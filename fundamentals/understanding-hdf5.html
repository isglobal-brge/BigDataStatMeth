<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Understanding HDF5 Storage – BigDataStatMeth</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../assets/favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-969ddfa49e00a70eb3423444dbc81f6c.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-d0f1cdce0779274a5ec1152cd33adb41.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-d6747531eee53fd58085a197f3afc013.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-d0f1cdce0779274a5ec1152cd33adb41.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<meta name="mermaid-theme" content="default">
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">


<link rel="stylesheet" href="../assets/css/custom.css">
</head>

<body class="nav-sidebar docked nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../assets/images/logo.png" alt="" class="navbar-logo light-content">
    <img src="../assets/images/logo.png" alt="" class="navbar-logo dark-content">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">BigDataStatMeth</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-fundamentals" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Fundamentals</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-fundamentals">    
        <li>
    <a class="dropdown-item" href="../fundamentals/big-data-problem.html">
 <span class="dropdown-text">The Big Data Problem</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../fundamentals/understanding-hdf5.html">
 <span class="dropdown-text">Understanding HDF5</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../fundamentals/blockwise-computing.html">
 <span class="dropdown-text">Block-Wise Computing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../fundamentals/linear-algebra.html">
 <span class="dropdown-text">Linear Algebra Essentials</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../tutorials/getting-started.html">
 <span class="dropdown-text">Getting Started</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../tutorials/working-hdf5-matrices.html">
 <span class="dropdown-text">Working with HDF5 Matrices</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../tutorials/first-analysis.html">
 <span class="dropdown-text">Your First Analysis</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-workflows" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Workflows</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-workflows">    
        <li>
    <a class="dropdown-item" href="../workflows/implementing-pca.html">
 <span class="dropdown-text">Implementing PCA</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../workflows/implementing-cca.html">
 <span class="dropdown-text">Implementing CCA</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../workflows/cross-platform.html">
 <span class="dropdown-text">Cross-Platform Workflows</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-developing-methods" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Developing Methods</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-developing-methods">    
        <li>
    <a class="dropdown-item" href="../developing-methods/cca-r-implementation.html">
 <span class="dropdown-text">CCA in R</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../developing-methods/cca-cpp-implementation.html">
 <span class="dropdown-text">CCA in C++</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-api-reference" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">API Reference</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-api-reference">    
        <li>
    <a class="dropdown-item" href="../api-reference/r/index.html">
 <span class="dropdown-text">R Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../api-reference/cpp/index.html">
 <span class="dropdown-text">C++ API</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../technical/performance.html"> 
<span class="menu-text">Technical</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/isglobal-brge/BigDataStatMeth"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://cran.r-project.org/package=BigDataStatMeth"> 
<span class="menu-text">CRAN</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../fundamentals/big-data-problem.html">Core Concepts</a></li><li class="breadcrumb-item"><a href="../fundamentals/understanding-hdf5.html">Understanding HDF5 Storage</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Core Concepts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../fundamentals/big-data-problem.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">The Big Data Problem in Genomics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../fundamentals/understanding-hdf5.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Understanding HDF5 Storage</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../fundamentals/blockwise-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Block-Wise Computing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../fundamentals/linear-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Algebra for Statistical Methods</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-youll-learn" id="toc-what-youll-learn" class="nav-link active" data-scroll-target="#what-youll-learn"><span class="header-section-number">0.1</span> What You’ll Learn</a></li>
  <li><a href="#the-problem-hdf5-solves" id="toc-the-problem-hdf5-solves" class="nav-link" data-scroll-target="#the-problem-hdf5-solves"><span class="header-section-number">1</span> The Problem HDF5 Solves</a></li>
  <li><a href="#what-is-hdf5" id="toc-what-is-hdf5" class="nav-link" data-scroll-target="#what-is-hdf5"><span class="header-section-number">2</span> What is HDF5?</a>
  <ul class="collapse">
  <li><a href="#the-hdf5-philosophy" id="toc-the-hdf5-philosophy" class="nav-link" data-scroll-target="#the-hdf5-philosophy"><span class="header-section-number">2.1</span> The HDF5 Philosophy</a></li>
  </ul></li>
  <li><a href="#hdf5-file-structure" id="toc-hdf5-file-structure" class="nav-link" data-scroll-target="#hdf5-file-structure"><span class="header-section-number">3</span> HDF5 File Structure</a>
  <ul class="collapse">
  <li><a href="#components-explained" id="toc-components-explained" class="nav-link" data-scroll-target="#components-explained"><span class="header-section-number">3.1</span> Components Explained</a></li>
  </ul></li>
  <li><a href="#creating-your-first-hdf5-file" id="toc-creating-your-first-hdf5-file" class="nav-link" data-scroll-target="#creating-your-first-hdf5-file"><span class="header-section-number">4</span> Creating Your First HDF5 File</a></li>
  <li><a href="#reading-data-the-power-of-partial-io" id="toc-reading-data-the-power-of-partial-io" class="nav-link" data-scroll-target="#reading-data-the-power-of-partial-io"><span class="header-section-number">5</span> Reading Data: The Power of Partial I/O</a></li>
  <li><a href="#chunking-how-hdf5-enables-efficient-access" id="toc-chunking-how-hdf5-enables-efficient-access" class="nav-link" data-scroll-target="#chunking-how-hdf5-enables-efficient-access"><span class="header-section-number">6</span> Chunking: How HDF5 Enables Efficient Access</a>
  <ul class="collapse">
  <li><a href="#what-is-chunking" id="toc-what-is-chunking" class="nav-link" data-scroll-target="#what-is-chunking"><span class="header-section-number">6.1</span> What is Chunking?</a></li>
  <li><a href="#why-chunking-matters" id="toc-why-chunking-matters" class="nav-link" data-scroll-target="#why-chunking-matters"><span class="header-section-number">6.2</span> Why Chunking Matters</a></li>
  </ul></li>
  <li><a href="#compression-saving-disk-space" id="toc-compression-saving-disk-space" class="nav-link" data-scroll-target="#compression-saving-disk-space"><span class="header-section-number">7</span> Compression: Saving Disk Space</a>
  <ul class="collapse">
  <li><a href="#compression-trade-offs" id="toc-compression-trade-offs" class="nav-link" data-scroll-target="#compression-trade-offs"><span class="header-section-number">7.1</span> Compression Trade-offs</a></li>
  </ul></li>
  <li><a href="#exploring-hdf5-files" id="toc-exploring-hdf5-files" class="nav-link" data-scroll-target="#exploring-hdf5-files"><span class="header-section-number">8</span> Exploring HDF5 Files</a>
  <ul class="collapse">
  <li><a href="#using-h5ls" id="toc-using-h5ls" class="nav-link" data-scroll-target="#using-h5ls"><span class="header-section-number">8.1</span> Using h5ls()</a></li>
  <li><a href="#using-hdfview-gui-tool" id="toc-using-hdfview-gui-tool" class="nav-link" data-scroll-target="#using-hdfview-gui-tool"><span class="header-section-number">8.2</span> Using HDFView (GUI Tool)</a></li>
  </ul></li>
  <li><a href="#how-bigdatastatmeth-uses-hdf5" id="toc-how-bigdatastatmeth-uses-hdf5" class="nav-link" data-scroll-target="#how-bigdatastatmeth-uses-hdf5"><span class="header-section-number">9</span> How BigDataStatMeth Uses HDF5</a></li>
  <li><a href="#practical-example-complete-workflow" id="toc-practical-example-complete-workflow" class="nav-link" data-scroll-target="#practical-example-complete-workflow"><span class="header-section-number">10</span> Practical Example: Complete Workflow</a></li>
  <li><a href="#interactive-exercise" id="toc-interactive-exercise" class="nav-link" data-scroll-target="#interactive-exercise"><span class="header-section-number">11</span> Interactive Exercise</a>
  <ul class="collapse">
  <li><a href="#practice-design-your-own-hdf5-structure" id="toc-practice-design-your-own-hdf5-structure" class="nav-link" data-scroll-target="#practice-design-your-own-hdf5-structure"><span class="header-section-number">11.1</span> Practice: Design Your Own HDF5 Structure</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">12</span> Key Takeaways</a>
  <ul class="collapse">
  <li><a href="#essential-concepts" id="toc-essential-concepts" class="nav-link" data-scroll-target="#essential-concepts"><span class="header-section-number">12.1</span> Essential Concepts</a></li>
  <li><a href="#when-to-use-hdf5" id="toc-when-to-use-hdf5" class="nav-link" data-scroll-target="#when-to-use-hdf5"><span class="header-section-number">12.2</span> When to Use HDF5</a></li>
  </ul></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps"><span class="header-section-number">13</span> Next Steps</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">14</span> Further Reading</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/isglobal-brge/BigDataStatMeth/edit/main/fundamentals/understanding-hdf5.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/isglobal-brge/BigDataStatMeth/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../fundamentals/big-data-problem.html">Core Concepts</a></li><li class="breadcrumb-item"><a href="../fundamentals/understanding-hdf5.html">Understanding HDF5 Storage</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Understanding HDF5 Storage</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">A Deep Dive into the Foundation of BigDataStatMeth</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="what-youll-learn" class="level3 learning-objectives" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="what-youll-learn"><span class="header-section-number">0.1</span> What You’ll Learn</h3>
<p>By the end of this section, you will:</p>
<ul>
<li>Understand what HDF5 is and why it exists</li>
<li>Grasp how HDF5 organizes data hierarchically</li>
<li>Learn how chunking enables efficient disk-based computing</li>
<li>Know when and why to use compression</li>
<li>Be able to create, inspect, and work with HDF5 files</li>
<li>Understand how BigDataStatMeth leverages HDF5 features</li>
</ul>
</section>
<section id="the-problem-hdf5-solves" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="the-problem-hdf5-solves"><span class="header-section-number">1</span> The Problem HDF5 Solves</h2>
<p>Imagine you’re analyzing genomic data: a matrix of 100,000 individuals × 50,000 genetic variants. Let’s do some quick math:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix dimensions</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n_individuals <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>n_variants <span class="ot">&lt;-</span> <span class="dv">50000</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Memory needed (assuming 8 bytes per double-precision number)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>memory_gb <span class="ot">&lt;-</span> (n_individuals <span class="sc">*</span> n_variants <span class="sc">*</span> <span class="dv">8</span>) <span class="sc">/</span> (<span class="dv">1024</span><span class="sc">^</span><span class="dv">3</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Memory required: %.1f GB</span><span class="sc">\n</span><span class="st">"</span>, memory_gb))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<pre><code>Memory required: 37.3 GB</code></pre>
<p><strong>The challenge:</strong> A typical laptop has 16GB RAM. Even a high-end workstation with 128GB would struggle when performing operations that create intermediate results.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>The Traditional Approach Fails
</div>
</div>
<div class="callout-body-container callout-body">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This would crash on most systems</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>genotype_matrix <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"100k_x_50k_genotypes.csv"</span>)  <span class="co"># ❌ Out of memory!</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>pca_result <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(genotype_matrix)                     <span class="co"># ❌ Never gets here</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>We need a fundamentally different approach.</p>
</div>
</div>
</section>
<section id="what-is-hdf5" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="what-is-hdf5"><span class="header-section-number">2</span> What is HDF5?</h2>
<p>When we encounter datasets that exceed our computer’s memory capacity, we face a fundamental question: how do we work with data we cannot fully load? Traditional file formats like CSV, RData, or even binary formats force us to read entire files into memory before we can work with them. This all-or-nothing approach creates an insurmountable barrier when data grows beyond available RAM.</p>
<p><strong>HDF5</strong> (Hierarchical Data Format version 5) emerged from this exact challenge in the scientific computing community. Developed by the National Center for Supercomputing Applications (NCSA) and now maintained by The HDF Group, HDF5 was designed by scientists who routinely work with terabytes of data from instruments, simulations, and experiments. It’s not just “another file format” - it’s fundamentally a <strong>database system optimized for scientific matrices and arrays</strong>.</p>
<p>The key innovation of HDF5 is deceptively simple but profoundly powerful: rather than treating a file as a monolithic block of data that must be read entirely, HDF5 treats files as structured databases where you can efficiently access exactly the pieces you need, when you need them. This changes everything about how we can work with large datasets.</p>
<section id="the-hdf5-philosophy" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="the-hdf5-philosophy"><span class="header-section-number">2.1</span> The HDF5 Philosophy</h3>
<p>HDF5 was designed around a core principle that directly addresses our big data problem:</p>
<blockquote class="blockquote">
<p>“Data should be accessible in pieces, without loading everything into memory.”</p>
</blockquote>
<p>This philosophy manifests in three revolutionary features that distinguish HDF5 from traditional file formats:</p>
<p><strong>1. Hierarchical Organization</strong></p>
<p>Just as you organize documents into folders and subfolders on your computer, HDF5 lets you organize datasets within groups. You might have a group called <code>/raw_data/</code> containing original measurements, another called <code>/processed/</code> for cleaned data, and <code>/results/</code> for analytical outputs. This organization isn’t just cosmetic - it helps both humans and computers understand the relationships between different pieces of data. When working on a genomics project, for example, you might organize by chromosome, by sample type, or by processing stage, all within a single file.</p>
<p><strong>2. Partial I/O</strong></p>
<p>This is where HDF5 truly shines. Imagine you have a matrix with 100,000 rows but only need to analyze the first 10,000. With a CSV file, you must read all 100,000 rows before you can work with any of them. With HDF5, you simply request rows 1-10,000, and only those rows are read from disk. The rest of the data stays untouched on disk, consuming zero memory. This selective reading extends to any dimension - rows, columns, or even arbitrary rectangular regions of your matrices. This capability is what makes disk-based computing practical.</p>
<p><strong>3. Self-Describing Metadata</strong></p>
<p>Every HDF5 dataset carries its own documentation. The file knows the dimensions of each matrix, the data type of each element, the names of rows and columns, when the data was created, and any other information you choose to store. This metadata travels with the data, so six months later, you (or a colleague) can open the file and immediately understand what it contains without consulting separate documentation. The data describes itself.</p>
<p>These three features combine to create a storage system where large datasets remain accessible and usable without overwhelming your computer’s memory. But understanding the features is just the beginning - seeing how HDF5 structures data internally helps us use it effectively.</p>
</section>
</section>
<section id="hdf5-file-structure" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="hdf5-file-structure"><span class="header-section-number">3</span> HDF5 File Structure</h2>
<p>To truly grasp how HDF5 enables efficient big data workflows, we need to understand its internal organization. Think of an HDF5 file as containing its own miniature filesystem - a hierarchy of containers and data, all within a single file.</p>
<p>At the top level, an HDF5 file can contain two types of objects: <strong>groups</strong> and <strong>datasets</strong>. Groups are like directories or folders, providing organizational structure. Datasets are where the actual data lives - these are your matrices, vectors, or arrays. Groups can contain other groups (nested hierarchies) and datasets. This creates a tree-like structure that can represent complex, multi-component analyses within a single file.</p>
<p>Here’s how this might look for a typical genomics analysis:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A[HDF5 File: analysis.hdf5] --&gt; B[Group: /data]
    A --&gt; C[Group: /results]
    A --&gt; D[Group: /metadata]
    
    B --&gt; B1[Dataset: genotypes&lt;br/&gt;100k × 50k matrix]
    B --&gt; B2[Dataset: phenotypes&lt;br/&gt;100k × 10 matrix]
    
    C --&gt; C1[Dataset: pca_components&lt;br/&gt;50k × 20 matrix]
    C --&gt; C2[Dataset: pca_scores&lt;br/&gt;100k × 20 matrix]
    
    D --&gt; D1[Attributes: creation_date]
    D --&gt; D2[Attributes: sample_ids]
    
    style A fill:#f0f8ff
    style B fill:#e8f6e8
    style C fill:#e8f6e8
    style D fill:#e8f6e8
    style B1 fill:#fff8e1
    style B2 fill:#fff8e1
    style C1 fill:#fff8e1
    style C2 fill:#fff8e1
</pre>
</div>
<p></p><figcaption> HDF5 hierarchical structure</figcaption> </figure><p></p>
</div>
</div>
</div>
<p>In this example, we have a single HDF5 file (<code>analysis.hdf5</code>) that contains everything related to a genetic association study. The file is organized into three main groups at the root level. The <code>/data</code> group holds our raw and processed data matrices - the genotype matrix with 100,000 individuals across 50,000 genetic variants, plus a smaller phenotype matrix with clinical measurements for those same individuals. The <code>/results</code> group stores the outputs of our PCA analysis - both the principal components (50,000 variants × 20 components) and the individual scores (100,000 individuals × 20 components). Finally, the <code>/metadata</code> group contains important information about the study, like when the data was collected and which samples correspond to which individuals.</p>
<p>This hierarchical organization does more than keep things tidy. It allows us to work with different parts of the analysis independently. We can read just the phenotype data without touching the much larger genotype matrix. We can update results without disturbing raw data. We can add new analyses in new groups without restructuring existing work. Everything stays together in one file, but nothing forces us to load everything at once.</p>
<section id="components-explained" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="components-explained"><span class="header-section-number">3.1</span> Components Explained</h3>
<p>To work effectively with HDF5, you need to understand three types of objects that make up this hierarchy. Each serves a distinct purpose in organizing and documenting your data.</p>
<p><strong>Groups</strong> are organizational containers that work exactly like folders on your computer’s filesystem. They provide structure and context for your data. A group can contain other groups (creating deeper hierarchies) and datasets. For instance, you might create a structure like <code>/data/genomics/chromosome1/variants</code> to organize genetic variants by chromosome. Groups help both humans and software understand how different pieces of data relate to each other. When you come back to an analysis months later, this organization helps you quickly locate what you need.</p>
<p><strong>Datasets</strong> are where your actual numerical data lives. These are the matrices, vectors, and arrays you’ll perform computations on. A dataset can be one-dimensional (a vector), two-dimensional (a matrix), or even higher-dimensional (tensors for complex data structures). Crucially, datasets in HDF5 can be arbitrarily large - they’re not limited by your computer’s RAM because the data resides on disk. Each dataset stores not just the numbers, but also information about data types, dimensions, and structure. This is what allows HDF5 to read just portions of a dataset efficiently.</p>
<p><strong>Attributes</strong> are small pieces of metadata attached to either groups or datasets. Think of attributes as post-it notes that document important information about your data. For a genotype dataset, attributes might store column names (which SNPs?), row names (which individuals?), the date of data collection, quality control thresholds used, or the genome build version. Attributes are always small and loaded entirely into memory, so they’re perfect for storing descriptive information that helps interpret the data. Unlike datasets, attributes are not designed for selective access - they’re meant to be read completely whenever you open a file.</p>
</section>
</section>
<section id="creating-your-first-hdf5-file" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="creating-your-first-hdf5-file"><span class="header-section-number">4</span> Creating Your First HDF5 File</h2>
<p>Now that you understand the conceptual foundation of HDF5 - its hierarchical structure, the power of partial I/O, and how chunking works under the hood - let’s make these ideas concrete by actually creating an HDF5 file. We’ll build a small example that mirrors a typical genomics study: genotype data for many individuals across many genetic variants, plus associated phenotype measurements.</p>
<p>The key function in BigDataStatMeth for creating HDF5 files is <code>bdCreate_hdf5_matrix()</code>. This function takes an R matrix (or data that can be converted to a matrix) and writes it to an HDF5 file with appropriate chunking, compression, and metadata. Behind the scenes, it’s handling all the complexity we discussed - choosing chunk sizes, setting up compression, creating the hierarchical structure - so you can focus on organizing your data logically.</p>
<p>Let’s walk through a complete example:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BigDataStatMeth)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rhdf5)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create some example data that mimics a genomics study</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># In real work, this data would come from your actual measurements</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>genotype_data <span class="ot">&lt;-</span> <span class="fu">matrix</span>(</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sample</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dv">1000</span> <span class="sc">*</span> <span class="dv">500</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>),  <span class="co"># 0, 1, 2 = genotype calls</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">nrow =</span> <span class="dv">1000</span>,  <span class="co"># 1000 individuals</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">ncol =</span> <span class="dv">500</span>    <span class="co"># 500 genetic variants</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>phenotype_data <span class="ot">&lt;-</span> <span class="fu">matrix</span>(</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rnorm</span>(<span class="dv">1000</span> <span class="sc">*</span> <span class="dv">10</span>),  <span class="co"># Continuous phenotype measurements</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">nrow =</span> <span class="dv">1000</span>,       <span class="co"># Same 1000 individuals</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">ncol =</span> <span class="dv">10</span>          <span class="co"># 10 measured traits</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create HDF5 file with hierarchical organization</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># This creates the file and writes the first dataset</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="fu">bdCreate_hdf5_matrix</span>(</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">filename =</span> <span class="st">"my_study.hdf5"</span>,    <span class="co"># Name of HDF5 file to create</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> genotype_data,         <span class="co"># Data to write</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="st">"data"</span>,                 <span class="co"># Group name (like a folder)</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">dataset =</span> <span class="st">"genotypes"</span>,          <span class="co"># Dataset name within the group</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">overwriteFile =</span> <span class="cn">TRUE</span>            <span class="co"># OK to overwrite if file exists</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Add second dataset to the same file</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Notice overwriteFile = FALSE so we add to existing file</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="fu">bdCreate_hdf5_matrix</span>(</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>  <span class="at">filename =</span> <span class="st">"my_study.hdf5"</span>,    <span class="co"># Same file as above</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> phenotype_data,        <span class="co"># Different data</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="st">"data"</span>,                 <span class="co"># Same group</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>  <span class="at">dataset =</span> <span class="st">"phenotypes"</span>,         <span class="co"># Different dataset name</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>  <span class="at">overwriteFile =</span> <span class="cn">FALSE</span>           <span class="co"># Don't overwrite the file, add to it</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the structure to see what we created</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="fu">h5ls</span>(<span class="st">"my_study.hdf5"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<pre><code>       group        name       otype dclass        dim
0          /        data   H5I_GROUP              
1     /data  genotypes H5I_DATASET  FLOAT 1000 x 500
2     /data phenotypes H5I_DATASET  FLOAT 1000 x 10</code></pre>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>What Just Happened Behind the Scenes?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let’s unpack what <code>bdCreate_hdf5_matrix()</code> did for us in those few lines of code:</p>
<p><strong>File and Structure Creation:</strong> The first call created a new HDF5 file on disk called <code>my_study.hdf5</code>. It automatically created the group <code>/data</code> (since it didn’t exist yet) and added the <code>genotypes</code> dataset within that group. The second call recognized the file already exists and added another dataset to the same group without disturbing the first one.</p>
<p><strong>Data Transfer:</strong> Your R matrices were written to disk in HDF5 format. The genotype matrix (1000 × 500 = 500,000 values) and phenotype matrix (1000 × 10 = 10,000 values) now live on disk, organized hierarchically. The original R objects still exist in memory - we’ve created copies on disk, not moved the data.</p>
<p><strong>Automatic Optimization:</strong> Behind the scenes, BigDataStatMeth chose appropriate chunk sizes for each dataset based on their dimensions. It applied default compression (level 4) to reduce file size. It stored metadata about dimensions and data types. All of this happened automatically without you needing to specify these technical details.</p>
<p><strong>Memory Efficiency:</strong> Notice that at no point did we need to have multiple copies of the data in memory. Each dataset was written directly from the R object to disk. This matters more with larger datasets - you can create multi-gigabyte HDF5 files without needing that much RAM, as long as each individual dataset fits in memory when you write it.</p>
<p>The <code>h5ls()</code> output shows our file structure. The <code>/</code> represents the root of the file, <code>data</code> is a group (H5I_GROUP), and within it are two datasets (H5I_DATASET) with their dimensions clearly visible.</p>
</div>
</div>
</section>
<section id="reading-data-the-power-of-partial-io" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="reading-data-the-power-of-partial-io"><span class="header-section-number">5</span> Reading Data: The Power of Partial I/O</h2>
<p>Now that we have data stored in HDF5 format, let’s explore what makes this format special: the ability to read just the portions we need.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read just the first 100 rows and 50 columns</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>subset_data <span class="ot">&lt;-</span> <span class="fu">h5read</span>(</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">"my_study.hdf5"</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">"data/genotypes"</span>,</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">index =</span> <span class="fu">list</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Check memory usage</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="fu">object.size</span>(subset_data)  <span class="co"># Only ~40 KB instead of ~3.8 MB!</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>This is <strong>fundamentally different</strong> from CSV or RData files where you must read everything.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TB
    T1["&lt;b&gt;Traditional Files (CSV, RData)&lt;/b&gt;"]
    T1 --&gt; A1[Full File&lt;br/&gt;37 GB]
    A1 --&gt; B1[Read ALL&lt;br/&gt;37 GB RAM]
    B1 --&gt; C1[Extract Subset&lt;br/&gt;~100 MB used]
    
    T2["&lt;b&gt;HDF5 File&lt;/b&gt;"]
    T2 --&gt; A2[HDF5 File&lt;br/&gt;37 GB]
    A2 --&gt; B2[Read SUBSET&lt;br/&gt;~100 MB RAM]
    B2 --&gt; C2[Work with Data&lt;br/&gt;~100 MB used]
    
    style T1 fill:#ffcccc,stroke:#cc0000,stroke-width:2px,color:#000
    style T2 fill:#ccffcc,stroke:#00cc00,stroke-width:2px,color:#000
    style A1 fill:#ffe8e8
    style B1 fill:#ffe8e8
    style C1 fill:#ffe8e8
    style A2 fill:#e8f6e8
    style B2 fill:#e8f6e8
    style C2 fill:#e8f6e8
</pre>
</div>
<p></p><figcaption> Partial I/O: Reading only needed data</figcaption> </figure><p></p>
</div>
</div>
</div>
</section>
<section id="chunking-how-hdf5-enables-efficient-access" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="chunking-how-hdf5-enables-efficient-access"><span class="header-section-number">6</span> Chunking: How HDF5 Enables Efficient Access</h2>
<p>We’ve established that HDF5 allows you to read just the portions of data you need - but how does it accomplish this efficiently? The answer lies in a fundamental design choice called <strong>chunking</strong>, which is perhaps the most important concept to understand when working with HDF5 files. Chunking is what transforms HDF5 from a simple file format into a high-performance data access system.</p>
<p>To appreciate why chunking matters, consider what happens with traditional file formats. When you store a matrix in a CSV file, the data is written sequentially: row 1, then row 2, then row 3, and so on. If you want to read just one column, you must scan through the entire file, extracting the relevant value from each row as you go. This means touching every single byte of a multi-gigabyte file just to access a tiny slice of data. It’s like having to read an entire book to find a single word.</p>
<p>HDF5 takes a fundamentally different approach by organizing data into rectangular blocks called chunks. This organization is built into how the data is stored on disk, not something that happens when you read the file. Understanding chunking helps you make better decisions about how to structure your data and which operations will be fast versus slow.</p>
<section id="what-is-chunking" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="what-is-chunking"><span class="header-section-number">6.1</span> What is Chunking?</h3>
<p>Instead of storing your matrix in a single continuous block (row-by-row or column-by-column), HDF5 divides it into rectangular <strong>chunks</strong> - think of them as tiles in a mosaic. Each chunk is stored as a contiguous unit on disk, meaning all the data in that chunk sits together in one place. When you request data that falls within a chunk, HDF5 can read that entire chunk in a single efficient disk operation.</p>
<p>The key insight is that related data - values that are likely to be accessed together - should live in the same chunk. If you typically access columns of data, you want chunks that contain complete column segments. If you work with rectangular regions, square chunks make sense. HDF5’s flexibility in chunk shapes lets you optimize for your specific access patterns.</p>
<p>Let’s visualize how a matrix gets divided into chunks:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    subgraph "Original Matrix (1000 × 500)"
        A[Chunk 1&lt;br/&gt;250×250]
        B[Chunk 2&lt;br/&gt;250×250]
        C[Chunk 3&lt;br/&gt;250×250]
        D[Chunk 4&lt;br/&gt;250×250]
        E[Chunk 5&lt;br/&gt;250×250]
        F[Chunk 6&lt;br/&gt;250×250]
        G[Chunk 7&lt;br/&gt;250×250]
        H[Chunk 8&lt;br/&gt;250×250]
    end
    
    style A fill:#f0f8ff
    style B fill:#e8f6e8
    style C fill:#fff8e1
    style D fill:#ffe8e8
    style E fill:#f0f8ff
    style F fill:#e8f6e8
    style G fill:#fff8e1
    style H fill:#ffe8e8
</pre>
</div>
<p></p><figcaption> Matrix chunking concept</figcaption> </figure><p></p>
</div>
</div>
</div>
<p>In this example, a 1000×500 matrix is divided into eight chunks of 250×250 elements each. Each colored block represents a separate chunk stored contiguously on disk. When you need data from a specific region of your matrix, HDF5 identifies which chunks contain that data and reads only those chunks. The chunks that aren’t needed stay on disk, consuming zero memory.</p>
<p>The coloring in the diagram isn’t just decorative - it helps visualize an important property: chunks are independent storage units. You can read chunk 1 without touching chunks 2 through 8. You can update chunk 5 without affecting any other chunk. This independence is what makes partial I/O possible and efficient.</p>
</section>
<section id="why-chunking-matters" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="why-chunking-matters"><span class="header-section-number">6.2</span> Why Chunking Matters</h3>
<p>The choice of chunk size and shape has profound implications for performance. To understand why, let’s walk through a concrete example that illustrates both good and bad chunking strategies.</p>
<p><strong>Example scenario:</strong> You want to read columns 1-250 of your matrix.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="">With Good Chunking</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false" href="">Without Chunking (Row-Major)</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<pre><code>Matrix chunked by 250×250 blocks
Reading columns 1-250 requires: Chunks 1, 3, 5, 7 (4 chunks)
Disk reads: 4 seek + read operations ✓ Efficient!</code></pre>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<pre><code>Matrix stored row-by-row
Reading columns 1-250 requires: Touching every row (1000 seek operations)
Disk reads: 1000 seek + read operations ✗ Slow!</code></pre>
</div>
</div>
</div>
<p>The difference is dramatic. With appropriate chunking, reading our column subset requires just 4 disk operations - one for each chunk that contains part of those columns. Without chunking (or with row-major storage), we need 1000 separate disk seeks, one for each row. Since disk seeks are typically measured in milliseconds while data transfer happens at gigabytes per second, the number of seeks dominates performance for partial reads.</p>
<p>This example illustrates a general principle: <strong>your chunk layout should match your access patterns</strong>. If you primarily access data by columns (common in statistical analysis), use chunks that span the full height of your matrix but only a portion of its width. If you work with rectangular regions, square chunks often work well. If you access entire rows, horizontal chunks make sense.</p>
<p>The good news is that BigDataStatMeth makes intelligent chunking decisions for you, optimizing for the block-wise statistical operations that are common in data analysis. But understanding chunking helps you recognize when certain operations will be fast (they align with chunk boundaries) versus slower (they require reading many partially-needed chunks).</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Chunk Size Matters
</div>
</div>
<div class="callout-body-container callout-body">
<p>Chunk size represents a fundamental trade-off in HDF5 performance. Several factors influence the optimal size:</p>
<p><strong>Too small chunks</strong> mean more metadata overhead and more disk seeks. If each chunk is only a few kilobytes, you’ll spend more time seeking to chunks than actually reading data. The metadata describing where each chunk lives can become a significant burden.</p>
<p><strong>Too large chunks</strong> mean reading more data than you need. If you want a single column but each chunk contains hundreds of columns, you’re transferring far more data from disk to memory than necessary. This wastes both I/O bandwidth and RAM.</p>
<p><strong>The sweet spot</strong> typically falls between 10KB and 1MB per chunk, though the exact optimum depends on your hardware (especially disk type - SSD versus hard drive) and access patterns. Modern SSDs are more forgiving of many small reads, while traditional hard drives strongly prefer fewer large reads.</p>
<p>BigDataStatMeth handles chunking automatically with sensible defaults, but it’s important to understand the philosophy behind these choices. The package takes a <strong>deliberately conservative approach</strong> to chunk sizing. Rather than trying to maximize performance, the defaults prioritize system stability and broad compatibility. This means that BigDataStatMeth’s automatic chunking probably isn’t the absolute fastest possible for your specific system - but it won’t overwhelm your RAM or cause your system to become unresponsive.</p>
<p>Why this conservatism? The package cannot know in advance what hardware you’re running on (laptop with 8GB RAM vs.&nbsp;server with 256GB), what else is running on your system, or your specific storage architecture (local SSD, network drive, cloud storage). Chunk sizes that work beautifully on a high-end workstation might cause out-of-memory errors on a laptop. The defaults aim for “works reliably everywhere” rather than “optimal for this specific configuration.”</p>
<p>If you know your system’s capabilities and access patterns well, you can override the defaults and potentially achieve better performance. But for most users, especially those new to HDF5, the conservative defaults provide a good balance: operations complete successfully without system issues, even if not at maximum theoretical speed. As you gain experience, you can experiment with more aggressive chunk sizes for your specific workflows.</p>
</div>
</div>
</section>
</section>
<section id="compression-saving-disk-space" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="compression-saving-disk-space"><span class="header-section-number">7</span> Compression: Saving Disk Space</h2>
<p>Storage space and I/O bandwidth are precious resources, especially when working with large datasets. HDF5 addresses both concerns through <strong>transparent compression</strong> - the data is automatically compressed when written and decompressed when read, without you having to manage the process explicitly. This compression happens at the chunk level, meaning each chunk is compressed independently. This chunk-wise compression preserves the ability to access arbitrary portions of your data efficiently.</p>
<p>The key word is “transparent.” From your perspective as a user, compressed and uncompressed datasets work identically. You read and write data using the same functions, and HDF5 handles the compression automatically. The only difference you’ll notice is in file size on disk and potentially in read/write performance, depending on whether your system is limited by disk I/O or CPU speed.</p>
<p>HDF5 uses the widely-tested gzip compression algorithm, which provides a good balance of compression ratio, speed, and universal support. The algorithm is lossless, meaning you get back exactly the data you put in - critical for scientific computing where accuracy matters. You control the compression level, trading off between better compression (smaller files, more CPU time) and faster operation (larger files, less CPU time).</p>
<p>Here’s how to use compression with BigDataStatMeth:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BigDataStatMeth uses compression by default</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">bdCreate_hdf5_matrix</span>(</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">filename =</span> <span class="st">"compressed.hdf5"</span>,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> genotype_data,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="st">"data"</span>,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">dataset =</span> <span class="st">"genotypes"</span>,</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">compression_level =</span> <span class="dv">6</span>  <span class="co"># 0 (none) to 9 (maximum)</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Check file sizes</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="fu">file.info</span>(<span class="st">"uncompressed.hdf5"</span>)<span class="sc">$</span>size <span class="sc">/</span> <span class="dv">1024</span><span class="sc">^</span><span class="dv">2</span>  <span class="co"># MB</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="fu">file.info</span>(<span class="st">"compressed.hdf5"</span>)<span class="sc">$</span>size <span class="sc">/</span> <span class="dv">1024</span><span class="sc">^</span><span class="dv">2</span>    <span class="co"># MB</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="compression-trade-offs" class="level3" data-number="7.1">
<h3 data-number="7.1" class="anchored" data-anchor-id="compression-trade-offs"><span class="header-section-number">7.1</span> Compression Trade-offs</h3>
<p>Choosing a compression level involves understanding the trade-off between file size and computational overhead. Higher compression levels examine more potential encoding strategies, finding more compact representations at the cost of more CPU cycles.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Level</th>
<th>Ratio</th>
<th>Speed</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1:1</td>
<td>Fastest</td>
<td>High-speed temporary files</td>
</tr>
<tr class="even">
<td>4-6</td>
<td>~2:1-5:1</td>
<td>Fast</td>
<td><strong>Default - good balance</strong></td>
</tr>
<tr class="odd">
<td>9</td>
<td>~3:1-8:1</td>
<td>Slower</td>
<td>Archival, limited disk space</td>
</tr>
</tbody>
</table>
<p>The compression ratio you achieve depends heavily on your data characteristics. Genomic data with many repeated values (like genotypes coded as 0, 1, 2) compresses extremely well, often achieving 5:1 or better ratios. Random floating-point numbers compress poorly because compression algorithms rely on finding patterns and redundancy in the data.</p>
<p>For most analytical workflows, levels 4-6 hit the sweet spot. They provide substantial space savings while adding minimal computational overhead. Modern CPUs can decompress data far faster than even fast SSDs can deliver it, so the decompression rarely becomes a bottleneck. In fact, compression can sometimes <em>improve</em> overall performance by reducing the amount of data that must be transferred from disk to memory - the time saved in I/O exceeds the time spent decompressing.</p>
<p>Level 0 (no compression) makes sense for temporary files where you prioritize speed over space, or when working with data that simply doesn’t compress well (already-compressed data, random noise, encrypted data). Level 9 is useful for archival storage where space is at a premium and you don’t mind slower write times, but it rarely makes sense for working files since levels 6-7 typically achieve nearly the same compression with noticeably better performance.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>BigDataStatMeth Default
</div>
</div>
<div class="callout-body-container callout-body">
<p>BigDataStatMeth uses compression level 4 by default, providing good compression ratios (typically 2-4× for typical genomic and statistical data) without significant performance penalty. This default works well for most use cases. The package applies compression automatically - you don’t need to think about it unless you have specific reasons to adjust the level.</p>
<p>For most users, the default compression is the right choice. It keeps your files manageable without slowing down your analysis. Only adjust compression if you have unusual requirements: no compression for maximum speed with temporary files, or higher compression for long-term storage of large datasets.</p>
</div>
</div>
</section>
</section>
<section id="exploring-hdf5-files" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="exploring-hdf5-files"><span class="header-section-number">8</span> Exploring HDF5 Files</h2>
<p>Once you’ve created HDF5 files, you’ll often want to inspect their contents to understand what data they contain and how it’s organized. HDF5 provides several tools for this exploration, both programmatic and visual. Understanding what’s in your files is essential for both your own work (remembering what analyses you’ve run) and for sharing data with colleagues who need to understand your file structure.</p>
<section id="using-h5ls" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="using-h5ls"><span class="header-section-number">8.1</span> Using h5ls()</h3>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># List contents of HDF5 file</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rhdf5)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">h5ls</span>(<span class="st">"my_study.hdf5"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<pre><code>       group        name       otype dclass        dim
0          /        data   H5I_GROUP              
1          /     results   H5I_GROUP              
2     /data  genotypes H5I_DATASET  FLOAT 1000 x 500
3     /data phenotypes H5I_DATASET  FLOAT 1000 x 10</code></pre>
</section>
<section id="using-hdfview-gui-tool" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="using-hdfview-gui-tool"><span class="header-section-number">8.2</span> Using HDFView (GUI Tool)</h3>
<p>For visual exploration, <a href="https://www.hdfgroup.org/downloads/hdfview/">HDFView</a> is an excellent free tool that provides a graphical interface to browse HDF5 file contents, inspect datasets, and view metadata.</p>
</section>
</section>
<section id="how-bigdatastatmeth-uses-hdf5" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="how-bigdatastatmeth-uses-hdf5"><span class="header-section-number">9</span> How BigDataStatMeth Uses HDF5</h2>
<p>BigDataStatMeth builds on HDF5’s capabilities:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Raw Data&lt;br/&gt;CSV, RData, GDS] --&gt; B[bdCreate_hdf5_matrix]
    B --&gt; C[HDF5 File&lt;br/&gt;Chunked &amp; Compressed]
    C --&gt; D[Block-wise&lt;br/&gt;Operations]
    D --&gt; E[Results&lt;br/&gt;Stored in HDF5]
    E --&gt; F[Extract to R&lt;br/&gt;or Keep on Disk]
    
    style C fill:#f0f8ff
    style D fill:#e8f6e8
    style E fill:#fff8e1
</pre>
</div>
<p></p><figcaption> BigDataStatMeth’s HDF5 workflow</figcaption> </figure><p></p>
</div>
</div>
</div>
<p><strong>Key features:</strong></p>
<ol type="1">
<li><strong>Automatic chunking</strong> optimized for statistical operations</li>
<li><strong>Metadata preservation</strong> (row names, column names)</li>
<li><strong>Block-wise algorithms</strong> that read/write chunks efficiently</li>
<li><strong>Result storage</strong> in same file for traceability</li>
</ol>
</section>
<section id="practical-example-complete-workflow" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="practical-example-complete-workflow"><span class="header-section-number">10</span> Practical Example: Complete Workflow</h2>
<p>Let’s put it all together with a realistic example:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BigDataStatMeth)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rhdf5)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create HDF5 file from existing data</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>large_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">10000</span> <span class="sc">*</span> <span class="dv">5000</span>), <span class="at">nrow =</span> <span class="dv">10000</span>, <span class="at">ncol =</span> <span class="dv">5000</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="fu">bdCreate_hdf5_matrix</span>(</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">filename =</span> <span class="st">"analysis.hdf5"</span>,</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> large_matrix,</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="st">"data"</span>,</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">dataset =</span> <span class="st">"expression_matrix"</span>,</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">overwriteFile =</span> <span class="cn">TRUE</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Perform SVD without loading full matrix</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>svd_result <span class="ot">&lt;-</span> <span class="fu">bdSVD_hdf5</span>(</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">filename =</span> <span class="st">"analysis.hdf5"</span>,</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="st">"data"</span>,</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">dataset =</span> <span class="st">"expression_matrix"</span>,</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">bcenter =</span> <span class="cn">TRUE</span>,</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">bscale =</span> <span class="cn">TRUE</span>,</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">k =</span> <span class="dv">20</span>  <span class="co"># Number of components</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Check what's in the file now</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a><span class="fu">h5ls</span>(<span class="st">"analysis.hdf5"</span>)</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Extract just the components you need</span></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>components <span class="ot">&lt;-</span> <span class="fu">h5read</span>(<span class="st">"analysis.hdf5"</span>, svd_result<span class="sc">$</span>ds_v)</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(components)  <span class="co"># 5000 × 20 (not 5000 × 5000!)</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Clean up</span></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="fu">h5closeAll</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>What We Achieved
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Stored 38GB dataset on disk</li>
<li>Performed PCA using ~500MB RAM</li>
<li>Kept results organized in same file</li>
<li>Can rerun analysis without re-reading data</li>
</ul>
</div>
</div>
</section>
<section id="interactive-exercise" class="level2 exercise" data-number="11">
<h2 class="exercise anchored" data-number="11" data-anchor-id="interactive-exercise"><span class="header-section-number">11</span> Interactive Exercise</h2>
<section id="practice-design-your-own-hdf5-structure" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="practice-design-your-own-hdf5-structure"><span class="header-section-number">11.1</span> Practice: Design Your Own HDF5 Structure</h3>
<p>The best way to internalize HDF5 concepts is to apply them to your own data. This exercise guides you through creating a multi-component HDF5 file and asks you to think about organizational decisions. There are no “correct” answers - the goal is to practice translating a research design into an HDF5 file structure and to consider how different organizational choices affect usability.</p>
<p><strong>This is a thinking exercise.</strong> We provide starter code and questions for reflection, but no solutions. The questions are designed to make you think about how you would structure real projects. Your answers will depend on your specific research questions and workflow.</p>
<p>Create an HDF5 file with your own data structure:</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create a hierarchical organization for a study</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BigDataStatMeth)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated study data</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>genomic_data <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">sample</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">2</span>, <span class="dv">5000</span><span class="sc">*</span><span class="dv">1000</span>, <span class="at">replace=</span><span class="cn">TRUE</span>), <span class="dv">5000</span>, <span class="dv">1000</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>expression_data <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">5000</span><span class="sc">*</span><span class="dv">200</span>), <span class="dv">5000</span>, <span class="dv">200</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>clinical_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">age =</span> <span class="fu">rnorm</span>(<span class="dv">5000</span>, <span class="dv">50</span>, <span class="dv">10</span>),</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">gender =</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">"M"</span>, <span class="st">"F"</span>), <span class="dv">5000</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Organize in HDF5</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="fu">bdCreate_hdf5_matrix</span>(</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>  <span class="st">"my_study.hdf5"</span>, genomic_data, </span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="st">"omics/genomics"</span>, <span class="at">dataset =</span> <span class="st">"snps"</span>,</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">overwriteFile =</span> <span class="cn">TRUE</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="fu">bdCreate_hdf5_matrix</span>(</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>  <span class="st">"my_study.hdf5"</span>, expression_data,</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="st">"omics/transcriptomics"</span>, <span class="at">dataset =</span> <span class="st">"genes"</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Explore your creation</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="fu">h5ls</span>(<span class="st">"my_study.hdf5"</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Questions for reflection:</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Longitudinal data organization:</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a><span class="co">#    - How would you organize data collected at multiple time points?</span></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Would you create separate groups for each timepoint (/timepoint1/, /timepoint2/)?</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Or group by data type with timepoint as a dimension within datasets?</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a><span class="co">#    - What are the trade-offs of each approach?</span></span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Multi-omic integration:</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="co">#    - You now have genomics and transcriptomics. What if you add:</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="co">#      * Proteomics data</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a><span class="co">#      * Metabolomics data</span></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a><span class="co">#      * Clinical phenotypes</span></span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a><span class="co">#    - How would you organize these to make cross-omic analyses easy?</span></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Should raw and processed data live in different groups?</span></span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Derived results storage:</span></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Where would you store PCA results for each omic?</span></span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Where would integration analysis results go?</span></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a><span class="co">#    - How do you link results back to the data they came from?</span></span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Should every analysis get its own group, or organize by analysis type?</span></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Metadata strategy:</span></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a><span class="co">#    - What attributes would you attach to each dataset?</span></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a><span class="co">#    - How would you document processing steps?</span></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a><span class="co">#    - What information needs to travel with the data for reproducibility?</span></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Try implementing one of these scenarios in code. The goal is not perfection,</span></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a><span class="co"># but practice thinking about data organization for real research projects.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Reflection, Not Solutions
</div>
</div>
<div class="callout-body-container callout-body">
<p>This exercise deliberately doesn’t provide “the answer” because there often isn’t a single correct way to organize complex data. Your organizational choices should reflect:</p>
<ul>
<li>Your specific research questions (what comparisons matter most?)</li>
<li>Your workflow (what data do you access together?)</li>
<li>Your collaboration needs (who else needs to use this data?)</li>
<li>Your analysis pipeline (what tools will read this file?)</li>
</ul>
<p>The experience of designing and trying different structures teaches more than following a prescribed solution. If you’re unsure, try multiple approaches and see which one feels more natural when you go to actually use the data.</p>
</div>
</div>
</section>
</section>
<section id="key-takeaways" class="level2 key-concept" data-number="12">
<h2 class="key-concept anchored" data-number="12" data-anchor-id="key-takeaways"><span class="header-section-number">12</span> Key Takeaways</h2>
<p>We’ve covered a lot of ground in understanding HDF5. Let’s consolidate the essential concepts you need to remember as you work with BigDataStatMeth and HDF5 files.</p>
<section id="essential-concepts" class="level3" data-number="12.1">
<h3 data-number="12.1" class="anchored" data-anchor-id="essential-concepts"><span class="header-section-number">12.1</span> Essential Concepts</h3>
<p>You’ve learned that HDF5 is fundamentally different from traditional file formats. Let’s review the key ideas:</p>
<p><strong>HDF5 is a database for matrices</strong>, not just a file format. This distinction matters because it changes how you think about data storage. Instead of “saving a file,” you’re building a database that can contain multiple related datasets, organized hierarchically, with built-in metadata.</p>
<p><strong>Hierarchical organization</strong> mirrors how you think about complex research projects. Just as you organize documents into folders, HDF5 lets you organize datasets into groups. This organization isn’t cosmetic - it helps both you and your analysis code understand the relationships between different data components.</p>
<p><strong>Chunking enables efficient partial I/O</strong> by organizing data into blocks that can be read independently. This is what makes the “read only what you need” promise real rather than theoretical. Understanding chunking helps you predict which operations will be fast and which will require reading more data than you’d prefer.</p>
<p><strong>Compression reduces disk usage</strong> without adding complexity to your code. HDF5 handles compression transparently, and the default settings work well for most statistical applications. You get smaller files essentially for free.</p>
<p><strong>Metadata is built-in</strong>, meaning your data carries its own documentation. Six months later, you (or a colleague) can open an HDF5 file and immediately understand what it contains, when it was created, and how to interpret the values.</p>
<p><strong>BigDataStatMeth automates</strong> these HDF5 best practices. The package makes intelligent decisions about chunking, compression, and metadata so you can focus on your analysis rather than storage engineering. But understanding what’s happening under the hood helps you use the package more effectively.</p>
</section>
<section id="when-to-use-hdf5" class="level3" data-number="12.2">
<h3 data-number="12.2" class="anchored" data-anchor-id="when-to-use-hdf5"><span class="header-section-number">12.2</span> When to Use HDF5</h3>
<p>Making the right choice about file formats matters for both productivity and practicality. Here’s how to decide:</p>
<p>✅ <strong>Use HDF5 when:</strong></p>
<ul>
<li><strong>Data exceeds available RAM</strong> - This is the primary use case. When you can’t load everything into memory, HDF5 lets you work with arbitrary-sized datasets by processing them in pieces.</li>
<li><strong>Need repeated access to subsets</strong> - If your workflow involves reading different portions of data at different times, HDF5’s partial I/O capabilities pay off quickly.</li>
<li><strong>Want organized, self-documenting storage</strong> - For complex projects with multiple data components, HDF5’s hierarchical structure and metadata support help keep everything organized and understandable.</li>
<li><strong>Sharing data across platforms</strong> - HDF5 is supported by R, Python, MATLAB, Julia, C++, and many other languages. It provides a common data format that works across your entire analysis ecosystem.</li>
</ul>
<p>❌ <strong>Don’t use HDF5 when:</strong></p>
<ul>
<li><strong>Data fits comfortably in RAM</strong> - If your dataset uses less than about 20% of your available memory, traditional formats (RData, CSV) are simpler. The overhead of HDF5 doesn’t provide benefits when everything fits in memory anyway.</li>
<li><strong>Need simple, human-readable formats</strong> - HDF5 files are binary and require special tools to inspect. If you need to open files in a text editor or want maximum simplicity, CSV or similar formats might be better despite their limitations.</li>
<li><strong>Working with highly irregular structures</strong> - HDF5 excels with matrices and arrays - data that has regular structure. Highly nested or irregular data structures might be better served by other formats designed for those use cases.</li>
</ul>
<p>The decision isn’t always clear-cut. Many projects start with data that fits in memory but grow over time. Starting with HDF5 can future-proof your analysis pipeline, especially if you anticipate scaling to larger datasets or want to leverage the organizational benefits even for medium-sized data.</p>
</section>
</section>
<section id="next-steps" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="next-steps"><span class="header-section-number">13</span> Next Steps</h2>
<p>You now have a solid foundation in HDF5 concepts and how BigDataStatMeth uses them. The natural next step is to understand how computational algorithms are adapted to work efficiently with disk-based data.</p>
<ul>
<li><a href="../fundamentals/blockwise-computing.html"><strong>Block-Wise Computing →</strong></a> Learn how algorithms are adapted for disk-based matrices</li>
<li><a href="../tutorials/working-hdf5-matrices.html"><strong>Working with HDF5 Matrices →</strong></a> Practical tutorial on data management</li>
<li><a href="../tutorials/first-analysis.html"><strong>Your First Analysis →</strong></a> Complete analytical workflow</li>
</ul>
</section>
<section id="further-reading" class="level2" data-number="14">
<h2 data-number="14" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">14</span> Further Reading</h2>
<p>If you want to deepen your understanding of HDF5 beyond what we’ve covered here, these resources provide different perspectives and more technical detail:</p>
<p><strong><a href="https://portal.hdfgroup.org/display/HDF5/HDF5+User+Guides">HDF5 User Guide</a></strong> - The official documentation from The HDF Group. This is comprehensive and authoritative, though quite technical. Good for understanding the full capabilities of HDF5 and diving into advanced features like parallel I/O, virtual datasets, and complex datatypes.</p>
<p><strong><a href="https://bioconductor.org/packages/release/bioc/html/rhdf5.html">rhdf5 Bioconductor Package</a></strong> - The R package that BigDataStatMeth builds upon for low-level HDF5 operations. The rhdf5 documentation provides examples of direct HDF5 manipulation if you need finer control than BigDataStatMeth’s convenience functions provide.</p>
<p><strong><a href="https://www.hdfgroup.org/downloads/hdfview/">HDFView</a></strong> - A free graphical tool for browsing HDF5 files visually. Essential for debugging file structure issues and understanding what your code has created. Works on Windows, Mac, and Linux.</p>
<p><strong>BigDataStatMeth Documentation</strong> - The <a href="../../api-reference/r-functions.qmd">complete API reference</a> covers all HDF5-related functions in detail, with additional examples and parameter explanations.</p>
<hr>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Questions or Feedback?
</div>
</div>
<div class="callout-body-container callout-body">
<p>If something is unclear or you’d like more examples, please <a href="https://github.com/isglobal-brge/BigDataStatMeth/issues">open an issue</a> on GitHub. We value your feedback to improve this educational material.</p>
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/isglobal-brge\.github\.io\/BigDataStatMeth\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Understanding HDF5 Storage"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "A Deep Dive into the Foundation of BigDataStatMeth"</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>::: {.learning-objectives}</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="fu">### What You'll Learn</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>By the end of this section, you will:</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand what HDF5 is and why it exists</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Grasp how HDF5 organizes data hierarchically</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learn how chunking enables efficient disk-based computing</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Know when and why to use compression</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Be able to create, inspect, and work with HDF5 files</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand how BigDataStatMeth leverages HDF5 features</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Problem HDF5 Solves</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>Imagine you're analyzing genomic data: a matrix of 100,000 individuals × 50,000 genetic variants. Let's do some quick math:</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix dimensions</span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>n_individuals <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>n_variants <span class="ot">&lt;-</span> <span class="dv">50000</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Memory needed (assuming 8 bytes per double-precision number)</span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>memory_gb <span class="ot">&lt;-</span> (n_individuals <span class="sc">*</span> n_variants <span class="sc">*</span> <span class="dv">8</span>) <span class="sc">/</span> (<span class="dv">1024</span><span class="sc">^</span><span class="dv">3</span>)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"Memory required: %.1f GB</span><span class="sc">\n</span><span class="st">"</span>, memory_gb))</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a><span class="in">Memory required: 37.3 GB</span></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>**The challenge:** A typical laptop has 16GB RAM. Even a high-end workstation with 128GB would struggle when performing operations that create intermediate results.</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Traditional Approach Fails</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a><span class="in">```r</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="co"># This would crash on most systems</span></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>genotype_matrix <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"100k_x_50k_genotypes.csv"</span>)  <span class="co"># ❌ Out of memory!</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>pca_result <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(genotype_matrix)                     <span class="co"># ❌ Never gets here</span></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>We need a fundamentally different approach.</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a><span class="fu">## What is HDF5?</span></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>When we encounter datasets that exceed our computer's memory capacity, we face a fundamental question: how do we work with data we cannot fully load? Traditional file formats like CSV, RData, or even binary formats force us to read entire files into memory before we can work with them. This all-or-nothing approach creates an insurmountable barrier when data grows beyond available RAM.</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>**HDF5** (Hierarchical Data Format version 5) emerged from this exact challenge in the scientific computing community. Developed by the National Center for Supercomputing Applications (NCSA) and now maintained by The HDF Group, HDF5 was designed by scientists who routinely work with terabytes of data from instruments, simulations, and experiments. It's not just "another file format" - it's fundamentally a **database system optimized for scientific matrices and arrays**.</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>The key innovation of HDF5 is deceptively simple but profoundly powerful: rather than treating a file as a monolithic block of data that must be read entirely, HDF5 treats files as structured databases where you can efficiently access exactly the pieces you need, when you need them. This changes everything about how we can work with large datasets.</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a><span class="fu">### The HDF5 Philosophy</span></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>HDF5 was designed around a core principle that directly addresses our big data problem:</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; "Data should be accessible in pieces, without loading everything into memory."</span></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a>This philosophy manifests in three revolutionary features that distinguish HDF5 from traditional file formats:</span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a>**1. Hierarchical Organization**</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a>Just as you organize documents into folders and subfolders on your computer, HDF5 lets you organize datasets within groups. You might have a group called <span class="in">`/raw_data/`</span> containing original measurements, another called <span class="in">`/processed/`</span> for cleaned data, and <span class="in">`/results/`</span> for analytical outputs. This organization isn't just cosmetic - it helps both humans and computers understand the relationships between different pieces of data. When working on a genomics project, for example, you might organize by chromosome, by sample type, or by processing stage, all within a single file.</span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>**2. Partial I/O**  </span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a>This is where HDF5 truly shines. Imagine you have a matrix with 100,000 rows but only need to analyze the first 10,000. With a CSV file, you must read all 100,000 rows before you can work with any of them. With HDF5, you simply request rows 1-10,000, and only those rows are read from disk. The rest of the data stays untouched on disk, consuming zero memory. This selective reading extends to any dimension - rows, columns, or even arbitrary rectangular regions of your matrices. This capability is what makes disk-based computing practical.</span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a>**3. Self-Describing Metadata**</span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a>Every HDF5 dataset carries its own documentation. The file knows the dimensions of each matrix, the data type of each element, the names of rows and columns, when the data was created, and any other information you choose to store. This metadata travels with the data, so six months later, you (or a colleague) can open the file and immediately understand what it contains without consulting separate documentation. The data describes itself.</span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a>These three features combine to create a storage system where large datasets remain accessible and usable without overwhelming your computer's memory. But understanding the features is just the beginning - seeing how HDF5 structures data internally helps us use it effectively.</span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a><span class="fu">## HDF5 File Structure</span></span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a>To truly grasp how HDF5 enables efficient big data workflows, we need to understand its internal organization. Think of an HDF5 file as containing its own miniature filesystem - a hierarchy of containers and data, all within a single file.</span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a>At the top level, an HDF5 file can contain two types of objects: **groups** and **datasets**. Groups are like directories or folders, providing organizational structure. Datasets are where the actual data lives - these are your matrices, vectors, or arrays. Groups can contain other groups (nested hierarchies) and datasets. This creates a tree-like structure that can represent complex, multi-component analyses within a single file.</span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a>Here's how this might look for a typical genomics analysis:</span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">HDF5 hierarchical structure</span><span class="ot">"</span></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a>%%| fig-alt: <span class="ot">"</span><span class="st">Diagram showing HDF5 file structure with groups and datasets</span><span class="ot">"</span></span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a>graph TD</span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a>    A[HDF5 File: analysis.hdf5] --&gt; B[Group: <span class="ot">/data]</span></span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a><span class="ot">    A --&gt; C</span><span class="ch">[</span><span class="bn">Group: /results</span><span class="ch">]</span></span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a><span class="ot">    A --&gt; D</span><span class="ch">[</span><span class="bn">Group: /metadata</span><span class="ch">]</span></span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a><span class="ot">    </span></span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a><span class="ot">    B --&gt; B1</span><span class="ch">[</span><span class="bn">Dataset: genotypes&lt;br/&gt;100k × 50k matrix</span><span class="ch">]</span></span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a><span class="ot">    B --&gt; B2</span><span class="ch">[</span><span class="bn">Dataset: phenotypes&lt;br/&gt;100k × 10 matrix</span><span class="ch">]</span></span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a><span class="ot">    </span></span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a><span class="ot">    C --&gt; C1</span><span class="ch">[</span><span class="bn">Dataset: pca_components&lt;br/&gt;50k × 20 matrix</span><span class="ch">]</span></span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a><span class="ot">    C --&gt; C2</span><span class="ch">[</span><span class="bn">Dataset: pca_scores&lt;br/&gt;100k × 20 matrix</span><span class="ch">]</span></span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a><span class="ot">    </span></span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a><span class="ot">    D --&gt; D1</span><span class="ch">[</span><span class="bn">Attributes: creation_date</span><span class="ch">]</span></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a><span class="ot">    D --&gt; D2</span><span class="ch">[</span><span class="bn">Attributes: sample_ids</span><span class="ch">]</span></span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a><span class="ot">    </span></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a><span class="ot">    style A fill:#f0f8ff</span></span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a><span class="ot">    style B fill:#e8f6e8</span></span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a><span class="ot">    style C fill:#e8f6e8</span></span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a><span class="ot">    style D fill:#e8f6e8</span></span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a><span class="ot">    style B1 fill:#fff8e1</span></span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a><span class="ot">    style B2 fill:#fff8e1</span></span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a><span class="ot">    style C1 fill:#fff8e1</span></span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a><span class="ot">    style C2 fill:#fff8e1</span></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a><span class="ot">```</span></span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a><span class="ot">In this example, we have a single HDF5 file </span><span class="ch">(</span><span class="ot">`analysis.hdf5`</span><span class="ch">)</span><span class="ot"> that contains everything related to a genetic association study. The file is organized into three main groups at the root level. The `/</span>data<span class="ot">`</span><span class="st"> group holds our raw and processed data matrices - the genotype matrix with 100,000 individuals across 50,000 genetic variants, plus a smaller phenotype matrix with clinical measurements for those same individuals. The </span><span class="ot">`/results` group stores the outputs of our PCA analysis - both the principal components </span><span class="ch">(</span><span class="ot">50,000 variants × 20 components</span><span class="ch">)</span><span class="ot"> and the individual scores </span><span class="ch">(</span><span class="ot">100,000 individuals × 20 components</span><span class="ch">)</span><span class="ot">. Finally, the `/m</span>etadata<span class="ot">`</span><span class="st"> group contains important information about the study, like when the data was collected and which samples correspond to which individuals.</span></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a><span class="st">This hierarchical organization does more than keep things tidy. It allows us to work with different parts of the analysis independently. We can read just the phenotype data without touching the much larger genotype matrix. We can update results without disturbing raw data. We can add new analyses in new groups without restructuring existing work. Everything stays together in one file, but nothing forces us to load everything at once.</span></span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a><span class="st">### Components Explained</span></span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a><span class="st">To work effectively with HDF5, you need to understand three types of objects that make up this hierarchy. Each serves a distinct purpose in organizing and documenting your data.</span></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a><span class="st">**Groups** are organizational containers that work exactly like folders on your computer's filesystem. They provide structure and context for your data. A group can contain other groups (creating deeper hierarchies) and datasets. For instance, you might create a structure like </span><span class="ot">`/data/g</span>enomics/chromosome1/variants<span class="ot">`</span><span class="st"> to organize genetic variants by chromosome. Groups help both humans and software understand how different pieces of data relate to each other. When you come back to an analysis months later, this organization helps you quickly locate what you need.</span></span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a><span class="st">**Datasets** are where your actual numerical data lives. These are the matrices, vectors, and arrays you'll perform computations on. A dataset can be one-dimensional (a vector), two-dimensional (a matrix), or even higher-dimensional (tensors for complex data structures). Crucially, datasets in HDF5 can be arbitrarily large - they're not limited by your computer's RAM because the data resides on disk. Each dataset stores not just the numbers, but also information about data types, dimensions, and structure. This is what allows HDF5 to read just portions of a dataset efficiently.</span></span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a><span class="st">**Attributes** are small pieces of metadata attached to either groups or datasets. Think of attributes as post-it notes that document important information about your data. For a genotype dataset, attributes might store column names (which SNPs?), row names (which individuals?), the date of data collection, quality control thresholds used, or the genome build version. Attributes are always small and loaded entirely into memory, so they're perfect for storing descriptive information that helps interpret the data. Unlike datasets, attributes are not designed for selective access - they're meant to be read completely whenever you open a file.</span></span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a><span class="st">## Creating Your First HDF5 File</span></span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a><span class="st">Now that you understand the conceptual foundation of HDF5 - its hierarchical structure, the power of partial I/O, and how chunking works under the hood - let's make these ideas concrete by actually creating an HDF5 file. We'll build a small example that mirrors a typical genomics study: genotype data for many individuals across many genetic variants, plus associated phenotype measurements.</span></span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a><span class="st">The key function in BigDataStatMeth for creating HDF5 files is </span><span class="ot">`</span>bdCreate_hdf5_matrix()<span class="ot">`</span><span class="st">. This function takes an R matrix (or data that can be converted to a matrix) and writes it to an HDF5 file with appropriate chunking, compression, and metadata. Behind the scenes, it's handling all the complexity we discussed - choosing chunk sizes, setting up compression, creating the hierarchical structure - so you can focus on organizing your data logically.</span></span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a><span class="st">Let's walk through a complete example:</span></span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a>library(BigDataStatMeth)</span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a>library(rhdf5)</span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a><span class="co"># Create some example data that mimics a genomics study</span></span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a><span class="co"># In real work, this data would come from your actual measurements</span></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a>set.seed(<span class="dv">123</span>)</span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a>genotype_data &lt;- matrix(</span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a>  sample(c(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dv">1000</span> <span class="ot">*</span> <span class="dv">500</span>, replace = TRUE),  <span class="co"># 0, 1, 2 = genotype calls</span></span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a>  nrow = <span class="dv">1000</span>,  <span class="co"># 1000 individuals</span></span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a>  ncol = <span class="dv">500</span>    <span class="co"># 500 genetic variants</span></span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a>phenotype_data &lt;- matrix(</span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a>  rnorm(<span class="dv">1000</span> <span class="ot">*</span> <span class="dv">10</span>),  <span class="co"># Continuous phenotype measurements</span></span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a>  nrow = <span class="dv">1000</span>,       <span class="co"># Same 1000 individuals</span></span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a>  ncol = <span class="dv">10</span>          <span class="co"># 10 measured traits</span></span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a><span class="co"># Create HDF5 file with hierarchical organization</span></span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a><span class="co"># This creates the file and writes the first dataset</span></span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a>bdCreate_hdf5_matrix(</span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a>  filename = <span class="ot">"</span><span class="st">my_study.hdf5</span><span class="ot">"</span>,    <span class="co"># Name of HDF5 file to create</span></span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a>  object = genotype_data,         <span class="co"># Data to write</span></span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a>  group = <span class="ot">"</span><span class="st">data</span><span class="ot">"</span>,                 <span class="co"># Group name (like a folder)</span></span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a>  dataset = <span class="ot">"</span><span class="st">genotypes</span><span class="ot">"</span>,          <span class="co"># Dataset name within the group</span></span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a>  overwriteFile = TRUE            <span class="co"># OK to overwrite if file exists</span></span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a><span class="co"># Add second dataset to the same file</span></span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a><span class="co"># Notice overwriteFile = FALSE so we add to existing file</span></span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a>bdCreate_hdf5_matrix(</span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a>  filename = <span class="ot">"</span><span class="st">my_study.hdf5</span><span class="ot">"</span>,    <span class="co"># Same file as above</span></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a>  object = phenotype_data,        <span class="co"># Different data</span></span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a>  group = <span class="ot">"</span><span class="st">data</span><span class="ot">"</span>,                 <span class="co"># Same group</span></span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a>  dataset = <span class="ot">"</span><span class="st">phenotypes</span><span class="ot">"</span>,         <span class="co"># Different dataset name</span></span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a>  overwriteFile = FALSE           <span class="co"># Don't overwrite the file, add to it</span></span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a><span class="co"># Inspect the structure to see what we created</span></span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a>h5ls(<span class="ot">"</span><span class="st">my_study.hdf5</span><span class="ot">"</span>)</span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a><span class="in">       group        name       otype dclass        dim</span></span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a><span class="in">0          /        data   H5I_GROUP              </span></span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a><span class="in">1     /data  genotypes H5I_DATASET  FLOAT 1000 x 500</span></span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a><span class="in">2     /data phenotypes H5I_DATASET  FLOAT 1000 x 10</span></span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a><span class="fu">### What Just Happened Behind the Scenes?</span></span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a>Let's unpack what <span class="in">`bdCreate_hdf5_matrix()`</span> did for us in those few lines of code:</span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a>**File and Structure Creation:** The first call created a new HDF5 file on disk called <span class="in">`my_study.hdf5`</span>. It automatically created the group <span class="in">`/data`</span> (since it didn't exist yet) and added the <span class="in">`genotypes`</span> dataset within that group. The second call recognized the file already exists and added another dataset to the same group without disturbing the first one.</span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a>**Data Transfer:** Your R matrices were written to disk in HDF5 format. The genotype matrix (1000 × 500 = 500,000 values) and phenotype matrix (1000 × 10 = 10,000 values) now live on disk, organized hierarchically. The original R objects still exist in memory - we've created copies on disk, not moved the data.</span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a>**Automatic Optimization:** Behind the scenes, BigDataStatMeth chose appropriate chunk sizes for each dataset based on their dimensions. It applied default compression (level 4) to reduce file size. It stored metadata about dimensions and data types. All of this happened automatically without you needing to specify these technical details.</span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a>**Memory Efficiency:** Notice that at no point did we need to have multiple copies of the data in memory. Each dataset was written directly from the R object to disk. This matters more with larger datasets - you can create multi-gigabyte HDF5 files without needing that much RAM, as long as each individual dataset fits in memory when you write it.</span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a>The <span class="in">`h5ls()`</span> output shows our file structure. The <span class="in">`/`</span> represents the root of the file, <span class="in">`data`</span> is a group (H5I_GROUP), and within it are two datasets (H5I_DATASET) with their dimensions clearly visible.</span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a><span class="fu">## Reading Data: The Power of Partial I/O</span></span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a>Now that we have data stored in HDF5 format, let's explore what makes this format special: the ability to read just the portions we need.</span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a><span class="co"># Read just the first 100 rows and 50 columns</span></span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a>subset_data <span class="ot">&lt;-</span> <span class="fu">h5read</span>(</span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a>  <span class="st">"my_study.hdf5"</span>,</span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a>  <span class="st">"data/genotypes"</span>,</span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a>  <span class="at">index =</span> <span class="fu">list</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>)</span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a><span class="co"># Check memory usage</span></span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a><span class="fu">object.size</span>(subset_data)  <span class="co"># Only ~40 KB instead of ~3.8 MB!</span></span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a>This is **fundamentally different** from CSV or RData files where you must read everything.</span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">Partial I/O: Reading only needed data</span><span class="ot">"</span></span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a>%%| fig-alt: <span class="ot">"</span><span class="st">Diagram comparing full file reading vs HDF5 partial reading</span><span class="ot">"</span></span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a>graph TB</span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a>    T1[<span class="ot">"</span><span class="st">&lt;b&gt;Traditional Files (CSV, RData)&lt;/b&gt;</span><span class="ot">"</span>]</span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a>    T1 --&gt; A1[Full File&lt;br/&gt;<span class="dv">37</span> GB]</span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a>    A1 --&gt; B1[Read ALL&lt;br/&gt;<span class="dv">37</span> GB RAM]</span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a>    B1 --&gt; C1[Extract Subset&lt;br/&gt;~<span class="dv">100</span> MB used]</span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a>    T2[<span class="ot">"</span><span class="st">&lt;b&gt;HDF5 File&lt;/b&gt;</span><span class="ot">"</span>]</span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a>    T2 --&gt; A2[HDF5 File&lt;br/&gt;<span class="dv">37</span> GB]</span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a>    A2 --&gt; B2[Read SUBSET&lt;br/&gt;~<span class="dv">100</span> MB RAM]</span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a>    B2 --&gt; C2[Work with Data&lt;br/&gt;~<span class="dv">100</span> MB used]</span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a>    style T1 fill:<span class="co">#ffcccc,stroke:#cc0000,stroke-width:2px,color:#000</span></span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a>    style T2 fill:<span class="co">#ccffcc,stroke:#00cc00,stroke-width:2px,color:#000</span></span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a>    style A1 fill:<span class="co">#ffe8e8</span></span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a>    style B1 fill:<span class="co">#ffe8e8</span></span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a>    style C1 fill:<span class="co">#ffe8e8</span></span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a>    style A2 fill:<span class="co">#e8f6e8</span></span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a>    style B2 fill:<span class="co">#e8f6e8</span></span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a>    style C2 fill:<span class="co">#e8f6e8</span></span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chunking: How HDF5 Enables Efficient Access</span></span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a>We've established that HDF5 allows you to read just the portions of data you need - but how does it accomplish this efficiently? The answer lies in a fundamental design choice called **chunking**, which is perhaps the most important concept to understand when working with HDF5 files. Chunking is what transforms HDF5 from a simple file format into a high-performance data access system.</span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a>To appreciate why chunking matters, consider what happens with traditional file formats. When you store a matrix in a CSV file, the data is written sequentially: row 1, then row 2, then row 3, and so on. If you want to read just one column, you must scan through the entire file, extracting the relevant value from each row as you go. This means touching every single byte of a multi-gigabyte file just to access a tiny slice of data. It's like having to read an entire book to find a single word.</span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a>HDF5 takes a fundamentally different approach by organizing data into rectangular blocks called chunks. This organization is built into how the data is stored on disk, not something that happens when you read the file. Understanding chunking helps you make better decisions about how to structure your data and which operations will be fast versus slow.</span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a><span class="fu">### What is Chunking?</span></span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a>Instead of storing your matrix in a single continuous block (row-by-row or column-by-column), HDF5 divides it into rectangular **chunks** - think of them as tiles in a mosaic. Each chunk is stored as a contiguous unit on disk, meaning all the data in that chunk sits together in one place. When you request data that falls within a chunk, HDF5 can read that entire chunk in a single efficient disk operation.</span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a>The key insight is that related data - values that are likely to be accessed together - should live in the same chunk. If you typically access columns of data, you want chunks that contain complete column segments. If you work with rectangular regions, square chunks make sense. HDF5's flexibility in chunk shapes lets you optimize for your specific access patterns.</span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a>Let's visualize how a matrix gets divided into chunks:</span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">Matrix chunking concept</span><span class="ot">"</span></span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a>%%| fig-alt: <span class="ot">"</span><span class="st">Visual representation of a matrix divided into chunks</span><span class="ot">"</span></span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a>graph TD</span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a>    subgraph <span class="ot">"</span><span class="st">Original Matrix (1000 × 500)</span><span class="ot">"</span></span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a>        A[Chunk <span class="dv">1</span>&lt;br/&gt;<span class="dv">250</span>×<span class="dv">250</span>]</span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a>        B[Chunk <span class="dv">2</span>&lt;br/&gt;<span class="dv">250</span>×<span class="dv">250</span>]</span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a>        C[Chunk <span class="dv">3</span>&lt;br/&gt;<span class="dv">250</span>×<span class="dv">250</span>]</span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a>        D[Chunk <span class="dv">4</span>&lt;br/&gt;<span class="dv">250</span>×<span class="dv">250</span>]</span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a>        E[Chunk <span class="dv">5</span>&lt;br/&gt;<span class="dv">250</span>×<span class="dv">250</span>]</span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a>        F[Chunk <span class="dv">6</span>&lt;br/&gt;<span class="dv">250</span>×<span class="dv">250</span>]</span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a>        G[Chunk <span class="dv">7</span>&lt;br/&gt;<span class="dv">250</span>×<span class="dv">250</span>]</span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a>        H[Chunk <span class="dv">8</span>&lt;br/&gt;<span class="dv">250</span>×<span class="dv">250</span>]</span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a>    end</span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a>    style A fill:<span class="co">#f0f8ff</span></span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a>    style B fill:<span class="co">#e8f6e8</span></span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a>    style C fill:<span class="co">#fff8e1</span></span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a>    style D fill:<span class="co">#ffe8e8</span></span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a>    style E fill:<span class="co">#f0f8ff</span></span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a>    style F fill:<span class="co">#e8f6e8</span></span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a>    style G fill:<span class="co">#fff8e1</span></span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a>    style H fill:<span class="co">#ffe8e8</span></span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a>In this example, a 1000×500 matrix is divided into eight chunks of 250×250 elements each. Each colored block represents a separate chunk stored contiguously on disk. When you need data from a specific region of your matrix, HDF5 identifies which chunks contain that data and reads only those chunks. The chunks that aren't needed stay on disk, consuming zero memory.</span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a>The coloring in the diagram isn't just decorative - it helps visualize an important property: chunks are independent storage units. You can read chunk 1 without touching chunks 2 through 8. You can update chunk 5 without affecting any other chunk. This independence is what makes partial I/O possible and efficient.</span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Chunking Matters</span></span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a>The choice of chunk size and shape has profound implications for performance. To understand why, let's walk through a concrete example that illustrates both good and bad chunking strategies.</span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a>**Example scenario:** You want to read columns 1-250 of your matrix.</span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a>::: {.panel-tabset}</span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a><span class="fu">### With Good Chunking</span></span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a><span class="in">Matrix chunked by 250×250 blocks</span></span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a><span class="in">Reading columns 1-250 requires: Chunks 1, 3, 5, 7 (4 chunks)</span></span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a><span class="in">Disk reads: 4 seek + read operations ✓ Efficient!</span></span>
<span id="cb14-323"><a href="#cb14-323" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a><span class="fu">### Without Chunking (Row-Major)</span></span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a><span class="in">Matrix stored row-by-row</span></span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a><span class="in">Reading columns 1-250 requires: Touching every row (1000 seek operations)</span></span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a><span class="in">Disk reads: 1000 seek + read operations ✗ Slow!</span></span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a>The difference is dramatic. With appropriate chunking, reading our column subset requires just 4 disk operations - one for each chunk that contains part of those columns. Without chunking (or with row-major storage), we need 1000 separate disk seeks, one for each row. Since disk seeks are typically measured in milliseconds while data transfer happens at gigabytes per second, the number of seeks dominates performance for partial reads.</span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a>This example illustrates a general principle: **your chunk layout should match your access patterns**. If you primarily access data by columns (common in statistical analysis), use chunks that span the full height of your matrix but only a portion of its width. If you work with rectangular regions, square chunks often work well. If you access entire rows, horizontal chunks make sense.</span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a>The good news is that BigDataStatMeth makes intelligent chunking decisions for you, optimizing for the block-wise statistical operations that are common in data analysis. But understanding chunking helps you recognize when certain operations will be fast (they align with chunk boundaries) versus slower (they require reading many partially-needed chunks).</span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a>::: {.callout-important}</span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chunk Size Matters</span></span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a>Chunk size represents a fundamental trade-off in HDF5 performance. Several factors influence the optimal size:</span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a>**Too small chunks** mean more metadata overhead and more disk seeks. If each chunk is only a few kilobytes, you'll spend more time seeking to chunks than actually reading data. The metadata describing where each chunk lives can become a significant burden.</span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a>**Too large chunks** mean reading more data than you need. If you want a single column but each chunk contains hundreds of columns, you're transferring far more data from disk to memory than necessary. This wastes both I/O bandwidth and RAM.</span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a>**The sweet spot** typically falls between 10KB and 1MB per chunk, though the exact optimum depends on your hardware (especially disk type - SSD versus hard drive) and access patterns. Modern SSDs are more forgiving of many small reads, while traditional hard drives strongly prefer fewer large reads.</span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a>BigDataStatMeth handles chunking automatically with sensible defaults, but it's important to understand the philosophy behind these choices. The package takes a **deliberately conservative approach** to chunk sizing. Rather than trying to maximize performance, the defaults prioritize system stability and broad compatibility. This means that BigDataStatMeth's automatic chunking probably isn't the absolute fastest possible for your specific system - but it won't overwhelm your RAM or cause your system to become unresponsive.</span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-354"><a href="#cb14-354" aria-hidden="true" tabindex="-1"></a>Why this conservatism? The package cannot know in advance what hardware you're running on (laptop with 8GB RAM vs. server with 256GB), what else is running on your system, or your specific storage architecture (local SSD, network drive, cloud storage). Chunk sizes that work beautifully on a high-end workstation might cause out-of-memory errors on a laptop. The defaults aim for "works reliably everywhere" rather than "optimal for this specific configuration."</span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a>If you know your system's capabilities and access patterns well, you can override the defaults and potentially achieve better performance. But for most users, especially those new to HDF5, the conservative defaults provide a good balance: operations complete successfully without system issues, even if not at maximum theoretical speed. As you gain experience, you can experiment with more aggressive chunk sizes for your specific workflows.</span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a><span class="fu">## Compression: Saving Disk Space</span></span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a>Storage space and I/O bandwidth are precious resources, especially when working with large datasets. HDF5 addresses both concerns through **transparent compression** - the data is automatically compressed when written and decompressed when read, without you having to manage the process explicitly. This compression happens at the chunk level, meaning each chunk is compressed independently. This chunk-wise compression preserves the ability to access arbitrary portions of your data efficiently.</span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a>The key word is "transparent." From your perspective as a user, compressed and uncompressed datasets work identically. You read and write data using the same functions, and HDF5 handles the compression automatically. The only difference you'll notice is in file size on disk and potentially in read/write performance, depending on whether your system is limited by disk I/O or CPU speed.</span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a>HDF5 uses the widely-tested gzip compression algorithm, which provides a good balance of compression ratio, speed, and universal support. The algorithm is lossless, meaning you get back exactly the data you put in - critical for scientific computing where accuracy matters. You control the compression level, trading off between better compression (smaller files, more CPU time) and faster operation (larger files, less CPU time).</span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a>Here's how to use compression with BigDataStatMeth:</span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a><span class="co"># BigDataStatMeth uses compression by default</span></span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a><span class="fu">bdCreate_hdf5_matrix</span>(</span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a>  <span class="at">filename =</span> <span class="st">"compressed.hdf5"</span>,</span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> genotype_data,</span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="st">"data"</span>,</span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a>  <span class="at">dataset =</span> <span class="st">"genotypes"</span>,</span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a>  <span class="at">compression_level =</span> <span class="dv">6</span>  <span class="co"># 0 (none) to 9 (maximum)</span></span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a><span class="co"># Check file sizes</span></span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a><span class="fu">file.info</span>(<span class="st">"uncompressed.hdf5"</span>)<span class="sc">$</span>size <span class="sc">/</span> <span class="dv">1024</span><span class="sc">^</span><span class="dv">2</span>  <span class="co"># MB</span></span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a><span class="fu">file.info</span>(<span class="st">"compressed.hdf5"</span>)<span class="sc">$</span>size <span class="sc">/</span> <span class="dv">1024</span><span class="sc">^</span><span class="dv">2</span>    <span class="co"># MB</span></span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a><span class="fu">### Compression Trade-offs</span></span>
<span id="cb14-389"><a href="#cb14-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-390"><a href="#cb14-390" aria-hidden="true" tabindex="-1"></a>Choosing a compression level involves understanding the trade-off between file size and computational overhead. Higher compression levels examine more potential encoding strategies, finding more compact representations at the cost of more CPU cycles.</span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Level <span class="pp">|</span> Ratio <span class="pp">|</span> Speed <span class="pp">|</span> When to Use <span class="pp">|</span></span>
<span id="cb14-393"><a href="#cb14-393" aria-hidden="true" tabindex="-1"></a><span class="pp">|-------|-------|-------|-------------|</span></span>
<span id="cb14-394"><a href="#cb14-394" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 0     <span class="pp">|</span> 1:1   <span class="pp">|</span> Fastest <span class="pp">|</span> High-speed temporary files <span class="pp">|</span></span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 4-6   <span class="pp">|</span> ~2:1-5:1 <span class="pp">|</span> Fast <span class="pp">|</span> **Default - good balance** <span class="pp">|</span></span>
<span id="cb14-396"><a href="#cb14-396" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 9     <span class="pp">|</span> ~3:1-8:1 <span class="pp">|</span> Slower <span class="pp">|</span> Archival, limited disk space <span class="pp">|</span></span>
<span id="cb14-397"><a href="#cb14-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a>The compression ratio you achieve depends heavily on your data characteristics. Genomic data with many repeated values (like genotypes coded as 0, 1, 2) compresses extremely well, often achieving 5:1 or better ratios. Random floating-point numbers compress poorly because compression algorithms rely on finding patterns and redundancy in the data.</span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-400"><a href="#cb14-400" aria-hidden="true" tabindex="-1"></a>For most analytical workflows, levels 4-6 hit the sweet spot. They provide substantial space savings while adding minimal computational overhead. Modern CPUs can decompress data far faster than even fast SSDs can deliver it, so the decompression rarely becomes a bottleneck. In fact, compression can sometimes *improve* overall performance by reducing the amount of data that must be transferred from disk to memory - the time saved in I/O exceeds the time spent decompressing.</span>
<span id="cb14-401"><a href="#cb14-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a>Level 0 (no compression) makes sense for temporary files where you prioritize speed over space, or when working with data that simply doesn't compress well (already-compressed data, random noise, encrypted data). Level 9 is useful for archival storage where space is at a premium and you don't mind slower write times, but it rarely makes sense for working files since levels 6-7 typically achieve nearly the same compression with noticeably better performance.</span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a><span class="fu">### BigDataStatMeth Default</span></span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a>BigDataStatMeth uses compression level 4 by default, providing good compression ratios (typically 2-4× for typical genomic and statistical data) without significant performance penalty. This default works well for most use cases. The package applies compression automatically - you don't need to think about it unless you have specific reasons to adjust the level.</span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a>For most users, the default compression is the right choice. It keeps your files manageable without slowing down your analysis. Only adjust compression if you have unusual requirements: no compression for maximum speed with temporary files, or higher compression for long-term storage of large datasets.</span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exploring HDF5 Files</span></span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a>Once you've created HDF5 files, you'll often want to inspect their contents to understand what data they contain and how it's organized. HDF5 provides several tools for this exploration, both programmatic and visual. Understanding what's in your files is essential for both your own work (remembering what analyses you've run) and for sharing data with colleagues who need to understand your file structure.</span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-416"><a href="#cb14-416" aria-hidden="true" tabindex="-1"></a><span class="fu">### Using h5ls()</span></span>
<span id="cb14-417"><a href="#cb14-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a><span class="co"># List contents of HDF5 file</span></span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rhdf5)</span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a><span class="fu">h5ls</span>(<span class="st">"my_study.hdf5"</span>)</span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a><span class="in">       group        name       otype dclass        dim</span></span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a><span class="in">0          /        data   H5I_GROUP              </span></span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a><span class="in">1          /     results   H5I_GROUP              </span></span>
<span id="cb14-432"><a href="#cb14-432" aria-hidden="true" tabindex="-1"></a><span class="in">2     /data  genotypes H5I_DATASET  FLOAT 1000 x 500</span></span>
<span id="cb14-433"><a href="#cb14-433" aria-hidden="true" tabindex="-1"></a><span class="in">3     /data phenotypes H5I_DATASET  FLOAT 1000 x 10</span></span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a><span class="fu">### Using HDFView (GUI Tool)</span></span>
<span id="cb14-437"><a href="#cb14-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-438"><a href="#cb14-438" aria-hidden="true" tabindex="-1"></a>For visual exploration, <span class="co">[</span><span class="ot">HDFView</span><span class="co">](https://www.hdfgroup.org/downloads/hdfview/)</span> is an excellent free tool that provides a graphical interface to browse HDF5 file contents, inspect datasets, and view metadata.</span>
<span id="cb14-439"><a href="#cb14-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-440"><a href="#cb14-440" aria-hidden="true" tabindex="-1"></a><span class="fu">## How BigDataStatMeth Uses HDF5</span></span>
<span id="cb14-441"><a href="#cb14-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-442"><a href="#cb14-442" aria-hidden="true" tabindex="-1"></a>BigDataStatMeth builds on HDF5's capabilities:</span>
<span id="cb14-443"><a href="#cb14-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-446"><a href="#cb14-446" aria-hidden="true" tabindex="-1"></a><span class="in">```{mermaid}</span></span>
<span id="cb14-447"><a href="#cb14-447" aria-hidden="true" tabindex="-1"></a>%%| fig-cap: <span class="ot">"</span><span class="st">BigDataStatMeth's HDF5 workflow</span><span class="ot">"</span></span>
<span id="cb14-448"><a href="#cb14-448" aria-hidden="true" tabindex="-1"></a>%%| fig-alt: <span class="ot">"</span><span class="st">Diagram showing workflow from data to results</span><span class="ot">"</span></span>
<span id="cb14-449"><a href="#cb14-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-450"><a href="#cb14-450" aria-hidden="true" tabindex="-1"></a>graph LR</span>
<span id="cb14-451"><a href="#cb14-451" aria-hidden="true" tabindex="-1"></a>    A[Raw Data&lt;br/&gt;CSV, RData, GDS] --&gt; B[bdCreate_hdf5_matrix]</span>
<span id="cb14-452"><a href="#cb14-452" aria-hidden="true" tabindex="-1"></a>    B --&gt; C[HDF5 File&lt;br/&gt;Chunked &amp; Compressed]</span>
<span id="cb14-453"><a href="#cb14-453" aria-hidden="true" tabindex="-1"></a>    C --&gt; D[Block-wise&lt;br/&gt;Operations]</span>
<span id="cb14-454"><a href="#cb14-454" aria-hidden="true" tabindex="-1"></a>    D --&gt; E[Results&lt;br/&gt;Stored in HDF5]</span>
<span id="cb14-455"><a href="#cb14-455" aria-hidden="true" tabindex="-1"></a>    E --&gt; F[Extract to R&lt;br/&gt;<span class="ot">or</span> Keep on Disk]</span>
<span id="cb14-456"><a href="#cb14-456" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-457"><a href="#cb14-457" aria-hidden="true" tabindex="-1"></a>    style C fill:<span class="co">#f0f8ff</span></span>
<span id="cb14-458"><a href="#cb14-458" aria-hidden="true" tabindex="-1"></a>    style D fill:<span class="co">#e8f6e8</span></span>
<span id="cb14-459"><a href="#cb14-459" aria-hidden="true" tabindex="-1"></a>    style E fill:<span class="co">#fff8e1</span></span>
<span id="cb14-460"><a href="#cb14-460" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-461"><a href="#cb14-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-462"><a href="#cb14-462" aria-hidden="true" tabindex="-1"></a>**Key features:**</span>
<span id="cb14-463"><a href="#cb14-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-464"><a href="#cb14-464" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Automatic chunking** optimized for statistical operations</span>
<span id="cb14-465"><a href="#cb14-465" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Metadata preservation** (row names, column names)</span>
<span id="cb14-466"><a href="#cb14-466" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Block-wise algorithms** that read/write chunks efficiently</span>
<span id="cb14-467"><a href="#cb14-467" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Result storage** in same file for traceability</span>
<span id="cb14-468"><a href="#cb14-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-469"><a href="#cb14-469" aria-hidden="true" tabindex="-1"></a><span class="fu">## Practical Example: Complete Workflow</span></span>
<span id="cb14-470"><a href="#cb14-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-471"><a href="#cb14-471" aria-hidden="true" tabindex="-1"></a>Let's put it all together with a realistic example:</span>
<span id="cb14-472"><a href="#cb14-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-475"><a href="#cb14-475" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb14-476"><a href="#cb14-476" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb14-477"><a href="#cb14-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-478"><a href="#cb14-478" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BigDataStatMeth)</span>
<span id="cb14-479"><a href="#cb14-479" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rhdf5)</span>
<span id="cb14-480"><a href="#cb14-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-481"><a href="#cb14-481" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create HDF5 file from existing data</span></span>
<span id="cb14-482"><a href="#cb14-482" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb14-483"><a href="#cb14-483" aria-hidden="true" tabindex="-1"></a>large_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">10000</span> <span class="sc">*</span> <span class="dv">5000</span>), <span class="at">nrow =</span> <span class="dv">10000</span>, <span class="at">ncol =</span> <span class="dv">5000</span>)</span>
<span id="cb14-484"><a href="#cb14-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-485"><a href="#cb14-485" aria-hidden="true" tabindex="-1"></a><span class="fu">bdCreate_hdf5_matrix</span>(</span>
<span id="cb14-486"><a href="#cb14-486" aria-hidden="true" tabindex="-1"></a>  <span class="at">filename =</span> <span class="st">"analysis.hdf5"</span>,</span>
<span id="cb14-487"><a href="#cb14-487" aria-hidden="true" tabindex="-1"></a>  <span class="at">object =</span> large_matrix,</span>
<span id="cb14-488"><a href="#cb14-488" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="st">"data"</span>,</span>
<span id="cb14-489"><a href="#cb14-489" aria-hidden="true" tabindex="-1"></a>  <span class="at">dataset =</span> <span class="st">"expression_matrix"</span>,</span>
<span id="cb14-490"><a href="#cb14-490" aria-hidden="true" tabindex="-1"></a>  <span class="at">overwriteFile =</span> <span class="cn">TRUE</span></span>
<span id="cb14-491"><a href="#cb14-491" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-492"><a href="#cb14-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-493"><a href="#cb14-493" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Perform SVD without loading full matrix</span></span>
<span id="cb14-494"><a href="#cb14-494" aria-hidden="true" tabindex="-1"></a>svd_result <span class="ot">&lt;-</span> <span class="fu">bdSVD_hdf5</span>(</span>
<span id="cb14-495"><a href="#cb14-495" aria-hidden="true" tabindex="-1"></a>  <span class="at">filename =</span> <span class="st">"analysis.hdf5"</span>,</span>
<span id="cb14-496"><a href="#cb14-496" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="st">"data"</span>,</span>
<span id="cb14-497"><a href="#cb14-497" aria-hidden="true" tabindex="-1"></a>  <span class="at">dataset =</span> <span class="st">"expression_matrix"</span>,</span>
<span id="cb14-498"><a href="#cb14-498" aria-hidden="true" tabindex="-1"></a>  <span class="at">bcenter =</span> <span class="cn">TRUE</span>,</span>
<span id="cb14-499"><a href="#cb14-499" aria-hidden="true" tabindex="-1"></a>  <span class="at">bscale =</span> <span class="cn">TRUE</span>,</span>
<span id="cb14-500"><a href="#cb14-500" aria-hidden="true" tabindex="-1"></a>  <span class="at">k =</span> <span class="dv">20</span>  <span class="co"># Number of components</span></span>
<span id="cb14-501"><a href="#cb14-501" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-502"><a href="#cb14-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-503"><a href="#cb14-503" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Check what's in the file now</span></span>
<span id="cb14-504"><a href="#cb14-504" aria-hidden="true" tabindex="-1"></a><span class="fu">h5ls</span>(<span class="st">"analysis.hdf5"</span>)</span>
<span id="cb14-505"><a href="#cb14-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-506"><a href="#cb14-506" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Extract just the components you need</span></span>
<span id="cb14-507"><a href="#cb14-507" aria-hidden="true" tabindex="-1"></a>components <span class="ot">&lt;-</span> <span class="fu">h5read</span>(<span class="st">"analysis.hdf5"</span>, svd_result<span class="sc">$</span>ds_v)</span>
<span id="cb14-508"><a href="#cb14-508" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(components)  <span class="co"># 5000 × 20 (not 5000 × 5000!)</span></span>
<span id="cb14-509"><a href="#cb14-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-510"><a href="#cb14-510" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Clean up</span></span>
<span id="cb14-511"><a href="#cb14-511" aria-hidden="true" tabindex="-1"></a><span class="fu">h5closeAll</span>()</span>
<span id="cb14-512"><a href="#cb14-512" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-513"><a href="#cb14-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-514"><a href="#cb14-514" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb14-515"><a href="#cb14-515" aria-hidden="true" tabindex="-1"></a><span class="fu">### What We Achieved</span></span>
<span id="cb14-516"><a href="#cb14-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-517"><a href="#cb14-517" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stored 38GB dataset on disk</span>
<span id="cb14-518"><a href="#cb14-518" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Performed PCA using ~500MB RAM</span>
<span id="cb14-519"><a href="#cb14-519" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Kept results organized in same file</span>
<span id="cb14-520"><a href="#cb14-520" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can rerun analysis without re-reading data</span>
<span id="cb14-521"><a href="#cb14-521" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-522"><a href="#cb14-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-523"><a href="#cb14-523" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interactive Exercise {.exercise}</span></span>
<span id="cb14-524"><a href="#cb14-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-525"><a href="#cb14-525" aria-hidden="true" tabindex="-1"></a><span class="fu">### Practice: Design Your Own HDF5 Structure</span></span>
<span id="cb14-526"><a href="#cb14-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-527"><a href="#cb14-527" aria-hidden="true" tabindex="-1"></a>The best way to internalize HDF5 concepts is to apply them to your own data. This exercise guides you through creating a multi-component HDF5 file and asks you to think about organizational decisions. There are no "correct" answers - the goal is to practice translating a research design into an HDF5 file structure and to consider how different organizational choices affect usability.</span>
<span id="cb14-528"><a href="#cb14-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-529"><a href="#cb14-529" aria-hidden="true" tabindex="-1"></a>**This is a thinking exercise.** We provide starter code and questions for reflection, but no solutions. The questions are designed to make you think about how you would structure real projects. Your answers will depend on your specific research questions and workflow.</span>
<span id="cb14-530"><a href="#cb14-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-531"><a href="#cb14-531" aria-hidden="true" tabindex="-1"></a>Create an HDF5 file with your own data structure:</span>
<span id="cb14-532"><a href="#cb14-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-535"><a href="#cb14-535" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb14-536"><a href="#cb14-536" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb14-537"><a href="#cb14-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-538"><a href="#cb14-538" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create a hierarchical organization for a study</span></span>
<span id="cb14-539"><a href="#cb14-539" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BigDataStatMeth)</span>
<span id="cb14-540"><a href="#cb14-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-541"><a href="#cb14-541" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated study data</span></span>
<span id="cb14-542"><a href="#cb14-542" aria-hidden="true" tabindex="-1"></a>genomic_data <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">sample</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">2</span>, <span class="dv">5000</span><span class="sc">*</span><span class="dv">1000</span>, <span class="at">replace=</span><span class="cn">TRUE</span>), <span class="dv">5000</span>, <span class="dv">1000</span>)</span>
<span id="cb14-543"><a href="#cb14-543" aria-hidden="true" tabindex="-1"></a>expression_data <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">5000</span><span class="sc">*</span><span class="dv">200</span>), <span class="dv">5000</span>, <span class="dv">200</span>)</span>
<span id="cb14-544"><a href="#cb14-544" aria-hidden="true" tabindex="-1"></a>clinical_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb14-545"><a href="#cb14-545" aria-hidden="true" tabindex="-1"></a>  <span class="at">age =</span> <span class="fu">rnorm</span>(<span class="dv">5000</span>, <span class="dv">50</span>, <span class="dv">10</span>),</span>
<span id="cb14-546"><a href="#cb14-546" aria-hidden="true" tabindex="-1"></a>  <span class="at">gender =</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">"M"</span>, <span class="st">"F"</span>), <span class="dv">5000</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb14-547"><a href="#cb14-547" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-548"><a href="#cb14-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-549"><a href="#cb14-549" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Organize in HDF5</span></span>
<span id="cb14-550"><a href="#cb14-550" aria-hidden="true" tabindex="-1"></a><span class="fu">bdCreate_hdf5_matrix</span>(</span>
<span id="cb14-551"><a href="#cb14-551" aria-hidden="true" tabindex="-1"></a>  <span class="st">"my_study.hdf5"</span>, genomic_data, </span>
<span id="cb14-552"><a href="#cb14-552" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="st">"omics/genomics"</span>, <span class="at">dataset =</span> <span class="st">"snps"</span>,</span>
<span id="cb14-553"><a href="#cb14-553" aria-hidden="true" tabindex="-1"></a>  <span class="at">overwriteFile =</span> <span class="cn">TRUE</span></span>
<span id="cb14-554"><a href="#cb14-554" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-555"><a href="#cb14-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-556"><a href="#cb14-556" aria-hidden="true" tabindex="-1"></a><span class="fu">bdCreate_hdf5_matrix</span>(</span>
<span id="cb14-557"><a href="#cb14-557" aria-hidden="true" tabindex="-1"></a>  <span class="st">"my_study.hdf5"</span>, expression_data,</span>
<span id="cb14-558"><a href="#cb14-558" aria-hidden="true" tabindex="-1"></a>  <span class="at">group =</span> <span class="st">"omics/transcriptomics"</span>, <span class="at">dataset =</span> <span class="st">"genes"</span></span>
<span id="cb14-559"><a href="#cb14-559" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-560"><a href="#cb14-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-561"><a href="#cb14-561" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Explore your creation</span></span>
<span id="cb14-562"><a href="#cb14-562" aria-hidden="true" tabindex="-1"></a><span class="fu">h5ls</span>(<span class="st">"my_study.hdf5"</span>)</span>
<span id="cb14-563"><a href="#cb14-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-564"><a href="#cb14-564" aria-hidden="true" tabindex="-1"></a><span class="co"># Questions for reflection:</span></span>
<span id="cb14-565"><a href="#cb14-565" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb14-566"><a href="#cb14-566" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Longitudinal data organization:</span></span>
<span id="cb14-567"><a href="#cb14-567" aria-hidden="true" tabindex="-1"></a><span class="co">#    - How would you organize data collected at multiple time points?</span></span>
<span id="cb14-568"><a href="#cb14-568" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Would you create separate groups for each timepoint (/timepoint1/, /timepoint2/)?</span></span>
<span id="cb14-569"><a href="#cb14-569" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Or group by data type with timepoint as a dimension within datasets?</span></span>
<span id="cb14-570"><a href="#cb14-570" aria-hidden="true" tabindex="-1"></a><span class="co">#    - What are the trade-offs of each approach?</span></span>
<span id="cb14-571"><a href="#cb14-571" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb14-572"><a href="#cb14-572" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Multi-omic integration:</span></span>
<span id="cb14-573"><a href="#cb14-573" aria-hidden="true" tabindex="-1"></a><span class="co">#    - You now have genomics and transcriptomics. What if you add:</span></span>
<span id="cb14-574"><a href="#cb14-574" aria-hidden="true" tabindex="-1"></a><span class="co">#      * Proteomics data</span></span>
<span id="cb14-575"><a href="#cb14-575" aria-hidden="true" tabindex="-1"></a><span class="co">#      * Metabolomics data</span></span>
<span id="cb14-576"><a href="#cb14-576" aria-hidden="true" tabindex="-1"></a><span class="co">#      * Clinical phenotypes</span></span>
<span id="cb14-577"><a href="#cb14-577" aria-hidden="true" tabindex="-1"></a><span class="co">#    - How would you organize these to make cross-omic analyses easy?</span></span>
<span id="cb14-578"><a href="#cb14-578" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Should raw and processed data live in different groups?</span></span>
<span id="cb14-579"><a href="#cb14-579" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb14-580"><a href="#cb14-580" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Derived results storage:</span></span>
<span id="cb14-581"><a href="#cb14-581" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Where would you store PCA results for each omic?</span></span>
<span id="cb14-582"><a href="#cb14-582" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Where would integration analysis results go?</span></span>
<span id="cb14-583"><a href="#cb14-583" aria-hidden="true" tabindex="-1"></a><span class="co">#    - How do you link results back to the data they came from?</span></span>
<span id="cb14-584"><a href="#cb14-584" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Should every analysis get its own group, or organize by analysis type?</span></span>
<span id="cb14-585"><a href="#cb14-585" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb14-586"><a href="#cb14-586" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Metadata strategy:</span></span>
<span id="cb14-587"><a href="#cb14-587" aria-hidden="true" tabindex="-1"></a><span class="co">#    - What attributes would you attach to each dataset?</span></span>
<span id="cb14-588"><a href="#cb14-588" aria-hidden="true" tabindex="-1"></a><span class="co">#    - How would you document processing steps?</span></span>
<span id="cb14-589"><a href="#cb14-589" aria-hidden="true" tabindex="-1"></a><span class="co">#    - What information needs to travel with the data for reproducibility?</span></span>
<span id="cb14-590"><a href="#cb14-590" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb14-591"><a href="#cb14-591" aria-hidden="true" tabindex="-1"></a><span class="co"># Try implementing one of these scenarios in code. The goal is not perfection,</span></span>
<span id="cb14-592"><a href="#cb14-592" aria-hidden="true" tabindex="-1"></a><span class="co"># but practice thinking about data organization for real research projects.</span></span>
<span id="cb14-593"><a href="#cb14-593" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-594"><a href="#cb14-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-595"><a href="#cb14-595" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb14-596"><a href="#cb14-596" aria-hidden="true" tabindex="-1"></a><span class="fu">### Reflection, Not Solutions</span></span>
<span id="cb14-597"><a href="#cb14-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-598"><a href="#cb14-598" aria-hidden="true" tabindex="-1"></a>This exercise deliberately doesn't provide "the answer" because there often isn't a single correct way to organize complex data. Your organizational choices should reflect:</span>
<span id="cb14-599"><a href="#cb14-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-600"><a href="#cb14-600" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Your specific research questions (what comparisons matter most?)</span>
<span id="cb14-601"><a href="#cb14-601" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Your workflow (what data do you access together?)</span>
<span id="cb14-602"><a href="#cb14-602" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Your collaboration needs (who else needs to use this data?)</span>
<span id="cb14-603"><a href="#cb14-603" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Your analysis pipeline (what tools will read this file?)</span>
<span id="cb14-604"><a href="#cb14-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-605"><a href="#cb14-605" aria-hidden="true" tabindex="-1"></a>The experience of designing and trying different structures teaches more than following a prescribed solution. If you're unsure, try multiple approaches and see which one feels more natural when you go to actually use the data.</span>
<span id="cb14-606"><a href="#cb14-606" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-607"><a href="#cb14-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-608"><a href="#cb14-608" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key Takeaways {.key-concept}</span></span>
<span id="cb14-609"><a href="#cb14-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-610"><a href="#cb14-610" aria-hidden="true" tabindex="-1"></a>We've covered a lot of ground in understanding HDF5. Let's consolidate the essential concepts you need to remember as you work with BigDataStatMeth and HDF5 files.</span>
<span id="cb14-611"><a href="#cb14-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-612"><a href="#cb14-612" aria-hidden="true" tabindex="-1"></a><span class="fu">### Essential Concepts</span></span>
<span id="cb14-613"><a href="#cb14-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-614"><a href="#cb14-614" aria-hidden="true" tabindex="-1"></a>You've learned that HDF5 is fundamentally different from traditional file formats. Let's review the key ideas:</span>
<span id="cb14-615"><a href="#cb14-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-616"><a href="#cb14-616" aria-hidden="true" tabindex="-1"></a>**HDF5 is a database for matrices**, not just a file format. This distinction matters because it changes how you think about data storage. Instead of "saving a file," you're building a database that can contain multiple related datasets, organized hierarchically, with built-in metadata.</span>
<span id="cb14-617"><a href="#cb14-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-618"><a href="#cb14-618" aria-hidden="true" tabindex="-1"></a>**Hierarchical organization** mirrors how you think about complex research projects. Just as you organize documents into folders, HDF5 lets you organize datasets into groups. This organization isn't cosmetic - it helps both you and your analysis code understand the relationships between different data components.</span>
<span id="cb14-619"><a href="#cb14-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-620"><a href="#cb14-620" aria-hidden="true" tabindex="-1"></a>**Chunking enables efficient partial I/O** by organizing data into blocks that can be read independently. This is what makes the "read only what you need" promise real rather than theoretical. Understanding chunking helps you predict which operations will be fast and which will require reading more data than you'd prefer.</span>
<span id="cb14-621"><a href="#cb14-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-622"><a href="#cb14-622" aria-hidden="true" tabindex="-1"></a>**Compression reduces disk usage** without adding complexity to your code. HDF5 handles compression transparently, and the default settings work well for most statistical applications. You get smaller files essentially for free.</span>
<span id="cb14-623"><a href="#cb14-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-624"><a href="#cb14-624" aria-hidden="true" tabindex="-1"></a>**Metadata is built-in**, meaning your data carries its own documentation. Six months later, you (or a colleague) can open an HDF5 file and immediately understand what it contains, when it was created, and how to interpret the values.</span>
<span id="cb14-625"><a href="#cb14-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-626"><a href="#cb14-626" aria-hidden="true" tabindex="-1"></a>**BigDataStatMeth automates** these HDF5 best practices. The package makes intelligent decisions about chunking, compression, and metadata so you can focus on your analysis rather than storage engineering. But understanding what's happening under the hood helps you use the package more effectively.</span>
<span id="cb14-627"><a href="#cb14-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-628"><a href="#cb14-628" aria-hidden="true" tabindex="-1"></a><span class="fu">### When to Use HDF5</span></span>
<span id="cb14-629"><a href="#cb14-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-630"><a href="#cb14-630" aria-hidden="true" tabindex="-1"></a>Making the right choice about file formats matters for both productivity and practicality. Here's how to decide:</span>
<span id="cb14-631"><a href="#cb14-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-632"><a href="#cb14-632" aria-hidden="true" tabindex="-1"></a>✅ **Use HDF5 when:**</span>
<span id="cb14-633"><a href="#cb14-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-634"><a href="#cb14-634" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data exceeds available RAM** - This is the primary use case. When you can't load everything into memory, HDF5 lets you work with arbitrary-sized datasets by processing them in pieces.</span>
<span id="cb14-635"><a href="#cb14-635" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Need repeated access to subsets** - If your workflow involves reading different portions of data at different times, HDF5's partial I/O capabilities pay off quickly.</span>
<span id="cb14-636"><a href="#cb14-636" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Want organized, self-documenting storage** - For complex projects with multiple data components, HDF5's hierarchical structure and metadata support help keep everything organized and understandable.</span>
<span id="cb14-637"><a href="#cb14-637" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sharing data across platforms** - HDF5 is supported by R, Python, MATLAB, Julia, C++, and many other languages. It provides a common data format that works across your entire analysis ecosystem.</span>
<span id="cb14-638"><a href="#cb14-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-639"><a href="#cb14-639" aria-hidden="true" tabindex="-1"></a>❌ **Don't use HDF5 when:**</span>
<span id="cb14-640"><a href="#cb14-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-641"><a href="#cb14-641" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data fits comfortably in RAM** - If your dataset uses less than about 20% of your available memory, traditional formats (RData, CSV) are simpler. The overhead of HDF5 doesn't provide benefits when everything fits in memory anyway.</span>
<span id="cb14-642"><a href="#cb14-642" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Need simple, human-readable formats** - HDF5 files are binary and require special tools to inspect. If you need to open files in a text editor or want maximum simplicity, CSV or similar formats might be better despite their limitations.</span>
<span id="cb14-643"><a href="#cb14-643" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Working with highly irregular structures** - HDF5 excels with matrices and arrays - data that has regular structure. Highly nested or irregular data structures might be better served by other formats designed for those use cases.</span>
<span id="cb14-644"><a href="#cb14-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-645"><a href="#cb14-645" aria-hidden="true" tabindex="-1"></a>The decision isn't always clear-cut. Many projects start with data that fits in memory but grow over time. Starting with HDF5 can future-proof your analysis pipeline, especially if you anticipate scaling to larger datasets or want to leverage the organizational benefits even for medium-sized data.</span>
<span id="cb14-646"><a href="#cb14-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-647"><a href="#cb14-647" aria-hidden="true" tabindex="-1"></a><span class="fu">## Next Steps</span></span>
<span id="cb14-648"><a href="#cb14-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-649"><a href="#cb14-649" aria-hidden="true" tabindex="-1"></a>You now have a solid foundation in HDF5 concepts and how BigDataStatMeth uses them. The natural next step is to understand how computational algorithms are adapted to work efficiently with disk-based data.</span>
<span id="cb14-650"><a href="#cb14-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-651"><a href="#cb14-651" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">**Block-Wise Computing →**</span><span class="co">](blockwise-computing.qmd)</span> Learn how algorithms are adapted for disk-based matrices</span>
<span id="cb14-652"><a href="#cb14-652" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">**Working with HDF5 Matrices →**</span><span class="co">](../tutorials/working-hdf5-matrices.qmd)</span> Practical tutorial on data management</span>
<span id="cb14-653"><a href="#cb14-653" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">**Your First Analysis →**</span><span class="co">](../tutorials/first-analysis.qmd)</span> Complete analytical workflow</span>
<span id="cb14-654"><a href="#cb14-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-655"><a href="#cb14-655" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further Reading</span></span>
<span id="cb14-656"><a href="#cb14-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-657"><a href="#cb14-657" aria-hidden="true" tabindex="-1"></a>If you want to deepen your understanding of HDF5 beyond what we've covered here, these resources provide different perspectives and more technical detail:</span>
<span id="cb14-658"><a href="#cb14-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-659"><a href="#cb14-659" aria-hidden="true" tabindex="-1"></a>**[HDF5 User Guide](https://portal.hdfgroup.org/display/HDF5/HDF5+User+Guides)** - The official documentation from The HDF Group. This is comprehensive and authoritative, though quite technical. Good for understanding the full capabilities of HDF5 and diving into advanced features like parallel I/O, virtual datasets, and complex datatypes.</span>
<span id="cb14-660"><a href="#cb14-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-661"><a href="#cb14-661" aria-hidden="true" tabindex="-1"></a>**[rhdf5 Bioconductor Package](https://bioconductor.org/packages/release/bioc/html/rhdf5.html)** - The R package that BigDataStatMeth builds upon for low-level HDF5 operations. The rhdf5 documentation provides examples of direct HDF5 manipulation if you need finer control than BigDataStatMeth's convenience functions provide.</span>
<span id="cb14-662"><a href="#cb14-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-663"><a href="#cb14-663" aria-hidden="true" tabindex="-1"></a>**[HDFView](https://www.hdfgroup.org/downloads/hdfview/)** - A free graphical tool for browsing HDF5 files visually. Essential for debugging file structure issues and understanding what your code has created. Works on Windows, Mac, and Linux.</span>
<span id="cb14-664"><a href="#cb14-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-665"><a href="#cb14-665" aria-hidden="true" tabindex="-1"></a>**BigDataStatMeth Documentation** - The <span class="co">[</span><span class="ot">complete API reference</span><span class="co">](../../api-reference/r-functions.qmd)</span> covers all HDF5-related functions in detail, with additional examples and parameter explanations.</span>
<span id="cb14-666"><a href="#cb14-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-667"><a href="#cb14-667" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb14-668"><a href="#cb14-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-669"><a href="#cb14-669" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-670"><a href="#cb14-670" aria-hidden="true" tabindex="-1"></a><span class="fu">### Questions or Feedback?</span></span>
<span id="cb14-671"><a href="#cb14-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-672"><a href="#cb14-672" aria-hidden="true" tabindex="-1"></a>If something is unclear or you'd like more examples, please <span class="co">[</span><span class="ot">open an issue</span><span class="co">](https://github.com/isglobal-brge/BigDataStatMeth/issues)</span> on GitHub. We value your feedback to improve this educational material.</span>
<span id="cb14-673"><a href="#cb14-673" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2025 BigDataStatMeth - ISGlobal BRGE</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/isglobal-brge/BigDataStatMeth/edit/main/fundamentals/understanding-hdf5.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/isglobal-brge/BigDataStatMeth/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/isglobal-brge/BigDataStatMeth">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item">
 License: GPL-3
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>