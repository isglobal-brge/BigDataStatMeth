<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>The Big Data Problem in Genomics – BigDataStatMeth</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../assets/favicon.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-969ddfa49e00a70eb3423444dbc81f6c.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-d0f1cdce0779274a5ec1152cd33adb41.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-d6747531eee53fd58085a197f3afc013.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-d0f1cdce0779274a5ec1152cd33adb41.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../assets/css/custom.css">
</head>

<body class="nav-sidebar docked nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../assets/images/logo.png" alt="" class="navbar-logo light-content">
    <img src="../assets/images/logo.png" alt="" class="navbar-logo dark-content">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">BigDataStatMeth</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-fundamentals" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Fundamentals</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-fundamentals">    
        <li>
    <a class="dropdown-item" href="../fundamentals/big-data-problem.html">
 <span class="dropdown-text">The Big Data Problem</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../fundamentals/understanding-hdf5.html">
 <span class="dropdown-text">Understanding HDF5</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../fundamentals/blockwise-computing.html">
 <span class="dropdown-text">Block-Wise Computing</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../fundamentals/linear-algebra.html">
 <span class="dropdown-text">Linear Algebra Essentials</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tutorials" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tutorials</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-tutorials">    
        <li>
    <a class="dropdown-item" href="../tutorials/getting-started.html">
 <span class="dropdown-text">Getting Started</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../tutorials/working-hdf5-matrices.html">
 <span class="dropdown-text">Working with HDF5 Matrices</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../tutorials/first-analysis.html">
 <span class="dropdown-text">Your First Analysis</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-workflows" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Workflows</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-workflows">    
        <li>
    <a class="dropdown-item" href="../workflows/implementing-pca.html">
 <span class="dropdown-text">Implementing PCA</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../workflows/implementing-cca.html">
 <span class="dropdown-text">Implementing CCA</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../workflows/cross-platform.html">
 <span class="dropdown-text">Cross-Platform Workflows</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-developing-methods" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Developing Methods</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-developing-methods">    
        <li>
    <a class="dropdown-item" href="../developing-methods/cca-r-implementation.html">
 <span class="dropdown-text">CCA in R</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../developing-methods/cca-cpp-implementation.html">
 <span class="dropdown-text">CCA in C++</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-api-reference" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">API Reference</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-api-reference">    
        <li>
    <a class="dropdown-item" href="../api-reference/r/index.html">
 <span class="dropdown-text">R Functions</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../api-reference/cpp/index.html">
 <span class="dropdown-text">C++ API</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../technical/performance.html"> 
<span class="menu-text">Technical</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/isglobal-brge/BigDataStatMeth"> <i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://cran.r-project.org/package=BigDataStatMeth"> 
<span class="menu-text">CRAN</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../fundamentals/big-data-problem.html">Core Concepts</a></li><li class="breadcrumb-item"><a href="../fundamentals/big-data-problem.html">The Big Data Problem in Genomics</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Core Concepts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../fundamentals/big-data-problem.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">The Big Data Problem in Genomics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../fundamentals/understanding-hdf5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Understanding HDF5 Storage</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../fundamentals/blockwise-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Block-Wise Computing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../fundamentals/linear-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear Algebra for Statistical Methods</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-youll-learn" id="toc-what-youll-learn" class="nav-link active" data-scroll-target="#what-youll-learn"><span class="header-section-number">0.1</span> What You’ll Learn</a></li>
  <li><a href="#the-promise-and-challenge-of-large-scale-data" id="toc-the-promise-and-challenge-of-large-scale-data" class="nav-link" data-scroll-target="#the-promise-and-challenge-of-large-scale-data"><span class="header-section-number">1</span> The Promise and Challenge of Large-Scale Data</a></li>
  <li><a href="#a-concrete-example-the-memory-wall" id="toc-a-concrete-example-the-memory-wall" class="nav-link" data-scroll-target="#a-concrete-example-the-memory-wall"><span class="header-section-number">2</span> A Concrete Example: The Memory Wall</a>
  <ul class="collapse">
  <li><a href="#the-size-calculation" id="toc-the-size-calculation" class="nav-link" data-scroll-target="#the-size-calculation"><span class="header-section-number">2.1</span> The Size Calculation</a></li>
  <li><a href="#the-practical-reality" id="toc-the-practical-reality" class="nav-link" data-scroll-target="#the-practical-reality"><span class="header-section-number">2.2</span> The Practical Reality</a></li>
  </ul></li>
  <li><a href="#traditional-approaches-and-their-limitations" id="toc-traditional-approaches-and-their-limitations" class="nav-link" data-scroll-target="#traditional-approaches-and-their-limitations"><span class="header-section-number">3</span> Traditional Approaches and Their Limitations</a>
  <ul class="collapse">
  <li><a href="#approach-1-reduce-the-data" id="toc-approach-1-reduce-the-data" class="nav-link" data-scroll-target="#approach-1-reduce-the-data"><span class="header-section-number">3.1</span> Approach 1: Reduce the Data</a></li>
  <li><a href="#approach-2-chunk-analysis-with-manual-merging" id="toc-approach-2-chunk-analysis-with-manual-merging" class="nav-link" data-scroll-target="#approach-2-chunk-analysis-with-manual-merging"><span class="header-section-number">3.2</span> Approach 2: Chunk Analysis with Manual Merging</a></li>
  <li><a href="#approach-3-use-specialized-hardware" id="toc-approach-3-use-specialized-hardware" class="nav-link" data-scroll-target="#approach-3-use-specialized-hardware"><span class="header-section-number">3.3</span> Approach 3: Use Specialized Hardware</a></li>
  <li><a href="#approach-4-reformulate-the-problem" id="toc-approach-4-reformulate-the-problem" class="nav-link" data-scroll-target="#approach-4-reformulate-the-problem"><span class="header-section-number">3.4</span> Approach 4: Reformulate the Problem</a></li>
  </ul></li>
  <li><a href="#the-root-cause-ram-as-a-fixed-resource" id="toc-the-root-cause-ram-as-a-fixed-resource" class="nav-link" data-scroll-target="#the-root-cause-ram-as-a-fixed-resource"><span class="header-section-number">4</span> The Root Cause: RAM as a Fixed Resource</a></li>
  <li><a href="#when-does-the-problem-actually-occur" id="toc-when-does-the-problem-actually-occur" class="nav-link" data-scroll-target="#when-does-the-problem-actually-occur"><span class="header-section-number">5</span> When Does the Problem Actually Occur?</a>
  <ul class="collapse">
  <li><a href="#rule-of-thumb-the-20-rule" id="toc-rule-of-thumb-the-20-rule" class="nav-link" data-scroll-target="#rule-of-thumb-the-20-rule"><span class="header-section-number">5.1</span> Rule of Thumb: The 20% Rule</a></li>
  <li><a href="#matrix-dimensions-as-thresholds" id="toc-matrix-dimensions-as-thresholds" class="nav-link" data-scroll-target="#matrix-dimensions-as-thresholds"><span class="header-section-number">5.2</span> Matrix Dimensions as Thresholds</a></li>
  <li><a href="#types-of-analysis-matter" id="toc-types-of-analysis-matter" class="nav-link" data-scroll-target="#types-of-analysis-matter"><span class="header-section-number">5.3</span> Types of Analysis Matter</a></li>
  </ul></li>
  <li><a href="#interactive-exercise" id="toc-interactive-exercise" class="nav-link" data-scroll-target="#interactive-exercise"><span class="header-section-number">6</span> Interactive Exercise</a>
  <ul class="collapse">
  <li><a href="#practice-calculate-your-datas-memory-requirements" id="toc-practice-calculate-your-datas-memory-requirements" class="nav-link" data-scroll-target="#practice-calculate-your-datas-memory-requirements"><span class="header-section-number">6.1</span> Practice: Calculate Your Data’s Memory Requirements</a></li>
  </ul></li>
  <li><a href="#the-path-forward" id="toc-the-path-forward" class="nav-link" data-scroll-target="#the-path-forward"><span class="header-section-number">7</span> The Path Forward</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">8</span> Key Takeaways</a>
  <ul class="collapse">
  <li><a href="#essential-concepts" id="toc-essential-concepts" class="nav-link" data-scroll-target="#essential-concepts"><span class="header-section-number">8.1</span> Essential Concepts</a></li>
  <li><a href="#when-to-use-disk-based-computing" id="toc-when-to-use-disk-based-computing" class="nav-link" data-scroll-target="#when-to-use-disk-based-computing"><span class="header-section-number">8.2</span> When to Use Disk-Based Computing</a></li>
  </ul></li>
  <li><a href="#next-steps" id="toc-next-steps" class="nav-link" data-scroll-target="#next-steps"><span class="header-section-number">9</span> Next Steps</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/isglobal-brge/BigDataStatMeth/edit/main/fundamentals/big-data-problem.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/isglobal-brge/BigDataStatMeth/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../fundamentals/big-data-problem.html">Core Concepts</a></li><li class="breadcrumb-item"><a href="../fundamentals/big-data-problem.html">The Big Data Problem in Genomics</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">The Big Data Problem in Genomics</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Understanding why traditional methods fail at scale</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="what-youll-learn" class="level3 learning-objectives" data-number="0.1">
<h3 data-number="0.1" class="anchored" data-anchor-id="what-youll-learn"><span class="header-section-number">0.1</span> What You’ll Learn</h3>
<p>By the end of this section, you will:</p>
<ul>
<li>Understand why traditional computing approaches fail with large datasets</li>
<li>Calculate memory requirements for your own data</li>
<li>Recognize the three fundamental constraints: memory, computation time, and disk I/O</li>
<li>Know when dataset size becomes a practical problem</li>
<li>Make informed decisions about when to use disk-based computing</li>
<li>Understand the trade-offs between in-memory and disk-based approaches</li>
</ul>
</section>
<section id="the-promise-and-challenge-of-large-scale-data" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="the-promise-and-challenge-of-large-scale-data"><span class="header-section-number">1</span> The Promise and Challenge of Large-Scale Data</h2>
<p>The data revolution has transformed scientific research across multiple disciplines. Geographic Information Systems (GIS) process satellite imagery with billions of pixels covering entire continents. Climate scientists analyze decades of high-resolution weather data from thousands of monitoring stations. Financial analysts track millions of transactions across global markets. Time series analysis in IoT applications handles streams from millions of sensors. Image analysis in medical diagnostics processes high-resolution scans with millions of voxels. Astronomers catalog billions of celestial objects from sky surveys.</p>
<p>Each of these fields has experienced the same transformation: data collection has outpaced our traditional computational tools. What worked perfectly for manageable datasets breaks down when you scale to the volumes now routinely collected. The mathematics remains the same, but the practical reality of computation changes fundamentally.</p>
<p>In this documentation, we’ll use <strong>genomics as our primary example</strong> because it illustrates the problem particularly clearly: genome-wide association studies (GWAS) routinely measure 500,000+ genetic variants across 50,000+ individuals, creating datasets that don’t fit in RAM. Transcriptomics studies quantify 20,000 genes across thousands of samples. Multi-omic integration combines genomics, transcriptomics, proteomics, and metabolomics for the same cohorts.</p>
<p>However, the concepts, solutions, and methods we discuss apply equally to any field working with large numerical matrices: satellite imagery is just a very large matrix of pixel values, time series data is observations-by-timepoints, financial data is transactions-by-features. The block-wise computational strategies and HDF5-based data management that BigDataStatMeth provides work regardless of whether your rows represent individuals, pixels, transactions, or time points.</p>
<p>We focus on genomics examples because: - The problem is widespread in this community - Dataset sizes clearly exceed typical RAM capacities<br>
- Statistical methods (PCA, regression, association tests) are well-defined - Results are scientifically important and well-understood</p>
<p>But as you read, remember that “individuals × genetic variants” could just as easily be “pixels × spectral bands,” “time points × sensors,” or “transactions × features.” The computational challenges and solutions are fundamentally the same.</p>
</section>
<section id="a-concrete-example-the-memory-wall" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="a-concrete-example-the-memory-wall"><span class="header-section-number">2</span> A Concrete Example: The Memory Wall</h2>
<p>Let’s make this concrete with a realistic scenario that many researchers face. You’re conducting a GWAS using data from the UK Biobank, which provides genetic data for 500,000 individuals across approximately 800,000 genetic variants (after standard quality control). This is not an unusually large dataset by modern standards - it’s actually quite typical for contemporary genetic research.</p>
<section id="the-size-calculation" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="the-size-calculation"><span class="header-section-number">2.1</span> The Size Calculation</h3>
<p>Each genetic variant is typically coded as 0, 1, or 2 (for a diploid organism like humans), representing the number of copies of the alternate allele. Storing this as a floating-point number requires 8 bytes per value (using R’s default numeric storage). Let’s calculate what this means:</p>
<pre><code>500,000 individuals × 800,000 variants × 8 bytes = 3.2 × 10^12 bytes

Converting to more familiar units:
= 3,200 GB
= 3.2 TB</code></pre>
<p>Three point two terabytes just for the genotype matrix. This doesn’t include:</p>
<ul>
<li>Sample identifiers and metadata (ancestry, phenotypes, covariates)</li>
<li>Variant annotations (chromosomal position, allele frequencies, functional annotations)</li>
<li>Quality control metrics</li>
<li>Derived variables or intermediate results</li>
<li>Any additional data layers (imputed variants, expression data, etc.)</li>
</ul>
</section>
<section id="the-practical-reality" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="the-practical-reality"><span class="header-section-number">2.2</span> The Practical Reality</h3>
<p>Most researchers don’t have access to machines with 3.2 TB of RAM. Even if you do, remember that R needs additional memory to actually <em>compute</em> with the data. A simple operation like computing a correlation matrix (<code>cor(X)</code>) requires substantially more memory than just storing <code>X</code>. Matrix operations typically need 2-3× the data size in working memory.</p>
<p>Furthermore, loading 3.2 TB into RAM, even on a machine that has the capacity, takes considerable time. The data must be read from disk, parsed, and structured in memory. On a typical system with disk read speeds of 500 MB/s, loading this dataset would require nearly two hours - and that’s assuming no bottlenecks, no compression to decompress, and optimal I/O conditions.</p>
<p>But memory isn’t the only bottleneck - <strong>computational time</strong> grows dramatically with data size. Consider computing a simple operation like a correlation matrix on this dataset:</p>
<pre><code>Computing cor(X) for 800,000 variants requires:
≈ (800,000)² / 2 pairwise correlations
= 320 billion correlations

At 1 million correlations per second (optimistic):
= 320,000 seconds  
= 89 hours
= Nearly 4 days of continuous computation</code></pre>
<p>And this is for a single operation. Real analyses involve many such operations iteratively: compute statistics, filter variants, recompute, fit models, validate, repeat. Each iteration compounds the time problem.</p>
<p>The computational complexity scales poorly: doubling your dataset size often quadruples or even octuples computation time, depending on the operation. A PCA that takes 10 minutes on 100,000 samples might take hours on 500,000 samples - not because of I/O, but because the number of arithmetic operations grows as <span class="math inline">O(n^2p)</span> or worse.</p>
<p>What happens when you try anyway? One of several things:</p>
<p>What happens when you try anyway? One of several things:</p>
<p><strong>Out-of-memory errors:</strong> R terminates your session with an error message. All your work is lost. If you’re running this on a shared cluster, you may have consumed significant resources that other users were depending on.</p>
<p><strong>System thrashing:</strong> If your operating system tries to be helpful by using swap space, your system grinds to a near halt. Operations that should take seconds take hours as the system constantly moves data between RAM and disk. Your entire computer becomes unresponsive.</p>
<p><strong>Prohibitive computation time:</strong> Even if the data technically fits and operations complete, they take so long that interactive analysis becomes impossible. Waiting hours or days for each exploratory step means you can’t iterate, can’t try alternative approaches, and can’t develop intuition about your data. Your research pace becomes limited by computational throughput rather than your thinking.</p>
<p>None of these outcomes moves your research forward.</p>
</section>
</section>
<section id="traditional-approaches-and-their-limitations" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="traditional-approaches-and-their-limitations"><span class="header-section-number">3</span> Traditional Approaches and Their Limitations</h2>
<p>Faced with this memory barrier, researchers have developed various workarounds. Each has merit in specific contexts, but each also has significant limitations that affect either what analyses you can perform or how efficiently you can work.</p>
<section id="approach-1-reduce-the-data" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="approach-1-reduce-the-data"><span class="header-section-number">3.1</span> Approach 1: Reduce the Data</h3>
<p><strong>Strategy:</strong> Use only a subset of the data that fits in memory.</p>
<p>Perhaps you analyze 50,000 samples instead of 500,000, or focus on 100,000 variants instead of 800,000. This immediately makes the problem tractable - a 50,000 × 100,000 matrix requires “only” 40 GB, which fits on a well-equipped workstation.</p>
<p><strong>The limitation:</strong> You’re throwing away information. Those 450,000 samples you excluded might contain the individuals with the phenotype you’re interested in. Those 700,000 variants you didn’t analyze might include the causal variant for the trait you’re studying. Statistical power drops dramatically with smaller sample sizes, and rare variants become impossible to analyze when you limit your sample.</p>
<p>This approach essentially says “we’ll solve the computational problem by avoiding it” - but at the cost of not actually answering your biological question with the full data you collected. It’s particularly problematic when the whole point of large-scale studies is to have adequate power to detect small effects or study rare variants.</p>
</section>
<section id="approach-2-chunk-analysis-with-manual-merging" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="approach-2-chunk-analysis-with-manual-merging"><span class="header-section-number">3.2</span> Approach 2: Chunk Analysis with Manual Merging</h3>
<p><strong>Strategy:</strong> Analyze chunks of data separately and manually combine results.</p>
<p>You might analyze variants 1-100,000, then variants 100,001-200,000, and so on. Each chunk fits in memory. For some analyses (like single-variant association tests), you can simply concatenate the results from each chunk.</p>
<p><strong>The limitation:</strong> This only works for embarrassingly parallel problems where chunks don’t need to interact. Many statistical methods require seeing all the data simultaneously. Principal component analysis (PCA) needs to consider all variants together to identify the major axes of variation. Regularized regression methods (LASSO, ridge regression) optimize over the entire feature space simultaneously. Cross-validation requires consistent data splits across the full dataset.</p>
<p>Even when chunking is possible in principle, implementing it correctly is error-prone. You must carefully track which chunks have been processed, ensure consistent handling of edge cases (variants near chunk boundaries), and manually verify that merged results are statistically valid. Every analysis becomes a custom programming project.</p>
</section>
<section id="approach-3-use-specialized-hardware" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="approach-3-use-specialized-hardware"><span class="header-section-number">3.3</span> Approach 3: Use Specialized Hardware</h3>
<p><strong>Strategy:</strong> Rent time on high-memory machines in the cloud or use institutional clusters with hundreds of GB of RAM.</p>
<p>Cloud providers offer machines with 512 GB, 1 TB, or even more RAM. Many universities operate shared computing clusters with similar capabilities.</p>
<p><strong>The limitation:</strong> This approach works but introduces friction into your research process. Cloud computing costs money - substantial money for large analyses that run for hours or days. Institutional clusters require learning job submission systems, waiting in queues, and debugging remotely when something goes wrong.</p>
<p>More fundamentally, this approach doesn’t scale to the next order of magnitude. As datasets continue to grow (and they will), even these large-memory machines become insufficient. You haven’t solved the underlying problem, just postponed confronting it. And interactive data exploration - the iterative process of trying ideas, examining results, and refining your approach - becomes cumbersome when every attempt requires submitting a batch job and waiting for results.</p>
</section>
<section id="approach-4-reformulate-the-problem" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="approach-4-reformulate-the-problem"><span class="header-section-number">3.4</span> Approach 4: Reformulate the Problem</h3>
<p><strong>Strategy:</strong> Develop mathematically equivalent algorithms that avoid ever materializing the full matrix in memory.</p>
<p>This is actually a sophisticated approach used in many modern tools. For example, some GWAS software never loads the full genotype matrix, instead reading small portions from disk as needed and maintaining summary statistics in memory.</p>
<p><strong>The limitation:</strong> This requires algorithm-specific implementations. Someone must write custom code for each statistical method, thinking carefully about how to decompose the computation. Most researchers aren’t in a position to do this themselves - they depend on software developers providing these specialized implementations. This means you’re limited to whatever methods someone has already implemented in this special way.</p>
<p>Moreover, these optimized tools often exist as standalone programs with their own file formats, their own configuration languages, and limited flexibility. Moving data between tools, combining methods in novel ways, or extending analyses beyond what the tool provides becomes difficult. Your computational constraints start dictating your scientific questions, rather than the other way around.</p>
</section>
</section>
<section id="the-root-cause-ram-as-a-fixed-resource" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="the-root-cause-ram-as-a-fixed-resource"><span class="header-section-number">4</span> The Root Cause: RAM as a Fixed Resource</h2>
<p>All these limitations stem from a single constraint: RAM is finite, and treating it as an unlimited resource forces us into uncomfortable compromises. Either we analyze less data, fragment our analyses, incur substantial costs, or limit ourselves to pre-packaged tools.</p>
<p>The traditional computing model assumes data lives in memory during analysis. Functions expect to receive complete data objects as inputs. Algorithms assume they can access any element of a matrix at any time. This made perfect sense when datasets were smaller - why complicate your code by reading data from disk when it fits comfortably in RAM?</p>
<p>But this assumption is no longer tenable for many modern datasets. We need a different model, one where:</p>
<ol type="1">
<li><strong>Data primarily lives on disk</strong>, with only actively needed portions in RAM</li>
<li><strong>Algorithms work with blocks</strong>, processing manageable chunks sequentially</li>
<li><strong>File format supports efficient partial access</strong>, making disk-based computing practical</li>
<li><strong>Tools are flexible</strong>, allowing complex multi-step analyses on out-of-memory data</li>
</ol>
<p>This is precisely what BigDataStatMeth provides, building on the HDF5 file format and block-wise computational strategies. Rather than forcing you to choose between data subsampling, hardware requirements, analysis limitations, or implementation complexity, the package lets you work with large datasets using the RAM you have available, implementing the statistical methods you need, without requiring expertise in high-performance computing.</p>
</section>
<section id="when-does-the-problem-actually-occur" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="when-does-the-problem-actually-occur"><span class="header-section-number">5</span> When Does the Problem Actually Occur?</h2>
<p>It’s worth being specific about when you’ll encounter these memory limitations. Not every genomics analysis requires special handling of large data. Understanding the thresholds helps you decide when to use specialized tools like BigDataStatMeth versus when simpler approaches suffice.</p>
<section id="rule-of-thumb-the-20-rule" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="rule-of-thumb-the-20-rule"><span class="header-section-number">5.1</span> Rule of Thumb: The 20% Rule</h3>
<p>A conservative guideline is that your data should use less than 20% of your available RAM to analyze comfortably. This leaves room for:</p>
<ul>
<li>R’s internal copies during operations</li>
<li>Intermediate results from computations</li>
<li>The operating system and other programs</li>
<li>Some safety margin for operations that temporarily spike memory usage</li>
</ul>
<p>If you have 16 GB of RAM, this means staying under about 3 GB of data. For 32 GB of RAM, keep data under 6 GB. These thresholds might seem generous, but they prevent the frustrating experience of operations failing partway through or systems becoming unresponsive.</p>
</section>
<section id="matrix-dimensions-as-thresholds" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="matrix-dimensions-as-thresholds"><span class="header-section-number">5.2</span> Matrix Dimensions as Thresholds</h3>
<p>Here are concrete matrix sizes and their memory requirements:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dimensions</th>
<th>Memory Required</th>
<th>Typical Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1,000 × 10,000</td>
<td>80 MB</td>
<td>Small pilot study, processed data</td>
</tr>
<tr class="even">
<td>10,000 × 10,000</td>
<td>800 MB</td>
<td>Medium study, targeted feature set</td>
</tr>
<tr class="odd">
<td>10,000 × 100,000</td>
<td>8 GB</td>
<td>Large study, single omic</td>
</tr>
<tr class="even">
<td>50,000 × 100,000</td>
<td>40 GB</td>
<td>Large cohort, genome-wide</td>
</tr>
<tr class="odd">
<td>100,000 × 500,000</td>
<td>400 GB</td>
<td>Biobank-scale, comprehensive</td>
</tr>
<tr class="even">
<td>500,000 × 800,000</td>
<td>3.2 TB</td>
<td>Full UK Biobank-scale GWAS</td>
</tr>
</tbody>
</table>
<p>The transition from “fits in memory” to “requires special handling” typically occurs around 10,000 × 100,000 for most researchers with standard workstations. Once you cross into the 40+ GB range, specialized approaches become not just helpful but necessary.</p>
</section>
<section id="types-of-analysis-matter" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="types-of-analysis-matter"><span class="header-section-number">5.3</span> Types of Analysis Matter</h3>
<p>The type of analysis also affects whether you hit memory limitations:</p>
<p><strong>Less demanding:</strong> Simple operations that process the data in a streaming fashion (reading sequentially without needing everything at once) often work with larger datasets. Computing means, frequencies, or single-variant tests can handle data that wouldn’t fit entirely in RAM.</p>
<p><strong>More demanding:</strong> Operations that require the full data simultaneously hit memory limits sooner. Matrix factorizations (PCA, SVD), model fitting across many features simultaneously, cross-validation, and resampling methods all need to “see” large portions of data at once.</p>
<p><strong>Most demanding:</strong> Operations that create large intermediate results exhaust memory even faster. Computing all pairwise correlations creates a new matrix of size features × features, which for 100,000 features would require nearly 80 GB just for the output, regardless of the input size.</p>
</section>
</section>
<section id="interactive-exercise" class="level2 exercise" data-number="6">
<h2 class="exercise anchored" data-number="6" data-anchor-id="interactive-exercise"><span class="header-section-number">6</span> Interactive Exercise</h2>
<section id="practice-calculate-your-datas-memory-requirements" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="practice-calculate-your-datas-memory-requirements"><span class="header-section-number">6.1</span> Practice: Calculate Your Data’s Memory Requirements</h3>
<p>The best way to internalize these concepts is to apply them to data you actually work with. This exercise helps you estimate whether your current or planned analyses will face memory constraints.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to estimate memory requirements</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>estimate_memory <span class="ot">&lt;-</span> <span class="cf">function</span>(n_samples, n_features, <span class="at">bytes_per_value =</span> <span class="dv">8</span>) {</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Basic storage</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  basic_gb <span class="ot">&lt;-</span> (n_samples <span class="sc">*</span> n_features <span class="sc">*</span> bytes_per_value) <span class="sc">/</span> (<span class="dv">1024</span><span class="sc">^</span><span class="dv">3</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Typical operations need 2-3x for intermediate results</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  working_gb <span class="ot">&lt;-</span> basic_gb <span class="sc">*</span> <span class="fl">2.5</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Memory Requirements:</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Data storage: %.2f GB</span><span class="sc">\n</span><span class="st">"</span>, basic_gb))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Working memory: %.2f GB</span><span class="sc">\n</span><span class="st">"</span>, working_gb))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Recommended RAM: %.2f GB</span><span class="sc">\n</span><span class="st">"</span>, working_gb <span class="sc">*</span> <span class="fl">1.5</span>))</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="fu">list</span>(<span class="at">storage =</span> basic_gb, <span class="at">working =</span> working_gb)))</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Your genomic study</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="fu">estimate_memory</span>(<span class="at">n_samples =</span> <span class="dv">10000</span>, <span class="at">n_features =</span> <span class="dv">500000</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Try with your own numbers</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="fu">estimate_memory</span>(<span class="at">n_samples =</span> ???, <span class="at">n_features =</span> ???)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Reflection Questions
</div>
</div>
<div class="callout-body-container callout-body">
<p>Think about these questions - there are no universal answers, as they depend on your specific situation:</p>
<ol type="1">
<li><strong>At what sample size would your analysis exceed your available RAM?</strong>
<ul>
<li>Consider both your current machine and any servers you have access to</li>
<li>Remember that R needs working memory beyond just data storage</li>
</ul></li>
<li><strong>Which operations in your workflow would become bottlenecks first?</strong>
<ul>
<li>Creating correlation matrices?</li>
<li>Matrix factorizations (PCA, SVD)?</li>
<li>Iterative model fitting?</li>
</ul></li>
<li><strong>Is your problem memory-limited or compute-limited?</strong>
<ul>
<li>If memory: disk-based computing helps</li>
<li>If compute time: you might need more cores or GPU acceleration</li>
<li>Often it’s both - understanding which dominates helps choose solutions</li>
</ul></li>
<li><strong>For your research question, do you need the full matrix simultaneously?</strong>
<ul>
<li>Some analyses can stream through data (single-variant tests)</li>
<li>Others need to “see” everything at once (PCA across all variants)</li>
<li>This affects whether block-wise approaches will work well</li>
</ul></li>
</ol>
<p>Try different scenarios. What if your sample size doubles? What if you add more phenotypes or additional omic layers? When does your current computational setup become insufficient?</p>
</div>
</div>
</section>
</section>
<section id="the-path-forward" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="the-path-forward"><span class="header-section-number">7</span> The Path Forward</h2>
<p>Understanding these limitations and when they occur helps you make informed decisions about your computational strategy. For small to medium datasets that fit comfortably in memory, traditional R approaches work wonderfully - they’re simpler, more flexible, and well-supported by the vast R ecosystem.</p>
<p>For datasets that push or exceed memory limits, BigDataStatMeth provides a different approach: disk-based computing with HDF5 files and block-wise algorithms. This isn’t necessarily “better” in absolute terms - it’s an appropriate tool for a specific problem. When your data exceeds memory, you need different computational strategies, and that’s what the package provides.</p>
<p>The remainder of this documentation shows you how to work effectively with large datasets using these strategies, covering both the conceptual understanding and practical implementation.</p>
</section>
<section id="key-takeaways" class="level2 key-concept" data-number="8">
<h2 class="key-concept anchored" data-number="8" data-anchor-id="key-takeaways"><span class="header-section-number">8</span> Key Takeaways</h2>
<p>Let’s consolidate the essential concepts about why big data creates computational challenges and when you need specialized approaches.</p>
<section id="essential-concepts" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="essential-concepts"><span class="header-section-number">8.1</span> Essential Concepts</h3>
<p><strong>The memory wall is real and unavoidable.</strong> When your data exceeds available RAM, traditional computing simply fails. This isn’t a software problem that better code can solve - it’s a fundamental hardware constraint. A 64 GB workstation cannot load a 200 GB matrix, period. Understanding when you’ll hit this wall helps you plan computational strategies before you’re stuck.</p>
<p><strong>Memory requirements grow faster than you expect.</strong> It’s not just about storing the data - operations need working memory for intermediate results. Matrix operations typically require 2-3× the data size in RAM. This means a “40 GB dataset” actually needs 80-120 GB of available memory for computation. The gap between data size and memory requirements catches many researchers by surprise.</p>
<p><strong>The three computational constraints</strong> - memory, compute time, and disk I/O - all matter, but they matter differently for different problems. A memory-constrained problem benefits from disk-based computing. A compute-constrained problem needs more cores or faster algorithms. An I/O-constrained problem needs better data organization or caching strategies. Identifying which constraint dominates your workflow determines which solutions will actually help.</p>
<p><strong>Dataset size is contextual, not absolute.</strong> Whether a dataset is “big” depends on your available resources, not just the number of bytes. A 20 GB dataset is “small” on a 256 GB server but “impossible” on an 8 GB laptop. Similarly, whether disk-based computing helps depends on the ratio of data size to available RAM, not the absolute size. A 50 GB dataset might work fine in-memory if you have 128 GB RAM, but requires disk-based approaches with 32 GB RAM.</p>
<p><strong>Operation type determines feasibility.</strong> Not all operations scale the same way. Element-wise operations (like adding a constant to every value) scale linearly and can stream through data. Operations requiring the full matrix simultaneously (like PCA) are harder to scale. Operations creating large outputs (like computing all pairwise correlations) can exhaust memory even when inputs fit. Understanding your analysis pipeline’s operation types helps predict where you’ll hit limitations.</p>
</section>
<section id="when-to-use-disk-based-computing" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="when-to-use-disk-based-computing"><span class="header-section-number">8.2</span> When to Use Disk-Based Computing</h3>
<p>Making the right choice about computational strategy matters for both productivity and practicality. Here’s guidance based on the challenges we’ve discussed:</p>
<p>✅ <strong>Use disk-based computing when:</strong></p>
<ul>
<li><p><strong>Data exceeds 20-30% of available RAM</strong> - This threshold gives enough headroom for intermediate results and operating system needs. Below this, in-memory approaches work fine. Above this, you start risking memory exhaustion during computation.</p></li>
<li><p><strong>Repeated partial access is your workflow</strong> - If you frequently access different subsets of data (different chromosomes, different time windows, different sample cohorts), HDF5’s partial I/O capabilities pay dividends. You read only what you need each time, keeping memory usage constant regardless of total data size.</p></li>
<li><p><strong>Multiple analysis types on same data</strong> - When you’ll run PCA, then regression, then association tests on the same dataset, keeping data in HDF5 format means you load it once and reuse it for all analyses. The upfront conversion cost amortizes across multiple uses.</p></li>
<li><p><strong>Sharing data across platforms</strong> - If your workflow spans R, Python, and command-line tools, HDF5 provides a common format all can read efficiently. This beats converting between CSV, RData, and other formats for each tool.</p></li>
</ul>
<p>❌ <strong>Use traditional in-memory computing when:</strong></p>
<ul>
<li><p><strong>Data comfortably fits in less than 20% of RAM</strong> - Traditional R approaches are simpler, more flexible, and better supported by the broader R ecosystem. Don’t add complexity when it’s not needed.</p></li>
<li><p><strong>One-off analysis with simple operations</strong> - If you’re doing a quick exploratory analysis you won’t repeat, the overhead of converting to HDF5 outweighs the benefits. Load the data, compute what you need, save results, and you’re done.</p></li>
<li><p><strong>Ultra-fast I/O is critical</strong> - Despite HDF5’s optimizations, RAM is always faster than disk. If your analysis involves thousands of tiny operations with random access patterns, in-memory processing wins. Disk-based computing excels at large sequential reads, not scattered tiny reads.</p></li>
<li><p><strong>You need maximum flexibility</strong> - R’s in-memory data structures support arbitrary manipulations trivially. With disk-based data, some operations become awkward or inefficient. If your workflow involves many ad-hoc transformations and exploratory manipulations, staying in memory (if possible) maintains flexibility.</p></li>
</ul>
<p>The decision isn’t always clear-cut, and many workflows benefit from a hybrid approach: use disk-based storage and computation for large matrices, but load summarized results into memory for final processing and visualization. Understanding these trade-offs helps you design efficient computational strategies for your specific needs.</p>
</section>
</section>
<section id="next-steps" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="next-steps"><span class="header-section-number">9</span> Next Steps</h2>
<p>Now that you understand why traditional approaches fail with large data, you’re ready to learn about the solution:</p>
<ul>
<li><a href="../fundamentals/understanding-hdf5.html"><strong>Understanding HDF5 →</strong></a> Learn how HDF5 enables disk-based data access</li>
<li><a href="../fundamentals/blockwise-computing.html"><strong>Block-Wise Computing →</strong></a> Understand how algorithms adapt to work with data blocks</li>
<li><a href="../tutorials/getting-started.html"><strong>Getting Started Tutorial →</strong></a> Start working with your own data</li>
</ul>
<hr>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Questions or Feedback?
</div>
</div>
<div class="callout-body-container callout-body">
<p>If you have questions about whether BigDataStatMeth is appropriate for your data size, or want to discuss specific use cases, please <a href="https://github.com/isglobal-brge/BigDataStatMeth/issues">open an issue</a> on GitHub.</p>
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/isglobal-brge\.github\.io\/BigDataStatMeth\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "The Big Data Problem in Genomics"</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> "Understanding why traditional methods fail at scale"</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>::: {.learning-objectives}</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="fu">### What You'll Learn</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>By the end of this section, you will:</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand why traditional computing approaches fail with large datasets</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate memory requirements for your own data</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Recognize the three fundamental constraints: memory, computation time, and disk I/O</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Know when dataset size becomes a practical problem</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Make informed decisions about when to use disk-based computing</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand the trade-offs between in-memory and disk-based approaches</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Promise and Challenge of Large-Scale Data</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>The data revolution has transformed scientific research across multiple disciplines. Geographic Information Systems (GIS) process satellite imagery with billions of pixels covering entire continents. Climate scientists analyze decades of high-resolution weather data from thousands of monitoring stations. Financial analysts track millions of transactions across global markets. Time series analysis in IoT applications handles streams from millions of sensors. Image analysis in medical diagnostics processes high-resolution scans with millions of voxels. Astronomers catalog billions of celestial objects from sky surveys.</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>Each of these fields has experienced the same transformation: data collection has outpaced our traditional computational tools. What worked perfectly for manageable datasets breaks down when you scale to the volumes now routinely collected. The mathematics remains the same, but the practical reality of computation changes fundamentally.</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>In this documentation, we'll use **genomics as our primary example** because it illustrates the problem particularly clearly: genome-wide association studies (GWAS) routinely measure 500,000+ genetic variants across 50,000+ individuals, creating datasets that don't fit in RAM. Transcriptomics studies quantify 20,000 genes across thousands of samples. Multi-omic integration combines genomics, transcriptomics, proteomics, and metabolomics for the same cohorts.</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>However, the concepts, solutions, and methods we discuss apply equally to any field working with large numerical matrices: satellite imagery is just a very large matrix of pixel values, time series data is observations-by-timepoints, financial data is transactions-by-features. The block-wise computational strategies and HDF5-based data management that BigDataStatMeth provides work regardless of whether your rows represent individuals, pixels, transactions, or time points.</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>We focus on genomics examples because:</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The problem is widespread in this community</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dataset sizes clearly exceed typical RAM capacities  </span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Statistical methods (PCA, regression, association tests) are well-defined</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Results are scientifically important and well-understood</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>But as you read, remember that "individuals × genetic variants" could just as easily be "pixels × spectral bands," "time points × sensors," or "transactions × features." The computational challenges and solutions are fundamentally the same.</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="fu">## A Concrete Example: The Memory Wall</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>Let's make this concrete with a realistic scenario that many researchers face. You're conducting a GWAS using data from the UK Biobank, which provides genetic data for 500,000 individuals across approximately 800,000 genetic variants (after standard quality control). This is not an unusually large dataset by modern standards - it's actually quite typical for contemporary genetic research.</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Size Calculation</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>Each genetic variant is typically coded as 0, 1, or 2 (for a diploid organism like humans), representing the number of copies of the alternate allele. Storing this as a floating-point number requires 8 bytes per value (using R's default numeric storage). Let's calculate what this means:</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="in">500,000 individuals × 800,000 variants × 8 bytes = 3.2 × 10^12 bytes</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="in">Converting to more familiar units:</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="in">= 3,200 GB</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="in">= 3.2 TB</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>Three point two terabytes just for the genotype matrix. This doesn't include:</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sample identifiers and metadata (ancestry, phenotypes, covariates)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Variant annotations (chromosomal position, allele frequencies, functional annotations)</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Quality control metrics</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Derived variables or intermediate results</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Any additional data layers (imputed variants, expression data, etc.)</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Practical Reality</span></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>Most researchers don't have access to machines with 3.2 TB of RAM. Even if you do, remember that R needs additional memory to actually *compute* with the data. A simple operation like computing a correlation matrix (<span class="in">`cor(X)`</span>) requires substantially more memory than just storing <span class="in">`X`</span>. Matrix operations typically need 2-3× the data size in working memory.</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>Furthermore, loading 3.2 TB into RAM, even on a machine that has the capacity, takes considerable time. The data must be read from disk, parsed, and structured in memory. On a typical system with disk read speeds of 500 MB/s, loading this dataset would require nearly two hours - and that's assuming no bottlenecks, no compression to decompress, and optimal I/O conditions.</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>But memory isn't the only bottleneck - **computational time** grows dramatically with data size. Consider computing a simple operation like a correlation matrix on this dataset:</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a><span class="in">Computing cor(X) for 800,000 variants requires:</span></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a><span class="in">≈ (800,000)² / 2 pairwise correlations</span></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a><span class="in">= 320 billion correlations</span></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a><span class="in">At 1 million correlations per second (optimistic):</span></span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a><span class="in">= 320,000 seconds  </span></span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a><span class="in">= 89 hours</span></span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a><span class="in">= Nearly 4 days of continuous computation</span></span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>And this is for a single operation. Real analyses involve many such operations iteratively: compute statistics, filter variants, recompute, fit models, validate, repeat. Each iteration compounds the time problem.</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>The computational complexity scales poorly: doubling your dataset size often quadruples or even octuples computation time, depending on the operation. A PCA that takes 10 minutes on 100,000 samples might take hours on 500,000 samples - not because of I/O, but because the number of arithmetic operations grows as $O(n^2p)$ or worse.</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>What happens when you try anyway? One of several things:</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>What happens when you try anyway? One of several things:</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>**Out-of-memory errors:** R terminates your session with an error message. All your work is lost. If you're running this on a shared cluster, you may have consumed significant resources that other users were depending on.</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>**System thrashing:** If your operating system tries to be helpful by using swap space, your system grinds to a near halt. Operations that should take seconds take hours as the system constantly moves data between RAM and disk. Your entire computer becomes unresponsive.</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>**Prohibitive computation time:** Even if the data technically fits and operations complete, they take so long that interactive analysis becomes impossible. Waiting hours or days for each exploratory step means you can't iterate, can't try alternative approaches, and can't develop intuition about your data. Your research pace becomes limited by computational throughput rather than your thinking.</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>None of these outcomes moves your research forward.</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a><span class="fu">## Traditional Approaches and Their Limitations</span></span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>Faced with this memory barrier, researchers have developed various workarounds. Each has merit in specific contexts, but each also has significant limitations that affect either what analyses you can perform or how efficiently you can work.</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a><span class="fu">### Approach 1: Reduce the Data</span></span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>**Strategy:** Use only a subset of the data that fits in memory.</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>Perhaps you analyze 50,000 samples instead of 500,000, or focus on 100,000 variants instead of 800,000. This immediately makes the problem tractable - a 50,000 × 100,000 matrix requires "only" 40 GB, which fits on a well-equipped workstation.</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>**The limitation:** You're throwing away information. Those 450,000 samples you excluded might contain the individuals with the phenotype you're interested in. Those 700,000 variants you didn't analyze might include the causal variant for the trait you're studying. Statistical power drops dramatically with smaller sample sizes, and rare variants become impossible to analyze when you limit your sample.</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>This approach essentially says "we'll solve the computational problem by avoiding it" - but at the cost of not actually answering your biological question with the full data you collected. It's particularly problematic when the whole point of large-scale studies is to have adequate power to detect small effects or study rare variants.</span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a><span class="fu">### Approach 2: Chunk Analysis with Manual Merging</span></span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>**Strategy:** Analyze chunks of data separately and manually combine results.</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>You might analyze variants 1-100,000, then variants 100,001-200,000, and so on. Each chunk fits in memory. For some analyses (like single-variant association tests), you can simply concatenate the results from each chunk.</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>**The limitation:** This only works for embarrassingly parallel problems where chunks don't need to interact. Many statistical methods require seeing all the data simultaneously. Principal component analysis (PCA) needs to consider all variants together to identify the major axes of variation. Regularized regression methods (LASSO, ridge regression) optimize over the entire feature space simultaneously. Cross-validation requires consistent data splits across the full dataset.</span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>Even when chunking is possible in principle, implementing it correctly is error-prone. You must carefully track which chunks have been processed, ensure consistent handling of edge cases (variants near chunk boundaries), and manually verify that merged results are statistically valid. Every analysis becomes a custom programming project.</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a><span class="fu">### Approach 3: Use Specialized Hardware</span></span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a>**Strategy:** Rent time on high-memory machines in the cloud or use institutional clusters with hundreds of GB of RAM.</span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a>Cloud providers offer machines with 512 GB, 1 TB, or even more RAM. Many universities operate shared computing clusters with similar capabilities.</span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>**The limitation:** This approach works but introduces friction into your research process. Cloud computing costs money - substantial money for large analyses that run for hours or days. Institutional clusters require learning job submission systems, waiting in queues, and debugging remotely when something goes wrong.</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>More fundamentally, this approach doesn't scale to the next order of magnitude. As datasets continue to grow (and they will), even these large-memory machines become insufficient. You haven't solved the underlying problem, just postponed confronting it. And interactive data exploration - the iterative process of trying ideas, examining results, and refining your approach - becomes cumbersome when every attempt requires submitting a batch job and waiting for results.</span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a><span class="fu">### Approach 4: Reformulate the Problem</span></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a>**Strategy:** Develop mathematically equivalent algorithms that avoid ever materializing the full matrix in memory.</span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a>This is actually a sophisticated approach used in many modern tools. For example, some GWAS software never loads the full genotype matrix, instead reading small portions from disk as needed and maintaining summary statistics in memory.</span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a>**The limitation:** This requires algorithm-specific implementations. Someone must write custom code for each statistical method, thinking carefully about how to decompose the computation. Most researchers aren't in a position to do this themselves - they depend on software developers providing these specialized implementations. This means you're limited to whatever methods someone has already implemented in this special way.</span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a>Moreover, these optimized tools often exist as standalone programs with their own file formats, their own configuration languages, and limited flexibility. Moving data between tools, combining methods in novel ways, or extending analyses beyond what the tool provides becomes difficult. Your computational constraints start dictating your scientific questions, rather than the other way around.</span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Root Cause: RAM as a Fixed Resource</span></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a>All these limitations stem from a single constraint: RAM is finite, and treating it as an unlimited resource forces us into uncomfortable compromises. Either we analyze less data, fragment our analyses, incur substantial costs, or limit ourselves to pre-packaged tools.</span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>The traditional computing model assumes data lives in memory during analysis. Functions expect to receive complete data objects as inputs. Algorithms assume they can access any element of a matrix at any time. This made perfect sense when datasets were smaller - why complicate your code by reading data from disk when it fits comfortably in RAM?</span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a>But this assumption is no longer tenable for many modern datasets. We need a different model, one where:</span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Data primarily lives on disk**, with only actively needed portions in RAM</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Algorithms work with blocks**, processing manageable chunks sequentially</span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**File format supports efficient partial access**, making disk-based computing practical</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Tools are flexible**, allowing complex multi-step analyses on out-of-memory data</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>This is precisely what BigDataStatMeth provides, building on the HDF5 file format and block-wise computational strategies. Rather than forcing you to choose between data subsampling, hardware requirements, analysis limitations, or implementation complexity, the package lets you work with large datasets using the RAM you have available, implementing the statistical methods you need, without requiring expertise in high-performance computing.</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a><span class="fu">## When Does the Problem Actually Occur?</span></span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>It's worth being specific about when you'll encounter these memory limitations. Not every genomics analysis requires special handling of large data. Understanding the thresholds helps you decide when to use specialized tools like BigDataStatMeth versus when simpler approaches suffice.</span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a><span class="fu">### Rule of Thumb: The 20% Rule</span></span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>A conservative guideline is that your data should use less than 20% of your available RAM to analyze comfortably. This leaves room for:</span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>R's internal copies during operations</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Intermediate results from computations</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The operating system and other programs</span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Some safety margin for operations that temporarily spike memory usage</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a>If you have 16 GB of RAM, this means staying under about 3 GB of data. For 32 GB of RAM, keep data under 6 GB. These thresholds might seem generous, but they prevent the frustrating experience of operations failing partway through or systems becoming unresponsive.</span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a><span class="fu">### Matrix Dimensions as Thresholds</span></span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>Here are concrete matrix sizes and their memory requirements:</span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Dimensions <span class="pp">|</span> Memory Required <span class="pp">|</span> Typical Use Case <span class="pp">|</span></span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a><span class="pp">|------------|----------------|------------------|</span></span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 1,000 × 10,000 <span class="pp">|</span> 80 MB <span class="pp">|</span> Small pilot study, processed data <span class="pp">|</span></span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 10,000 × 10,000 <span class="pp">|</span> 800 MB <span class="pp">|</span> Medium study, targeted feature set <span class="pp">|</span></span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 10,000 × 100,000 <span class="pp">|</span> 8 GB <span class="pp">|</span> Large study, single omic <span class="pp">|</span></span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 50,000 × 100,000 <span class="pp">|</span> 40 GB <span class="pp">|</span> Large cohort, genome-wide <span class="pp">|</span></span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 100,000 × 500,000 <span class="pp">|</span> 400 GB <span class="pp">|</span> Biobank-scale, comprehensive <span class="pp">|</span></span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 500,000 × 800,000 <span class="pp">|</span> 3.2 TB <span class="pp">|</span> Full UK Biobank-scale GWAS <span class="pp">|</span></span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a>The transition from "fits in memory" to "requires special handling" typically occurs around 10,000 × 100,000 for most researchers with standard workstations. Once you cross into the 40+ GB range, specialized approaches become not just helpful but necessary.</span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a><span class="fu">### Types of Analysis Matter</span></span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>The type of analysis also affects whether you hit memory limitations:</span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a>**Less demanding:** Simple operations that process the data in a streaming fashion (reading sequentially without needing everything at once) often work with larger datasets. Computing means, frequencies, or single-variant tests can handle data that wouldn't fit entirely in RAM.</span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a>**More demanding:** Operations that require the full data simultaneously hit memory limits sooner. Matrix factorizations (PCA, SVD), model fitting across many features simultaneously, cross-validation, and resampling methods all need to "see" large portions of data at once.</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a>**Most demanding:** Operations that create large intermediate results exhaust memory even faster. Computing all pairwise correlations creates a new matrix of size features × features, which for 100,000 features would require nearly 80 GB just for the output, regardless of the input size.</span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a><span class="fu">## Interactive Exercise {.exercise}</span></span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a><span class="fu">### Practice: Calculate Your Data's Memory Requirements</span></span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>The best way to internalize these concepts is to apply them to data you actually work with. This exercise helps you estimate whether your current or planned analyses will face memory constraints.</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to estimate memory requirements</span></span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>estimate_memory <span class="ot">&lt;-</span> <span class="cf">function</span>(n_samples, n_features, <span class="at">bytes_per_value =</span> <span class="dv">8</span>) {</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Basic storage</span></span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a>  basic_gb <span class="ot">&lt;-</span> (n_samples <span class="sc">*</span> n_features <span class="sc">*</span> bytes_per_value) <span class="sc">/</span> (<span class="dv">1024</span><span class="sc">^</span><span class="dv">3</span>)</span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Typical operations need 2-3x for intermediate results</span></span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a>  working_gb <span class="ot">&lt;-</span> basic_gb <span class="sc">*</span> <span class="fl">2.5</span></span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">"Memory Requirements:</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Data storage: %.2f GB</span><span class="sc">\n</span><span class="st">"</span>, basic_gb))</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Working memory: %.2f GB</span><span class="sc">\n</span><span class="st">"</span>, working_gb))</span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">"  Recommended RAM: %.2f GB</span><span class="sc">\n</span><span class="st">"</span>, working_gb <span class="sc">*</span> <span class="fl">1.5</span>))</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">invisible</span>(<span class="fu">list</span>(<span class="at">storage =</span> basic_gb, <span class="at">working =</span> working_gb)))</span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Your genomic study</span></span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a><span class="fu">estimate_memory</span>(<span class="at">n_samples =</span> <span class="dv">10000</span>, <span class="at">n_features =</span> <span class="dv">500000</span>)</span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a><span class="co"># Try with your own numbers</span></span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a><span class="fu">estimate_memory</span>(<span class="at">n_samples =</span> ???, <span class="at">n_features =</span> ???)</span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a><span class="fu">### Reflection Questions</span></span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a>Think about these questions - there are no universal answers, as they depend on your specific situation:</span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**At what sample size would your analysis exceed your available RAM?**</span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Consider both your current machine and any servers you have access to</span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Remember that R needs working memory beyond just data storage</span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Which operations in your workflow would become bottlenecks first?**</span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Creating correlation matrices? </span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Matrix factorizations (PCA, SVD)?</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Iterative model fitting?</span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Is your problem memory-limited or compute-limited?**</span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>If memory: disk-based computing helps</span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>If compute time: you might need more cores or GPU acceleration</span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Often it's both - understanding which dominates helps choose solutions</span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**For your research question, do you need the full matrix simultaneously?**</span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Some analyses can stream through data (single-variant tests)</span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Others need to "see" everything at once (PCA across all variants)</span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>This affects whether block-wise approaches will work well</span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a>Try different scenarios. What if your sample size doubles? What if you add more phenotypes or additional omic layers? When does your current computational setup become insufficient?</span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Path Forward</span></span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a>Understanding these limitations and when they occur helps you make informed decisions about your computational strategy. For small to medium datasets that fit comfortably in memory, traditional R approaches work wonderfully - they're simpler, more flexible, and well-supported by the vast R ecosystem.</span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a>For datasets that push or exceed memory limits, BigDataStatMeth provides a different approach: disk-based computing with HDF5 files and block-wise algorithms. This isn't necessarily "better" in absolute terms - it's an appropriate tool for a specific problem. When your data exceeds memory, you need different computational strategies, and that's what the package provides.</span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a>The remainder of this documentation shows you how to work effectively with large datasets using these strategies, covering both the conceptual understanding and practical implementation.</span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key Takeaways {.key-concept}</span></span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>Let's consolidate the essential concepts about why big data creates computational challenges and when you need specialized approaches.</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a><span class="fu">### Essential Concepts</span></span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a>**The memory wall is real and unavoidable.** When your data exceeds available RAM, traditional computing simply fails. This isn't a software problem that better code can solve - it's a fundamental hardware constraint. A 64 GB workstation cannot load a 200 GB matrix, period. Understanding when you'll hit this wall helps you plan computational strategies before you're stuck.</span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>**Memory requirements grow faster than you expect.** It's not just about storing the data - operations need working memory for intermediate results. Matrix operations typically require 2-3× the data size in RAM. This means a "40 GB dataset" actually needs 80-120 GB of available memory for computation. The gap between data size and memory requirements catches many researchers by surprise.</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>**The three computational constraints** - memory, compute time, and disk I/O - all matter, but they matter differently for different problems. A memory-constrained problem benefits from disk-based computing. A compute-constrained problem needs more cores or faster algorithms. An I/O-constrained problem needs better data organization or caching strategies. Identifying which constraint dominates your workflow determines which solutions will actually help.</span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a>**Dataset size is contextual, not absolute.** Whether a dataset is "big" depends on your available resources, not just the number of bytes. A 20 GB dataset is "small" on a 256 GB server but "impossible" on an 8 GB laptop. Similarly, whether disk-based computing helps depends on the ratio of data size to available RAM, not the absolute size. A 50 GB dataset might work fine in-memory if you have 128 GB RAM, but requires disk-based approaches with 32 GB RAM.</span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a>**Operation type determines feasibility.** Not all operations scale the same way. Element-wise operations (like adding a constant to every value) scale linearly and can stream through data. Operations requiring the full matrix simultaneously (like PCA) are harder to scale. Operations creating large outputs (like computing all pairwise correlations) can exhaust memory even when inputs fit. Understanding your analysis pipeline's operation types helps predict where you'll hit limitations.</span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a><span class="fu">### When to Use Disk-Based Computing</span></span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a>Making the right choice about computational strategy matters for both productivity and practicality. Here's guidance based on the challenges we've discussed:</span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a>✅ **Use disk-based computing when:**</span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data exceeds 20-30% of available RAM** - This threshold gives enough headroom for intermediate results and operating system needs. Below this, in-memory approaches work fine. Above this, you start risking memory exhaustion during computation.</span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Repeated partial access is your workflow** - If you frequently access different subsets of data (different chromosomes, different time windows, different sample cohorts), HDF5's partial I/O capabilities pay dividends. You read only what you need each time, keeping memory usage constant regardless of total data size.</span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multiple analysis types on same data** - When you'll run PCA, then regression, then association tests on the same dataset, keeping data in HDF5 format means you load it once and reuse it for all analyses. The upfront conversion cost amortizes across multiple uses.</span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sharing data across platforms** - If your workflow spans R, Python, and command-line tools, HDF5 provides a common format all can read efficiently. This beats converting between CSV, RData, and other formats for each tool.</span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a>❌ **Use traditional in-memory computing when:**</span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Data comfortably fits in less than 20% of RAM** - Traditional R approaches are simpler, more flexible, and better supported by the broader R ecosystem. Don't add complexity when it's not needed.</span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**One-off analysis with simple operations** - If you're doing a quick exploratory analysis you won't repeat, the overhead of converting to HDF5 outweighs the benefits. Load the data, compute what you need, save results, and you're done.</span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Ultra-fast I/O is critical** - Despite HDF5's optimizations, RAM is always faster than disk. If your analysis involves thousands of tiny operations with random access patterns, in-memory processing wins. Disk-based computing excels at large sequential reads, not scattered tiny reads.</span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**You need maximum flexibility** - R's in-memory data structures support arbitrary manipulations trivially. With disk-based data, some operations become awkward or inefficient. If your workflow involves many ad-hoc transformations and exploratory manipulations, staying in memory (if possible) maintains flexibility.</span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a>The decision isn't always clear-cut, and many workflows benefit from a hybrid approach: use disk-based storage and computation for large matrices, but load summarized results into memory for final processing and visualization. Understanding these trade-offs helps you design efficient computational strategies for your specific needs.</span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a><span class="fu">## Next Steps</span></span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a>Now that you understand why traditional approaches fail with large data, you're ready to learn about the solution:</span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">**Understanding HDF5 →**</span><span class="co">](understanding-hdf5.qmd)</span> Learn how HDF5 enables disk-based data access</span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">**Block-Wise Computing →**</span><span class="co">](blockwise-computing.qmd)</span> Understand how algorithms adapt to work with data blocks</span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">**Getting Started Tutorial →**</span><span class="co">](../tutorials/getting-started.qmd)</span> Start working with your own data</span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a><span class="fu">### Questions or Feedback?</span></span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a>If you have questions about whether BigDataStatMeth is appropriate for your data size, or want to discuss specific use cases, please <span class="co">[</span><span class="ot">open an issue</span><span class="co">](https://github.com/isglobal-brge/BigDataStatMeth/issues)</span> on GitHub.</span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2025 BigDataStatMeth - ISGlobal BRGE</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/isglobal-brge/BigDataStatMeth/edit/main/fundamentals/big-data-problem.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/isglobal-brge/BigDataStatMeth/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/isglobal-brge/BigDataStatMeth">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item">
 License: GPL-3
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>