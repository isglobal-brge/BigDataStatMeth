[
  {
    "objectID": "workflows/index.html",
    "href": "workflows/index.html",
    "title": "Practical Workflows",
    "section": "",
    "text": "NoteReal Research, Real Solutions\n\n\n\nPractical workflows demonstrate how BigDataStatMeth solves genuine computational challenges. These are complete analyses from start to finish that you can adapt to your own research."
  },
  {
    "objectID": "workflows/index.html#complete-examples",
    "href": "workflows/index.html#complete-examples",
    "title": "Practical Workflows",
    "section": "1 Complete Examples",
    "text": "1 Complete Examples\nUnlike tutorials that teach concepts in isolation, these workflows present end-to-end analyses. You’ll see how operations combine, how results are validated, and how to handle problems that arise in real research."
  },
  {
    "objectID": "workflows/index.html#available-workflows",
    "href": "workflows/index.html#available-workflows",
    "title": "Practical Workflows",
    "section": "2 Available Workflows",
    "text": "2 Available Workflows\n\n\n\n\n\n\n\n\nTipImplementing PCA\n\n\n\nDimensionality reduction\nPrincipal component analysis for large-scale genomic data. Learn to identify major axes of variation and reveal population structure.\nStart workflow →\n\n\n\n\n\n\n\n\n\n\nWarningImplementing CCA\n\n\n\nMulti-omics integration\nCanonical correlation analysis for integrating different data types. Identify relationships between gene expression and DNA methylation using block-wise algorithms.\nStart workflow →\n\n\n\n\n\n\n\n\n\n\nImportantCross-Platform Workflows\n\n\n\nInteroperability\nWork seamlessly across different systems and programming languages. Pass data between R, Python, and C++ using HDF5 as a common format.\nExplore integration →\n\n\n\n\n\n\n\n\n\n\n\nCautionAdapt These Patterns\n\n\n\nThese workflows are designed to be adapted to your research needs. The patterns you learn—how operations are sequenced, how results are validated, how errors are diagnosed—apply across many different analyses.\n\n\n\n\n\n\n\n\n\nTipContinue Learning\n\n\n\nReady to develop your own methods? See Developing Methods for detailed implementation examples in both R and C++, or consult the API Reference for function documentation."
  },
  {
    "objectID": "workflows/implementing-cca.html",
    "href": "workflows/implementing-cca.html",
    "title": "Implementing CCA",
    "section": "",
    "text": "Canonical Correlation Analysis (CCA) identifies coordinated relationships between two high-dimensional datasets by finding linear combinations that maximize correlation. In multi-omic studies, CCA reveals how different molecular layers interact—for instance, how gene expression patterns relate to DNA methylation profiles. This workflow demonstrates CCA on real TCGA cancer data, showing how methylation and expression coordinate across different cancer types.\nUnlike pairwise correlation analyses that examine individual feature relationships, CCA discovers the fundamental patterns linking entire datasets. Think of it as finding the “canonical” dimensions where methylation and expression vary together most strongly. This workflow uses the bdCCA_hdf5() function from BDStatMethExamples to perform block-wise CCA on data stored in HDF5 format, enabling analysis of datasets too large for memory.\n\n\nBy the end of this workflow, you will:\n\nImport multi-omic data directly from URLs into HDF5 format\nUse bdCCA_hdf5() from BDStatMethExamples for block-wise CCA\nUnderstand how block-wise processing enables large-scale CCA\nExtract canonical correlations, coefficients, and sample scores\nVisualize canonical variates colored by biological groups\nInterpret CCA results in the context of cancer biology\nIdentify which canonical components separate cancer types\nUse CCA results stored in HDF5 for downstream analyses",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "workflows/implementing-cca.html#overview",
    "href": "workflows/implementing-cca.html#overview",
    "title": "Implementing CCA",
    "section": "",
    "text": "Canonical Correlation Analysis (CCA) identifies coordinated relationships between two high-dimensional datasets by finding linear combinations that maximize correlation. In multi-omic studies, CCA reveals how different molecular layers interact—for instance, how gene expression patterns relate to DNA methylation profiles. This workflow demonstrates CCA on real TCGA cancer data, showing how methylation and expression coordinate across different cancer types.\nUnlike pairwise correlation analyses that examine individual feature relationships, CCA discovers the fundamental patterns linking entire datasets. Think of it as finding the “canonical” dimensions where methylation and expression vary together most strongly. This workflow uses the bdCCA_hdf5() function from BDStatMethExamples to perform block-wise CCA on data stored in HDF5 format, enabling analysis of datasets too large for memory.\n\n\nBy the end of this workflow, you will:\n\nImport multi-omic data directly from URLs into HDF5 format\nUse bdCCA_hdf5() from BDStatMethExamples for block-wise CCA\nUnderstand how block-wise processing enables large-scale CCA\nExtract canonical correlations, coefficients, and sample scores\nVisualize canonical variates colored by biological groups\nInterpret CCA results in the context of cancer biology\nIdentify which canonical components separate cancer types\nUse CCA results stored in HDF5 for downstream analyses",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "workflows/implementing-cca.html#prerequisites",
    "href": "workflows/implementing-cca.html#prerequisites",
    "title": "Implementing CCA",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\n\n2.1 Install and Load Packages\nCheck if BDStatMethExamples is already installed to avoid reinstalling:\n\n# Check and install BDStatMethExamples if needed\nif (!require(\"BDStatMethExamples\", quietly = TRUE)) {\n  if (!require(\"devtools\", quietly = TRUE)) {\n    install.packages(\"devtools\")\n  }\n  devtools::install_github(\"dpelegri/BDStatMethExamples\")\n}\n\n# Load required packages\nlibrary(BDStatMethExamples)\nlibrary(BigDataStatMeth)\nlibrary(rhdf5)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(gridExtra)",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "workflows/implementing-cca.html#the-dataset",
    "href": "workflows/implementing-cca.html#the-dataset",
    "title": "Implementing CCA",
    "section": "3 The Dataset",
    "text": "3 The Dataset\nWe’ll analyze matched methylation and RNA-seq data from TCGA (The Cancer Genome Atlas):\n\nDataset X (RNA-seq): 2,171 samples × 500 gene expression values\nDataset Y (Methylation): Same 2,171 samples × 339 CpG methylation sites\nMetadata: Cancer type labels for each sample\nSource: TCGA multi-cancer study\nGoal: Identify methylation-expression patterns that differ by cancer type\n\nThe data represents multiple cancer types (BRCA, LUAD, KIRC, COAD, PRAD), allowing us to see if canonical variates capture cancer-specific molecular signatures. Samples are pre-matched—row order is identical in both datasets, ensuring correct sample alignment for CCA.",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "workflows/implementing-cca.html#step-1-import-data-to-hdf5",
    "href": "workflows/implementing-cca.html#step-1-import-data-to-hdf5",
    "title": "Implementing CCA",
    "section": "4 Step 1: Import Data to HDF5",
    "text": "4 Step 1: Import Data to HDF5\n\n4.1 Download and Import RNA-seq Data\nThe bdImportData_hdf5() function downloads, extracts, and imports data directly from a URL:\n\n# Base URL for TCGA CCA example data\nfilecommon &lt;- paste0(\"https://raw.githubusercontent.com/isglobal-brge/\",\n                     \"Supplementary-Material/master/Pelegri-Siso_2025/\",\n                     \"application_examples/CCA/data/\")\n\n# RNA-seq expression data URL\nXfile &lt;- paste0(filecommon, \"RNA_data_small.zip\")\n\n# Import RNA data as X matrix in HDF5 file\nimpX &lt;- bdImportData_hdf5(\n  inFile = Xfile,\n  destFile = \"cca_tcga_small.hdf5\",\n  destGroup = \"data\",\n  destDataset = \"X\",\n  header = TRUE,\n  rownames = FALSE,\n  sep = \",\",\n  overwrite = TRUE,\n  overwriteFile = TRUE\n)\n\n\ncat(\"✓ RNA-seq data imported\\n\")\n\n✓ RNA-seq data imported\n\ncat(\"  Dimensions:\", impX$dims[1], \"samples ×\", impX$dims[2], \"genes\\n\")\n\n  Dimensions: samples × genes\n\n\nThe function automatically: 1. Downloads RNA_data_small.zip from GitHub 2. Extracts the CSV file 3. Imports into HDF5 at /data/X 4. Cleans up temporary files\n\n\n4.2 Import Methylation Data\nAdd methylation data to the same HDF5 file:\n\n# Methylation data URL\nYfile &lt;- paste0(filecommon, \"Methyl_data_small.zip\")\n\n# Import methylation as Y matrix (same file, different dataset)\nimpY &lt;- bdImportData_hdf5(\n  inFile = Yfile,\n  destFile = \"cca_tcga_small.hdf5\",\n  destGroup = \"data\",\n  destDataset = \"Y\",\n  header = TRUE,\n  rownames = FALSE,\n  sep = \",\",\n  overwrite = TRUE,\n  overwriteFile = FALSE  # Don't overwrite file, just add dataset\n)\n\n\ncat(\"✓ Methylation data imported\\n\")\n\n✓ Methylation data imported\n\n# Verify both datasets are in the file\nh5ls(\"cca_tcga_small.hdf5\")\n\n              group        name       otype   dclass        dim\n0                 /        data   H5I_GROUP                    \n1             /data .X_dimnames   H5I_GROUP                    \n2 /data/.X_dimnames           2 H5I_DATASET COMPOUND        500\n3             /data .Y_dimnames   H5I_GROUP                    \n4 /data/.Y_dimnames           2 H5I_DATASET COMPOUND        339\n5             /data           X H5I_DATASET    FLOAT 2171 x 500\n6             /data           Y H5I_DATASET    FLOAT 2171 x 339\n\n\n\n\n\n\n\n\nNoteSample Matching in Pre-Prepared Data\n\n\n\nThe TCGA datasets used here have been pre-processed to ensure sample alignment. Rows in the RNA-seq matrix correspond exactly to rows in the methylation matrix by position. Sample names are not stored in these files—alignment is by row order.\nIn your own analyses, always verify: - Same samples present in both datasets - Identical sample order - No missing or duplicated samples\nMisaligned samples produce mathematically valid but biologically meaningless CCA results.",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "workflows/implementing-cca.html#step-2-perform-cca",
    "href": "workflows/implementing-cca.html#step-2-perform-cca",
    "title": "Implementing CCA",
    "section": "5 Step 2: Perform CCA",
    "text": "5 Step 2: Perform CCA\n\n5.1 Understanding CCA Parameters\nThe bdCCA_hdf5() function performs CCA using block-wise QR decomposition:\n\nfilename: HDF5 file containing both X and Y datasets\nX, Y: Paths to datasets within the file (e.g., “data/X”, “data/Y”)\nm: Number of blocks for QR decomposition\nbcenter: Center data by column means (default TRUE)\nbscale: Scale by standard deviations (default FALSE)\noverwriteResults: Overwrite existing CCA results (TRUE for reanalysis)\nkeepInteResults: Keep intermediate QR steps for inspection\n\n\n\n5.2 Run CCA with Block-Wise Processing\n\n# Number of blocks for processing\nm &lt;- 4\n\n# Perform CCA\nbdCCA_hdf5(\n  filename = \"cca_tcga_small.hdf5\",\n  X = \"data/X\",\n  Y = \"data/Y\",\n  m = m,\n  bcenter = TRUE,\n  bscale = FALSE,\n  overwriteResults = TRUE,\n  keepInteResults = TRUE\n)\n\nDataset has been splitted, results can be found in Step1/Xrows/X\nXRt dataset has been recomposed from blocks\nDataset has been splitted, results can be found in Step4/splitted/XRt.Q\nXQ dataset has been recomposed from blocks\nDataset has been splitted, results can be found in Step1/Yrows/Y\nYRt dataset has been recomposed from blocks\nDataset has been splitted, results can be found in Step4/splitted/YRt.Q\nYQ dataset has been recomposed from blocks\n\n\n\ncat(\"✓ CCA complete\\n\")\n\n✓ CCA complete\n\ncat(\"Results stored in /Results/ group\\n\")\n\nResults stored in /Results/ group\n\n# Examine output structure\nh5ls(\"cca_tcga_small.hdf5\")\n\n                      group            name       otype   dclass        dim\n0                         /      NORMALIZED   H5I_GROUP                    \n1               /NORMALIZED            data   H5I_GROUP                    \n2          /NORMALIZED/data               X H5I_DATASET    FLOAT 2171 x 500\n3          /NORMALIZED/data               Y H5I_DATASET    FLOAT 2171 x 339\n4          /NORMALIZED/data          mean.X H5I_DATASET    FLOAT    500 x 1\n5          /NORMALIZED/data          mean.Y H5I_DATASET    FLOAT    339 x 1\n6          /NORMALIZED/data            sd.X H5I_DATASET    FLOAT    500 x 1\n7          /NORMALIZED/data            sd.Y H5I_DATASET    FLOAT    339 x 1\n8                         /         Results   H5I_GROUP                    \n9                  /Results .xcoef_dimnames   H5I_GROUP                    \n10 /Results/.xcoef_dimnames               1 H5I_DATASET COMPOUND        500\n11                 /Results .ycoef_dimnames   H5I_GROUP                    \n12 /Results/.ycoef_dimnames               1 H5I_DATASET COMPOUND        339\n13                 /Results             cor H5I_DATASET    FLOAT      1 x 1\n14                 /Results         xcenter H5I_DATASET    FLOAT    500 x 1\n15                 /Results           xcoef H5I_DATASET    FLOAT  500 x 339\n16                 /Results         xscores H5I_DATASET    FLOAT 2171 x 339\n17                 /Results         ycenter H5I_DATASET    FLOAT    339 x 1\n18                 /Results           ycoef H5I_DATASET    FLOAT  339 x 339\n19                 /Results         yscores H5I_DATASET    FLOAT 2171 x 339\n20                        /             SVD   H5I_GROUP                    \n21                     /SVD CrossProd_XQ_YQ   H5I_GROUP                    \n22     /SVD/CrossProd_XQ_YQ               d H5I_DATASET    FLOAT    1 x 339\n23     /SVD/CrossProd_XQ_YQ               u H5I_DATASET    FLOAT  500 x 339\n24     /SVD/CrossProd_XQ_YQ               v H5I_DATASET    FLOAT  339 x 339\n25                        /            data   H5I_GROUP                    \n26                    /data     .X_dimnames   H5I_GROUP                    \n27        /data/.X_dimnames               2 H5I_DATASET COMPOUND        500\n28                    /data     .Y_dimnames   H5I_GROUP                    \n29        /data/.Y_dimnames               2 H5I_DATASET COMPOUND        339\n30                    /data               X H5I_DATASET    FLOAT 2171 x 500\n31                    /data               Y H5I_DATASET    FLOAT 2171 x 339\n\n\nThe function internally:\n\nNormalizes X and Y (centers by column means)\nSplits each matrix into m blocks by rows\nComputes QR decomposition on each block\nMerges block QR results hierarchically\nComputes cross-product of Q matrices (Q_X^T Q_Y)\nPerforms SVD to extract canonical correlations\nSolves for canonical coefficients from QR factors\nComputes canonical variates (sample scores)\n\nAll intermediate steps (Step1-Step7) are preserved because keepInteResults = TRUE, allowing you to inspect the block-wise computation if needed.\n\n\n\n\n\n\nTipChoosing the Number of Blocks (m)\n\n\n\nFor this dataset (2,171 samples × 500 and 339 features): - m = 4: Good balance of memory and speed - Larger m: Reduces memory usage but increases computation time - Smaller m: Faster but requires more memory\nFor genome-wide data (50,000 samples × 500,000 features), increase to m = 8-16.",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "workflows/implementing-cca.html#step-3-load-metadata-and-results",
    "href": "workflows/implementing-cca.html#step-3-load-metadata-and-results",
    "title": "Implementing CCA",
    "section": "6 Step 3: Load Metadata and Results",
    "text": "6 Step 3: Load Metadata and Results\n\n6.1 Load Sample Metadata\nMetadata contains cancer type labels for visualizing results:\n\n# Load metadata with cancer type annotations\nurlfile &lt;- paste0(\"https://raw.githubusercontent.com/isglobal-brge/\",\n                  \"Supplementary-Material/master/Pelegri-Siso_2025/\",\n                  \"application_examples/CCA/data/metadata.csv\")\n\nmetadata &lt;- read.csv(urlfile)\n\n\ncat(\"✓ Metadata loaded\\n\")\n\n✓ Metadata loaded\n\ncat(\"  Samples:\", nrow(metadata), \"\\n\")\n\n  Samples: 2171 \n\ncat(\"  Cancer types:\\n\")\n\n  Cancer types:\n\nprint(table(metadata$cancer))\n\n\nBRCA COAD  LGG LUAD \n 868  296  530  477 \n\n\nThe metadata contains: - Sample identifiers - Cancer type labels (BRCA, LUAD, KIRC, COAD, PRAD) - Additional clinical variables\n\n\n6.2 Extract CCA Results\nLoad canonical correlations and sample scores from the HDF5 file:\n\n# Open HDF5 file\nh5f &lt;- H5Fopen(\"cca_tcga_small.hdf5\")\n\n# Extract canonical correlations matrix\ncor &lt;- as.data.frame(h5f$Results$cor)\n\n# Get dimension names for labeling\ndimnames_labels &lt;- (h5f$Results$.xcoef_dimnames$`1`)[1:dim(cor)[1], 1]\ncolnames(cor) &lt;- dimnames_labels\nrownames(cor) &lt;- dimnames_labels\n\n# Extract X canonical variates (RNA scores)\nxscores &lt;- as.data.frame(h5f$Results$xscores)\ncolnames(xscores) &lt;- sprintf(\"CCAX_%s\", seq(1:dim(xscores)[2]))\n\n# Extract Y canonical variates (methylation scores)\nyscores &lt;- as.data.frame(h5f$Results$yscores)\ncolnames(yscores) &lt;- sprintf(\"CCAY_%s\", seq(1:dim(yscores)[2]))\n\n# Close HDF5 file\nh5closeAll()\n\n\ncat(\"✓ CCA results extracted\\n\")\n\n✓ CCA results extracted\n\ncat(\"\\n  X scores dimensions:\", dim(xscores)[1], \"×\", dim(xscores)[2], \"\\n\")\n\n\n  X scores dimensions: 2171 × 339 \n\ncat(\"  Y scores dimensions:\", dim(yscores)[1], \"×\", dim(yscores)[2], \"\\n\")\n\n  Y scores dimensions: 2171 × 339 \n\n\n\n\n\n\n\n\nNoteUnderstanding Canonical Correlations\n\n\n\nThe diagonal of the cor matrix contains canonical correlations: - First value: Strongest correlation between X and Y patterns - Subsequent values: Progressively weaker correlations - Typically only first 3-5 components are interpretable\nFor this TCGA data, expect correlations around 0.6-0.8 for the first component, representing coordinated methylation-expression patterns across cancer types.",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "workflows/implementing-cca.html#step-4-visualize-canonical-variates",
    "href": "workflows/implementing-cca.html#step-4-visualize-canonical-variates",
    "title": "Implementing CCA",
    "section": "7 Step 4: Visualize Canonical Variates",
    "text": "7 Step 4: Visualize Canonical Variates\n\n7.1 Combine Data for Plotting\nMerge canonical variates with metadata to color points by cancer type:\n\n# Combine metadata with canonical scores\nfull_data &lt;- cbind(metadata, xscores, yscores)\n\n\ncat(\"✓ Data prepared for plotting\\n\")\n\n✓ Data prepared for plotting\n\ncat(\"  Combined dimensions:\", dim(full_data)[1], \"×\", dim(full_data)[2], \"\\n\")\n\n  Combined dimensions: 2171 × 682 \n\n\n\n\n7.2 Define Plot Theme for Screen Display\nCreate theme optimized for on-screen viewing (smaller text than PNG export):\n\nscreen_theme &lt;- theme(\n  axis.text = element_text(size = 10, face = \"bold\"),\n  axis.title = element_text(size = 12, face = \"bold\"),\n  legend.background = element_blank(),\n  legend.key = element_blank(),\n  legend.position = \"bottom\",\n  legend.text = element_text(size = 9),\n  panel.background = element_rect(fill = \"white\", colour = \"black\"),\n  strip.background = element_blank(),\n  plot.background = element_blank(),\n  panel.grid = element_blank(),\n  legend.title = element_blank()\n)\n\n\n\n7.3 Create Canonical Variate Scatter Plots\nGenerate three plots showing different canonical component pairs:\n\n# Plot 1: CCAX_1 vs CCAY_2\np1 &lt;- full_data %&gt;%\n  ggplot(aes(x = CCAX_1, y = CCAY_2, color = as.factor(cancer))) +\n  geom_point(size = 1.8) +\n  stat_ellipse(linewidth = 1) +\n  screen_theme +\n  guides(colour = guide_legend(override.aes = list(size = 4))) +\n  labs(x = \"RNA CC1\", y = \"Methylation CC2\")\n\n# Plot 2: CCAX_2 vs CCAY_1\np2 &lt;- full_data %&gt;%\n  ggplot(aes(x = CCAX_2, y = CCAY_1, color = as.factor(cancer))) +\n  geom_point(size = 1.8) +\n  stat_ellipse(linewidth = 1) +\n  screen_theme +\n  guides(colour = guide_legend(override.aes = list(size = 4))) +\n  labs(x = \"RNA CC2\", y = \"Methylation CC1\")\n\n# Plot 3: CCAX_3 vs CCAY_4\np3 &lt;- full_data %&gt;%\n  ggplot(aes(x = CCAX_3, y = CCAY_4, color = as.factor(cancer))) +\n  geom_point(size = 1.8) +\n  stat_ellipse(linewidth = 1) +\n  screen_theme +\n  guides(colour = guide_legend(override.aes = list(size = 4))) +\n  labs(x = \"RNA CC3\", y = \"Methylation CC4\")\n\n\ncat(\"✓ Plots created\\n\")\n\n✓ Plots created\n\n\n\n\n7.4 Display Combined Figure\nShow the three plots side-by-side on screen:\n\n# Display on screen\ngrid.arrange(p1, p2, p3, nrow = 1)\n\n\n\n\n\n\n\n\nThe resulting figure shows: - Left panel (CCAX_1 vs CCAY_2): Primary RNA expression pattern vs. secondary methylation pattern - Middle panel (CCAX_2 vs CCAY_1): Secondary RNA pattern vs. primary methylation pattern\n- Right panel (CCAX_3 vs CCAY_4): Tertiary patterns in both omics\nEllipses around each cancer type reveal: - Which cancer types cluster together (similar molecular profiles) - Which canonical components best separate cancer types - Whether methylation or expression drives cancer-specific patterns\n\n\n\n\n\n\nImportantInterpreting the Canonical Variate Plots\n\n\n\nEach point represents one sample, colored by cancer type. Ellipses show 95% confidence regions for each cancer.\nWhat the axes mean: - CCAX_n: RNA expression canonical variate n (linear combination of genes) - CCAY_n: Methylation canonical variate n (linear combination of CpG sites)\nWhat to look for: - Separated ellipses: Cancer types with distinct molecular profiles - Overlapping ellipses: Cancer types with similar profiles - Component pairs: Different components capture different biological patterns\nIf BRCA separates clearly in panel 1 but not panel 2, the first canonical components capture BRCA-specific methylation-expression coordination.",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "workflows/implementing-cca.html#step-5-extract-feature-contributions",
    "href": "workflows/implementing-cca.html#step-5-extract-feature-contributions",
    "title": "Implementing CCA",
    "section": "8 Step 5: Extract Feature Contributions",
    "text": "8 Step 5: Extract Feature Contributions\n\n8.1 Identify Top Contributing Features\nTo understand which genes and CpG sites drive the canonical patterns, examine the canonical coefficients:\n\n# Open file to get coefficients\nh5f &lt;- H5Fopen(\"cca_tcga_small.hdf5\")\n\n# X coefficients (gene contributions)\nxcoef &lt;- as.data.frame(h5f$Results$xcoef)\ncolnames(xcoef) &lt;- sprintf(\"CC%s\", 1:ncol(xcoef))\n\n# Get gene names from dimnames\ngene_names &lt;- h5f$data$.X_dimnames$`2`[, 1]\nrownames(xcoef) &lt;- gene_names\n\n# Y coefficients (CpG contributions)\nycoef &lt;- as.data.frame(h5f$Results$ycoef)\ncolnames(ycoef) &lt;- sprintf(\"CC%s\", 1:ncol(ycoef))\n\n# Get CpG names from dimnames\ncpg_names &lt;- h5f$data$.Y_dimnames$`2`[, 1]\nrownames(ycoef) &lt;- cpg_names\n\nH5Fclose(h5f)\n\n\n# Find top contributors to first canonical component\ncat(\"\\nTop 10 genes for CC1:\\n\")\n\n\nTop 10 genes for CC1:\n\nxcoef$feature &lt;- rownames(xcoef)\nx_top &lt;- xcoef[order(abs(xcoef$CC1), decreasing = TRUE), ]\nprint(head(x_top[, c(\"feature\", \"CC1\")], 10))\n\n        feature           CC1\nACTRT2   ACTRT2  5.278068e-04\nACTL7A   ACTL7A  4.320488e-04\nACTRT1   ACTRT1 -2.620451e-04\nAADACL4 AADACL4 -2.498018e-04\nADAD1     ADAD1 -2.382528e-04\nACSM4     ACSM4  1.946739e-04\nACTL9     ACTL9  1.878009e-04\nADAM3A   ADAM3A -1.686120e-04\nACCSL     ACCSL -1.316626e-04\nAFM         AFM -8.923667e-05\n\ncat(\"\\nTop 10 CpG sites for CC1:\\n\")\n\n\nTop 10 CpG sites for CC1:\n\nycoef$feature &lt;- rownames(ycoef)\ny_top &lt;- ycoef[order(abs(ycoef$CC1), decreasing = TRUE), ]\nprint(head(y_top[, c(\"feature\", \"CC1\")], 10))\n\n              feature         CC1\ncg00009970 cg00009970 -0.02123091\ncg00010659 cg00010659 -0.02119275\ncg00020297 cg00020297 -0.02038494\ncg00011247 cg00011247  0.02036400\ncg00019809 cg00019809  0.02032894\ncg00004072 cg00004072 -0.01805912\ncg00003784 cg00003784  0.01743918\ncg00013410 cg00013410  0.01644359\ncg00009412 cg00009412  0.01501612\ncg00016156 cg00016156 -0.01408071\n\n\nHigh absolute coefficients indicate features that contribute strongly to the canonical variate. These features are candidates for: - Pathway enrichment analysis - Genomic region analysis (for CpG sites) - Mechanistic follow-up studies",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "workflows/implementing-cca.html#interactive-exercise",
    "href": "workflows/implementing-cca.html#interactive-exercise",
    "title": "Implementing CCA",
    "section": "9 Interactive Exercise",
    "text": "9 Interactive Exercise\n\n9.1 Practice: Exploring CCA Results\nUnderstanding CCA output helps you apply it effectively to your own multi-omic data.\n\n# Exercise 1: Try different numbers of blocks\n# Re-run CCA with m = 2, m = 8\n# How does computation time change?\n# Do canonical correlations differ?\n\nbdCCA_hdf5(\"cca_tcga_small.hdf5\", \"data/X\", \"data/Y\", \n           m = 2, overwriteResults = TRUE)\nbdCCA_hdf5(\"cca_tcga_small.hdf5\", \"data/X\", \"data/Y\", \n           m = 8, overwriteResults = TRUE)\n\n# Exercise 2: Explore intermediate results\n# keepInteResults = TRUE saved Step1-Step7\n# What's in each step?\n\nh5ls(\"cca_tcga_small.hdf5\")\n# Browse Step1 through Step7 to see block-wise computation\n\n# Exercise 3: Create additional plots\n# Try other canonical component pairs\n# Which components best separate specific cancer types?\n\np4 &lt;- full_data %&gt;%\n  ggplot(aes(x = CCAX_1, y = CCAY_1, color = as.factor(cancer))) +\n  geom_point(size = 1.8) +\n  stat_ellipse(linewidth = 1) +\n  screen_theme\n\ngrid.arrange(p4)\n\n# Exercise 4: Examine more features\n# Look at top 50 genes and CpGs\n# Do biological pathways emerge?\n\n\n\n\n\n\n\nTipReflection Questions\n\n\n\n1. Block-Wise Processing: - Why does m = 4 work well for this dataset? - What would happen if m = 1 (no blocking)? - When would you need m = 16?\n2. Canonical Components: - Why plot CCAX_1 vs CCAY_2 instead of CCAX_1 vs CCAY_1? - What does it mean if cancer types separate in CC1 but not CC2? - How many components should you examine?\n3. Biological Interpretation: - Which cancer types cluster together? Why might that be? - Do RNA patterns or methylation patterns better separate cancers? - How would you validate top contributing genes/CpGs?\n4. Method Limitations: - CCA finds correlations—does this mean causation? - What if samples weren’t pre-matched? - Could batch effects produce apparent CCA patterns?\n5. Next Steps: - How would you use canonical scores for classification? - What other multi-omic combinations could you analyze? - When would you need regularized CCA instead?\nThese questions guide thoughtful application of CCA beyond following the workflow mechanically.",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "workflows/implementing-cca.html#key-takeaways",
    "href": "workflows/implementing-cca.html#key-takeaways",
    "title": "Implementing CCA",
    "section": "10 Key Takeaways",
    "text": "10 Key Takeaways\nLet’s consolidate what you’ve learned about applying CCA to multi-omic cancer data.\n\n10.1 Essential Concepts\nBlock-wise processing makes large-scale CCA tractable. The TCGA dataset used here (2,171 samples × 500 and 339 features) could fit in memory, but the same bdCCA_hdf5() function scales to genome-wide data (50,000 samples × 500,000 features) that would overwhelm traditional CCA implementations. By splitting matrices into blocks, computing QR decomposition on each block, and merging results hierarchically, block-wise CCA achieves identical mathematical results to full-matrix CCA but with dramatically reduced memory requirements. The m parameter controls this trade-off—larger m means smaller memory footprint but longer computation time.\nSample alignment is critical but often pre-handled. CCA correlates row i of X with row i of Y—if samples are mis-ordered, you’re correlating patient A’s expression with patient B’s methylation. The TCGA datasets used here have been pre-processed to ensure alignment by row position. In your own analyses, verify sample matching before running CCA. Many published datasets provide pre-aligned matrices; if not, explicitly match samples by identifier and reorder matrices accordingly. One misalignment invalidates the entire analysis, and CCA won’t warn you—it will happily find “patterns” in the noise.\nCanonical components capture different biological patterns. The first canonical component (CC1) represents the strongest coordinated methylation-expression pattern across all samples. Subsequent components capture progressively weaker, orthogonal patterns. Plotting different component pairs (CCAX_1 vs CCAY_2, CCAX_2 vs CCAY_1, etc.) reveals that different biological signatures emerge in different components. In the TCGA cancer data, specific cancer types may separate clearly in some component pairs but not others, suggesting those components capture cancer-type-specific molecular coordination. Not all components are biologically meaningful—typically only the first 3-5 warrant interpretation.\nEllipse separation indicates molecular heterogeneity. In the canonical variate plots, well-separated ellipses for different cancer types indicate those cancers have distinct methylation-expression coordination patterns. Overlapping ellipses suggest similar molecular profiles. This visualization is powerful for exploratory analysis—it reveals which cancer types share common regulatory mechanisms (overlapping) versus having unique signatures (separated). If BRCA and LUAD separate in the first component pair but KIRC and COAD overlap, this suggests BRCA and LUAD have distinctive methylation-expression relationships while KIRC and COAD share common patterns.\nCanonical coefficients identify key molecular features. High absolute coefficients indicate features (genes or CpG sites) that contribute strongly to the canonical variate. These aren’t necessarily the features with highest individual variance or correlation—CCA identifies features whose coordinated patterns across datasets drive the canonical relationship. For the top-loading genes, pathway enrichment analysis reveals which biological processes are coordinated with methylation. For top CpG sites, genomic region analysis shows which regulatory elements are involved. These features are mechanistic starting points for understanding the biology underlying the CCA patterns.\nHDF5 storage preserves all results for downstream use. Unlike traditional CCA that returns objects in R’s workspace, bdCCA_hdf5() stores everything in the HDF5 file—canonical correlations, coefficients, sample scores, and intermediate steps. This enables reproducible analyses where you can return to the file months later and extract exactly what you need without re-running CCA. Other tools can read HDF5 files (Python, MATLAB, command-line tools), making results accessible across computational environments. The structured HDF5 format also documents the analysis provenance through its hierarchical organization.\n\n\n10.2 When to Use CCA\nUnderstanding when block-wise HDF5 CCA provides value versus when simpler approaches suffice guides effective multi-omic analysis.\n✅ Use bdCCA_hdf5() when:\n\nData too large for memory - When loading both X and Y matrices simultaneously causes memory errors, block-wise processing becomes essential. For TCGA-scale analyses (thousands of samples, hundreds of thousands of features), bdCCA_hdf5() enables CCA that would otherwise be impossible.\nMulti-cancer or multi-condition studies - When exploring how molecular coordination varies across biological groups (cancer types, developmental stages, treatment responses), CCA can reveal group-specific patterns that univariate analyses miss. The canonical variate plots with group coloring immediately show which conditions have distinct versus shared molecular signatures.\nIntegrating matched multi-omic datasets - When you have methylation + expression, CNV + expression, proteomics + transcriptomics, or other paired omics from the same samples, CCA identifies the coordinated dimensions of variation. This is CCA’s primary strength—finding relationships between high-dimensional paired datasets.\nExploratory multi-omic analysis - When you don’t have a specific hypothesis about which features relate but want to discover the main patterns of coordination, CCA provides unsupervised dimensionality reduction that respects the relationship between datasets. It’s often a first step before more targeted analyses.\n\n❌ Simpler approaches work better when:\n\nData fits in memory comfortably - If R’s cancor() runs without issues, use it. Base R CCA is simpler, better documented, and integrates with more packages. Only add HDF5 complexity when memory constraints force it.\nYou want specific feature-feature relationships - If the question is “which CpGs correlate with which genes,” compute pairwise correlations with FDR correction. CCA answers “how do CpG profiles relate to gene profiles as coordinated patterns,” which is different. For feature-level relationships, pairwise approaches are more interpretable.\nSample size is small - With 50 samples and 10,000 features per dataset, CCA will overfit dramatically. You need many more samples than features in either dataset for stable CCA. With small samples, reduce features first (through filtering, pathway aggregation, or supervised selection) or use regularized CCA methods.\nDatasets aren’t biologically linked - If analyzing brain methylation from one cohort and liver expression from a different cohort, CCA will find correlations (it always does), but they won’t reflect biology. CCA requires matched samples measuring different aspects of the same biological system.\nYou need causal inference - CCA identifies correlation, not causation. If your scientific question requires understanding whether methylation regulates expression or vice versa, you need causal inference methods (Mendelian randomization, perturbation studies, longitudinal data) not CCA.\n\nThe key principle: use block-wise HDF5 CCA when you have large, matched multi-omic datasets and want to discover coordinated patterns across conditions. For smaller data, specific relationships, or causal questions, other approaches are more appropriate.",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "workflows/implementing-cca.html#next-steps",
    "href": "workflows/implementing-cca.html#next-steps",
    "title": "Implementing CCA",
    "section": "11 Next Steps",
    "text": "11 Next Steps\nUnderstand the implementation:\n\nCCA Implementation in R - How bdCCA_hdf5() works internally\nCCA Implementation in C++ - Performance optimization\n\nExtend this analysis:\n\nPerform pathway enrichment on top-loading genes\nMap top CpG sites to genomic regions and regulatory elements\nUse canonical scores as features for cancer type classification\nTest which canonical components predict survival outcomes\n\nAdvanced CCA topics:\n\nSparse CCA for automatic feature selection\nRegularized CCA for high-dimensional settings (p &gt;&gt; n)\nKernel CCA for capturing nonlinear relationships\nMulti-block CCA for integrating &gt;2 omics types",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "workflows/implementing-cca.html#cleanup",
    "href": "workflows/implementing-cca.html#cleanup",
    "title": "Implementing CCA",
    "section": "12 Cleanup",
    "text": "12 Cleanup\n\n# Close any open HDF5 connections\nh5closeAll()\n\n# Note: All CCA results remain in cca_tcga_small.hdf5\n# Accessible for downstream analyses without re-running CCA",
    "crumbs": [
      "Complete Examples",
      "Implementing CCA"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html",
    "href": "tutorials/working-hdf5-matrices.html",
    "title": "Working with HDF5 Matrices",
    "section": "",
    "text": "This tutorial teaches you the practical skills for working efficiently with HDF5 files: converting data from common formats, organizing complex projects with multiple datasets, and performing routine file operations. Think of this as learning the file system before learning programming - you need to understand how to organize and access your data before diving into statistical analyses.\n\n\nBy the end of this tutorial, you will:\n\nConvert text files (CSV, TSV) to HDF5 format efficiently\nOrganize multiple related datasets in one HDF5 file\nUse hierarchical groups to structure complex projects\nAdd, inspect, and remove datasets safely\nRead data efficiently (whole datasets vs. subsets)\nApply best practices for HDF5 file organization\nUnderstand when to use rhdf5 vs. BigDataStatMeth functions",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html#overview",
    "href": "tutorials/working-hdf5-matrices.html#overview",
    "title": "Working with HDF5 Matrices",
    "section": "",
    "text": "This tutorial teaches you the practical skills for working efficiently with HDF5 files: converting data from common formats, organizing complex projects with multiple datasets, and performing routine file operations. Think of this as learning the file system before learning programming - you need to understand how to organize and access your data before diving into statistical analyses.\n\n\nBy the end of this tutorial, you will:\n\nConvert text files (CSV, TSV) to HDF5 format efficiently\nOrganize multiple related datasets in one HDF5 file\nUse hierarchical groups to structure complex projects\nAdd, inspect, and remove datasets safely\nRead data efficiently (whole datasets vs. subsets)\nApply best practices for HDF5 file organization\nUnderstand when to use rhdf5 vs. BigDataStatMeth functions",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html#prerequisites",
    "href": "tutorials/working-hdf5-matrices.html#prerequisites",
    "title": "Working with HDF5 Matrices",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nComplete the Getting Started tutorial first. You should have:\n\nBigDataStatMeth installed and working\nBasic understanding of HDF5 structure (groups and datasets)\nR and rhdf5 loaded\n\n\nlibrary(BigDataStatMeth)\nlibrary(rhdf5)",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html#converting-text-files-to-hdf5",
    "href": "tutorials/working-hdf5-matrices.html#converting-text-files-to-hdf5",
    "title": "Working with HDF5 Matrices",
    "section": "3 Converting Text Files to HDF5",
    "text": "3 Converting Text Files to HDF5\n\n3.1 The Problem\nYou have large text files (CSV, TSV, or custom delimiters) that won’t fit comfortably in RAM. Loading them with read.csv() or read.table() is slow or impossible.\n\n\n3.2 The Solution\nUse bdImportTextFile_hdf5() to convert directly to HDF5 format, processing the file in chunks.\n\n\n3.3 Example: Converting a CSV File\nWe’ll use a real clinical dataset (colesterol.csv) with cholesterol and metabolic measurements:\n\n# Check if file exists (should be in same directory as tutorial)\nif (!file.exists(\"colesterol.csv\")) {\n  stop(\"colesterol.csv not found. Please ensure it's in the working directory.\")\n}\n\n# Check file size\nfile_size_kb &lt;- file.info(\"colesterol.csv\")$size / 1024\ncat(\"File size:\", round(file_size_kb, 1), \"KB\\n\")\n\nFile size: 152.1 KB\n\n# Preview first few lines\ncat(\"\\nFirst few lines of the CSV:\\n\")\n\n\nFirst few lines of the CSV:\n\nhead(read.csv(\"colesterol.csv\", nrows = 3))\n\n  TCholesterol      Age  Insulin Creatinine      BUN     LLDR Triglycerides\n1     223.7348 55.25039 14.90246  0.9026095 10.22067 1.450970      199.0737\n2     248.1820 53.47404 25.12592  0.8710345 17.10225 1.002655      169.7877\n3     180.2071 58.54268 12.95197  0.8882435 11.21530 1.073924      150.2938\n     HDL_C     LDL_C Sex\n1 38.37616 107.09286   0\n2 36.35374  83.66443   1\n3 55.91920 108.02967   1\n\n\nNow convert to HDF5:\n\n# Convert CSV to HDF5\nresult &lt;- bdImportTextFile_hdf5(\n  filename = \"colesterol.csv\",\n  sep = \",\",                        # CSV uses commas\n  outputfile = \"clinical_data.hdf5\",\n  outGroup = \"clinical\",\n  outDataset = \"measurements\",\n  header = TRUE,                    # First row is column names\n  overwrite = TRUE\n)\n\ncat(\"✓ File converted to HDF5\\n\")\n\n✓ File converted to HDF5\n\ncat(\"Output file:\", result$fn, \"\\n\")\n\nOutput file: clinical_data.hdf5 \n\ncat(\"Output dataset:\", result$ds, \"\\n\")\n\nOutput dataset: clinical/measurements \n\n\n\n\n\n\n\n\nNoteImportant: Numeric Data Only\n\n\n\nbdImportTextFile_hdf5() handles numeric data. The first column (if character-based IDs) is stored separately as row names. Column names are stored in a separate dataset.\nThe numeric matrix contains only the numerical measurements (Age, Cholesterol, Insulin, etc.).\n\n\n\n\n3.4 Verify the Conversion\n\n# Inspect structure\nh5ls(result$fn)\n\n                             group                   name       otype   dclass\n0                                /               clinical   H5I_GROUP         \n1                        /clinical .measurements_dimnames   H5I_GROUP         \n2 /clinical/.measurements_dimnames                      2 H5I_DATASET COMPOUND\n3                        /clinical           measurements H5I_DATASET    FLOAT\n        dim\n0          \n1          \n2        10\n3 1000 x 10\n\n# Read back a portion\nconverted_data &lt;- h5read(result$fn, result$ds, \n                         index = list(1:5, NULL))\nconverted_data\n\n         [,1]     [,2]     [,3]      [,4]     [,5]     [,6]     [,7]     [,8]\n[1,] 223.7348 55.25039 14.90246 0.9026095 10.22067 1.450970 199.0737 38.37616\n[2,] 248.1820 53.47404 25.12592 0.8710345 17.10225 1.002655 169.7877 36.35374\n[3,] 180.2071 58.54268 12.95197 0.8882435 11.21530 1.073924 150.2938 55.91920\n[4,] 200.1522 62.58284 28.27819 0.9361357 12.79399 1.129373 150.0377 51.89241\n[5,] 234.3281 68.70254 13.21404 0.7775238 11.39689 1.235655 161.8820 45.25873\n          [,9] [,10]\n[1,] 107.09286     0\n[2,]  83.66443     1\n[3,] 108.02967     1\n[4,] 108.83731     1\n[5,]  94.98369     1\n\n# Read column names\ncolnames_data &lt;- h5read(result$fn, result$ds_cols)[,1]\ncolnames_data\n\n [1] \"TCholesterol\"  \"Age\"           \"Insulin\"       \"Creatinine\"   \n [5] \"BUN\"           \"LLDR\"          \"Triglycerides\" \"HDL_C\"        \n [9] \"LDL_C\"         \"Sex\"          \n\n\n\n\n3.5 Working with Different Delimiters\n\n# Tab-separated file\nbdImportTextFile_hdf5(\n  filename = \"data.tsv\",\n  sep = \"\\t\",\n  outputfile = \"data.hdf5\",\n  outGroup = \"imported\",\n  outDataset = \"data\"\n)\n\n# Custom delimiter (e.g., semicolon)\nbdImportTextFile_hdf5(\n  filename = \"data.txt\",\n  sep = \";\",\n  outputfile = \"data.hdf5\",\n  outGroup = \"imported\",\n  outDataset = \"data\"\n)",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html#managing-multiple-datasets",
    "href": "tutorials/working-hdf5-matrices.html#managing-multiple-datasets",
    "title": "Working with HDF5 Matrices",
    "section": "4 Managing Multiple Datasets",
    "text": "4 Managing Multiple Datasets\nOne HDF5 file can store multiple datasets, organized in groups. This is ideal for related analyses.\n\n4.1 Creating a Multi-Dataset File\n\n# Create three related datasets\nset.seed(100)\ngenotype &lt;- matrix(sample(0:2, 1000*500, replace = TRUE), 1000, 500)\nphenotype &lt;- matrix(rnorm(1000*10), 1000, 10)\ncovariates &lt;- matrix(rnorm(1000*5), 1000, 5)\n\n# Store in same file, different groups\nproject_file &lt;- \"multi_dataset_project.hdf5\"\n\nbdCreate_hdf5_matrix(\n  filename = project_file,\n  object = genotype,\n  group = \"genetics\",\n  dataset = \"snps\",\n  overwriteFile = TRUE\n)\n\n$fn\n[1] \"multi_dataset_project.hdf5\"\n\n$ds\n[1] \"genetics/snps\"\n\nbdCreate_hdf5_matrix(\n  filename = project_file,\n  object = phenotype,\n  group = \"phenotypes\",\n  dataset = \"traits\"\n)\n\n$fn\n[1] \"multi_dataset_project.hdf5\"\n\n$ds\n[1] \"phenotypes/traits\"\n\nbdCreate_hdf5_matrix(\n  filename = project_file,\n  object = covariates,\n  group = \"phenotypes\",\n  dataset = \"covariates\"\n)\n\n$fn\n[1] \"multi_dataset_project.hdf5\"\n\n$ds\n[1] \"phenotypes/covariates\"\n\ncat(\"✓ Multi-dataset file created\\n\")\n\n✓ Multi-dataset file created\n\n\n\n\n4.2 Inspecting File Contents\n\n# List all contents\nh5ls(project_file)\n\n        group       name       otype  dclass        dim\n0           /   genetics   H5I_GROUP                   \n1   /genetics       snps H5I_DATASET INTEGER 1000 x 500\n2           / phenotypes   H5I_GROUP                   \n3 /phenotypes covariates H5I_DATASET   FLOAT   1000 x 5\n4 /phenotypes     traits H5I_DATASET   FLOAT  1000 x 10\n\n\n\n\n\n\n\n\nTipOrganizing Your Data\n\n\n\nGood organization strategies:\nBy data type: - /genetics/ - SNP matrices, genomic data - /phenotypes/ - Clinical measurements, traits - /results/ - Analysis outputs (PCA, regression)\nBy processing stage: - /raw/ - Original imported data - /qc/ - Quality-controlled data - /normalized/ - Normalized, ready for analysis\nBy analysis: - /pca_analysis/ - PCA components, loadings - /gwas_results/ - Association results",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html#adding-and-removing-datasets",
    "href": "tutorials/working-hdf5-matrices.html#adding-and-removing-datasets",
    "title": "Working with HDF5 Matrices",
    "section": "5 Adding and Removing Datasets",
    "text": "5 Adding and Removing Datasets\n\n5.1 Adding New Datasets\n\n# Add another dataset to existing file\nquality_scores &lt;- matrix(runif(1000*500), 1000, 500)\n\nbdCreate_hdf5_matrix(\n  filename = project_file,\n  object = quality_scores,\n  group = \"quality_control\",\n  dataset = \"scores\",\n  overwriteFile = FALSE  # Don't overwrite the file\n)\n\n$fn\n[1] \"multi_dataset_project.hdf5\"\n\n$ds\n[1] \"quality_control/scores\"\n\n# Verify it was added\nh5ls(project_file)\n\n             group            name       otype  dclass        dim\n0                /        genetics   H5I_GROUP                   \n1        /genetics            snps H5I_DATASET INTEGER 1000 x 500\n2                /      phenotypes   H5I_GROUP                   \n3      /phenotypes      covariates H5I_DATASET   FLOAT   1000 x 5\n4      /phenotypes          traits H5I_DATASET   FLOAT  1000 x 10\n5                / quality_control   H5I_GROUP                   \n6 /quality_control          scores H5I_DATASET   FLOAT 1000 x 500\n\n\n\n\n5.2 Removing Datasets\nTo remove datasets, use rhdf5 functions:\n\n# Remove a specific dataset\nh5delete(project_file, \"/quality_control/scores\")\n\n# Remove entire group\nh5delete(project_file, \"/quality_control\")\n\n\n\n\n\n\n\nWarningCareful with Deletion\n\n\n\nHDF5 doesn’t reclaim disk space immediately after deletion. The space is marked as free but the file size doesn’t shrink. To reclaim space, you need to repack the file using HDF5 command-line tools (h5repack).",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html#efficient-data-access",
    "href": "tutorials/working-hdf5-matrices.html#efficient-data-access",
    "title": "Working with HDF5 Matrices",
    "section": "6 Efficient Data Access",
    "text": "6 Efficient Data Access\nOne of the most powerful aspects of HDF5 is partial reading - you can access just the data you need without loading gigabytes into memory. This is the secret to working with datasets larger than your RAM.\n\n\n\n\n\n\nNoteWhy We Use rhdf5 for Data Access\n\n\n\nYou’ll notice we use rhdf5’s h5read() function for reading data. This is intentional! BigDataStatMeth focuses on computation (matrix operations, statistical methods, block-wise algorithms), while rhdf5 excels at file I/O (reading, writing, inspecting).\nDivision of labor: - rhdf5: File operations, data access, inspection - BigDataStatMeth: Matrix algebra, statistics, block-wise processing\nThere’s no need to reimplement what rhdf5 already does perfectly.\n\n\n\n6.1 Reading Subsets\nInstead of loading entire matrices (which might be 100 GB), read just what you need:\n\n# Read first 100 samples and first 50 SNPs\nsubset_data &lt;- h5read(\n  project_file, \n  \"/genetics/snps\",\n  index = list(1:100, 1:50)\n)\n\ncat(\"Subset dimensions:\", dim(subset_data), \"\\n\")\n\nSubset dimensions: 100 50 \n\ncat(\"Preview of data (first 5×5):\\n\")\n\nPreview of data (first 5×5):\n\nprint(subset_data[1:5, 1:5])\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    1    0    0    2\n[2,]    2    2    1    1    1\n[3,]    1    1    0    1    2\n[4,]    2    2    0    2    2\n[5,]    0    2    0    2    0\n\ncat(\"\\n✓ Loaded only\", prod(dim(subset_data)), \"values\\n\")\n\n\n✓ Loaded only 5000 values\n\ncat(\"  Full dataset would have been:\", 1000 * 2000, \"values\\n\")\n\n  Full dataset would have been: 2e+06 values\n\n\n\n\n6.2 Flexible Indexing\nYou can read specific columns, skip rows, or sample randomly:\n\n# Read specific columns only (useful for specific variables)\ncolumn_subset &lt;- h5read(\n  project_file,\n  \"/genetics/snps\",\n  index = list(NULL, c(1, 10, 20, 30, 40))  # Just these 5 columns\n)\n\ncat(\"\\nColumn subset dimensions:\", dim(column_subset), \"\\n\")\n\n\nColumn subset dimensions: 1000 5 \n\ncat(\"Selected columns: 1, 10, 20, 30, 40\\n\")\n\nSelected columns: 1, 10, 20, 30, 40\n\n# Read every 10th row (useful for quick checks)\nsparse_sample &lt;- h5read(\n  project_file,\n  \"/genetics/snps\",\n  index = list(seq(1, 1000, by = 10), 1:10)\n)\n\ncat(\"\\nSparse sample dimensions:\", dim(sparse_sample), \"\\n\")\n\n\nSparse sample dimensions: 100 10 \n\ncat(\"Reading every 10th row reduces data by 90%\\n\")\n\nReading every 10th row reduces data by 90%\n\n\n\n\n\n\n\n\nTipSmart Reading Strategies\n\n\n\nFor exploration:\nh5read(file, dataset, index = list(1:100, NULL))  # First 100 rows\nFor specific variables:\nh5read(file, dataset, index = list(NULL, c(5, 10, 15)))  # Specific columns\nFor random sampling:\nrows &lt;- sample(1:10000, 500)  # Random 500 rows\nh5read(file, dataset, index = list(rows, NULL))\nThe key principle: Load only what you need. This is how you analyze 100 GB files on a 16 GB laptop.",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html#working-with-large-files",
    "href": "tutorials/working-hdf5-matrices.html#working-with-large-files",
    "title": "Working with HDF5 Matrices",
    "section": "7 Working with Large Files",
    "text": "7 Working with Large Files\n\n7.1 Understanding BigDataStatMeth’s Internal Management\nWhen you use BigDataStatMeth functions like bdblockmult_hdf5() or bdSVD_hdf5(), the package handles all the complexity internally:\n\nOpens and closes files automatically\nReads data in optimal block sizes\nWrites results efficiently\nManages memory usage\n\nYou simply call the function and let BigDataStatMeth do the work:\n\n# BigDataStatMeth handles everything internally:\nresult &lt;- bdblockmult_hdf5(\n  filename = \"huge_file.hdf5\",\n  group = \"data\",\n  A = \"matrix_A\",\n  B = \"matrix_B\"\n)\n# ↑ Behind the scenes:\n#   - File opened\n#   - Data read in blocks\n#   - Computation performed\n#   - Results written\n#   - File closed\n# You don't see any of this!\n\n\n\n\n\n\n\nImportantManual File Management: C++ API Only\n\n\n\nWith BigDataStatMeth’s R functions, you never need to manually open/close files or manage blocks. The package does it all.\nManual control is only available when developing new methods using the C++ API. This is for advanced users creating custom statistical methods who need fine-grained control over: - Block sizes - Memory allocation\n- Read/write patterns - Custom algorithms\nFor 99% of users, the automatic management is exactly what you want.\n\n\n\n\n7.2 Checking File Information\nSometimes you just want to know about the data without reading it:\n\n# Get file size\nfile_size_mb &lt;- file.info(project_file)$size / (1024^2)\ncat(\"File size:\", round(file_size_mb, 2), \"MB\\n\")\n\nFile size: 2.78 MB\n\n# Get dataset dimensions without reading data\nh5file &lt;- H5Fopen(project_file)\ndim_snps &lt;- dim(h5file$genetics$snps)\ncat(\"\\nSNP matrix dimensions:\", dim_snps[1], \"samples ×\", dim_snps[2], \"SNPs\\n\")\n\n\nSNP matrix dimensions: 1000 samples × 500 SNPs\n\ndim_traits &lt;- dim(h5file$phenotypes$traits)\ncat(\"Trait matrix dimensions:\", dim_traits[1], \"samples ×\", dim_traits[2], \"traits\\n\")\n\nTrait matrix dimensions: 1000 samples × 10 traits\n\nH5Fclose(h5file)\n\nThis is useful for: - Planning analyses (how much memory will I need?) - Verifying dimensions before operations - Checking if files were created correctly H5Fclose(h5file)\ncat(“SNP matrix dimensions:”, dim_snps[1], “×”, dim_snps[2], “”)\n\n---\n\n## Data Organization Best Practices\n\n### 1. Use Descriptive Names\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Good naming\nbdCreate_hdf5_matrix(filename = \"study.hdf5\",\n                     object = data,\n                     group = \"gwas_diabetes_2024\",\n                     dataset = \"genotypes_qc_filtered\")\n\n# Avoid cryptic names\nbdCreate_hdf5_matrix(filename = \"data.hdf5\",\n                     object = data,\n                     group = \"g1\",\n                     dataset = \"d\")\n:::\n\n\n7.3 2. Document Your Structure\nKeep a README or comments file describing your HDF5 organization:\n\n# Create a text description\nstructure_doc &lt;- \"\nHDF5 File Structure\n==================\n/genetics/snps - Genotype matrix (samples × SNPs)\n/phenotypes/traits - Clinical measurements  \n/phenotypes/covariates - Age, sex, PCs\n/results/pca - PCA results from genetics data\n\"\n\n# You could store this as an attribute or separate file\nwriteLines(structure_doc, \"project_structure.txt\")\n\n\n\n7.4 3. Use Consistent Dimensions\nAlways keep samples in rows and features in columns. This is the standard convention in statistics and makes combining datasets much easier:\n\n# Consistent convention\ngenotypes  # n_samples × n_snps\nphenotypes # n_samples × n_traits\nresults    # n_samples × n_components\n\n# All have samples in rows → easy to merge/join\n\n\n\n\n\n\n\nImportantUnderstanding Row-Major vs Column-Major Storage\n\n\n\nThis is critical to understand when viewing HDF5 files directly!\nR uses column-major order (like Fortran): - Data is stored column by column in memory - Matrix[i, j] means: row i, column j\nHDF5 uses row-major order (like C/C++): - Data is stored row by row in memory\n- Dataset[i, j] means: row i, column j\n\n7.5 What This Means for You\nWhen you create a matrix in R:\n# In R: 3 samples (rows) × 5 SNPs (columns)\ngenotypes &lt;- matrix(1:15, nrow = 3, ncol = 5)\n#      [,1] [,2] [,3] [,4] [,5]\n# [1,]    1    4    7   10   13\n# [2,]    2    5    8   11   14\n# [3,]    3    6    9   12   15\n\n# Save to HDF5\nbdCreate_hdf5_matrix(filename = \"test.hdf5\", \n                     object = genotypes,\n                     group = \"data\", dataset = \"geno\")\nWhen you view it in HDFView or h5dump, you’ll see the transpose:\n# In HDFView: 5 × 3 (appears transposed!)\n# [0,0] [0,1] [0,2]\n#   1     2     3\n# [1,0] [1,1] [1,2]  \n#   4     5     6\n# [2,0] [2,1] [2,2]\n#   7     8     9\n\n\n7.6 Don’t Panic!\nThis is normal and expected. BigDataStatMeth handles the conversion automatically:\n✓ When you read with h5read() → You get the correct R matrix\n✓ When you compute with bdCrossprod_hdf5() → Dimensions are correct\n✓ When you save results → They’re stored correctly for R\nThe transposition is handled transparently. You only notice it if you inspect files with external tools like HDFView.\n\n\n7.7 Key Takeaway\nIn R code: Think in R dimensions (samples × features)\nIn HDFView: Expect to see transposed dimensions\nIn BigDataStatMeth: Everything works correctly - dimensions match R expectations\nThe moral: Trust BigDataStatMeth to handle the storage details. Your matrices will always have the dimensions you expect in R.",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html#interactive-exercise",
    "href": "tutorials/working-hdf5-matrices.html#interactive-exercise",
    "title": "Working with HDF5 Matrices",
    "section": "8 Interactive Exercise",
    "text": "8 Interactive Exercise\n\n8.1 Practice: Designing Your Project’s HDF5 Structure\nEffective HDF5 organization makes the difference between a maintainable project and a confusing mess six months later. This exercise helps you think through organization before starting real analyses.\n\n# Exercise: Plan and create an HDF5 structure for your project\n\n# Scenario: You're analyzing a multi-omic study with:\n# - Genomic data (SNPs): 50,000 individuals × 500,000 variants\n# - Transcriptomic data (RNA-seq): same 50,000 individuals × 20,000 genes\n# - Phenotype data: 50,000 individuals × 50 measurements\n# - Sample metadata: IDs, age, sex, ancestry\n\n# Your task: Design the HDF5 structure\n\n# Option 1: Everything in one file\nproject_file &lt;- \"my_study.hdf5\"\n\n# How would you organize groups?\n# /raw_data/genomics/snps\n# /raw_data/transcriptomics/expression\n# /raw_data/phenotypes/measurements\n# /metadata/samples\n# /quality_control/filtered_snps\n# /quality_control/normalized_expression\n# /results/pca\n# /results/associations\n\n# Try creating this structure:\nbdCreate_hdf5_matrix(project_file, dummy_data, \n                     group = \"/raw_data/genomics\", \n                     dataset = \"snps\")\n# ... add more datasets\n\n# Option 2: Separate files per data type\n# genomics.hdf5, transcriptomics.hdf5, phenotypes.hdf5\n# When is this better? When worse?\n\n\n\n\n\n\n\nTipReflection Questions\n\n\n\nThink through these design decisions for your actual or planned projects:\n1. File Organization Strategy: - One large file vs. multiple smaller files? - For your project, which approach makes more sense? - How does your team typically share data? - Will different people access different parts?\n2. Group Hierarchy: - How deep should your folder structure go? - /data/type/version vs. /data_type_version? - Can you navigate your structure 6 months from now? - Would a new collaborator understand it?\n3. Dataset Naming: - “matrix1” vs. “filtered_normalized_snps_maf05”? - Descriptive names vs. short names? - How do you indicate processing steps? - Do you need version numbers in names?\n4. Metadata Management: - Where do row/column names go? - How do you store processing parameters? - Document analysis dates and software versions? - Link between datasets (which samples are in which matrix)?\n5. Practical Constraints: - How much disk space do you have? - Will files be transferred between systems? - Need to share via email/FTP? (file size limits) - Backup strategy for large files?\n6. Future Expansion: - What if you add more samples next year? - New data types (proteomics, metabolomics)? - Reanalysis with different parameters? - Where does each new piece go?\nThere’s no universal “correct” organization - it depends on your project scale, team structure, and workflow. The goal is thinking through these questions before you have 50 analysis scripts depending on a particular structure.",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html#cleanup",
    "href": "tutorials/working-hdf5-matrices.html#cleanup",
    "title": "Working with HDF5 Matrices",
    "section": "9 Cleanup",
    "text": "9 Cleanup\n\n# Close all HDF5 connections\nh5closeAll()\n\n# Remove generated HDF5 file\nfile.remove(\"clinical_data.hdf5\",\n            \"multi_dataset_project.hdf5\")\n\n[1] TRUE TRUE\n\ncat(\"✓ Tutorial cleanup complete\\n\")\n\n✓ Tutorial cleanup complete\n\ncat(\"Note: colesterol.csv is kept for reference\\n\")\n\nNote: colesterol.csv is kept for reference",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html#what-youve-learned",
    "href": "tutorials/working-hdf5-matrices.html#what-youve-learned",
    "title": "Working with HDF5 Matrices",
    "section": "10 What You’ve Learned",
    "text": "10 What You’ve Learned\n✅ Convert text files (CSV, TSV) to HDF5 format\n✅ Manage multiple datasets in one HDF5 file\n✅ Organize data using groups and descriptive names\n✅ Access data efficiently (subsets, slicing)\n✅ Apply best practices for file organization",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html#next-steps",
    "href": "tutorials/working-hdf5-matrices.html#next-steps",
    "title": "Working with HDF5 Matrices",
    "section": "11 Next Steps",
    "text": "11 Next Steps\nContinue the tutorial series:\n\n✅ Getting Started\n✅ Working with HDF5 Matrices (you are here)\n→ Your First Analysis - Complete analysis workflow\n\nReady for real analysis:\n\nImplementing PCA - Full PCA workflow on genomic data\nImplementing CCA - Canonical correlation analysis",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/working-hdf5-matrices.html#key-takeaways",
    "href": "tutorials/working-hdf5-matrices.html#key-takeaways",
    "title": "Working with HDF5 Matrices",
    "section": "12 Key Takeaways",
    "text": "12 Key Takeaways\nLet’s consolidate your understanding of HDF5 file management and organization for big data projects.\n\n12.1 Essential Concepts\nHDF5 is a data management system, not just a file format. It provides hierarchical organization (groups and datasets), partial I/O (read only what you need), and cross-platform compatibility. This makes it fundamentally different from flat file formats like CSV. You’re not just storing data - you’re organizing an entire analysis project in a structured, queryable format.\nFile conversion is a one-time investment that pays dividends throughout your project. Converting a 50 GB CSV file to HDF5 might take 20 minutes, but every subsequent analysis benefits from fast partial reads, organized structure, and efficient access patterns. The upfront cost amortizes across dozens or hundreds of analyses over the project lifetime.\nOrganization decisions made early are hard to change later. Once you have 50 analysis scripts expecting data at /raw_data/genomics/snps, reorganizing the file breaks everything. Take time designing your structure before populating it. A bit of planning prevents substantial refactoring pain later.\nBigDataStatMeth and rhdf5 serve different purposes. BigDataStatMeth creates and operates on matrices (the computational work). rhdf5 provides file I/O and inspection (reading, writing, navigating). You need both: BigDataStatMeth for analysis, rhdf5 for file management. Understanding this division of labor prevents confusion about which tool does what.\nRow-major vs. column-major storage is handled automatically. R uses column-major order, HDF5 uses row-major. BigDataStatMeth manages this conversion transparently. Your matrices always have the dimensions you expect in R code, even though they’re stored differently on disk. You only notice the difference if you inspect files with external tools like HDFView - and even then, it’s just a visual curiosity, not a problem.\nEfficient HDF5 use means reading strategically. Don’t read entire 100 GB matrices to access a 1 MB subset. Use dimension slicing, work with row/column blocks, and read only required data. The ability to partially read files is HDF5’s main advantage - leverage it. This is why understanding dataset dimensions and organization matters.\n\n\n12.2 When to Use HDF5 File Management\nUnderstanding when HDF5’s organizational features help versus when simpler approaches suffice guides practical decisions about file structure and management.\n✅ Use hierarchical HDF5 organization when:\n\nYou have multiple related datasets - Genomics + transcriptomics + phenotypes all in one project. Groups like /genetics/, /expression/, /clinical/ keep everything organized in a single file rather than managing dozens of separate files.\nYour project will grow over time - Starting organized prevents refactoring pain. If you’ll add more samples, time points, or data types later, hierarchical structure accommodates growth naturally.\nMultiple team members access the same data - Clear organization (/raw_data/, /quality_control/, /results/) makes files self-documenting. Collaborators can navigate without constant explanations.\nYou need to document processing steps - Dataset names like genotypes_filtered_maf005_hwe1e6 encode processing history. The file structure itself documents your analysis pipeline.\n\n✅ Use rhdf5 for file operations when:\n\nYou need to inspect file contents - h5ls() shows structure, h5read() retrieves data. rhdf5 provides the file navigation and I/O tools.\nYou’re managing metadata - Row names, column names, attributes, and documentation all go through rhdf5 functions.\nYou need partial data access - Reading specific rows, columns, or blocks uses rhdf5’s indexing capabilities: h5read(file, dataset, index = list(rows, cols)).\n\n❌ Simpler approaches work better when:\n\nYou have a single dataset - If your entire project is one matrix, elaborate HDF5 organization adds no value. A simple flat file or single-group HDF5 suffices.\nData won’t grow or change - For static datasets that are finalized and won’t be extended, elaborate organization provides minimal benefit over simple naming.\nQuick, temporary analyses - If you’re not building a reusable analysis pipeline, the organizational overhead outweighs benefits. Use whatever gets you to results fastest.\nYou never need partial reads - If you always process entire matrices, HDF5’s partial I/O advantage disappears. The organizational features may still help, but the performance benefit is minimal.\n\nUnderstanding these trade-offs helps you invest effort where it pays off - elaborate organization for complex, growing projects; simple structure for straightforward, static analyses.",
    "crumbs": [
      "Getting Started",
      "Working with HDF5 Matrices"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html",
    "href": "tutorials/getting-started.html",
    "title": "Getting Started",
    "section": "",
    "text": "BigDataStatMeth enables statistical analysis on datasets too large for RAM by using HDF5 file storage and block-wise processing. This tutorial guides you through installation and your first analysis, ensuring you have a working environment before diving into more complex operations.\nThink of this as setting up your laboratory bench before starting experiments. You’ll verify that all your tools work correctly with small, manageable examples before scaling to real big data analyses. This approach prevents frustrating debugging sessions later when you’re working with large files.\n\n\nBy the end of this tutorial, you will:\n\nInstall BigDataStatMeth and all required dependencies correctly\nCreate your first HDF5 matrix from R data\nPerform basic matrix operations on HDF5-stored data\nUnderstand the HDF5 file structure and how to navigate it\nVerify your installation works correctly with test examples\nKnow where to find help when you encounter issues\nBe prepared for more advanced tutorials",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#overview",
    "href": "tutorials/getting-started.html#overview",
    "title": "Getting Started",
    "section": "",
    "text": "BigDataStatMeth enables statistical analysis on datasets too large for RAM by using HDF5 file storage and block-wise processing. This tutorial guides you through installation and your first analysis, ensuring you have a working environment before diving into more complex operations.\nThink of this as setting up your laboratory bench before starting experiments. You’ll verify that all your tools work correctly with small, manageable examples before scaling to real big data analyses. This approach prevents frustrating debugging sessions later when you’re working with large files.\n\n\nBy the end of this tutorial, you will:\n\nInstall BigDataStatMeth and all required dependencies correctly\nCreate your first HDF5 matrix from R data\nPerform basic matrix operations on HDF5-stored data\nUnderstand the HDF5 file structure and how to navigate it\nVerify your installation works correctly with test examples\nKnow where to find help when you encounter issues\nBe prepared for more advanced tutorials",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#prerequisites",
    "href": "tutorials/getting-started.html#prerequisites",
    "title": "Getting Started",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\n\n2.1 System Requirements\nOperating System: - Linux (recommended) - macOS - Windows (requires Rtools - see below)\nR Version: - R ≥ 4.0.0 (check with R.version.string)\nRAM: - Minimum: 4 GB - Recommended: 8+ GB for comfortable work\nDisk Space: - ~500 MB for package and dependencies - Additional space for your HDF5 data files\n\n\n\n\n\n\nImportantWindows Users: Install Rtools First\n\n\n\nBigDataStatMeth is compiled from C++ source code. Windows users must install Rtools before installing the package.\nDownload the version matching your R installation from the link above.",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#step-1-install-dependencies",
    "href": "tutorials/getting-started.html#step-1-install-dependencies",
    "title": "Getting Started",
    "section": "3 Step 1: Install Dependencies",
    "text": "3 Step 1: Install Dependencies\nBigDataStatMeth requires packages from both CRAN and Bioconductor.\n\n3.1 Install BiocManager\n\n# BiocManager helps install Bioconductor packages\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\n\n\n3.2 Install Required Packages\n\n# CRAN packages\ninstall.packages(c(\"Matrix\", \"RcppEigen\", \"RSpectra\"))\n\n# Bioconductor packages  \nBiocManager::install(c(\"rhdf5\", \"HDF5Array\"))\n\n\n\n\n\n\n\nTipTroubleshooting Dependencies\n\n\n\nIf installation fails:\n\nUpdate R: Some packages require recent R versions\nUpdate BiocManager: Run BiocManager::install() with no arguments\nCheck compilation tools: Especially on Windows/macOS\nInstallation logs: Look for specific error messages about missing libraries",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#step-2-install-bigdatastatmeth",
    "href": "tutorials/getting-started.html#step-2-install-bigdatastatmeth",
    "title": "Getting Started",
    "section": "4 Step 2: Install BigDataStatMeth",
    "text": "4 Step 2: Install BigDataStatMeth\n\n4.1 From CRAN (Recommended - Stable Version)\nThe stable version is available on CRAN:\n\ninstall.packages(\"BigDataStatMeth\")\n\nThis installs the latest stable, tested release.\n\n\n4.2 From GitHub (Development Version)\nFor the latest development features:\n\n# Install devtools if needed\nif (!requireNamespace(\"devtools\", quietly = TRUE))\n    install.packages(\"devtools\")\n\n# Install development version from GitHub\ndevtools::install_github(\"isglobal-brge/BigDataStatMeth\")\n\n\n\n\n\n\n\nNoteWhich Version Should I Use?\n\n\n\nUse CRAN version if: - You want maximum stability - You’re doing production analysis - You prefer well-tested releases\nUse GitHub version if: - You need the latest features - You’re contributing to development - You want to test new functionality",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#step-3-load-and-verify",
    "href": "tutorials/getting-started.html#step-3-load-and-verify",
    "title": "Getting Started",
    "section": "5 Step 3: Load and Verify",
    "text": "5 Step 3: Load and Verify\n\n5.1 Load the Package\n\nlibrary(BigDataStatMeth)\n\nIf no errors appear, the package loaded successfully!\n\n\n\n\n\n\nImportantAbout Loading rhdf5\n\n\n\nYou might see examples that load library(rhdf5). This is only necessary when you use rhdf5 functions directly like h5ls(), h5read(), or H5Fopen().\nBigDataStatMeth already depends on rhdf5 internally, so you don’t need to load it for BigDataStatMeth functions. However, in this tutorial we’ll use some rhdf5 functions for file inspection, so we’ll load it when needed.\nKey point: BigDataStatMeth implements its own high-level functions. We use rhdf5’s inspection functions because they’re already excellent and there’s no need to reimplement them.\n\n\n\n\n5.2 Quick Verification\nRun this simple test to verify everything works:\n\n# Create a small test matrix\ntest_matrix &lt;- matrix(rnorm(100), nrow = 10, ncol = 10)\n\n# Create HDF5 file\ntest_file &lt;- \"verification_test.hdf5\"\nbdCreate_hdf5_matrix(\n  filename = test_file,\n  object = test_matrix,\n  group = \"test\",\n  dataset = \"data\",\n  overwriteFile = TRUE\n)\n\n$fn\n[1] \"verification_test.hdf5\"\n\n$ds\n[1] \"test/data\"\n\n# Verify file was created\nif (file.exists(test_file)) {\n  cat(\"✓ Installation verified!\\n\")\n  cat(\"✓ HDF5 file created successfully\\n\")\n  \n  # Clean up\n  file.remove(test_file)\n} else {\n  cat(\"✗ Installation issue - file not created\\n\")\n}\n\n✓ Installation verified!\n✓ HDF5 file created successfully\n\n\n[1] TRUE\n\n\nExpected output:\n✓ Installation verified!\n✓ HDF5 file created successfully\n\n\n\n\n\n\nWarningIf Verification Fails\n\n\n\nCommon issues:\n\n“Package not found”: Restart R session\n“HDF5 library error”: Reinstall rhdf5 package\n\nPermission denied: Check write permissions in working directory\nSymbol not found: Recompile package from source\n\nTry sessionInfo() to check loaded packages and versions.",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#step-4-your-first-hdf5-dataset",
    "href": "tutorials/getting-started.html#step-4-your-first-hdf5-dataset",
    "title": "Getting Started",
    "section": "6 Step 4: Your First HDF5 Dataset",
    "text": "6 Step 4: Your First HDF5 Dataset\nNow let’s create a realistic dataset and perform basic operations.\n\n6.1 Create Sample Data\n\n# Simulate a genomic dataset: 1,000 samples × 5,000 SNPs\nset.seed(123)\nn_samples &lt;- 1000\nn_snps &lt;- 5000\n\ngenotype_data &lt;- matrix(\n  sample(0:2, n_samples * n_snps, replace = TRUE),\n  nrow = n_samples,\n  ncol = n_snps\n)\n\n# Add meaningful row/column names\nrownames(genotype_data) &lt;- paste0(\"Sample_\", 1:n_samples)\ncolnames(genotype_data) &lt;- paste0(\"SNP_\", 1:n_snps)\n\n# Check size\nformat(object.size(genotype_data), units = \"MB\")\n\n[1] \"19.5 Mb\"\n\n\n\n\n6.2 Save to HDF5\n\n# Create HDF5 file\ndata_file &lt;- \"my_first_dataset.hdf5\"\n\nbdCreate_hdf5_matrix(\n  filename = data_file,\n  object = genotype_data,\n  group = \"genotypes\",\n  dataset = \"data\",\n  overwriteFile = TRUE\n)\n\n$fn\n[1] \"my_first_dataset.hdf5\"\n\n$ds\n[1] \"genotypes/data\"\n\ncat(\"✓ Dataset saved to HDF5\\n\")\n\n✓ Dataset saved to HDF5\n\n\n\n\n\n\n\n\nNoteHDF5 File Structure\n\n\n\nHDF5 files organize data hierarchically:\n\nFile: my_first_dataset.hdf5\n\nGroup: genotypes (like a folder)\n\nDataset: data (the actual matrix)\n\n\n\nThink of groups as folders and datasets as files inside them.\n\n\n\n\n6.3 Inspect the File\nFor file inspection, we use rhdf5 functions:\n\nlibrary(rhdf5)  # Needed for h5ls()\n\n# List file contents\nh5ls(data_file)\n\n                      group           name       otype   dclass         dim\n0                         /      genotypes   H5I_GROUP                     \n1                /genotypes .data_dimnames   H5I_GROUP                     \n2 /genotypes/.data_dimnames              1 H5I_DATASET COMPOUND        1000\n3 /genotypes/.data_dimnames              2 H5I_DATASET COMPOUND        5000\n4                /genotypes           data H5I_DATASET  INTEGER 1000 x 5000\n\n\n\n\n6.4 Read Data Back\n\n# Read a small portion\nsmall_chunk &lt;- h5read(data_file, \"/genotypes/data\", \n                      index = list(1:5, 1:10))\nsmall_chunk\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    2    1    2    2    2    0    1    0    2     0\n[2,]    2    2    1    2    2    1    0    0    0     2\n[3,]    2    1    0    0    2    2    1    0    0     0\n[4,]    1    0    0    0    2    1    0    0    1     1\n[5,]    2    2    0    1    0    2    0    2    2     2\n\n# Verify it matches original\nall.equal(small_chunk, genotype_data[1:5, 1:10])\n\n[1] \"Attributes: &lt; Length mismatch: comparison on first 1 components &gt;\"\n\n\n\n\n\n\n\n\nNoteUnderstanding all.equal()\n\n\n\nIf you see output like \"Attributes: &lt; Component 'dimnames': target is NULL, current is list &gt;\", this means the matrices are numerically identical but have different attributes (row/column names).\nThis is expected: HDF5 stores dimension names separately from the data matrix. The numeric values are identical, which is what matters for calculations. You can verify with:\n\n# Compare just the numeric values\nall.equal(as.numeric(small_chunk), as.numeric(genotype_data[1:5, 1:10]))\n\n[1] TRUE",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#step-5-basic-operations",
    "href": "tutorials/getting-started.html#step-5-basic-operations",
    "title": "Getting Started",
    "section": "7 Step 5: Basic Operations",
    "text": "7 Step 5: Basic Operations\nLet’s perform operations directly on the HDF5 file using BigDataStatMeth functions.\n\n7.1 Matrix Multiplication\n\n# Create two smaller matrices for demonstration\nset.seed(456)\nA &lt;- matrix(rnorm(500 * 100), nrow = 500, ncol = 100)\nB &lt;- matrix(rnorm(100 * 200), nrow = 100, ncol = 200)\n\n# Save to HDF5\nexample_file &lt;- \"operations_example.hdf5\"\nbdCreate_hdf5_matrix(filename = example_file, object = A,\n                     group = \"matrices\", dataset = \"A\",\n                     overwriteFile = TRUE)\n\n$fn\n[1] \"operations_example.hdf5\"\n\n$ds\n[1] \"matrices/A\"\n\nbdCreate_hdf5_matrix(filename = example_file, object = B,\n                     group = \"matrices\", dataset = \"B\")\n\n$fn\n[1] \"operations_example.hdf5\"\n\n$ds\n[1] \"matrices/B\"\n\n# Perform block-wise multiplication\nresult &lt;- bdblockmult_hdf5(\n  filename = example_file,\n  group = \"matrices\",\n  A = \"A\",\n  B = \"B\",\n  outgroup = \"results\"\n)\n\n# Read and display a portion of the result\nresult_sample &lt;- h5read(result$fn, result$ds, index = list(1:5, 1:5))\ncat(\"Result preview (first 5×5):\\n\")\n\nResult preview (first 5×5):\n\nprint(result_sample)\n\n          [,1]      [,2]       [,3]       [,4]     [,5]\n[1,]  3.966529 -1.596117 -10.575227  -1.523170 13.37335\n[2,] 10.496300 -4.726875  16.389046   3.817424 16.47317\n[3,] -8.331547 -9.299947  -3.084136   4.030101 11.75505\n[4,] -5.159786  2.319629   2.815899  -3.407731 -5.72410\n[5,]  3.384955 18.492893   1.697668 -13.890756 10.48865\n\ncat(\"\\n✓ Matrix multiplication completed\\n\")\n\n\n✓ Matrix multiplication completed\n\ncat(\"Result stored in:\", result$ds, \"\\n\")\n\nResult stored in: results/A_x_B \n\n\n\n\n\n\n\n\nTipMemory Efficiency in Action\n\n\n\nNotice we multiplied a 500×100 matrix by a 100×200 matrix without loading the full result into memory. BigDataStatMeth processed this in blocks and wrote directly to HDF5.\nFor truly large matrices (100 GB+), this same code works identically - just takes longer.\n\n\n\n\n7.2 Crossproduct Operation\n\n# Compute t(A) %*% A\ncrossprod_result &lt;- bdCrossprod_hdf5(\n  filename = example_file,\n  group = \"matrices\",\n  A = \"A\"\n)\n\n# Read and display a portion\ncrossprod_sample &lt;- h5read(crossprod_result$fn, crossprod_result$ds, \n                           index = list(1:5, 1:5))\ncat(\"Crossproduct result preview (first 5×5):\\n\")\n\nCrossproduct result preview (first 5×5):\n\nprint(crossprod_sample)\n\n           [,1]       [,2]       [,3]       [,4]      [,5]\n[1,] 475.947503 -26.983942   1.705196  18.276113  22.03469\n[2,] -26.983942 488.288982 -24.322274   6.712965 -14.34956\n[3,]   1.705196 -24.322274 498.798075   9.800220  52.82859\n[4,]  18.276113   6.712965   9.800220 511.565405 -26.92909\n[5,]  22.034686 -14.349559  52.828589 -26.929093 438.22361\n\ncat(\"\\n✓ Crossprod completed\\n\")\n\n\n✓ Crossprod completed\n\ncat(\"Result dimensions should be 100 × 100\\n\")\n\nResult dimensions should be 100 × 100\n\n# Verify dimensions\nh5ls(example_file)\n\n      group            name       otype dclass       dim\n0         /          OUTPUT   H5I_GROUP                 \n1   /OUTPUT CrossProd_A_x_A H5I_DATASET  FLOAT 100 x 100\n2         /        matrices   H5I_GROUP                 \n3 /matrices               A H5I_DATASET  FLOAT 500 x 100\n4 /matrices               B H5I_DATASET  FLOAT 100 x 200\n5         /         results   H5I_GROUP                 \n6  /results           A_x_B H5I_DATASET  FLOAT 500 x 200",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#step-6-clean-up",
    "href": "tutorials/getting-started.html#step-6-clean-up",
    "title": "Getting Started",
    "section": "8 Step 6: Clean Up",
    "text": "8 Step 6: Clean Up\n\n# Close any open HDF5 connections\nh5closeAll()\n\n# Remove test files (optional)\nfile.remove(data_file, example_file)\n\n[1] TRUE TRUE",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#interactive-exercise",
    "href": "tutorials/getting-started.html#interactive-exercise",
    "title": "Getting Started",
    "section": "9 Interactive Exercise",
    "text": "9 Interactive Exercise\n\n9.1 Practice: Creating Your Own HDF5 Workflow\nNow that you’ve seen the basic operations, try designing a small workflow with data relevant to your work. This helps internalize the concepts through hands-on practice.\n\n# Exercise: Create a mini-analysis workflow\n\n# 1. Generate or load your own data\nmy_data &lt;- matrix(rnorm(1000 * 500), nrow = 1000, ncol = 500)  \n# Replace with: read.csv(), or data from your field\n\n# 2. Save to HDF5 with meaningful names\nbdCreate_hdf5_matrix(\n  filename = \"my_analysis.hdf5\",\n  object = my_data,\n  group = \"raw_data\",           # Choose descriptive name\n  dataset = \"measurements\",     # What does this represent?\n  overwriteFile = TRUE\n)\n\n# 3. Perform an operation\nresult &lt;- bdCrossprod_hdf5(\n  filename = \"my_analysis.hdf5\",\n  group = \"raw_data\",\n  A = \"measurements\",\n  outgroup = \"processed\"\n)\n\n# 4. Verify the result\nh5ls(\"my_analysis.hdf5\")  # Examine the structure\n\n\n\n\n\n\n\nTipReflection Questions\n\n\n\nAs you work through this exercise, consider:\n1. File Organization: - How are you structuring your HDF5 file? (groups and datasets) - If you added more data tomorrow, where would it go? - Could someone else understand your organization scheme?\n2. Memory vs. Disk: - At what data size would your operation fail in memory? - How much disk space does your HDF5 file use? - Compare: object size in R (object.size(my_data)) vs. file size on disk\n3. Operation Choice: - Why did you choose bdCrossprod_hdf5() over other operations? - What statistical question does this operation answer? - Could you achieve the same result by composing multiple simpler operations?\n4. Error Handling: - What happens if you try to create a dataset that already exists? - Try it: What does the error message tell you? - How do you fix it? (overwriteDataset = TRUE or different name?)\n5. Scaling Up: - Your test used 1,000 × 500. What about 100,000 × 50,000? - Which operations would work unchanged? - Which would need adjustment (block sizes, memory)?\nDon’t worry about “correct” answers - the goal is developing intuition about when and how to use these tools. Each analysis scenario is unique, and hands-on experimentation builds understanding better than reading alone.",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#what-youve-accomplished",
    "href": "tutorials/getting-started.html#what-youve-accomplished",
    "title": "Getting Started",
    "section": "10 What You’ve Accomplished",
    "text": "10 What You’ve Accomplished\n✅ Installed BigDataStatMeth and all dependencies\n✅ Created your first HDF5 dataset\n✅ Performed block-wise operations on HDF5 data\n✅ Understood the HDF5 file structure (groups and datasets)\n✅ Verified that block-wise processing works correctly",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#next-steps",
    "href": "tutorials/getting-started.html#next-steps",
    "title": "Getting Started",
    "section": "11 Next Steps",
    "text": "11 Next Steps\nNow that you have BigDataStatMeth working, continue learning:\nContinue the tutorial series:\n\n✅ Getting Started (you are here)\n→ Working with HDF5 Matrices - File operations, data conversion, and management\n→ Your First Analysis - Complete workflow from raw data to results\n\nExplore practical workflows:\n\nImplementing PCA - Principal Component Analysis on genomic data\nImplementing CCA - Canonical Correlation Analysis\n\nDive deeper into concepts:\n\nUnderstanding HDF5 - How HDF5 storage works\nBlock-Wise Computing - Algorithms behind the scenes",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#getting-help",
    "href": "tutorials/getting-started.html#getting-help",
    "title": "Getting Started",
    "section": "12 Getting Help",
    "text": "12 Getting Help\nIf you encounter issues:\n\nCheck documentation: Most functions have detailed help: ?bdCreate_hdf5_matrix\nReview examples: Package vignettes contain working code\nGitHub Issues: Report bugs at isglobal-brge/BigDataStatMeth\n\n\n\n\n\n\n\nTipQuick Troubleshooting Commands\n\n\n\n\n# Check package version\npackageVersion(\"BigDataStatMeth\")\n\n# View all loaded packages\nsessionInfo()\n\n# Test HDF5 installation\nrhdf5::h5version()\n\n# Check working directory (where files are created)\ngetwd()",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "tutorials/getting-started.html#key-takeaways",
    "href": "tutorials/getting-started.html#key-takeaways",
    "title": "Getting Started",
    "section": "13 Key Takeaways",
    "text": "13 Key Takeaways\nLet’s consolidate what you’ve learned about setting up and using BigDataStatMeth for the first time.\n\n13.1 Essential Concepts\nInstallation creates the foundation for all subsequent work with BigDataStatMeth. Without properly installed dependencies (rhdf5, HDF5Array, compilation tools), nothing else works. Windows users face additional complexity requiring Rtools, but following the installation sequence systematically prevents most problems. Testing with small examples immediately after installation catches configuration issues before you invest time in real analyses.\nHDF5 files are organized hierarchically like a file system, with groups acting as folders and datasets as files. This structure isn’t just organizational convenience - it enables efficient partial I/O where you read only the data you need. Creating your first HDF5 file teaches this fundamental paradigm: data lives on disk, accessed selectively, rather than entirely in RAM. Good organization from the start saves confusion when projects grow complex.\nBlock-wise processing happens automatically behind BigDataStatMeth’s functions. You don’t partition matrices manually or manage memory explicitly - the package handles block sizes, I/O patterns, and result aggregation internally. When you call bdCrossprod_hdf5(), it looks like a single function call, but executes sophisticated block-wise algorithms transparently. This abstraction is the package’s main value: complexity hidden, scaling achieved.\nVerification prevents wasted effort. Testing installations with small examples (100×100 matrices) catches problems when they’re easy to fix. If basic operations fail on tiny test data, they won’t mysteriously work on real 100,000×100,000 matrices. Small-scale testing establishes that your environment works correctly before investing hours generating or converting large datasets.\nThe R API suffices for most users. Functions like bdCreate_hdf5_matrix(), bdSVD_hdf5(), and bdCrossprod_hdf5() provide complete functionality for standard analyses. The C++ API exists for developers implementing novel statistical methods who need fine-grained control over algorithms and memory management. Unless you’re developing new methods from scratch, the R interface provides everything needed.\n\n\n13.2 When to Use BigDataStatMeth\nMaking informed decisions about when BigDataStatMeth helps versus when simpler approaches suffice saves time and prevents unnecessary complexity.\n✅ Use BigDataStatMeth when:\n\nData exceeds 30% of available RAM - This threshold provides headroom for intermediate computations and operating system needs. Below 30%, traditional R approaches work fine. Above 30%, you risk memory exhaustion during operations, and disk-based computing becomes necessary.\nYou’re starting a new analysis project - Converting data to HDF5 at the beginning avoids migration pain later. It’s easier to start organized than to reorganize mid-project when you discover your data has grown beyond memory limits.\nMultiple analyses will reuse the same data - Converting to HDF5 once pays off when you’ll run PCA, then regression, then association tests on the same dataset. The upfront conversion cost amortizes across repeated analyses.\nYour workflow spans multiple tools - If you work in R, Python, and command-line tools, HDF5 provides a common format all can read efficiently. This beats converting between CSV, RData, and tool-specific formats repeatedly.\n\n❌ Traditional R works better when:\n\nData comfortably fits in less than 20% of RAM - If data &lt;- read.csv(file) works without issues, stick with familiar R approaches. Traditional methods are simpler, more flexible, and better supported by the broader R ecosystem. Don’t add complexity unnecessarily.\nYou’re doing one-off exploratory analysis - For quick investigations you won’t repeat, the HDF5 conversion overhead outweighs benefits. Load data, explore, save key results, discard working data, and you’re done.\nYou need maximum flexibility - In-memory R data structures support arbitrary manipulations trivially: reshape, subset, transform however you want. HDF5 adds structure, which aids organization but constrains spontaneous manipulations. If your workflow involves many ad-hoc transformations, staying in memory maintains flexibility.\n\nThe key question isn’t just “can my data fit in memory?” but “does my workflow benefit from disk-based computing?” Sometimes the answer is obvious (500 GB dataset, 32 GB RAM → yes). Sometimes it’s contextual (40 GB dataset, 64 GB RAM → depends on specific operations needed). Understanding your analysis requirements and computational resources helps make this decision rationally rather than by trial and error.",
    "crumbs": [
      "Getting Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "technical/performance.html",
    "href": "technical/performance.html",
    "title": "Performance Optimization",
    "section": "",
    "text": "Performance optimization in BigDataStatMeth requires understanding a fundamental truth: you cannot simultaneously maximize speed, minimize memory usage, and maintain perfect numerical accuracy. Every optimization decision involves tradeoffs, and the “best” configuration depends entirely on your specific constraints—hardware limitations, dataset characteristics, and analysis requirements.\nConsider a common scenario: you have a 100GB dataset and need to perform PCA. One configuration processes it in 10 minutes using 8GB RAM. Another finishes in 5 minutes but requires 64GB RAM. Which is “better”? If you only have 16GB available, the second configuration is impossible—the first configuration is objectively superior for your situation. If you have 128GB RAM but are analyzing hundreds of datasets, the 5-minute option saves hours of total computation time. Neither configuration is universally optimal; context determines the right choice.\nThis guide teaches you to make these decisions systematically. You’ll learn to identify where your computation spends time (profiling), understand how parameters affect performance (tuning), and recognize when you’re fighting fundamental limitations (bottlenecks). The goal isn’t to memorize optimal parameter values—those don’t exist—but to develop intuition for balancing constraints in your specific environment.\nThe strategies here apply broadly across BigDataStatMeth’s operations: PCA, SVD, CCA, matrix operations, statistical tests. While optimal values vary by dataset and hardware, the underlying principles remain constant. Master these principles, and you can optimize any workflow.\n\n\n\nIdentify performance bottlenecks in your workflows using profiling tools\nUnderstand how block size (k, q) controls the memory-speed tradeoff\nTune thread counts while recognizing parallelization limits\nOptimize HDF5 chunk sizes to match your access patterns\nBalance numerical accuracy against computational efficiency\nRecognize fundamental bottlenecks (I/O, memory bandwidth, Amdahl’s law)\nMake informed optimization decisions based on your constraints\nBenchmark systematically to measure actual improvements"
  },
  {
    "objectID": "technical/performance.html#overview",
    "href": "technical/performance.html#overview",
    "title": "Performance Optimization",
    "section": "",
    "text": "Performance optimization in BigDataStatMeth requires understanding a fundamental truth: you cannot simultaneously maximize speed, minimize memory usage, and maintain perfect numerical accuracy. Every optimization decision involves tradeoffs, and the “best” configuration depends entirely on your specific constraints—hardware limitations, dataset characteristics, and analysis requirements.\nConsider a common scenario: you have a 100GB dataset and need to perform PCA. One configuration processes it in 10 minutes using 8GB RAM. Another finishes in 5 minutes but requires 64GB RAM. Which is “better”? If you only have 16GB available, the second configuration is impossible—the first configuration is objectively superior for your situation. If you have 128GB RAM but are analyzing hundreds of datasets, the 5-minute option saves hours of total computation time. Neither configuration is universally optimal; context determines the right choice.\nThis guide teaches you to make these decisions systematically. You’ll learn to identify where your computation spends time (profiling), understand how parameters affect performance (tuning), and recognize when you’re fighting fundamental limitations (bottlenecks). The goal isn’t to memorize optimal parameter values—those don’t exist—but to develop intuition for balancing constraints in your specific environment.\nThe strategies here apply broadly across BigDataStatMeth’s operations: PCA, SVD, CCA, matrix operations, statistical tests. While optimal values vary by dataset and hardware, the underlying principles remain constant. Master these principles, and you can optimize any workflow.\n\n\n\nIdentify performance bottlenecks in your workflows using profiling tools\nUnderstand how block size (k, q) controls the memory-speed tradeoff\nTune thread counts while recognizing parallelization limits\nOptimize HDF5 chunk sizes to match your access patterns\nBalance numerical accuracy against computational efficiency\nRecognize fundamental bottlenecks (I/O, memory bandwidth, Amdahl’s law)\nMake informed optimization decisions based on your constraints\nBenchmark systematically to measure actual improvements"
  },
  {
    "objectID": "technical/performance.html#understanding-the-three-constraints",
    "href": "technical/performance.html#understanding-the-three-constraints",
    "title": "Performance Optimization",
    "section": "2 Understanding the Three Constraints",
    "text": "2 Understanding the Three Constraints\nEvery optimization navigates three competing constraints. Understanding why these constraints conflict helps you make better tradeoff decisions.\n\n\n\n\n\nflowchart LR\n    A[Memory] &lt;--&gt; B[Speed]\n    B &lt;--&gt; C[Accuracy]\n    C &lt;--&gt; A\n    \n    style A fill:#e8f6e8\n    style B fill:#e7f3ff\n    style C fill:#fff8e1\n\n\n\n\n\n\n\n2.1 Memory vs Speed\nBlock-wise computing divides matrices into smaller pieces to control memory usage. Smaller blocks mean lower peak memory—you can process datasets much larger than your RAM. But smaller blocks come with costs: more operations to combine results, more HDF5 reads/writes, more overhead from setup and coordination.\nThink of it like moving furniture. Disassembling a table into many small pieces lets you fit it through doorways (memory constraint), but assembly and disassembly take time (speed penalty). Keeping the table intact is faster but requires a truck large enough to carry it whole (memory requirement). The optimal approach depends on what size “truck” (RAM) you have available.\nMathematically, if you double the number of blocks (k), you roughly halve peak memory usage but may increase computation time by 20-50% depending on the operation. For pure computation (matrix multiplication, SVD), the overhead is lower. For I/O-heavy operations (reading data, writing results), overhead dominates.\n\n\n2.2 Speed vs Accuracy\nSome algorithms offer approximate solutions faster than exact solutions. Block-wise SVD, for example, can compute fewer iterations to converge faster, accepting slightly lower precision in singular values. For exploratory data analysis—visualizing principal components, identifying outliers, getting a general sense of data structure—high precision matters less than rapid iteration. You’re happy with “close enough” if it lets you test ten parameter combinations instead of one.\nFor publication-quality results, accuracy cannot be compromised. The difference between a singular value of 42.157 and 42.154 might seem trivial, but reviewers and reproducibility standards demand exact, verifiable computations. In these cases, you accept longer computation times to guarantee numerical precision.\nBigDataStatMeth defaults to conservative, accurate algorithms. When speed matters more than the last decimal place, you can adjust—but always verify results remain scientifically valid for your purposes.\n\n\n2.3 Memory vs Accuracy\nThis constraint is subtler and less obvious. Extreme block sizes can affect numerical stability. Very small blocks (dozens of rows) may create ill-conditioned subproblems—matrices where numerical algorithms struggle with rounding errors. Very large blocks approaching memory limits may cause intermittent failures when memory fragmentation prevents allocation despite “enough” RAM theoretically existing.\nThe practical guidance: stay in reasonable ranges. Blocks with 1,000-10,000 rows typically work well. Below 500 rows per block, watch for numerical warnings. Above 50,000 rows per block, ensure you have substantial memory headroom. BigDataStatMeth’s algorithms handle edge cases, but respecting these guidelines avoids potential issues."
  },
  {
    "objectID": "technical/performance.html#quick-start-decision-tree",
    "href": "technical/performance.html#quick-start-decision-tree",
    "title": "Performance Optimization",
    "section": "3 Quick Start: Decision Tree",
    "text": "3 Quick Start: Decision Tree\nBefore diving into theory, here’s a practical decision tree for choosing starting configurations. These values aren’t optimal—they’re reasonable starting points based on common scenarios. Always profile and tune for your specific case.\n\n\n\n\n\nflowchart TD\n    Start[Start: Choose Configuration] --&gt; DataSize{Dataset Size?}\n    \n    DataSize --&gt;|&lt; 10 GB| Small[Small Dataset]\n    DataSize --&gt;|10-100 GB| Medium[Medium Dataset]\n    DataSize --&gt;|&gt; 100 GB| Large[Large Dataset]\n    \n    Small --&gt; SmallMem{Available RAM?}\n    SmallMem --&gt;|&gt; 16 GB| SmallConfig[k=2, q=1&lt;br/&gt;threads=4]\n    SmallMem --&gt;|&lt; 16 GB| SmallConfigMem[k=4, q=1&lt;br/&gt;threads=2]\n    \n    Medium --&gt; MediumMem{Available RAM?}\n    MediumMem --&gt;|&gt; 32 GB| MediumConfig[k=4, q=1&lt;br/&gt;threads=8]\n    MediumMem --&gt;|16-32 GB| MediumConfigMed[k=8, q=1&lt;br/&gt;threads=4]\n    MediumMem --&gt;|&lt; 16 GB| MediumConfigLow[k=16, q=2&lt;br/&gt;threads=2]\n    \n    Large --&gt; LargeMem{Available RAM?}\n    LargeMem --&gt;|&gt; 64 GB| LargeConfig[k=8, q=2&lt;br/&gt;threads=8]\n    LargeMem --&gt;|&lt; 64 GB| LargeConfigMem[k=32, q=2&lt;br/&gt;threads=4]\n    \n    SmallConfig --&gt; Profile[Profile & Tune Iteratively]\n    SmallConfigMem --&gt; Profile\n    MediumConfig --&gt; Profile\n    MediumConfigMed --&gt; Profile\n    MediumConfigLow --&gt; Profile\n    LargeConfig --&gt; Profile\n    LargeConfigMem --&gt; Profile\n    \n    style Start fill:#f0f8ff\n    style Profile fill:#e8f6e8\n\n\n\n\n\n\nHow to use this tree:\nStart by estimating your dataset size in GB: (rows × cols × 8 bytes) / (1024^3). Then follow the tree based on your available RAM. The resulting k, q, and threads values are starting points—safe configurations unlikely to crash but not necessarily optimal. After establishing this baseline, profile to identify bottlenecks and tune iteratively.\nFor example, if you have a 50GB dataset and 32GB RAM, the tree suggests k=8, q=1, threads=8. This configuration splits data into 8 blocks (each ~6.25GB), processes in a single hierarchy level, and uses 8 threads. Peak memory usage will be roughly 8-10GB (safe for 32GB RAM), and 8 threads should parallelize well on modern CPUs. But your specific data might benefit from k=4 (fewer blocks, faster) or k=16 (more blocks if memory pressure appears). The tree gets you started; profiling reveals improvements."
  },
  {
    "objectID": "technical/performance.html#key-parameters-deep-dive",
    "href": "technical/performance.html#key-parameters-deep-dive",
    "title": "Performance Optimization",
    "section": "4 Key Parameters Deep Dive",
    "text": "4 Key Parameters Deep Dive\n\n4.1 Block Size: The k Parameter\nThe k parameter fundamentally controls BigDataStatMeth’s memory-speed tradeoff. Understanding k deeply helps you tune confidently.\nWhat k does mechanically: For a matrix with n rows, setting k=4 creates 4 blocks of approximately n/4 rows each (the last block may be slightly larger if n doesn’t divide evenly). Each block is processed sequentially or in parallel depending on the operation. Results from blocks are combined—summed, stacked, or merged—to produce final output.\nMemory impact: Peak memory usage equals approximately the size of the largest block plus overhead for intermediate results. If your matrix is 100,000 rows × 20,000 columns (16GB), setting k=4 creates blocks of ~25,000 × 20,000 (4GB each). Peak usage will be roughly 5-6GB (one block + overhead). Doubling k to 8 halves block size to 2GB and reduces peak memory to 3-4GB. This linear relationship makes memory management straightforward: if hitting memory limits, double k.\nSpeed impact: More blocks mean more operations. Consider matrix multiplication: A × B with A split into 4 blocks requires 4 block multiplications then combining results. With 8 blocks, you need 8 block multiplications. Pure computation time increases roughly linearly with k. But I/O time (reading from HDF5, writing results) also increases—more blocks mean more separate read/write operations, each with overhead. For operations where I/O dominates (simple additions, subtractions), doubling k might increase total time by 80-100%. For computation-heavy operations (SVD, matrix inversion), time increases by 20-40%.\nChoosing k strategically:\nStart with this formula:\n# Calculate reasonable k based on available memory\ndataset_size_gb &lt;- (n_rows * n_cols * 8) / (1024^3)\navailable_gb &lt;- 16  # Your available RAM\nsafety_factor &lt;- 0.7  # Use 70% of available RAM (headroom for overhead)\n\nk_minimum &lt;- ceiling(dataset_size_gb / (available_gb * safety_factor))\nk_safe &lt;- max(2, k_minimum)  # Never use k=1 (no blocking benefits)\n\ncat(\"Recommended k:\", k_safe, \"\\n\")\nThis formula ensures your blocks fit comfortably in memory. But you might adjust:\n\nIf memory is abundant: Reduce k below the calculated value for speed\nIf computation is very slow: Try k/2 to reduce overhead (if memory allows)\nIf memory pressure appears: Increase k to provide more headroom\nIf using many threads: Ensure k ≥ threads (otherwise threads sit idle)\n\nGuidelines by use case:\n\n\n\n\n\n\n\n\nScenario\nRecommended k\nReasoning\n\n\n\n\nExploratory analysis, abundant RAM\nk = 2-4\nMinimize overhead, iterate quickly\n\n\nProduction analysis, moderate RAM\nk = 8-16\nBalance memory safety and speed\n\n\nMemory-constrained server\nk = 16-32\nGuarantee successful completion\n\n\nRepeated analysis on same data\nk = 4-8\nModerate value, tune based on profiling\n\n\n\nRemember: these are starting points. Profile your actual workflow to find your optimal k.\n\n\n4.2 Hierarchy Levels: The q Parameter\nThe q parameter controls hierarchy depth in multi-level algorithms, primarily block-wise SVD. Understanding when hierarchy helps requires grasping how block results combine.\nSingle-level (q=1) approach: Compute SVD on each of k blocks independently, producing k sets of singular vectors. Then combine all k results simultaneously—stack the k vectors, compute SVD of the combined matrix. This final SVD operates on a k×p matrix where p is the number of retained components. For k=8 and p=50, the final SVD processes an 8×50 matrix—tiny, fast.\nTwo-level (q=2) approach: Compute SVD on each of k blocks (first level). Group results into pairs or quadruplets, compute SVD on each group (second level). Finally, combine group results (top level). For k=16, first level produces 16 results, second level combines these into 4 intermediate results, top level combines those 4 into final output.\nWhen does hierarchy help?\nHierarchy reduces peak memory during combination. With q=1 and k=32, combining 32 results simultaneously might require substantial memory—allocating arrays for 32 matrices, stacking them, computing SVD. With q=2, you combine 32 results in groups of 4 (8 groups), then combine 8 intermediate results—peak memory is lower because you never hold all 32 original results simultaneously.\nThe cost: more intermediate operations. Two-level hierarchy does roughly 1.3-1.5× as much computation as single-level. Three-level increases this to 1.5-2×. Unless memory-constrained, extra computation isn’t worthwhile.\nDecision guide:\n\nq=1: Default for k ≤ 8, or when memory isn’t limiting\nq=2: Use when k &gt; 8 AND memory is tight (less than 2× block size available)\nq&gt;2: Rarely needed; only for extreme cases with k &gt; 32 and severe memory limits\n\nExample decision: You have k=16 blocks and 16GB RAM. Each block uses ~3GB. With q=1, combining 16 results needs perhaps 6GB peak (manageable). With q=2, peak is ~4GB (safer but slower). If your system has 32GB RAM, q=1 is fine—extra headroom makes hierarchy unnecessary. If RAM is exactly 16GB and other processes use 8GB, q=2 provides safety margin.\n\n\n4.3 Thread Count: Parallel Processing\nThreading enables processing multiple blocks simultaneously, dramatically reducing wall-clock time. But effective threading requires understanding both software parallelism and hardware limitations.\nHow BigDataStatMeth uses threads: When you specify threads=4, eligible operations distribute work across 4 CPU cores. For block-wise operations, different blocks compute in parallel—while core 1 processes block 1, core 2 handles block 2, etc. For matrix operations using BLAS/LAPACK (multiplication, decompositions), underlying libraries may parallelize internally. Not all operations parallelize equally: some are inherently sequential, others embarrassingly parallel.\nWhy more threads isn’t always better:\nThis is crucial to understand. Adding threads only helps when:\n\nOperations are parallelizable: Sequential algorithms gain nothing from threads\nEnough independent work exists: With k=4 blocks and threads=8, 4 threads sit idle\nComputation dominates I/O: If most time is disk access, parallel computation doesn’t help\nMemory bandwidth suffices: Multiple cores reading simultaneously can saturate bandwidth\nCPU cores are available: Oversubscription (more threads than cores) causes thrashing\n\nUnderstanding bottlenecks: Three fundamental bottlenecks limit threading efficiency:\nI/O Bottleneck: Hard drives (even SSDs) have finite bandwidth. If 4 threads each try to read 1GB/second from a disk with 2GB/second total bandwidth, they compete—each gets only 500MB/second. Adding more threads doesn’t increase total bandwidth; it just divides it further. This is why simple operations (addition, scaling) often show poor threading efficiency—computation is fast, but reading data from HDF5 is slow. All threads wait for I/O.\nMemory Bandwidth Bottleneck: Modern CPUs are fast, but RAM is comparatively slow. If 8 threads each try to read 8GB/second from RAM with 25GB/second total bandwidth, they get ~3GB/second each—less than needed. Memory-intensive operations (large matrix multiplications, cache-unfriendly access patterns) can saturate memory bandwidth. Adding more threads can actually slow things down by increasing cache misses and memory contention.\nAmdahl’s Law (Inherent Sequentiality): Some parts of algorithms cannot parallelize. If 20% of your computation is inherently sequential, maximum possible speedup is 5× regardless of thread count. With 4 threads, you might achieve 3.2× speedup (80% of theoretical maximum). With 8 threads, maybe 3.8× speedup—adding 4 threads gave only 0.6× improvement because sequential portions dominate.\nMeasuring threading efficiency:\nlibrary(microbenchmark)\n\n# Measure speedup with different thread counts\nmb &lt;- microbenchmark(\n  t1 = bdSVD_hdf5(\"data.hdf5\", \"data\", \"matrix\", k=8, threads=1),\n  t2 = bdSVD_hdf5(\"data.hdf5\", \"data\", \"matrix\", k=8, threads=2),\n  t4 = bdSVD_hdf5(\"data.hdf5\", \"data\", \"matrix\", k=8, threads=4),\n  t8 = bdSVD_hdf5(\"data.hdf5\", \"data\", \"matrix\", k=8, threads=8),\n  times = 3\n)\n\n# Calculate speedup and efficiency\ntimes &lt;- summary(mb)$median\nspeedup &lt;- times[1] / times\nefficiency &lt;- speedup / c(1, 2, 4, 8)\n\nprint(data.frame(\n  threads = c(1, 2, 4, 8),\n  time_sec = round(times, 2),\n  speedup = round(speedup, 2),\n  efficiency = round(efficiency, 2)\n))\nInterpreting efficiency:\n\n90-100% efficiency: Ideal scaling, rare in practice\n70-90% efficiency: Excellent scaling, computation dominates\n50-70% efficiency: Good scaling, acceptable for most use cases\n30-50% efficiency: Moderate scaling, likely hitting I/O or memory bandwidth\n&lt;30% efficiency: Poor scaling, fundamental bottleneck exists\n\nIf you see 4 threads achieving only 1.5× speedup (37% efficiency), investigate:\n\nIs operation I/O-heavy? (Check if system time &gt;&gt; user time)\nIs memory bandwidth saturated? (Monitor with tools like htop or iostat)\nIs BLAS library already parallel? (Check with sessionInfo())\nAre blocks too small? (Very small blocks have high overhead relative to computation)\n\nPractical thread guidelines:\n\n\n\n\n\n\n\n\nHardware\nRecommended Threads\nNotes\n\n\n\n\nLaptop (4 cores)\nthreads = 2-4\nUse all cores unless thermal throttling appears\n\n\nWorkstation (8-16 cores)\nthreads = 4-8\nStart with half, increase if efficiency good\n\n\nShared server (32+ cores)\nthreads = n_cores / n_users\nBe considerate of others\n\n\nCloud instance (variable)\nthreads = physical_cores\nAvoid hyperthreads for consistency\n\n\nHigh-memory, low-core\nthreads = n_cores\nCPU-bound, use all available\n\n\n\nBLAS library interaction:\nBigDataStatMeth uses BLAS/LAPACK for matrix operations. Some BLAS implementations (Intel MKL, OpenBLAS) are multi-threaded by default. If BLAS uses 4 threads and you specify threads=4, you may have 16 total threads (4 × 4), causing oversubscription.\nCheck BLAS configuration:\nsessionInfo()\n# Look for BLAS/LAPACK implementation\n# If using OpenBLAS or MKL, they may be threaded\n\n# Limit BLAS threads if needed (Linux/Mac)\nSys.setenv(OPENBLAS_NUM_THREADS = 1)\nSys.setenv(MKL_NUM_THREADS = 1)\nIf BLAS is single-threaded (reference BLAS), BigDataStatMeth’s threading provides all parallelism. If BLAS is multi-threaded, consider reducing BigDataStatMeth threads to avoid oversubscription.\n\n\n4.4 HDF5 Chunk Size\nHDF5 stores data in chunks—contiguous blocks on disk. Chunk size dramatically affects I/O performance but is set when creating datasets, not when computing. If you control dataset creation (via bdCreate_hdf5_matrix), chunk size becomes a key optimization lever.\nHow chunking affects I/O: When you read a single element from HDF5, the entire chunk containing it loads into memory. Read 100 elements scattered across 100 different chunks, and HDF5 performs 100 disk seeks, loading 100 chunks. Read 100 elements all in one chunk, and HDF5 performs 1 disk seek, loading 1 chunk. Sequential access to well-chunked data is fast; random access to poorly-chunked data is slow.\nBigDataStatMeth access patterns:\nMost BigDataStatMeth operations access data in rectangular blocks—reading several rows and all columns, or all rows and several columns. Optimal chunking aligns chunks with typical access blocks.\n\nRow-wise operations (processing samples): Chunk by rows. If reading 1,000 rows at a time, chunk size ~1,000 rows × all columns\nColumn-wise operations (processing features): Chunk by columns. If reading 100 columns at a time, chunk size ~all rows × 100 columns\nBlock operations (typical for BigDataStatMeth): Square-ish chunks work well. For 100,000 × 10,000 matrix read in blocks of 10,000 × 10,000, use 10,000 × 10,000 chunks\n\nChunk size guidelines:\n\nTarget 1-10 MB chunks: Very small chunks (&lt;100 KB) have excessive overhead. Very large chunks (&gt;100 MB) waste space and time loading unused data.\nMatch access pattern: If you always read full rows, make chunks span full rows. If you read columns, span full columns.\nConsider compression: Larger chunks compress better (more data for compression algorithms to find patterns). If using compression, prefer larger chunks (5-10 MB).\nBalance with cache: Chunks should fit comfortably in CPU cache (L3 cache often 8-16 MB). Chunks larger than cache cause cache thrashing.\n\nExample chunking decisions:\n# Genomics: 50,000 samples × 500,000 SNPs\n# Typical access: process 5,000 samples at a time (row-wise)\nbdCreate_hdf5_matrix(\n  filename = \"genomics.hdf5\",\n  object = genotype_matrix,\n  group = \"data\",\n  dataset = \"snps\",\n  chunk_rows = 5000,      # Matches block size\n  chunk_cols = 500000     # All columns\n)\n\n# Gene expression: 100 samples × 20,000 genes\n# Typical access: analyze all samples for subset of genes (column-wise)\nbdCreate_hdf5_matrix(\n  filename = \"expression.hdf5\",\n  object = expression_matrix,\n  group = \"data\",\n  dataset = \"genes\",\n  chunk_rows = 100,       # All samples\n  chunk_cols = 2000       # ~10 chunks column-wise\n)\n\n# General matrix: 10,000 × 10,000\n# Typical access: square blocks\nbdCreate_hdf5_matrix(\n  filename = \"matrix.hdf5\",\n  object = large_matrix,\n  group = \"data\",\n  dataset = \"values\",\n  chunk_rows = 1000,      # Square chunks\n  chunk_cols = 1000       # ~100 total chunks\n)\nWhen chunk size matters most:\n\nVery large datasets (&gt;10 GB) where I/O is significant fraction of time\nRepeated access to same data (poor chunking penalizes every access)\nNetwork storage (NFS, cloud) where disk seeks are expensive\nCompression enabled (larger chunks compress better)\n\nWhen chunk size matters less:\n\nSmall datasets (&lt;1 GB) that fit entirely in cache\nSingle-pass operations (read once, compute, done)\nLocal SSD storage where seeks are cheap\nComputation-heavy operations where I/O is &lt;10% of time\n\nIf profiling shows system time (I/O) is &gt;30% of elapsed time, investigate chunking. Otherwise, default chunking is likely fine."
  },
  {
    "objectID": "technical/performance.html#profiling-finding-the-bottleneck",
    "href": "technical/performance.html#profiling-finding-the-bottleneck",
    "title": "Performance Optimization",
    "section": "5 Profiling: Finding the Bottleneck",
    "text": "5 Profiling: Finding the Bottleneck\nBefore optimizing, you must identify where time actually goes. Optimizing the wrong part of your code wastes effort and provides minimal benefit. Profiling reveals the truth.\n\n5.1 Why Profile First\nIntuition about bottlenecks is often wrong. You might assume SVD computation dominates, but profiling reveals 60% of time is normalization. You might think I/O is the problem, but profiling shows memory allocation overhead. Without measurement, optimization is guesswork.\nThe profiling workflow:\n\nEstablish baseline: Run with conservative parameters, measure total time\nProfile: Identify which operations consume time\nPrioritize: Focus on operations using &gt;20% of total time\nOptimize: Tune those specific operations\nVerify: Ensure optimization helped and results remain correct\nRepeat: Profile again to find next bottleneck\n\n\n\n5.2 Basic Timing with system.time()\nThe simplest profiling tool is system.time():\ntiming &lt;- system.time({\n  result &lt;- bdPCA_hdf5(\n    filename = \"genomics.hdf5\",\n    group = \"data\",\n    dataset = \"genotypes\",\n    k = 8,\n    q = 1,\n    threads = 4\n  )\n})\n\nprint(timing)\n#    user  system elapsed \n#  45.231   2.109  12.847\nInterpreting the output:\n\nuser: CPU time spent in R code and user-level functions. This is “computation time”—the sum of all CPU time across all threads. With 4 threads, user time can exceed elapsed time (4 threads × 10 seconds = 40 seconds user time in 10 seconds elapsed).\nsystem: CPU time spent in system calls—primarily I/O (reading/writing files), memory allocation, and inter-process communication. High system time indicates I/O or memory management overhead.\nelapsed: Wall-clock time—what you experience. This is total time from start to finish, regardless of threading.\n\nCommon patterns and what they mean:\n# Pattern 1: user &gt;&gt; elapsed (e.g., user=45, elapsed=13)\n# Good parallelization! 4 threads did 45 seconds of work in 13 seconds\n# Speedup: 45/13 = 3.46× with 4 threads (87% efficiency)\n\n# Pattern 2: system high (e.g., user=20, system=15, elapsed=35)\n# I/O bottleneck! System time (15s) is 43% of elapsed time\n# Solution: Optimize HDF5 chunk size, use faster storage, or cache data\n\n# Pattern 3: elapsed &gt;&gt; user + system (e.g., user=10, system=2, elapsed=40)\n# Waiting for something! CPU idle most of the time\n# Possible causes: disk very slow, network storage, memory swapping\n# Check: iostat, iotop, or system monitor during execution\n\n# Pattern 4: user ≈ elapsed, threads=4 (e.g., user=15, elapsed=14)\n# No parallelization benefit! Work is sequential or I/O-bound\n# Solution: Reduce threads (they're not helping), optimize sequential portions\n\n\n5.3 Detailed Profiling with Rprof()\nFor deeper insight, use Rprof() to identify which specific functions consume time:\n# Start profiling\nRprof(\"pca_profile.out\", memory.profiling = TRUE)\n\n# Run your analysis\nresult &lt;- bdPCA_hdf5(\n  filename = \"large_data.hdf5\",\n  group = \"data\",\n  dataset = \"matrix\",\n  k = 16,\n  q = 2,\n  threads = 8\n)\n\n# Stop profiling\nRprof(NULL)\n\n# Analyze results\nprofile_summary &lt;- summaryRprof(\"pca_profile.out\")\n\n# Print time spent in each function\nprint(profile_summary$by.self)\nExample output and interpretation:\n           self.time self.pct total.time total.pct\nbdSVD_hdf5      24.5     48.5       35.2      69.8\nbdNormalize     12.3     24.4       12.8      25.4\nh5read           8.2     16.3        8.2      16.3\ncrossprod        3.1      6.1        3.1       6.1\nThis output tells a clear story:\n\nbdSVD_hdf5 (48.5% self time): SVD computation dominates—this is the primary optimization target\nbdNormalize (24.4% self time): Normalization takes 1/4 of total time—consider pre-normalizing if running multiple analyses\nh5read (16.3% self time): I/O is significant but not dominant—chunking optimization might help but won’t transform performance\ncrossprod (6.1% self time): Minor contributor—optimizing this won’t noticeably improve total time\n\nMemory profiling insights:\n# Memory allocation by function\nprint(profile_summary$by.total[1:10, c(\"total.time\", \"mem.total\")])\nShows which functions allocate most memory. High memory allocation indicates where increasing k (more blocks) would help most.\n\n\n5.4 Visual Profiling with profvis\nFor interactive exploration, profvis provides flamegraphs showing time distribution visually:\nlibrary(profvis)\n\nprofvis({\n  bdSVD_hdf5(\n    filename = \"data.hdf5\",\n    group = \"data\",\n    dataset = \"matrix\",\n    k = 8,\n    threads = 4\n  )\n})\nThe interactive visualization shows:\n\nHorizontal axis: Time from start to finish\nVertical axis: Call stack (functions calling functions)\nWidth of bars: Time spent in each function\nColors: Different colors for different code sections\n\nClick on bars to zoom in, hover to see details. Look for:\n\nWide bars: Functions consuming significant time (optimization targets)\nDeep stacks: Many nested function calls (potential overhead)\nMemory spikes: Sudden memory allocations (candidates for blocking)"
  },
  {
    "objectID": "technical/performance.html#optimization-workflows",
    "href": "technical/performance.html#optimization-workflows",
    "title": "Performance Optimization",
    "section": "6 Optimization Workflows",
    "text": "6 Optimization Workflows\nDifferent bottlenecks require different optimization strategies. These workflows guide you through systematic tuning.\n\n6.1 Memory-Constrained Environment\nSymptoms you’re memory-constrained:\n\nFrequent swapping (system very slow, disk activity constant)\n“Cannot allocate vector of size X” errors\nR crashes with memory-related errors\nSystem monitor shows RAM usage at 95-100%\n\nThe optimization strategy:\n\n\n\n\n\nflowchart TD\n    A[Memory Issues Detected] --&gt; B[Increase k by 2x]\n    B --&gt; C{Issues resolved?}\n    C --&gt;|Yes| H[Success: Monitor future runs]\n    C --&gt;|No| D[Add hierarchy: q=2]\n    D --&gt; E{Issues resolved?}\n    E --&gt;|Yes| H\n    E --&gt;|No| F[Reduce threads by half]\n    F --&gt; G{Issues resolved?}\n    G --&gt;|Yes| H\n    G --&gt;|No| I[Process in stages OR&lt;br/&gt;upgrade hardware]\n    \n    style A fill:#ffe8e8\n    style H fill:#e8f6e8\n    style I fill:#fff8e1\n\n\n\n\n\n\nStep-by-step implementation:\nStep 1: Increase block count (k)\nDoubling k halves memory usage. If currently using k=4 and hitting memory limits, try k=8:\n# Original configuration (memory issues)\nresult &lt;- bdPCA_hdf5(\n  filename = \"large_data.hdf5\",\n  group = \"data\",\n  dataset = \"matrix\",\n  k = 4,\n  q = 1,\n  threads = 4\n)\n# Error or very slow due to swapping\n\n# Optimized: Double k\nresult &lt;- bdPCA_hdf5(\n  filename = \"large_data.hdf5\",\n  group = \"data\",\n  dataset = \"matrix\",\n  k = 8,    # Doubled\n  q = 1,\n  threads = 4\n)\n# Should complete successfully, though slower\nStep 2: Add hierarchy if still problematic\nIf k=8 still causes issues, add q=2 to reduce peak memory during combination:\nresult &lt;- bdPCA_hdf5(\n  filename = \"large_data.hdf5\",\n  group = \"data\",\n  dataset = \"matrix\",\n  k = 8,\n  q = 2,    # Added hierarchy\n  threads = 4\n)\nStep 3: Reduce threading if memory pressure persists\nMore threads means more blocks loaded simultaneously. Reducing threads decreases parallel memory pressure:\nresult &lt;- bdPCA_hdf5(\n  filename = \"large_data.hdf5\",\n  group = \"data\",\n  dataset = \"matrix\",\n  k = 8,\n  q = 2,\n  threads = 2    # Reduced from 4\n)\nStep 4: Process in stages\nIf still problematic, break analysis into smaller stages, writing intermediate results to HDF5:\n# Stage 1: Normalize (writes to HDF5)\nbdNormalize_hdf5(\n  filename = \"large_data.hdf5\",\n  group = \"data\",\n  dataset = \"matrix\",\n  bcenter = TRUE,\n  bscale = FALSE\n)\n\n# Stage 2: QR decomposition (done internally, writes to HDF5)\n# Stage 3: SVD on reduced data (smaller working set)\nresult &lt;- bdPCA_hdf5(\n  filename = \"large_data.hdf5\",\n  group = \"NORMALIZED/data\",\n  dataset = \"matrix\",\n  bcenter = FALSE,  # Already normalized\n  bscale = FALSE,\n  k = 8,\n  q = 2,\n  threads = 2\n)\nEach stage has smaller peak memory than attempting everything at once.\n\n\n6.2 Speed-Constrained Environment\nSymptoms you’re speed-constrained:\n\nComputation takes too long for your workflow\nMemory usage is well below capacity (30-50% of available RAM)\nWaiting for results interrupts iterative analysis\nSystem monitor shows RAM available but computation slow\n\nThe optimization strategy:\n\n\n\n\n\nflowchart TD\n    A[Speed Issues Detected] --&gt; B[Check memory usage]\n    B --&gt; C{RAM &lt; 70% used?}\n    C --&gt;|Yes| D[Decrease k by half]\n    D --&gt; E{Faster?}\n    E --&gt;|Yes| F[Increase threads]\n    F --&gt; G{Faster?}\n    G --&gt;|Yes| J[Optimize HDF5 chunking]\n    C --&gt;|No| H[Memory limiting speed&lt;br/&gt;Use balanced approach]\n    E --&gt;|No| I[Check I/O bottleneck]\n    G --&gt;|No| K[Check parallelization limits]\n    J --&gt; L[Success: Optimal configuration]\n    \n    style A fill:#fff8e1\n    style L fill:#e8f6e8\n\n\n\n\n\n\nStep-by-step implementation:\nStep 1: Reduce block count if memory allows\nFewer blocks mean less overhead. If memory usage is low, try reducing k:\n# Current: Slow but plenty of RAM available\nresult &lt;- bdPCA_hdf5(\"data.hdf5\", \"data\", \"matrix\", \n                     k=8, threads=4)\n# Elapsed: 45 seconds, RAM usage: 8GB / 32GB available\n\n# Optimized: Fewer blocks\nresult &lt;- bdPCA_hdf5(\"data.hdf5\", \"data\", \"matrix\",\n                     k=4, threads=4)  # Halved k\n# Elapsed: 28 seconds (38% faster)\nStep 2: Increase thread count\nMore threads reduce wall-clock time for parallelizable operations:\n# Check available cores\nn_cores &lt;- parallel::detectCores()\ncat(\"Available cores:\", n_cores, \"\\n\")\n\n# Use more threads\nresult &lt;- bdPCA_hdf5(\"data.hdf5\", \"data\", \"matrix\",\n                     k=4, threads=n_cores)\n# Measure speedup vs fewer threads\nMonitor efficiency—if adding threads provides minimal speedup, you’ve hit a bottleneck (I/O, memory bandwidth, or Amdahl’s law).\nStep 3: Pre-normalize data\nIf running multiple analyses on the same data, normalize once and reuse:\n# Normalize once\nbdNormalize_hdf5(\"data.hdf5\", \"data\", \"matrix\",\n                 bcenter=TRUE, bscale=FALSE)\n\n# Run analyses without re-normalizing\nfor (param in parameter_grid) {\n  result &lt;- bdPCA_hdf5(\n    filename = \"data.hdf5\",\n    group = \"NORMALIZED/data\",\n    dataset = \"matrix\",\n    bcenter = FALSE,  # Skip normalization\n    bscale = FALSE,\n    k = 4,\n    threads = 8\n    # Other parameters vary\n  )\n  # Process result\n}\nThis eliminates redundant normalization across runs, saving 20-40% of time in iterative workflows.\nStep 4: Optimize HDF5 chunk size\nIf profiling shows significant I/O time, recreate dataset with optimal chunking:\n# Read existing data\noriginal_data &lt;- h5read(\"data.hdf5\", \"data/matrix\")\n\n# Recreate with optimized chunks\nbdCreate_hdf5_matrix(\n  filename = \"data_optimized.hdf5\",\n  object = original_data,\n  group = \"data\",\n  dataset = \"matrix\",\n  chunk_rows = nrow(original_data) / 4,  # Match k=4\n  chunk_cols = ncol(original_data),\n  compression = 0  # Disable compression for max speed\n)\n\n# Use optimized dataset\nresult &lt;- bdPCA_hdf5(\"data_optimized.hdf5\", \"data\", \"matrix\",\n                     k=4, threads=8)\nOptimal chunking can reduce I/O time by 50-80% for I/O-heavy operations.\n\n\n6.3 Balanced Optimization: Iterative Tuning\nWhen neither memory nor speed is catastrophically limiting, tune iteratively to find the sweet spot:\nPhase 1: Establish baseline\n# Conservative configuration (guaranteed to work)\ncat(\"Testing baseline configuration...\\n\")\nbaseline_time &lt;- system.time({\n  baseline_result &lt;- bdPCA_hdf5(\n    filename = \"data.hdf5\",\n    group = \"data\",\n    dataset = \"matrix\",\n    k = 8,\n    q = 1,\n    threads = 2\n  )\n})\n\ncat(\"Baseline time:\", baseline_time[\"elapsed\"], \"seconds\\n\")\ncat(\"Peak memory: Check system monitor\\n\")\nPhase 2: Test reduced blocking\ncat(\"Testing k=4 (fewer blocks)...\\n\")\ntest1_time &lt;- system.time({\n  test1_result &lt;- bdPCA_hdf5(\n    filename = \"data.hdf5\",\n    group = \"data\",\n    dataset = \"matrix\",\n    k = 4,  # Halved\n    q = 1,\n    threads = 2\n  )\n})\n\nimprovement1 &lt;- (baseline_time[\"elapsed\"] - test1_time[\"elapsed\"]) / \n                baseline_time[\"elapsed\"] * 100\n\ncat(\"k=4 time:\", test1_time[\"elapsed\"], \"seconds\\n\")\ncat(\"Improvement:\", round(improvement1, 1), \"%\\n\")\nPhase 3: Test increased threading\ncat(\"Testing threads=4 (more parallelism)...\\n\")\ntest2_time &lt;- system.time({\n  test2_result &lt;- bdPCA_hdf5(\n    filename = \"data.hdf5\",\n    group = \"data\",\n    dataset = \"matrix\",\n    k = 4,\n    q = 1,\n    threads = 4  # Doubled\n  )\n})\n\nimprovement2 &lt;- (test1_time[\"elapsed\"] - test2_time[\"elapsed\"]) / \n                test1_time[\"elapsed\"] * 100\n\ncat(\"threads=4 time:\", test2_time[\"elapsed\"], \"seconds\\n\")\ncat(\"Additional improvement:\", round(improvement2, 1), \"%\\n\")\nPhase 4: Verify and select\n# Verify results match baseline\ncat(\"Verifying correctness...\\n\")\nall.equal(\n  baseline_result$components[,1:5],\n  test2_result$components[,1:5],\n  tolerance = 1e-10\n)\n\n# Choose best configuration\nconfigurations &lt;- data.frame(\n  config = c(\"Baseline\", \"k=4\", \"k=4,t=4\"),\n  time = c(baseline_time[\"elapsed\"], \n           test1_time[\"elapsed\"],\n           test2_time[\"elapsed\"]),\n  speedup = c(1.0,\n              baseline_time[\"elapsed\"] / test1_time[\"elapsed\"],\n              baseline_time[\"elapsed\"] / test2_time[\"elapsed\"])\n)\n\nprint(configurations)\n\ncat(\"\\nRecommended configuration: k=4, threads=4\\n\")\nThis systematic approach finds good configurations without exhaustive testing."
  },
  {
    "objectID": "technical/performance.html#common-performance-pitfalls",
    "href": "technical/performance.html#common-performance-pitfalls",
    "title": "Performance Optimization",
    "section": "7 Common Performance Pitfalls",
    "text": "7 Common Performance Pitfalls\nExperience reveals recurring mistakes that degrade performance. Recognizing these patterns helps you avoid them.\n\n7.1 Pitfall 1: Excessive Blocking\nSymptom: Computation is very slow despite low memory usage (30-40% of RAM used).\nCause: Block count (k) is much larger than necessary, creating excessive overhead from operations and I/O.\nWhy this happens: Overly conservative memory estimates or copying someone else’s configuration without adapting to your hardware.\nExample:\n# Poor configuration\nsystem.time({\n  result &lt;- bdSVD_hdf5(\"data.hdf5\", \"data\", \"small\", k=32)\n})\n#   elapsed: 45 seconds\n# Dataset: 10,000×5,000 (400 MB)\n# Available RAM: 16 GB (plenty of headroom!)\n\n# Good configuration\nsystem.time({\n  result &lt;- bdSVD_hdf5(\"data.hdf5\", \"data\", \"small\", k=2)\n})\n#   elapsed: 8 seconds (82% faster)\nFix: Calculate appropriate k based on actual memory needs:\n# Calculate dataset size\nn_rows &lt;- 10000\nn_cols &lt;- 5000\ndataset_size_gb &lt;- (n_rows * n_cols * 8) / (1024^3)\ncat(\"Dataset size:\", round(dataset_size_gb, 2), \"GB\\n\")\n\n# Available RAM\navailable_gb &lt;- 16\nk_needed &lt;- ceiling(dataset_size_gb / (available_gb * 0.5))\ncat(\"Minimum k needed:\", k_needed, \"\\n\")\ncat(\"Recommended k:\", max(2, k_needed), \"\\n\")\nRule: Use smallest k that keeps memory usage comfortably below limits (60-70% of RAM).\n\n\n\n7.2 Pitfall 2: Thread Oversubscription\nSymptom: Adding more threads doesn’t improve performance or actually makes it worse.\nCause: More threads than physical CPU cores, causing context switching overhead and cache thrashing.\nWhy this happens: Confusing physical cores with hyperthreads/logical cores, or assuming “more threads = more speed.”\nExample:\n# System: 4 physical cores (8 hyperthreads)\n\n# Poor: threads=16 (4× oversubscription)\nsystem.time({\n  result &lt;- bdPCA_hdf5(\"data.hdf5\", \"data\", \"matrix\", \n                       k=8, threads=16)\n})\n#   elapsed: 25 seconds\n# Context switching overhead, cache thrashing\n\n# Good: threads=4 (matches physical cores)\nsystem.time({\n  result &lt;- bdPCA_hdf5(\"data.hdf5\", \"data\", \"matrix\",\n                       k=8, threads=4)\n})\n#   elapsed: 12 seconds (52% faster)\nUnderstanding the problem: When threads exceed physical cores, the operating system rapidly switches between them (context switching). Each switch: saves one thread’s state, loads another’s state, invalidates CPU caches. With 16 threads on 4 cores, each core switches between 4 threads. Switching costs accumulate, and cache efficiency plummets—data constantly evicted before use.\nFix: Match threads to physical cores:\n# Find physical cores (not hyperthreads)\nn_physical &lt;- parallel::detectCores(logical = FALSE)\ncat(\"Physical cores:\", n_physical, \"\\n\")\n\n# Use physical cores as upper bound\noptimal_threads &lt;- min(n_physical, 8)  # Cap at 8 for safety\nAdditional check—BLAS threading:\nSome BLAS libraries (MKL, OpenBLAS) use threading internally. If BLAS uses 4 threads and you specify threads=4 in BigDataStatMeth, you have 4×4=16 threads—oversubscription even with 16 physical cores.\n# Check BLAS configuration\nsessionInfo()\n# Look for \"BLAS: \" line\n\n# If multi-threaded BLAS, limit BigDataStatMeth threads\nSys.setenv(OPENBLAS_NUM_THREADS = 1)  # Disable BLAS threading\n# OR\n# Use fewer BigDataStatMeth threads to account for BLAS threading\n\n\n\n7.3 Pitfall 3: Redundant Operations\nSymptom: Repeated analyses (parameter sweeps, cross-validation) each take the same amount of time, including time for operations that don’t change between runs.\nCause: Not leveraging HDF5’s persistence to reuse computed results.\nWhy this happens: Treating each analysis as independent rather than recognizing shared computations.\nExample:\n# Poor: Normalize every iteration\nparameter_values &lt;- c(2, 5, 10, 20, 50)\n\nfor (n_components in parameter_values) {\n  result &lt;- bdPCA_hdf5(\n    filename = \"data.hdf5\",\n    group = \"data\",\n    dataset = \"raw\",\n    bcenter = TRUE,   # Normalizes every iteration\n    bscale = TRUE,\n    k = 4,\n    # n_components varies\n  )\n  # Process result\n}\n# Total time: 5 × (normalization + PCA) = 5 × 60s = 300s\n\n# Good: Normalize once, reuse\nbdNormalize_hdf5(\n  filename = \"data.hdf5\",\n  group = \"data\",\n  dataset = \"raw\",\n  bcenter = TRUE,\n  bscale = TRUE\n)\n\nfor (n_components in parameter_values) {\n  result &lt;- bdPCA_hdf5(\n    filename = \"data.hdf5\",\n    group = \"NORMALIZED/data\",  # Pre-normalized\n    dataset = \"raw\",\n    bcenter = FALSE,  # Skip normalization\n    bscale = FALSE,\n    k = 4,\n    # n_components varies\n  )\n  # Process result\n}\n# Total time: normalization + 5 × PCA = 50s + 5×10s = 100s (67% faster)\nUnderstanding the savings: If normalization takes 20% of total time and you run 10 analyses, normalizing once saves 9/10 × 20% = 18% of total workflow time. For workflows with hundreds of runs (bootstrapping, cross-validation), savings multiply dramatically.\nFix strategy:\n\nIdentify operations that don’t change between runs\nPerform them once, store results in HDF5\nReuse stored results in subsequent runs\n\nApplies to normalization, QR decomposition (for CCA), centering, scaling—any operation deterministic and shared across runs."
  },
  {
    "objectID": "technical/performance.html#benchmarking-measuring-real-performance",
    "href": "technical/performance.html#benchmarking-measuring-real-performance",
    "title": "Performance Optimization",
    "section": "8 Benchmarking: Measuring Real Performance",
    "text": "8 Benchmarking: Measuring Real Performance\nTo validate optimizations, benchmark systematically with real data and hardware. The following framework provides structure for comprehensive performance testing.\n\n8.1 Systematic Benchmarking Framework\nlibrary(microbenchmark)\n\n# Define configurations to test\ntest_configs &lt;- expand.grid(\n  k = c(2, 4, 8, 16),\n  threads = c(1, 2, 4, 8),\n  stringsAsFactors = FALSE\n)\n\n# Run each configuration multiple times\nresults &lt;- lapply(seq_len(nrow(test_configs)), function(i) {\n  cfg &lt;- test_configs[i, ]\n  \n  cat(\"Testing k=\", cfg$k, \", threads=\", cfg$threads, \"...\\n\", sep=\"\")\n  \n  timing &lt;- system.time({\n    bdSVD_hdf5(\n      file = \"benchmark_data.hdf5\",\n      group = \"data\",\n      dataset = \"matrix\",\n      k = cfg$k,\n      threads = cfg$threads,\n      bcenter = TRUE,\n      bscale = FALSE\n    )\n  })\n  \n  data.frame(\n    k = cfg$k,\n    threads = cfg$threads,\n    user = timing[[\"user\"]],\n    system = timing[[\"system\"]],\n    elapsed = timing[[\"elapsed\"]]\n  )\n})\n\nbenchmark_df &lt;- do.call(rbind, results)\n\n# Analyze results\ncat(\"\\nBenchmark Results:\\n\")\nprint(benchmark_df)\n\n# Find optimal configuration\noptimal_idx &lt;- which.min(benchmark_df$elapsed)\noptimal &lt;- benchmark_df[optimal_idx, ]\n\ncat(\"\\nOptimal configuration:\\n\")\ncat(\"  k =\", optimal$k, \"\\n\")\ncat(\"  threads =\", optimal$threads, \"\\n\")\ncat(\"  elapsed time =\", round(optimal$elapsed, 2), \"seconds\\n\")\n\n\n8.2 Visualizing Performance\nlibrary(ggplot2)\n\n# Plot: k vs time for each thread count\nggplot(benchmark_df, aes(x=k, y=elapsed, color=factor(threads))) +\n  geom_line(size=1) +\n  geom_point(size=3) +\n  scale_x_log2(breaks=c(2,4,8,16)) +\n  scale_color_brewer(palette=\"Set1\", name=\"Threads\") +\n  labs(\n    title=\"SVD Performance: Block Size vs Thread Count\",\n    subtitle=paste(\"Dataset:\", nrow, \"×\", ncol, \"- Hardware:\", RAM, \"GB RAM\"),\n    x=\"Number of Blocks (k)\",\n    y=\"Elapsed Time (seconds)\"\n  ) +\n  theme_minimal(base_size=14) +\n  theme(legend.position=\"right\")\n\n\n8.3 Interpreting Benchmarks\nLook for these patterns:\n\nOptimal k: Time decreases as k decreases, then levels off or increases. The minimum is your optimal k for this dataset/hardware combination.\nThreading efficiency: Compare times across thread counts for fixed k. If doubling threads doesn’t halve time, you’re hitting parallelization limits.\nInteraction effects: Sometimes optimal k changes with thread count. With few threads, smaller k is best (less overhead). With many threads, larger k helps (more blocks to parallelize).\n\nExample interpretation:\nk=2,  threads=1: 48.3s\nk=2,  threads=4: 14.2s  (speedup: 3.4×, efficiency: 85%)\nk=8,  threads=4: 18.7s\nk=16, threads=4: 26.1s\n\nConclusion: k=2 optimal (data fits in memory), threads=4 good efficiency"
  },
  {
    "objectID": "technical/performance.html#real-world-case-study-1000-genomes-pca",
    "href": "technical/performance.html#real-world-case-study-1000-genomes-pca",
    "title": "Performance Optimization",
    "section": "9 Real-World Case Study: 1000 Genomes PCA",
    "text": "9 Real-World Case Study: 1000 Genomes PCA\nLet’s walk through complete optimization of a real analysis: PCA on 1000 Genomes data.\nDataset: 2,504 samples × 500 genes from 1000 Genomes Project\nHardware: Standard workstation (16 GB RAM, 4 physical cores)\nGoal: Optimize PCA to minimize analysis time while ensuring correctness\n\n9.1 Initial Baseline\n# Conservative starting configuration\ncat(\"=== Baseline Configuration ===\\n\")\nbaseline_time &lt;- system.time({\n  baseline &lt;- bdPCA_hdf5(\n    filename = \"1000G.hdf5\",\n    group = \"data\",\n    dataset = \"genotypes\",\n    bcenter = TRUE,\n    bscale = FALSE,\n    k = 8,    # Conservative blocking\n    q = 1,\n    threads = 2  # Conservative threading\n  )\n})\n\ncat(\"Elapsed time:\", baseline_time[\"elapsed\"], \"seconds\\n\")\ncat(\"User time:\", baseline_time[\"user\"], \"seconds\\n\")\ncat(\"System time:\", baseline_time[\"system\"], \"seconds\\n\\n\")\n\n# Typical output:\n#   Elapsed time: 23.45 seconds\n#   User time: 41.23 seconds\n#   System time: 3.12 seconds\nInitial observations:\n\nUser time &gt;&gt; elapsed time: Parallelization working (2 threads doing ~1.8× work)\nSystem time moderate (~13% of elapsed): I/O not dominant but noticeable\nMemory usage: ~4 GB peak (plenty of headroom with 16 GB available)\n\n\n\n9.2 Profile to Identify Bottlenecks\ncat(\"=== Profiling to Find Bottlenecks ===\\n\")\nRprof(\"pca_profile.out\")\n\nbdPCA_hdf5(\n  filename = \"1000G.hdf5\",\n  group = \"data\",\n  dataset = \"genotypes\",\n  bcenter = TRUE,\n  bscale = FALSE,\n  k = 8,\n  q = 1,\n  threads = 2\n)\n\nRprof(NULL)\n\n# Analyze profile\nprofile &lt;- summaryRprof(\"pca_profile.out\")\nprint(profile$by.self[1:10,])\n\n# Typical output shows:\n#   bdSVD_hdf5:      60% of time (primary bottleneck)\n#   bdNormalize:     25% of time (opportunity for pre-computation)\n#   h5read:          12% of time (I/O overhead)\n#   Other:           3% of time\nBottleneck identified: SVD computation dominates (60%), normalization is significant (25%), I/O is noticeable (12%).\nOptimization strategy based on profiling:\n\nReduce k to speed up SVD (we have memory headroom)\nIncrease threads to parallelize SVD better (4 cores available)\nPre-normalize data to eliminate repeated normalization\nConsider HDF5 chunking if I/O remains problematic after other optimizations\n\n\n\n9.3 Optimization 1: Reduce Block Count\ncat(\"=== Optimization 1: Reduce k ===\\n\")\nopt1_time &lt;- system.time({\n  opt1 &lt;- bdPCA_hdf5(\n    filename = \"1000G.hdf5\",\n    group = \"data\",\n    dataset = \"genotypes\",\n    bcenter = TRUE,\n    bscale = FALSE,\n    k = 4,    # Halved from 8\n    q = 1,\n    threads = 2\n  )\n})\n\ncat(\"Elapsed time:\", opt1_time[\"elapsed\"], \"seconds\\n\")\nimprovement1 &lt;- (baseline_time[\"elapsed\"] - opt1_time[\"elapsed\"]) / \n                baseline_time[\"elapsed\"] * 100\ncat(\"Improvement:\", round(improvement1, 1), \"%\\n\\n\")\n\n# Typical output:\n#   Elapsed time: 18.32 seconds\n#   Improvement: 21.9%\nWhy this helped: Reducing from k=8 to k=4 halves the number of block operations. SVD on 4 blocks is faster than SVD on 8 blocks. Memory usage increased from 4GB to 6GB—well within our 16GB capacity.\n\n\n9.4 Optimization 2: Increase Threading\ncat(\"=== Optimization 2: Increase threads ===\\n\")\nopt2_time &lt;- system.time({\n  opt2 &lt;- bdPCA_hdf5(\n    filename = \"1000G.hdf5\",\n    group = \"data\",\n    dataset = \"genotypes\",\n    bcenter = TRUE,\n    bscale = FALSE,\n    k = 4,\n    q = 1,\n    threads = 4  # Doubled from 2\n  )\n})\n\ncat(\"Elapsed time:\", opt2_time[\"elapsed\"], \"seconds\\n\")\nimprovement2 &lt;- (opt1_time[\"elapsed\"] - opt2_time[\"elapsed\"]) / \n                opt1_time[\"elapsed\"] * 100\ncat(\"Additional improvement:\", round(improvement2, 1), \"%\\n\")\ntotal_improvement &lt;- (baseline_time[\"elapsed\"] - opt2_time[\"elapsed\"]) / \n                     baseline_time[\"elapsed\"] * 100\ncat(\"Total improvement vs baseline:\", round(total_improvement, 1), \"%\\n\\n\")\n\n# Calculate threading efficiency\nspeedup &lt;- opt1_time[\"elapsed\"] / opt2_time[\"elapsed\"]\nefficiency &lt;- speedup / 2  # Doubled threads\ncat(\"Threading efficiency:\", round(efficiency * 100, 1), \"%\\n\\n\")\n\n# Typical output:\n#   Elapsed time: 12.84 seconds\n#   Additional improvement: 29.9%\n#   Total improvement vs baseline: 45.3%\n#   Threading efficiency: 71.2%\nWhy this helped: With k=4, we have 4 blocks that can compute in parallel. Using 4 threads (matching our 4 physical cores) provides good parallelization. Efficiency of 71% is solid—some overhead from coordination and I/O, but most computation parallelizes well.\n\n\n9.5 Optimization 3: Pre-normalize Data\ncat(\"=== Optimization 3: Pre-normalize ===\\n\")\n\n# Normalize once\nnorm_time &lt;- system.time({\n  bdNormalize_hdf5(\n    filename = \"1000G.hdf5\",\n    group = \"data\",\n    dataset = \"genotypes\",\n    bcenter = TRUE,\n    bscale = FALSE\n  )\n})\n\ncat(\"Normalization time (one-time cost):\", norm_time[\"elapsed\"], \"seconds\\n\")\n\n# Run PCA on pre-normalized data\nopt3_time &lt;- system.time({\n  opt3 &lt;- bdPCA_hdf5(\n    filename = \"1000G.hdf5\",\n    group = \"NORMALIZED/data\",\n    dataset = \"genotypes\",\n    bcenter = FALSE,  # Already normalized\n    bscale = FALSE,\n    k = 4,\n    q = 1,\n    threads = 4\n  )\n})\n\ncat(\"PCA time (using pre-normalized):\", opt3_time[\"elapsed\"], \"seconds\\n\")\nimprovement3 &lt;- (opt2_time[\"elapsed\"] - opt3_time[\"elapsed\"]) / \n                opt2_time[\"elapsed\"] * 100\ncat(\"Additional improvement:\", round(improvement3, 1), \"%\\n\")\n\nfinal_improvement &lt;- (baseline_time[\"elapsed\"] - opt3_time[\"elapsed\"]) / \n                     baseline_time[\"elapsed\"] * 100\ncat(\"FINAL improvement vs baseline:\", round(final_improvement, 1), \"%\\n\\n\")\n\n# Typical output:\n#   Normalization time: 4.23 seconds\n#   PCA time: 9.18 seconds\n#   Additional improvement: 28.5%\n#   FINAL improvement vs baseline: 60.9%\nWhy this helped: Profiling showed normalization consumed 25% of time. By pre-normalizing, we eliminate that 25% from each subsequent PCA run. If running only once, the one-time normalization cost negates some benefit. But for multiple runs (common in parameter tuning, cross-validation), savings multiply.\nAmortized analysis: If running 10 PCAs with different parameters:\n\nWithout pre-normalization: 10 × 12.84s = 128.4s total\nWith pre-normalization: 4.23s + 10 × 9.18s = 96.0s total (25% faster)\n\nFor 100 runs: 4.23s + 100 × 9.18s = 922s vs 1284s (28% faster)\n\n\n9.6 Verification of Correctness\ncat(\"=== Verifying Correctness ===\\n\")\n\n# Compare principal components\npc_match &lt;- all.equal(\n  baseline$components[, 1:10],\n  opt3$components[, 1:10],\n  tolerance = 1e-10\n)\n\ncat(\"Principal components match:\", pc_match, \"\\n\")\n\n# Compare variance explained\nvar_match &lt;- all.equal(\n  baseline$variance_prop[1:10],\n  opt3$variance_prop[1:10],\n  tolerance = 1e-10\n)\n\ncat(\"Variance explained matches:\", var_match, \"\\n\\n\")\n\n# Typical output:\n#   Principal components match: TRUE\n#   Variance explained matches: TRUE\nCritical step: Always verify optimization doesn’t change results. Numerical differences within tolerance (1e-10) are acceptable—floating-point arithmetic isn’t perfectly deterministic. But results should be essentially identical.\n\n\n9.7 Final Configuration Summary\ncat(\"=== Final Optimization Summary ===\\n\\n\")\n\nsummary_df &lt;- data.frame(\n  Configuration = c(\"Baseline\", \"Opt1: k=4\", \"Opt2: +threads\", \"Opt3: +prenorm\"),\n  Time_seconds = c(\n    baseline_time[\"elapsed\"],\n    opt1_time[\"elapsed\"],\n    opt2_time[\"elapsed\"],\n    opt3_time[\"elapsed\"]\n  ),\n  Speedup = c(\n    1.0,\n    baseline_time[\"elapsed\"] / opt1_time[\"elapsed\"],\n    baseline_time[\"elapsed\"] / opt2_time[\"elapsed\"],\n    baseline_time[\"elapsed\"] / opt3_time[\"elapsed\"]\n  ),\n  Config_details = c(\n    \"k=8, threads=2\",\n    \"k=4, threads=2\",\n    \"k=4, threads=4\",\n    \"k=4, threads=4, prenorm\"\n  )\n)\n\nprint(summary_df)\n\ncat(\"\\nFinal configuration:\\n\")\ncat(\"  - Pre-normalize data once\\n\")\ncat(\"  - k = 4 (balance speed/memory)\\n\")\ncat(\"  - threads = 4 (match physical cores)\\n\")\ncat(\"  - Result: 60.9% faster than baseline\\n\")\ncat(\"  - Verified: Results identical to baseline\\n\")\nKey lessons from case study:\n\nProfile identified the right targets: Without profiling, we might have optimized I/O (only 12% of time) instead of SVD (60% of time)\nIncremental optimization works: Each step provided measurable benefit (22%, 30%, 29%)\nMemory headroom enabled speed: Having 16GB for a 2GB dataset let us reduce k aggressively\nPre-computation pays off: One-time normalization cost amortizes over multiple runs\nVerification is essential: Always confirm results unchanged after optimization"
  },
  {
    "objectID": "technical/performance.html#interactive-exercise",
    "href": "technical/performance.html#interactive-exercise",
    "title": "Performance Optimization",
    "section": "10 Interactive Exercise",
    "text": "10 Interactive Exercise\n\n10.1 Apply Optimization to Your Data\nUse this exercise to practice optimization on your own datasets:\n# ==========================================\n# Exercise 1: Baseline Measurement\n# ==========================================\n\ncat(\"Exercise 1: Establishing Baseline\\n\")\ncat(\"----------------------------------\\n\")\n\n# TODO: Replace with your actual file, group, dataset\nyour_file &lt;- \"your_data.hdf5\"\nyour_group &lt;- \"your_group\"\nyour_dataset &lt;- \"your_dataset\"\n\nbaseline_time &lt;- system.time({\n  baseline_result &lt;- bdPCA_hdf5(\n    filename = your_file,\n    group = your_group,\n    dataset = your_dataset,\n    k = 4,\n    threads = 2\n  )\n})\n\ncat(\"Baseline time:\", round(baseline_time[\"elapsed\"], 2), \"seconds\\n\")\ncat(\"User time:\", round(baseline_time[\"user\"], 2), \"seconds\\n\")\ncat(\"System time:\", round(baseline_time[\"system\"], 2), \"seconds\\n\\n\")\n\n# ==========================================\n# Exercise 2: Test Different k Values\n# ==========================================\n\ncat(\"Exercise 2: Testing Block Sizes (k)\\n\")\ncat(\"------------------------------------\\n\")\n\nk_values &lt;- c(2, 4, 8, 16)\nk_times &lt;- numeric(length(k_values))\n\nfor (i in seq_along(k_values)) {\n  cat(\"Testing k =\", k_values[i], \"... \")\n  \n  t &lt;- system.time({\n    bdPCA_hdf5(your_file, your_group, your_dataset,\n               k=k_values[i], threads=2)\n  })\n  \n  k_times[i] &lt;- t[\"elapsed\"]\n  cat(round(k_times[i], 2), \"seconds\\n\")\n}\n\n# Find optimal k\noptimal_k &lt;- k_values[which.min(k_times)]\ncat(\"\\nOptimal k:\", optimal_k, \"\\n\")\ncat(\"Best time:\", round(min(k_times), 2), \"seconds\\n\\n\")\n\n# Plot k vs time\nplot(k_values, k_times, type=\"b\", pch=19,\n     xlab=\"Number of Blocks (k)\", ylab=\"Time (seconds)\",\n     main=\"Impact of Block Size on Performance\",\n     col=\"blue\", lwd=2)\ngrid()\n\n# ==========================================\n# Exercise 3: Test Different Thread Counts\n# ==========================================\n\ncat(\"Exercise 3: Testing Thread Counts\\n\")\ncat(\"----------------------------------\\n\")\n\nthread_values &lt;- c(1, 2, 4, 8)\nthread_times &lt;- numeric(length(thread_values))\n\nfor (i in seq_along(thread_values)) {\n  cat(\"Testing threads =\", thread_values[i], \"... \")\n  \n  t &lt;- system.time({\n    bdPCA_hdf5(your_file, your_group, your_dataset,\n               k=optimal_k, threads=thread_values[i])\n  })\n  \n  thread_times[i] &lt;- t[\"elapsed\"]\n  cat(round(thread_times[i], 2), \"seconds\\n\")\n}\n\n# Calculate speedup and efficiency\nspeedup &lt;- thread_times[1] / thread_times\nefficiency &lt;- speedup / thread_values\n\ncat(\"\\nThreading Analysis:\\n\")\nresults_df &lt;- data.frame(\n  Threads = thread_values,\n  Time = round(thread_times, 2),\n  Speedup = round(speedup, 2),\n  Efficiency = paste0(round(efficiency * 100, 1), \"%\")\n)\nprint(results_df)\n\n# ==========================================\n# Exercise 4: Measure Memory Usage\n# ==========================================\n\ncat(\"\\nExercise 4: Memory Usage by Configuration\\n\")\ncat(\"-------------------------------------------\\n\")\n\nlibrary(pryr)\n\nmeasure_memory &lt;- function(k_val) {\n  mem_before &lt;- mem_used()\n  \n  bdPCA_hdf5(your_file, your_group, your_dataset,\n             k=k_val, threads=2)\n  \n  mem_after &lt;- mem_used()\n  (mem_after - mem_before) / 1024^2  # Convert to MB\n}\n\nmemory_usage &lt;- sapply(k_values, measure_memory)\n\ncat(\"\\nMemory Usage by k:\\n\")\nmem_df &lt;- data.frame(\n  k = k_values,\n  Memory_MB = round(memory_usage, 1)\n)\nprint(mem_df)\n\n# ==========================================\n# Summary and Recommendations\n# ==========================================\n\ncat(\"\\n\\n========================================\\n\")\ncat(\"OPTIMIZATION SUMMARY\\n\")\ncat(\"========================================\\n\\n\")\n\ncat(\"Baseline configuration: k=4, threads=2\\n\")\ncat(\"Baseline time:\", round(baseline_time[\"elapsed\"], 2), \"seconds\\n\\n\")\n\ncat(\"Optimal configuration:\\n\")\noptimal_threads &lt;- thread_values[which.min(thread_times)]\ncat(\"  k =\", optimal_k, \"\\n\")\ncat(\"  threads =\", optimal_threads, \"\\n\")\ncat(\"  time =\", round(min(thread_times), 2), \"seconds\\n\\n\")\n\nimprovement &lt;- (baseline_time[\"elapsed\"] - min(thread_times)) / \n               baseline_time[\"elapsed\"] * 100\ncat(\"Improvement:\", round(improvement, 1), \"%\\n\\n\")\n\ncat(\"Threading efficiency:\", \n    round(efficiency[which.min(thread_times)] * 100, 1), \"%\\n\")\n\nif (efficiency[which.min(thread_times)] &lt; 0.5) {\n  cat(\"\\nNote: Threading efficiency &lt;50% suggests bottleneck\\n\")\n  cat(\"Possible causes: I/O limitation, memory bandwidth, or BLAS threading\\n\")\n}\n\n\n\n\n\n\nTipReflection Questions\n\n\n\nAfter completing the exercises, consider:\nAbout k (block size):\n\nHow does k affect time in your data? Is relationship linear?\nWhat’s the memory usage at your optimal k? Could you use smaller k?\nDoes optimal k change with different thread counts?\n\nAbout threading:\n\nWhat speedup did you achieve with maximum threads?\nWhat’s your threading efficiency? Is it &gt;50%?\nIf efficiency is low, what bottleneck might exist? (Check user vs system time)\n\nAbout your workflow:\n\nCould you pre-normalize to save time in repeated runs?\nIs I/O (system time) significant? (&gt;20% of elapsed time?)\nWould different HDF5 chunking help if I/O is high?\nWhat’s the best tradeoff for your specific needs?\n\nDecision making:\n\nWhich configuration would you use for production? Why?\nHow would your choice change if RAM was limited?\nHow would your choice change if speed was critical?\nWhat evidence supports your configuration choice?"
  },
  {
    "objectID": "technical/performance.html#key-takeaways",
    "href": "technical/performance.html#key-takeaways",
    "title": "Performance Optimization",
    "section": "11 Key Takeaways",
    "text": "11 Key Takeaways\nLet’s consolidate essential principles that guide effective performance optimization.\nOptimization is about tradeoffs, not absolutes. There’s no universally “best” configuration—optimal settings depend on your dataset size, available hardware, and whether memory or speed limits you more. Understanding why these tradeoffs exist helps you make intelligent decisions for your specific situation. When memory is tight, accept slower computation with more blocking. When speed matters and memory is abundant, minimize blocking for faster execution.\nProfile before optimizing. Intuition about bottlenecks is frequently wrong. You might assume I/O dominates when actually computation takes 80% of time, or vice versa. Profiling with system.time(), Rprof(), or profvis reveals where time actually goes. Focus optimization effort on operations consuming &gt;20% of total time—optimizing minor contributors wastes effort with minimal benefit.\nBlock size (k) controls the fundamental memory-speed tradeoff. Smaller k means fewer, larger blocks—faster but more RAM required. Larger k means many small blocks—slower but memory-efficient. Start with k that uses 50-70% of available RAM. If hitting memory limits, increase k. If memory usage is low and speed matters, decrease k. The relationship is roughly linear: doubling k halves memory usage and increases time by 20-50% depending on operation intensity.\nThreading scales only when operations parallelize. More threads help when work divides independently (processing blocks in parallel, parallel BLAS operations). Threading doesn’t help when operations are sequential, I/O-bound, or memory-bandwidth-limited. If 4 threads provide &lt;2× speedup compared to 1 thread, you’ve hit a fundamental bottleneck—adding more threads won’t help. Common bottlenecks: disk I/O (all threads wait for disk), memory bandwidth (all threads compete for RAM access), Amdahl’s law (inherent sequential portions limit parallelism).\nHDF5 chunk size affects I/O performance dramatically. Optimal chunking aligns chunks with your access patterns—row-wise chunks for row-wise operations, column-wise chunks for column-wise operations. Target 1-10 MB chunks that match typical block sizes. If profiling shows high system time (&gt;30% of elapsed time), investigate chunk size. Proper chunking can reduce I/O time by 50-80% for I/O-heavy operations.\nLeverage HDF5 persistence to avoid redundant computation. Operations like normalization, centering, and scaling are deterministic—same input always produces same output. Compute these operations once, store results in HDF5, and reuse them in subsequent analyses. For workflows with repeated analyses (parameter sweeps, cross-validation, bootstrapping), pre-computation saves enormous time. A one-time normalization cost amortizes over hundreds of runs.\nAlways verify correctness after optimization. Faster incorrect results are worse than slower correct results. After each optimization, verify results match baseline within numerical tolerance. Test with all.equal(result1, result2, tolerance=1e-10). If results differ beyond floating-point noise, optimization introduced a bug—revert and investigate.\nIterate systematically. Start with conservative configuration guaranteed to complete successfully. Profile to identify bottlenecks. Optimize the biggest bottleneck. Measure improvement. Verify correctness. Repeat. Change one parameter at a time so you know what helped. Keep what works, revert what doesn’t. Document your final configuration and the reasoning behind it."
  },
  {
    "objectID": "technical/performance.html#next-steps",
    "href": "technical/performance.html#next-steps",
    "title": "Performance Optimization",
    "section": "12 Next Steps",
    "text": "12 Next Steps\nApply to your data:\n\nProfile your current workflows to identify where time goes\nExperiment with different k and thread values systematically\nMeasure actual improvements—don’t assume\nVerify results remain correct after each optimization\n\nExplore advanced optimization:\n\nStudy how different operations scale (SVD vs matrix multiplication vs statistical tests)\nExperiment with HDF5 chunking if I/O is your bottleneck\nConsider hybrid strategies: pre-compute expensive operations, cache intermediate results\nLearn about your BLAS library configuration (single-threaded vs multi-threaded)\n\nDeepen understanding:\n\nRead about Block-wise Computing mathematical foundations\nExplore HDF5 storage optimization in detail\nStudy CCA implementation as complex workflow example\nReview BigDataStatMeth paper for algorithmic details and theoretical analysis\n\nShare knowledge:\n\nDocument configurations that work well for your datasets and hardware\nShare benchmarking results with colleagues analyzing similar data\nContribute performance findings to the community\nHelp others optimize their workflows based on your experience\n\nPerformance optimization is empirical and iterative. What works for one dataset on one machine might not work for another dataset on different hardware. The principles here provide a systematic framework for discovering what works in your environment. Apply them methodically, measure carefully, and you’ll achieve substantial improvements in your BigDataStatMeth workflows."
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html",
    "href": "fundamentals/understanding-hdf5.html",
    "title": "Understanding HDF5 Storage",
    "section": "",
    "text": "By the end of this section, you will:\n\nUnderstand what HDF5 is and why it exists\nGrasp how HDF5 organizes data hierarchically\nLearn how chunking enables efficient disk-based computing\nKnow when and why to use compression\nBe able to create, inspect, and work with HDF5 files\nUnderstand how BigDataStatMeth leverages HDF5 features",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#the-problem-hdf5-solves",
    "href": "fundamentals/understanding-hdf5.html#the-problem-hdf5-solves",
    "title": "Understanding HDF5 Storage",
    "section": "1 The Problem HDF5 Solves",
    "text": "1 The Problem HDF5 Solves\nImagine you’re analyzing genomic data: a matrix of 100,000 individuals × 50,000 genetic variants. Let’s do some quick math:\n# Matrix dimensions\nn_individuals &lt;- 100000\nn_variants &lt;- 50000\n\n# Memory needed (assuming 8 bytes per double-precision number)\nmemory_gb &lt;- (n_individuals * n_variants * 8) / (1024^3)\ncat(sprintf(\"Memory required: %.1f GB\\n\", memory_gb))\nMemory required: 37.3 GB\nThe challenge: A typical laptop has 16GB RAM. Even a high-end workstation with 128GB would struggle when performing operations that create intermediate results.\n\n\n\n\n\n\nNoteThe Traditional Approach Fails\n\n\n\n# This would crash on most systems\ngenotype_matrix &lt;- read.csv(\"100k_x_50k_genotypes.csv\")  # ❌ Out of memory!\npca_result &lt;- prcomp(genotype_matrix)                     # ❌ Never gets here\nWe need a fundamentally different approach.",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#what-is-hdf5",
    "href": "fundamentals/understanding-hdf5.html#what-is-hdf5",
    "title": "Understanding HDF5 Storage",
    "section": "2 What is HDF5?",
    "text": "2 What is HDF5?\nWhen we encounter datasets that exceed our computer’s memory capacity, we face a fundamental question: how do we work with data we cannot fully load? Traditional file formats like CSV, RData, or even binary formats force us to read entire files into memory before we can work with them. This all-or-nothing approach creates an insurmountable barrier when data grows beyond available RAM.\nHDF5 (Hierarchical Data Format version 5) emerged from this exact challenge in the scientific computing community. Developed by the National Center for Supercomputing Applications (NCSA) and now maintained by The HDF Group, HDF5 was designed by scientists who routinely work with terabytes of data from instruments, simulations, and experiments. It’s not just “another file format” - it’s fundamentally a database system optimized for scientific matrices and arrays.\nThe key innovation of HDF5 is deceptively simple but profoundly powerful: rather than treating a file as a monolithic block of data that must be read entirely, HDF5 treats files as structured databases where you can efficiently access exactly the pieces you need, when you need them. This changes everything about how we can work with large datasets.\n\n2.1 The HDF5 Philosophy\nHDF5 was designed around a core principle that directly addresses our big data problem:\n\n“Data should be accessible in pieces, without loading everything into memory.”\n\nThis philosophy manifests in three revolutionary features that distinguish HDF5 from traditional file formats:\n1. Hierarchical Organization\nJust as you organize documents into folders and subfolders on your computer, HDF5 lets you organize datasets within groups. You might have a group called /raw_data/ containing original measurements, another called /processed/ for cleaned data, and /results/ for analytical outputs. This organization isn’t just cosmetic - it helps both humans and computers understand the relationships between different pieces of data. When working on a genomics project, for example, you might organize by chromosome, by sample type, or by processing stage, all within a single file.\n2. Partial I/O\nThis is where HDF5 truly shines. Imagine you have a matrix with 100,000 rows but only need to analyze the first 10,000. With a CSV file, you must read all 100,000 rows before you can work with any of them. With HDF5, you simply request rows 1-10,000, and only those rows are read from disk. The rest of the data stays untouched on disk, consuming zero memory. This selective reading extends to any dimension - rows, columns, or even arbitrary rectangular regions of your matrices. This capability is what makes disk-based computing practical.\n3. Self-Describing Metadata\nEvery HDF5 dataset carries its own documentation. The file knows the dimensions of each matrix, the data type of each element, the names of rows and columns, when the data was created, and any other information you choose to store. This metadata travels with the data, so six months later, you (or a colleague) can open the file and immediately understand what it contains without consulting separate documentation. The data describes itself.\nThese three features combine to create a storage system where large datasets remain accessible and usable without overwhelming your computer’s memory. But understanding the features is just the beginning - seeing how HDF5 structures data internally helps us use it effectively.",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#hdf5-file-structure",
    "href": "fundamentals/understanding-hdf5.html#hdf5-file-structure",
    "title": "Understanding HDF5 Storage",
    "section": "3 HDF5 File Structure",
    "text": "3 HDF5 File Structure\nTo truly grasp how HDF5 enables efficient big data workflows, we need to understand its internal organization. Think of an HDF5 file as containing its own miniature filesystem - a hierarchy of containers and data, all within a single file.\nAt the top level, an HDF5 file can contain two types of objects: groups and datasets. Groups are like directories or folders, providing organizational structure. Datasets are where the actual data lives - these are your matrices, vectors, or arrays. Groups can contain other groups (nested hierarchies) and datasets. This creates a tree-like structure that can represent complex, multi-component analyses within a single file.\nHere’s how this might look for a typical genomics analysis:\n\n\n\n\n\ngraph TD\n    A[HDF5 File: analysis.hdf5] --&gt; B[Group: /data]\n    A --&gt; C[Group: /results]\n    A --&gt; D[Group: /metadata]\n    \n    B --&gt; B1[Dataset: genotypes&lt;br/&gt;100k × 50k matrix]\n    B --&gt; B2[Dataset: phenotypes&lt;br/&gt;100k × 10 matrix]\n    \n    C --&gt; C1[Dataset: pca_components&lt;br/&gt;50k × 20 matrix]\n    C --&gt; C2[Dataset: pca_scores&lt;br/&gt;100k × 20 matrix]\n    \n    D --&gt; D1[Attributes: creation_date]\n    D --&gt; D2[Attributes: sample_ids]\n    \n    style A fill:#f0f8ff\n    style B fill:#e8f6e8\n    style C fill:#e8f6e8\n    style D fill:#e8f6e8\n    style B1 fill:#fff8e1\n    style B2 fill:#fff8e1\n    style C1 fill:#fff8e1\n    style C2 fill:#fff8e1\n\n\n HDF5 hierarchical structure \n\n\n\nIn this example, we have a single HDF5 file (analysis.hdf5) that contains everything related to a genetic association study. The file is organized into three main groups at the root level. The /data group holds our raw and processed data matrices - the genotype matrix with 100,000 individuals across 50,000 genetic variants, plus a smaller phenotype matrix with clinical measurements for those same individuals. The /results group stores the outputs of our PCA analysis - both the principal components (50,000 variants × 20 components) and the individual scores (100,000 individuals × 20 components). Finally, the /metadata group contains important information about the study, like when the data was collected and which samples correspond to which individuals.\nThis hierarchical organization does more than keep things tidy. It allows us to work with different parts of the analysis independently. We can read just the phenotype data without touching the much larger genotype matrix. We can update results without disturbing raw data. We can add new analyses in new groups without restructuring existing work. Everything stays together in one file, but nothing forces us to load everything at once.\n\n3.1 Components Explained\nTo work effectively with HDF5, you need to understand three types of objects that make up this hierarchy. Each serves a distinct purpose in organizing and documenting your data.\nGroups are organizational containers that work exactly like folders on your computer’s filesystem. They provide structure and context for your data. A group can contain other groups (creating deeper hierarchies) and datasets. For instance, you might create a structure like /data/genomics/chromosome1/variants to organize genetic variants by chromosome. Groups help both humans and software understand how different pieces of data relate to each other. When you come back to an analysis months later, this organization helps you quickly locate what you need.\nDatasets are where your actual numerical data lives. These are the matrices, vectors, and arrays you’ll perform computations on. A dataset can be one-dimensional (a vector), two-dimensional (a matrix), or even higher-dimensional (tensors for complex data structures). Crucially, datasets in HDF5 can be arbitrarily large - they’re not limited by your computer’s RAM because the data resides on disk. Each dataset stores not just the numbers, but also information about data types, dimensions, and structure. This is what allows HDF5 to read just portions of a dataset efficiently.\nAttributes are small pieces of metadata attached to either groups or datasets. Think of attributes as post-it notes that document important information about your data. For a genotype dataset, attributes might store column names (which SNPs?), row names (which individuals?), the date of data collection, quality control thresholds used, or the genome build version. Attributes are always small and loaded entirely into memory, so they’re perfect for storing descriptive information that helps interpret the data. Unlike datasets, attributes are not designed for selective access - they’re meant to be read completely whenever you open a file.",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#creating-your-first-hdf5-file",
    "href": "fundamentals/understanding-hdf5.html#creating-your-first-hdf5-file",
    "title": "Understanding HDF5 Storage",
    "section": "4 Creating Your First HDF5 File",
    "text": "4 Creating Your First HDF5 File\nNow that you understand the conceptual foundation of HDF5 - its hierarchical structure, the power of partial I/O, and how chunking works under the hood - let’s make these ideas concrete by actually creating an HDF5 file. We’ll build a small example that mirrors a typical genomics study: genotype data for many individuals across many genetic variants, plus associated phenotype measurements.\nThe key function in BigDataStatMeth for creating HDF5 files is bdCreate_hdf5_matrix(). This function takes an R matrix (or data that can be converted to a matrix) and writes it to an HDF5 file with appropriate chunking, compression, and metadata. Behind the scenes, it’s handling all the complexity we discussed - choosing chunk sizes, setting up compression, creating the hierarchical structure - so you can focus on organizing your data logically.\nLet’s walk through a complete example:\n\nlibrary(BigDataStatMeth)\nlibrary(rhdf5)\n\n# Create some example data that mimics a genomics study\n# In real work, this data would come from your actual measurements\nset.seed(123)\ngenotype_data &lt;- matrix(\n  sample(c(0, 1, 2), 1000 * 500, replace = TRUE),  # 0, 1, 2 = genotype calls\n  nrow = 1000,  # 1000 individuals\n  ncol = 500    # 500 genetic variants\n)\n\nphenotype_data &lt;- matrix(\n  rnorm(1000 * 10),  # Continuous phenotype measurements\n  nrow = 1000,       # Same 1000 individuals\n  ncol = 10          # 10 measured traits\n)\n\n# Create HDF5 file with hierarchical organization\n# This creates the file and writes the first dataset\nbdCreate_hdf5_matrix(\n  filename = \"my_study.hdf5\",    # Name of HDF5 file to create\n  object = genotype_data,         # Data to write\n  group = \"data\",                 # Group name (like a folder)\n  dataset = \"genotypes\",          # Dataset name within the group\n  overwriteFile = TRUE            # OK to overwrite if file exists\n)\n\n# Add second dataset to the same file\n# Notice overwriteFile = FALSE so we add to existing file\nbdCreate_hdf5_matrix(\n  filename = \"my_study.hdf5\",    # Same file as above\n  object = phenotype_data,        # Different data\n  group = \"data\",                 # Same group\n  dataset = \"phenotypes\",         # Different dataset name\n  overwriteFile = FALSE           # Don't overwrite the file, add to it\n)\n\n# Inspect the structure to see what we created\nh5ls(\"my_study.hdf5\")\n\n       group        name       otype dclass        dim\n0          /        data   H5I_GROUP              \n1     /data  genotypes H5I_DATASET  FLOAT 1000 x 500\n2     /data phenotypes H5I_DATASET  FLOAT 1000 x 10\n\n\n\n\n\n\nTipWhat Just Happened Behind the Scenes?\n\n\n\nLet’s unpack what bdCreate_hdf5_matrix() did for us in those few lines of code:\nFile and Structure Creation: The first call created a new HDF5 file on disk called my_study.hdf5. It automatically created the group /data (since it didn’t exist yet) and added the genotypes dataset within that group. The second call recognized the file already exists and added another dataset to the same group without disturbing the first one.\nData Transfer: Your R matrices were written to disk in HDF5 format. The genotype matrix (1000 × 500 = 500,000 values) and phenotype matrix (1000 × 10 = 10,000 values) now live on disk, organized hierarchically. The original R objects still exist in memory - we’ve created copies on disk, not moved the data.\nAutomatic Optimization: Behind the scenes, BigDataStatMeth chose appropriate chunk sizes for each dataset based on their dimensions. It applied default compression (level 4) to reduce file size. It stored metadata about dimensions and data types. All of this happened automatically without you needing to specify these technical details.\nMemory Efficiency: Notice that at no point did we need to have multiple copies of the data in memory. Each dataset was written directly from the R object to disk. This matters more with larger datasets - you can create multi-gigabyte HDF5 files without needing that much RAM, as long as each individual dataset fits in memory when you write it.\nThe h5ls() output shows our file structure. The / represents the root of the file, data is a group (H5I_GROUP), and within it are two datasets (H5I_DATASET) with their dimensions clearly visible.",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#reading-data-the-power-of-partial-io",
    "href": "fundamentals/understanding-hdf5.html#reading-data-the-power-of-partial-io",
    "title": "Understanding HDF5 Storage",
    "section": "5 Reading Data: The Power of Partial I/O",
    "text": "5 Reading Data: The Power of Partial I/O\nNow that we have data stored in HDF5 format, let’s explore what makes this format special: the ability to read just the portions we need.\n\n# Read just the first 100 rows and 50 columns\nsubset_data &lt;- h5read(\n  \"my_study.hdf5\",\n  \"data/genotypes\",\n  index = list(1:100, 1:50)\n)\n\n# Check memory usage\nobject.size(subset_data)  # Only ~40 KB instead of ~3.8 MB!\n\nThis is fundamentally different from CSV or RData files where you must read everything.\n\n\n\n\n\ngraph TB\n    T1[\"&lt;b&gt;Traditional Files (CSV, RData)&lt;/b&gt;\"]\n    T1 --&gt; A1[Full File&lt;br/&gt;37 GB]\n    A1 --&gt; B1[Read ALL&lt;br/&gt;37 GB RAM]\n    B1 --&gt; C1[Extract Subset&lt;br/&gt;~100 MB used]\n    \n    T2[\"&lt;b&gt;HDF5 File&lt;/b&gt;\"]\n    T2 --&gt; A2[HDF5 File&lt;br/&gt;37 GB]\n    A2 --&gt; B2[Read SUBSET&lt;br/&gt;~100 MB RAM]\n    B2 --&gt; C2[Work with Data&lt;br/&gt;~100 MB used]\n    \n    style T1 fill:#ffcccc,stroke:#cc0000,stroke-width:2px,color:#000\n    style T2 fill:#ccffcc,stroke:#00cc00,stroke-width:2px,color:#000\n    style A1 fill:#ffe8e8\n    style B1 fill:#ffe8e8\n    style C1 fill:#ffe8e8\n    style A2 fill:#e8f6e8\n    style B2 fill:#e8f6e8\n    style C2 fill:#e8f6e8\n\n\n Partial I/O: Reading only needed data",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#chunking-how-hdf5-enables-efficient-access",
    "href": "fundamentals/understanding-hdf5.html#chunking-how-hdf5-enables-efficient-access",
    "title": "Understanding HDF5 Storage",
    "section": "6 Chunking: How HDF5 Enables Efficient Access",
    "text": "6 Chunking: How HDF5 Enables Efficient Access\nWe’ve established that HDF5 allows you to read just the portions of data you need - but how does it accomplish this efficiently? The answer lies in a fundamental design choice called chunking, which is perhaps the most important concept to understand when working with HDF5 files. Chunking is what transforms HDF5 from a simple file format into a high-performance data access system.\nTo appreciate why chunking matters, consider what happens with traditional file formats. When you store a matrix in a CSV file, the data is written sequentially: row 1, then row 2, then row 3, and so on. If you want to read just one column, you must scan through the entire file, extracting the relevant value from each row as you go. This means touching every single byte of a multi-gigabyte file just to access a tiny slice of data. It’s like having to read an entire book to find a single word.\nHDF5 takes a fundamentally different approach by organizing data into rectangular blocks called chunks. This organization is built into how the data is stored on disk, not something that happens when you read the file. Understanding chunking helps you make better decisions about how to structure your data and which operations will be fast versus slow.\n\n6.1 What is Chunking?\nInstead of storing your matrix in a single continuous block (row-by-row or column-by-column), HDF5 divides it into rectangular chunks - think of them as tiles in a mosaic. Each chunk is stored as a contiguous unit on disk, meaning all the data in that chunk sits together in one place. When you request data that falls within a chunk, HDF5 can read that entire chunk in a single efficient disk operation.\nThe key insight is that related data - values that are likely to be accessed together - should live in the same chunk. If you typically access columns of data, you want chunks that contain complete column segments. If you work with rectangular regions, square chunks make sense. HDF5’s flexibility in chunk shapes lets you optimize for your specific access patterns.\nLet’s visualize how a matrix gets divided into chunks:\n\n\n\n\n\ngraph TD\n    subgraph \"Original Matrix (1000 × 500)\"\n        A[Chunk 1&lt;br/&gt;250×250]\n        B[Chunk 2&lt;br/&gt;250×250]\n        C[Chunk 3&lt;br/&gt;250×250]\n        D[Chunk 4&lt;br/&gt;250×250]\n        E[Chunk 5&lt;br/&gt;250×250]\n        F[Chunk 6&lt;br/&gt;250×250]\n        G[Chunk 7&lt;br/&gt;250×250]\n        H[Chunk 8&lt;br/&gt;250×250]\n    end\n    \n    style A fill:#f0f8ff\n    style B fill:#e8f6e8\n    style C fill:#fff8e1\n    style D fill:#ffe8e8\n    style E fill:#f0f8ff\n    style F fill:#e8f6e8\n    style G fill:#fff8e1\n    style H fill:#ffe8e8\n\n\n Matrix chunking concept \n\n\n\nIn this example, a 1000×500 matrix is divided into eight chunks of 250×250 elements each. Each colored block represents a separate chunk stored contiguously on disk. When you need data from a specific region of your matrix, HDF5 identifies which chunks contain that data and reads only those chunks. The chunks that aren’t needed stay on disk, consuming zero memory.\nThe coloring in the diagram isn’t just decorative - it helps visualize an important property: chunks are independent storage units. You can read chunk 1 without touching chunks 2 through 8. You can update chunk 5 without affecting any other chunk. This independence is what makes partial I/O possible and efficient.\n\n\n6.2 Why Chunking Matters\nThe choice of chunk size and shape has profound implications for performance. To understand why, let’s walk through a concrete example that illustrates both good and bad chunking strategies.\nExample scenario: You want to read columns 1-250 of your matrix.\n\nWith Good ChunkingWithout Chunking (Row-Major)\n\n\nMatrix chunked by 250×250 blocks\nReading columns 1-250 requires: Chunks 1, 3, 5, 7 (4 chunks)\nDisk reads: 4 seek + read operations ✓ Efficient!\n\n\nMatrix stored row-by-row\nReading columns 1-250 requires: Touching every row (1000 seek operations)\nDisk reads: 1000 seek + read operations ✗ Slow!\n\n\n\nThe difference is dramatic. With appropriate chunking, reading our column subset requires just 4 disk operations - one for each chunk that contains part of those columns. Without chunking (or with row-major storage), we need 1000 separate disk seeks, one for each row. Since disk seeks are typically measured in milliseconds while data transfer happens at gigabytes per second, the number of seeks dominates performance for partial reads.\nThis example illustrates a general principle: your chunk layout should match your access patterns. If you primarily access data by columns (common in statistical analysis), use chunks that span the full height of your matrix but only a portion of its width. If you work with rectangular regions, square chunks often work well. If you access entire rows, horizontal chunks make sense.\nThe good news is that BigDataStatMeth makes intelligent chunking decisions for you, optimizing for the block-wise statistical operations that are common in data analysis. But understanding chunking helps you recognize when certain operations will be fast (they align with chunk boundaries) versus slower (they require reading many partially-needed chunks).\n\n\n\n\n\n\nImportantChunk Size Matters\n\n\n\nChunk size represents a fundamental trade-off in HDF5 performance. Several factors influence the optimal size:\nToo small chunks mean more metadata overhead and more disk seeks. If each chunk is only a few kilobytes, you’ll spend more time seeking to chunks than actually reading data. The metadata describing where each chunk lives can become a significant burden.\nToo large chunks mean reading more data than you need. If you want a single column but each chunk contains hundreds of columns, you’re transferring far more data from disk to memory than necessary. This wastes both I/O bandwidth and RAM.\nThe sweet spot typically falls between 10KB and 1MB per chunk, though the exact optimum depends on your hardware (especially disk type - SSD versus hard drive) and access patterns. Modern SSDs are more forgiving of many small reads, while traditional hard drives strongly prefer fewer large reads.\nBigDataStatMeth handles chunking automatically with sensible defaults, but it’s important to understand the philosophy behind these choices. The package takes a deliberately conservative approach to chunk sizing. Rather than trying to maximize performance, the defaults prioritize system stability and broad compatibility. This means that BigDataStatMeth’s automatic chunking probably isn’t the absolute fastest possible for your specific system - but it won’t overwhelm your RAM or cause your system to become unresponsive.\nWhy this conservatism? The package cannot know in advance what hardware you’re running on (laptop with 8GB RAM vs. server with 256GB), what else is running on your system, or your specific storage architecture (local SSD, network drive, cloud storage). Chunk sizes that work beautifully on a high-end workstation might cause out-of-memory errors on a laptop. The defaults aim for “works reliably everywhere” rather than “optimal for this specific configuration.”\nIf you know your system’s capabilities and access patterns well, you can override the defaults and potentially achieve better performance. But for most users, especially those new to HDF5, the conservative defaults provide a good balance: operations complete successfully without system issues, even if not at maximum theoretical speed. As you gain experience, you can experiment with more aggressive chunk sizes for your specific workflows.",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#compression-saving-disk-space",
    "href": "fundamentals/understanding-hdf5.html#compression-saving-disk-space",
    "title": "Understanding HDF5 Storage",
    "section": "7 Compression: Saving Disk Space",
    "text": "7 Compression: Saving Disk Space\nStorage space and I/O bandwidth are precious resources, especially when working with large datasets. HDF5 addresses both concerns through transparent compression - the data is automatically compressed when written and decompressed when read, without you having to manage the process explicitly. This compression happens at the chunk level, meaning each chunk is compressed independently. This chunk-wise compression preserves the ability to access arbitrary portions of your data efficiently.\nThe key word is “transparent.” From your perspective as a user, compressed and uncompressed datasets work identically. You read and write data using the same functions, and HDF5 handles the compression automatically. The only difference you’ll notice is in file size on disk and potentially in read/write performance, depending on whether your system is limited by disk I/O or CPU speed.\nHDF5 uses the widely-tested gzip compression algorithm, which provides a good balance of compression ratio, speed, and universal support. The algorithm is lossless, meaning you get back exactly the data you put in - critical for scientific computing where accuracy matters. You control the compression level, trading off between better compression (smaller files, more CPU time) and faster operation (larger files, less CPU time).\nHere’s how to use compression with BigDataStatMeth:\n\n# BigDataStatMeth uses compression by default\nbdCreate_hdf5_matrix(\n  filename = \"compressed.hdf5\",\n  object = genotype_data,\n  group = \"data\",\n  dataset = \"genotypes\",\n  compression_level = 6  # 0 (none) to 9 (maximum)\n)\n\n# Check file sizes\nfile.info(\"uncompressed.hdf5\")$size / 1024^2  # MB\nfile.info(\"compressed.hdf5\")$size / 1024^2    # MB\n\n\n7.1 Compression Trade-offs\nChoosing a compression level involves understanding the trade-off between file size and computational overhead. Higher compression levels examine more potential encoding strategies, finding more compact representations at the cost of more CPU cycles.\n\n\n\nLevel\nRatio\nSpeed\nWhen to Use\n\n\n\n\n0\n1:1\nFastest\nHigh-speed temporary files\n\n\n4-6\n~2:1-5:1\nFast\nDefault - good balance\n\n\n9\n~3:1-8:1\nSlower\nArchival, limited disk space\n\n\n\nThe compression ratio you achieve depends heavily on your data characteristics. Genomic data with many repeated values (like genotypes coded as 0, 1, 2) compresses extremely well, often achieving 5:1 or better ratios. Random floating-point numbers compress poorly because compression algorithms rely on finding patterns and redundancy in the data.\nFor most analytical workflows, levels 4-6 hit the sweet spot. They provide substantial space savings while adding minimal computational overhead. Modern CPUs can decompress data far faster than even fast SSDs can deliver it, so the decompression rarely becomes a bottleneck. In fact, compression can sometimes improve overall performance by reducing the amount of data that must be transferred from disk to memory - the time saved in I/O exceeds the time spent decompressing.\nLevel 0 (no compression) makes sense for temporary files where you prioritize speed over space, or when working with data that simply doesn’t compress well (already-compressed data, random noise, encrypted data). Level 9 is useful for archival storage where space is at a premium and you don’t mind slower write times, but it rarely makes sense for working files since levels 6-7 typically achieve nearly the same compression with noticeably better performance.\n\n\n\n\n\n\nNoteBigDataStatMeth Default\n\n\n\nBigDataStatMeth uses compression level 4 by default, providing good compression ratios (typically 2-4× for typical genomic and statistical data) without significant performance penalty. This default works well for most use cases. The package applies compression automatically - you don’t need to think about it unless you have specific reasons to adjust the level.\nFor most users, the default compression is the right choice. It keeps your files manageable without slowing down your analysis. Only adjust compression if you have unusual requirements: no compression for maximum speed with temporary files, or higher compression for long-term storage of large datasets.",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#exploring-hdf5-files",
    "href": "fundamentals/understanding-hdf5.html#exploring-hdf5-files",
    "title": "Understanding HDF5 Storage",
    "section": "8 Exploring HDF5 Files",
    "text": "8 Exploring HDF5 Files\nOnce you’ve created HDF5 files, you’ll often want to inspect their contents to understand what data they contain and how it’s organized. HDF5 provides several tools for this exploration, both programmatic and visual. Understanding what’s in your files is essential for both your own work (remembering what analyses you’ve run) and for sharing data with colleagues who need to understand your file structure.\n\n8.1 Using h5ls()\n\n# List contents of HDF5 file\nlibrary(rhdf5)\nh5ls(\"my_study.hdf5\")\n\n       group        name       otype dclass        dim\n0          /        data   H5I_GROUP              \n1          /     results   H5I_GROUP              \n2     /data  genotypes H5I_DATASET  FLOAT 1000 x 500\n3     /data phenotypes H5I_DATASET  FLOAT 1000 x 10\n\n\n8.2 Using HDFView (GUI Tool)\nFor visual exploration, HDFView is an excellent free tool that provides a graphical interface to browse HDF5 file contents, inspect datasets, and view metadata.",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#how-bigdatastatmeth-uses-hdf5",
    "href": "fundamentals/understanding-hdf5.html#how-bigdatastatmeth-uses-hdf5",
    "title": "Understanding HDF5 Storage",
    "section": "9 How BigDataStatMeth Uses HDF5",
    "text": "9 How BigDataStatMeth Uses HDF5\nBigDataStatMeth builds on HDF5’s capabilities:\n\n\n\n\n\ngraph LR\n    A[Raw Data&lt;br/&gt;CSV, RData, GDS] --&gt; B[bdCreate_hdf5_matrix]\n    B --&gt; C[HDF5 File&lt;br/&gt;Chunked & Compressed]\n    C --&gt; D[Block-wise&lt;br/&gt;Operations]\n    D --&gt; E[Results&lt;br/&gt;Stored in HDF5]\n    E --&gt; F[Extract to R&lt;br/&gt;or Keep on Disk]\n    \n    style C fill:#f0f8ff\n    style D fill:#e8f6e8\n    style E fill:#fff8e1\n\n\n BigDataStatMeth’s HDF5 workflow \n\n\n\nKey features:\n\nAutomatic chunking optimized for statistical operations\nMetadata preservation (row names, column names)\nBlock-wise algorithms that read/write chunks efficiently\nResult storage in same file for traceability",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#practical-example-complete-workflow",
    "href": "fundamentals/understanding-hdf5.html#practical-example-complete-workflow",
    "title": "Understanding HDF5 Storage",
    "section": "10 Practical Example: Complete Workflow",
    "text": "10 Practical Example: Complete Workflow\nLet’s put it all together with a realistic example:\n\nlibrary(BigDataStatMeth)\nlibrary(rhdf5)\n\n# 1. Create HDF5 file from existing data\nset.seed(42)\nlarge_matrix &lt;- matrix(rnorm(10000 * 5000), nrow = 10000, ncol = 5000)\n\nbdCreate_hdf5_matrix(\n  filename = \"analysis.hdf5\",\n  object = large_matrix,\n  group = \"data\",\n  dataset = \"expression_matrix\",\n  overwriteFile = TRUE\n)\n\n# 2. Perform SVD without loading full matrix\nsvd_result &lt;- bdSVD_hdf5(\n  filename = \"analysis.hdf5\",\n  group = \"data\",\n  dataset = \"expression_matrix\",\n  bcenter = TRUE,\n  bscale = TRUE,\n  k = 20  # Number of components\n)\n\n# 3. Check what's in the file now\nh5ls(\"analysis.hdf5\")\n\n# 4. Extract just the components you need\ncomponents &lt;- h5read(\"analysis.hdf5\", svd_result$ds_v)\ndim(components)  # 5000 × 20 (not 5000 × 5000!)\n\n# 5. Clean up\nh5closeAll()\n\n\n\n\n\n\n\nTipWhat We Achieved\n\n\n\n\nStored 38GB dataset on disk\nPerformed PCA using ~500MB RAM\nKept results organized in same file\nCan rerun analysis without re-reading data",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#interactive-exercise",
    "href": "fundamentals/understanding-hdf5.html#interactive-exercise",
    "title": "Understanding HDF5 Storage",
    "section": "11 Interactive Exercise",
    "text": "11 Interactive Exercise\n\n11.1 Practice: Design Your Own HDF5 Structure\nThe best way to internalize HDF5 concepts is to apply them to your own data. This exercise guides you through creating a multi-component HDF5 file and asks you to think about organizational decisions. There are no “correct” answers - the goal is to practice translating a research design into an HDF5 file structure and to consider how different organizational choices affect usability.\nThis is a thinking exercise. We provide starter code and questions for reflection, but no solutions. The questions are designed to make you think about how you would structure real projects. Your answers will depend on your specific research questions and workflow.\nCreate an HDF5 file with your own data structure:\n\n# 1. Create a hierarchical organization for a study\nlibrary(BigDataStatMeth)\n\n# Simulated study data\ngenomic_data &lt;- matrix(sample(0:2, 5000*1000, replace=TRUE), 5000, 1000)\nexpression_data &lt;- matrix(rnorm(5000*200), 5000, 200)\nclinical_data &lt;- data.frame(\n  age = rnorm(5000, 50, 10),\n  gender = sample(c(\"M\", \"F\"), 5000, replace = TRUE)\n)\n\n# 2. Organize in HDF5\nbdCreate_hdf5_matrix(\n  \"my_study.hdf5\", genomic_data, \n  group = \"omics/genomics\", dataset = \"snps\",\n  overwriteFile = TRUE\n)\n\nbdCreate_hdf5_matrix(\n  \"my_study.hdf5\", expression_data,\n  group = \"omics/transcriptomics\", dataset = \"genes\"\n)\n\n# 3. Explore your creation\nh5ls(\"my_study.hdf5\")\n\n# Questions for reflection:\n# \n# 1. Longitudinal data organization:\n#    - How would you organize data collected at multiple time points?\n#    - Would you create separate groups for each timepoint (/timepoint1/, /timepoint2/)?\n#    - Or group by data type with timepoint as a dimension within datasets?\n#    - What are the trade-offs of each approach?\n#\n# 2. Multi-omic integration:\n#    - You now have genomics and transcriptomics. What if you add:\n#      * Proteomics data\n#      * Metabolomics data\n#      * Clinical phenotypes\n#    - How would you organize these to make cross-omic analyses easy?\n#    - Should raw and processed data live in different groups?\n#\n# 3. Derived results storage:\n#    - Where would you store PCA results for each omic?\n#    - Where would integration analysis results go?\n#    - How do you link results back to the data they came from?\n#    - Should every analysis get its own group, or organize by analysis type?\n#\n# 4. Metadata strategy:\n#    - What attributes would you attach to each dataset?\n#    - How would you document processing steps?\n#    - What information needs to travel with the data for reproducibility?\n#\n# Try implementing one of these scenarios in code. The goal is not perfection,\n# but practice thinking about data organization for real research projects.\n\n\n\n\n\n\n\nTipReflection, Not Solutions\n\n\n\nThis exercise deliberately doesn’t provide “the answer” because there often isn’t a single correct way to organize complex data. Your organizational choices should reflect:\n\nYour specific research questions (what comparisons matter most?)\nYour workflow (what data do you access together?)\nYour collaboration needs (who else needs to use this data?)\nYour analysis pipeline (what tools will read this file?)\n\nThe experience of designing and trying different structures teaches more than following a prescribed solution. If you’re unsure, try multiple approaches and see which one feels more natural when you go to actually use the data.",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#key-takeaways",
    "href": "fundamentals/understanding-hdf5.html#key-takeaways",
    "title": "Understanding HDF5 Storage",
    "section": "12 Key Takeaways",
    "text": "12 Key Takeaways\nWe’ve covered a lot of ground in understanding HDF5. Let’s consolidate the essential concepts you need to remember as you work with BigDataStatMeth and HDF5 files.\n\n12.1 Essential Concepts\nYou’ve learned that HDF5 is fundamentally different from traditional file formats. Let’s review the key ideas:\nHDF5 is a database for matrices, not just a file format. This distinction matters because it changes how you think about data storage. Instead of “saving a file,” you’re building a database that can contain multiple related datasets, organized hierarchically, with built-in metadata.\nHierarchical organization mirrors how you think about complex research projects. Just as you organize documents into folders, HDF5 lets you organize datasets into groups. This organization isn’t cosmetic - it helps both you and your analysis code understand the relationships between different data components.\nChunking enables efficient partial I/O by organizing data into blocks that can be read independently. This is what makes the “read only what you need” promise real rather than theoretical. Understanding chunking helps you predict which operations will be fast and which will require reading more data than you’d prefer.\nCompression reduces disk usage without adding complexity to your code. HDF5 handles compression transparently, and the default settings work well for most statistical applications. You get smaller files essentially for free.\nMetadata is built-in, meaning your data carries its own documentation. Six months later, you (or a colleague) can open an HDF5 file and immediately understand what it contains, when it was created, and how to interpret the values.\nBigDataStatMeth automates these HDF5 best practices. The package makes intelligent decisions about chunking, compression, and metadata so you can focus on your analysis rather than storage engineering. But understanding what’s happening under the hood helps you use the package more effectively.\n\n\n12.2 When to Use HDF5\nMaking the right choice about file formats matters for both productivity and practicality. Here’s how to decide:\n✅ Use HDF5 when:\n\nData exceeds available RAM - This is the primary use case. When you can’t load everything into memory, HDF5 lets you work with arbitrary-sized datasets by processing them in pieces.\nNeed repeated access to subsets - If your workflow involves reading different portions of data at different times, HDF5’s partial I/O capabilities pay off quickly.\nWant organized, self-documenting storage - For complex projects with multiple data components, HDF5’s hierarchical structure and metadata support help keep everything organized and understandable.\nSharing data across platforms - HDF5 is supported by R, Python, MATLAB, Julia, C++, and many other languages. It provides a common data format that works across your entire analysis ecosystem.\n\n❌ Don’t use HDF5 when:\n\nData fits comfortably in RAM - If your dataset uses less than about 20% of your available memory, traditional formats (RData, CSV) are simpler. The overhead of HDF5 doesn’t provide benefits when everything fits in memory anyway.\nNeed simple, human-readable formats - HDF5 files are binary and require special tools to inspect. If you need to open files in a text editor or want maximum simplicity, CSV or similar formats might be better despite their limitations.\nWorking with highly irregular structures - HDF5 excels with matrices and arrays - data that has regular structure. Highly nested or irregular data structures might be better served by other formats designed for those use cases.\n\nThe decision isn’t always clear-cut. Many projects start with data that fits in memory but grow over time. Starting with HDF5 can future-proof your analysis pipeline, especially if you anticipate scaling to larger datasets or want to leverage the organizational benefits even for medium-sized data.",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#next-steps",
    "href": "fundamentals/understanding-hdf5.html#next-steps",
    "title": "Understanding HDF5 Storage",
    "section": "13 Next Steps",
    "text": "13 Next Steps\nYou now have a solid foundation in HDF5 concepts and how BigDataStatMeth uses them. The natural next step is to understand how computational algorithms are adapted to work efficiently with disk-based data.\n\nBlock-Wise Computing → Learn how algorithms are adapted for disk-based matrices\nWorking with HDF5 Matrices → Practical tutorial on data management\nYour First Analysis → Complete analytical workflow",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/understanding-hdf5.html#further-reading",
    "href": "fundamentals/understanding-hdf5.html#further-reading",
    "title": "Understanding HDF5 Storage",
    "section": "14 Further Reading",
    "text": "14 Further Reading\nIf you want to deepen your understanding of HDF5 beyond what we’ve covered here, these resources provide different perspectives and more technical detail:\nHDF5 User Guide - The official documentation from The HDF Group. This is comprehensive and authoritative, though quite technical. Good for understanding the full capabilities of HDF5 and diving into advanced features like parallel I/O, virtual datasets, and complex datatypes.\nrhdf5 Bioconductor Package - The R package that BigDataStatMeth builds upon for low-level HDF5 operations. The rhdf5 documentation provides examples of direct HDF5 manipulation if you need finer control than BigDataStatMeth’s convenience functions provide.\nHDFView - A free graphical tool for browsing HDF5 files visually. Essential for debugging file structure issues and understanding what your code has created. Works on Windows, Mac, and Linux.\nBigDataStatMeth Documentation - The complete API reference covers all HDF5-related functions in detail, with additional examples and parameter explanations.\n\n\n\n\n\n\n\nNoteQuestions or Feedback?\n\n\n\nIf something is unclear or you’d like more examples, please open an issue on GitHub. We value your feedback to improve this educational material.",
    "crumbs": [
      "Core Concepts",
      "Understanding HDF5 Storage"
    ]
  },
  {
    "objectID": "fundamentals/index.html",
    "href": "fundamentals/index.html",
    "title": "Fundamentals",
    "section": "",
    "text": "NoteWhat You’ll Learn Here\n\n\n\nUnderstanding the foundations of BigDataStatMeth is essential for working effectively with large-scale datasets. This section covers the core concepts that make scalable statistical computing possible."
  },
  {
    "objectID": "fundamentals/index.html#why-these-fundamentals-matter",
    "href": "fundamentals/index.html#why-these-fundamentals-matter",
    "title": "Fundamentals",
    "section": "1 Why These Fundamentals Matter",
    "text": "1 Why These Fundamentals Matter\nModern genomic datasets routinely exceed available memory. BigDataStatMeth solves this challenge through intelligent design choices that let you work with data that would otherwise be impossible to analyze on standard workstations.\nThese fundamentals provide the practical knowledge you need to develop new statistical methods, optimize existing analyses, and understand why certain approaches work better than others at scale."
  },
  {
    "objectID": "fundamentals/index.html#core-concepts",
    "href": "fundamentals/index.html#core-concepts",
    "title": "Fundamentals",
    "section": "2 Core Concepts",
    "text": "2 Core Concepts\n\n\n\n\n\n\n\n\nTipThe Big Data Problem\n\n\n\nWhy do genomic datasets break traditional statistical software? What happens when your data exceeds available RAM?\nUnderstanding these challenges is the first step toward solving them.\nExplore the challenge →\n\n\n\n\n\n\n\n\n\n\nWarningUnderstanding HDF5\n\n\n\nThe storage technology that makes working with large datasets practical. HDF5 provides random access to data subsets without loading entire files into memory.\nLearn about HDF5 →\n\n\n\n\n\n\n\n\n\n\nImportantBlock-Wise Computing\n\n\n\nThe algorithmic strategy that makes large-scale analysis possible. Learn how to divide matrices into manageable pieces and process them efficiently.\nDiscover the algorithms →\n\n\n\n\n\n\n\n\n\n\nCautionLinear Algebra Essentials\n\n\n\nMatrix operations that form the foundation of statistical methods. Understand when to use each operation and how they work at scale.\nMaster the mathematics →\n\n\n\n\n\n\n\n\n\n\n\nNoteReady to Apply These Concepts?\n\n\n\nOnce you understand these fundamentals, you’ll be ready to work through the Tutorials section or explore Practical Workflows to see complete analyses."
  },
  {
    "objectID": "fundamentals/big-data-problem.html",
    "href": "fundamentals/big-data-problem.html",
    "title": "The Big Data Problem in Genomics",
    "section": "",
    "text": "By the end of this section, you will:\n\nUnderstand why traditional computing approaches fail with large datasets\nCalculate memory requirements for your own data\nRecognize the three fundamental constraints: memory, computation time, and disk I/O\nKnow when dataset size becomes a practical problem\nMake informed decisions about when to use disk-based computing\nUnderstand the trade-offs between in-memory and disk-based approaches",
    "crumbs": [
      "Core Concepts",
      "The Big Data Problem in Genomics"
    ]
  },
  {
    "objectID": "fundamentals/big-data-problem.html#the-promise-and-challenge-of-large-scale-data",
    "href": "fundamentals/big-data-problem.html#the-promise-and-challenge-of-large-scale-data",
    "title": "The Big Data Problem in Genomics",
    "section": "1 The Promise and Challenge of Large-Scale Data",
    "text": "1 The Promise and Challenge of Large-Scale Data\nThe data revolution has transformed scientific research across multiple disciplines. Geographic Information Systems (GIS) process satellite imagery with billions of pixels covering entire continents. Climate scientists analyze decades of high-resolution weather data from thousands of monitoring stations. Financial analysts track millions of transactions across global markets. Time series analysis in IoT applications handles streams from millions of sensors. Image analysis in medical diagnostics processes high-resolution scans with millions of voxels. Astronomers catalog billions of celestial objects from sky surveys.\nEach of these fields has experienced the same transformation: data collection has outpaced our traditional computational tools. What worked perfectly for manageable datasets breaks down when you scale to the volumes now routinely collected. The mathematics remains the same, but the practical reality of computation changes fundamentally.\nIn this documentation, we’ll use genomics as our primary example because it illustrates the problem particularly clearly: genome-wide association studies (GWAS) routinely measure 500,000+ genetic variants across 50,000+ individuals, creating datasets that don’t fit in RAM. Transcriptomics studies quantify 20,000 genes across thousands of samples. Multi-omic integration combines genomics, transcriptomics, proteomics, and metabolomics for the same cohorts.\nHowever, the concepts, solutions, and methods we discuss apply equally to any field working with large numerical matrices: satellite imagery is just a very large matrix of pixel values, time series data is observations-by-timepoints, financial data is transactions-by-features. The block-wise computational strategies and HDF5-based data management that BigDataStatMeth provides work regardless of whether your rows represent individuals, pixels, transactions, or time points.\nWe focus on genomics examples because: - The problem is widespread in this community - Dataset sizes clearly exceed typical RAM capacities\n- Statistical methods (PCA, regression, association tests) are well-defined - Results are scientifically important and well-understood\nBut as you read, remember that “individuals × genetic variants” could just as easily be “pixels × spectral bands,” “time points × sensors,” or “transactions × features.” The computational challenges and solutions are fundamentally the same.",
    "crumbs": [
      "Core Concepts",
      "The Big Data Problem in Genomics"
    ]
  },
  {
    "objectID": "fundamentals/big-data-problem.html#a-concrete-example-the-memory-wall",
    "href": "fundamentals/big-data-problem.html#a-concrete-example-the-memory-wall",
    "title": "The Big Data Problem in Genomics",
    "section": "2 A Concrete Example: The Memory Wall",
    "text": "2 A Concrete Example: The Memory Wall\nLet’s make this concrete with a realistic scenario that many researchers face. You’re conducting a GWAS using data from the UK Biobank, which provides genetic data for 500,000 individuals across approximately 800,000 genetic variants (after standard quality control). This is not an unusually large dataset by modern standards - it’s actually quite typical for contemporary genetic research.\n\n2.1 The Size Calculation\nEach genetic variant is typically coded as 0, 1, or 2 (for a diploid organism like humans), representing the number of copies of the alternate allele. Storing this as a floating-point number requires 8 bytes per value (using R’s default numeric storage). Let’s calculate what this means:\n500,000 individuals × 800,000 variants × 8 bytes = 3.2 × 10^12 bytes\n\nConverting to more familiar units:\n= 3,200 GB\n= 3.2 TB\nThree point two terabytes just for the genotype matrix. This doesn’t include:\n\nSample identifiers and metadata (ancestry, phenotypes, covariates)\nVariant annotations (chromosomal position, allele frequencies, functional annotations)\nQuality control metrics\nDerived variables or intermediate results\nAny additional data layers (imputed variants, expression data, etc.)\n\n\n\n2.2 The Practical Reality\nMost researchers don’t have access to machines with 3.2 TB of RAM. Even if you do, remember that R needs additional memory to actually compute with the data. A simple operation like computing a correlation matrix (cor(X)) requires substantially more memory than just storing X. Matrix operations typically need 2-3× the data size in working memory.\nFurthermore, loading 3.2 TB into RAM, even on a machine that has the capacity, takes considerable time. The data must be read from disk, parsed, and structured in memory. On a typical system with disk read speeds of 500 MB/s, loading this dataset would require nearly two hours - and that’s assuming no bottlenecks, no compression to decompress, and optimal I/O conditions.\nBut memory isn’t the only bottleneck - computational time grows dramatically with data size. Consider computing a simple operation like a correlation matrix on this dataset:\nComputing cor(X) for 800,000 variants requires:\n≈ (800,000)² / 2 pairwise correlations\n= 320 billion correlations\n\nAt 1 million correlations per second (optimistic):\n= 320,000 seconds  \n= 89 hours\n= Nearly 4 days of continuous computation\nAnd this is for a single operation. Real analyses involve many such operations iteratively: compute statistics, filter variants, recompute, fit models, validate, repeat. Each iteration compounds the time problem.\nThe computational complexity scales poorly: doubling your dataset size often quadruples or even octuples computation time, depending on the operation. A PCA that takes 10 minutes on 100,000 samples might take hours on 500,000 samples - not because of I/O, but because the number of arithmetic operations grows as O(n^2p) or worse.\nWhat happens when you try anyway? One of several things:\nWhat happens when you try anyway? One of several things:\nOut-of-memory errors: R terminates your session with an error message. All your work is lost. If you’re running this on a shared cluster, you may have consumed significant resources that other users were depending on.\nSystem thrashing: If your operating system tries to be helpful by using swap space, your system grinds to a near halt. Operations that should take seconds take hours as the system constantly moves data between RAM and disk. Your entire computer becomes unresponsive.\nProhibitive computation time: Even if the data technically fits and operations complete, they take so long that interactive analysis becomes impossible. Waiting hours or days for each exploratory step means you can’t iterate, can’t try alternative approaches, and can’t develop intuition about your data. Your research pace becomes limited by computational throughput rather than your thinking.\nNone of these outcomes moves your research forward.",
    "crumbs": [
      "Core Concepts",
      "The Big Data Problem in Genomics"
    ]
  },
  {
    "objectID": "fundamentals/big-data-problem.html#traditional-approaches-and-their-limitations",
    "href": "fundamentals/big-data-problem.html#traditional-approaches-and-their-limitations",
    "title": "The Big Data Problem in Genomics",
    "section": "3 Traditional Approaches and Their Limitations",
    "text": "3 Traditional Approaches and Their Limitations\nFaced with this memory barrier, researchers have developed various workarounds. Each has merit in specific contexts, but each also has significant limitations that affect either what analyses you can perform or how efficiently you can work.\n\n3.1 Approach 1: Reduce the Data\nStrategy: Use only a subset of the data that fits in memory.\nPerhaps you analyze 50,000 samples instead of 500,000, or focus on 100,000 variants instead of 800,000. This immediately makes the problem tractable - a 50,000 × 100,000 matrix requires “only” 40 GB, which fits on a well-equipped workstation.\nThe limitation: You’re throwing away information. Those 450,000 samples you excluded might contain the individuals with the phenotype you’re interested in. Those 700,000 variants you didn’t analyze might include the causal variant for the trait you’re studying. Statistical power drops dramatically with smaller sample sizes, and rare variants become impossible to analyze when you limit your sample.\nThis approach essentially says “we’ll solve the computational problem by avoiding it” - but at the cost of not actually answering your biological question with the full data you collected. It’s particularly problematic when the whole point of large-scale studies is to have adequate power to detect small effects or study rare variants.\n\n\n3.2 Approach 2: Chunk Analysis with Manual Merging\nStrategy: Analyze chunks of data separately and manually combine results.\nYou might analyze variants 1-100,000, then variants 100,001-200,000, and so on. Each chunk fits in memory. For some analyses (like single-variant association tests), you can simply concatenate the results from each chunk.\nThe limitation: This only works for embarrassingly parallel problems where chunks don’t need to interact. Many statistical methods require seeing all the data simultaneously. Principal component analysis (PCA) needs to consider all variants together to identify the major axes of variation. Regularized regression methods (LASSO, ridge regression) optimize over the entire feature space simultaneously. Cross-validation requires consistent data splits across the full dataset.\nEven when chunking is possible in principle, implementing it correctly is error-prone. You must carefully track which chunks have been processed, ensure consistent handling of edge cases (variants near chunk boundaries), and manually verify that merged results are statistically valid. Every analysis becomes a custom programming project.\n\n\n3.3 Approach 3: Use Specialized Hardware\nStrategy: Rent time on high-memory machines in the cloud or use institutional clusters with hundreds of GB of RAM.\nCloud providers offer machines with 512 GB, 1 TB, or even more RAM. Many universities operate shared computing clusters with similar capabilities.\nThe limitation: This approach works but introduces friction into your research process. Cloud computing costs money - substantial money for large analyses that run for hours or days. Institutional clusters require learning job submission systems, waiting in queues, and debugging remotely when something goes wrong.\nMore fundamentally, this approach doesn’t scale to the next order of magnitude. As datasets continue to grow (and they will), even these large-memory machines become insufficient. You haven’t solved the underlying problem, just postponed confronting it. And interactive data exploration - the iterative process of trying ideas, examining results, and refining your approach - becomes cumbersome when every attempt requires submitting a batch job and waiting for results.\n\n\n3.4 Approach 4: Reformulate the Problem\nStrategy: Develop mathematically equivalent algorithms that avoid ever materializing the full matrix in memory.\nThis is actually a sophisticated approach used in many modern tools. For example, some GWAS software never loads the full genotype matrix, instead reading small portions from disk as needed and maintaining summary statistics in memory.\nThe limitation: This requires algorithm-specific implementations. Someone must write custom code for each statistical method, thinking carefully about how to decompose the computation. Most researchers aren’t in a position to do this themselves - they depend on software developers providing these specialized implementations. This means you’re limited to whatever methods someone has already implemented in this special way.\nMoreover, these optimized tools often exist as standalone programs with their own file formats, their own configuration languages, and limited flexibility. Moving data between tools, combining methods in novel ways, or extending analyses beyond what the tool provides becomes difficult. Your computational constraints start dictating your scientific questions, rather than the other way around.",
    "crumbs": [
      "Core Concepts",
      "The Big Data Problem in Genomics"
    ]
  },
  {
    "objectID": "fundamentals/big-data-problem.html#the-root-cause-ram-as-a-fixed-resource",
    "href": "fundamentals/big-data-problem.html#the-root-cause-ram-as-a-fixed-resource",
    "title": "The Big Data Problem in Genomics",
    "section": "4 The Root Cause: RAM as a Fixed Resource",
    "text": "4 The Root Cause: RAM as a Fixed Resource\nAll these limitations stem from a single constraint: RAM is finite, and treating it as an unlimited resource forces us into uncomfortable compromises. Either we analyze less data, fragment our analyses, incur substantial costs, or limit ourselves to pre-packaged tools.\nThe traditional computing model assumes data lives in memory during analysis. Functions expect to receive complete data objects as inputs. Algorithms assume they can access any element of a matrix at any time. This made perfect sense when datasets were smaller - why complicate your code by reading data from disk when it fits comfortably in RAM?\nBut this assumption is no longer tenable for many modern datasets. We need a different model, one where:\n\nData primarily lives on disk, with only actively needed portions in RAM\nAlgorithms work with blocks, processing manageable chunks sequentially\nFile format supports efficient partial access, making disk-based computing practical\nTools are flexible, allowing complex multi-step analyses on out-of-memory data\n\nThis is precisely what BigDataStatMeth provides, building on the HDF5 file format and block-wise computational strategies. Rather than forcing you to choose between data subsampling, hardware requirements, analysis limitations, or implementation complexity, the package lets you work with large datasets using the RAM you have available, implementing the statistical methods you need, without requiring expertise in high-performance computing.",
    "crumbs": [
      "Core Concepts",
      "The Big Data Problem in Genomics"
    ]
  },
  {
    "objectID": "fundamentals/big-data-problem.html#when-does-the-problem-actually-occur",
    "href": "fundamentals/big-data-problem.html#when-does-the-problem-actually-occur",
    "title": "The Big Data Problem in Genomics",
    "section": "5 When Does the Problem Actually Occur?",
    "text": "5 When Does the Problem Actually Occur?\nIt’s worth being specific about when you’ll encounter these memory limitations. Not every genomics analysis requires special handling of large data. Understanding the thresholds helps you decide when to use specialized tools like BigDataStatMeth versus when simpler approaches suffice.\n\n5.1 Rule of Thumb: The 20% Rule\nA conservative guideline is that your data should use less than 20% of your available RAM to analyze comfortably. This leaves room for:\n\nR’s internal copies during operations\nIntermediate results from computations\nThe operating system and other programs\nSome safety margin for operations that temporarily spike memory usage\n\nIf you have 16 GB of RAM, this means staying under about 3 GB of data. For 32 GB of RAM, keep data under 6 GB. These thresholds might seem generous, but they prevent the frustrating experience of operations failing partway through or systems becoming unresponsive.\n\n\n5.2 Matrix Dimensions as Thresholds\nHere are concrete matrix sizes and their memory requirements:\n\n\n\nDimensions\nMemory Required\nTypical Use Case\n\n\n\n\n1,000 × 10,000\n80 MB\nSmall pilot study, processed data\n\n\n10,000 × 10,000\n800 MB\nMedium study, targeted feature set\n\n\n10,000 × 100,000\n8 GB\nLarge study, single omic\n\n\n50,000 × 100,000\n40 GB\nLarge cohort, genome-wide\n\n\n100,000 × 500,000\n400 GB\nBiobank-scale, comprehensive\n\n\n500,000 × 800,000\n3.2 TB\nFull UK Biobank-scale GWAS\n\n\n\nThe transition from “fits in memory” to “requires special handling” typically occurs around 10,000 × 100,000 for most researchers with standard workstations. Once you cross into the 40+ GB range, specialized approaches become not just helpful but necessary.\n\n\n5.3 Types of Analysis Matter\nThe type of analysis also affects whether you hit memory limitations:\nLess demanding: Simple operations that process the data in a streaming fashion (reading sequentially without needing everything at once) often work with larger datasets. Computing means, frequencies, or single-variant tests can handle data that wouldn’t fit entirely in RAM.\nMore demanding: Operations that require the full data simultaneously hit memory limits sooner. Matrix factorizations (PCA, SVD), model fitting across many features simultaneously, cross-validation, and resampling methods all need to “see” large portions of data at once.\nMost demanding: Operations that create large intermediate results exhaust memory even faster. Computing all pairwise correlations creates a new matrix of size features × features, which for 100,000 features would require nearly 80 GB just for the output, regardless of the input size.",
    "crumbs": [
      "Core Concepts",
      "The Big Data Problem in Genomics"
    ]
  },
  {
    "objectID": "fundamentals/big-data-problem.html#interactive-exercise",
    "href": "fundamentals/big-data-problem.html#interactive-exercise",
    "title": "The Big Data Problem in Genomics",
    "section": "6 Interactive Exercise",
    "text": "6 Interactive Exercise\n\n6.1 Practice: Calculate Your Data’s Memory Requirements\nThe best way to internalize these concepts is to apply them to data you actually work with. This exercise helps you estimate whether your current or planned analyses will face memory constraints.\n\n# Function to estimate memory requirements\nestimate_memory &lt;- function(n_samples, n_features, bytes_per_value = 8) {\n  # Basic storage\n  basic_gb &lt;- (n_samples * n_features * bytes_per_value) / (1024^3)\n  \n  # Typical operations need 2-3x for intermediate results\n  working_gb &lt;- basic_gb * 2.5\n  \n  cat(\"Memory Requirements:\\n\")\n  cat(sprintf(\"  Data storage: %.2f GB\\n\", basic_gb))\n  cat(sprintf(\"  Working memory: %.2f GB\\n\", working_gb))\n  cat(sprintf(\"  Recommended RAM: %.2f GB\\n\", working_gb * 1.5))\n  \n  return(invisible(list(storage = basic_gb, working = working_gb)))\n}\n\n# Example: Your genomic study\nestimate_memory(n_samples = 10000, n_features = 500000)\n\n# Try with your own numbers\nestimate_memory(n_samples = ???, n_features = ???)\n\n\n\n\n\n\n\nTipReflection Questions\n\n\n\nThink about these questions - there are no universal answers, as they depend on your specific situation:\n\nAt what sample size would your analysis exceed your available RAM?\n\nConsider both your current machine and any servers you have access to\nRemember that R needs working memory beyond just data storage\n\nWhich operations in your workflow would become bottlenecks first?\n\nCreating correlation matrices?\nMatrix factorizations (PCA, SVD)?\nIterative model fitting?\n\nIs your problem memory-limited or compute-limited?\n\nIf memory: disk-based computing helps\nIf compute time: you might need more cores or GPU acceleration\nOften it’s both - understanding which dominates helps choose solutions\n\nFor your research question, do you need the full matrix simultaneously?\n\nSome analyses can stream through data (single-variant tests)\nOthers need to “see” everything at once (PCA across all variants)\nThis affects whether block-wise approaches will work well\n\n\nTry different scenarios. What if your sample size doubles? What if you add more phenotypes or additional omic layers? When does your current computational setup become insufficient?",
    "crumbs": [
      "Core Concepts",
      "The Big Data Problem in Genomics"
    ]
  },
  {
    "objectID": "fundamentals/big-data-problem.html#the-path-forward",
    "href": "fundamentals/big-data-problem.html#the-path-forward",
    "title": "The Big Data Problem in Genomics",
    "section": "7 The Path Forward",
    "text": "7 The Path Forward\nUnderstanding these limitations and when they occur helps you make informed decisions about your computational strategy. For small to medium datasets that fit comfortably in memory, traditional R approaches work wonderfully - they’re simpler, more flexible, and well-supported by the vast R ecosystem.\nFor datasets that push or exceed memory limits, BigDataStatMeth provides a different approach: disk-based computing with HDF5 files and block-wise algorithms. This isn’t necessarily “better” in absolute terms - it’s an appropriate tool for a specific problem. When your data exceeds memory, you need different computational strategies, and that’s what the package provides.\nThe remainder of this documentation shows you how to work effectively with large datasets using these strategies, covering both the conceptual understanding and practical implementation.",
    "crumbs": [
      "Core Concepts",
      "The Big Data Problem in Genomics"
    ]
  },
  {
    "objectID": "fundamentals/big-data-problem.html#key-takeaways",
    "href": "fundamentals/big-data-problem.html#key-takeaways",
    "title": "The Big Data Problem in Genomics",
    "section": "8 Key Takeaways",
    "text": "8 Key Takeaways\nLet’s consolidate the essential concepts about why big data creates computational challenges and when you need specialized approaches.\n\n8.1 Essential Concepts\nThe memory wall is real and unavoidable. When your data exceeds available RAM, traditional computing simply fails. This isn’t a software problem that better code can solve - it’s a fundamental hardware constraint. A 64 GB workstation cannot load a 200 GB matrix, period. Understanding when you’ll hit this wall helps you plan computational strategies before you’re stuck.\nMemory requirements grow faster than you expect. It’s not just about storing the data - operations need working memory for intermediate results. Matrix operations typically require 2-3× the data size in RAM. This means a “40 GB dataset” actually needs 80-120 GB of available memory for computation. The gap between data size and memory requirements catches many researchers by surprise.\nThe three computational constraints - memory, compute time, and disk I/O - all matter, but they matter differently for different problems. A memory-constrained problem benefits from disk-based computing. A compute-constrained problem needs more cores or faster algorithms. An I/O-constrained problem needs better data organization or caching strategies. Identifying which constraint dominates your workflow determines which solutions will actually help.\nDataset size is contextual, not absolute. Whether a dataset is “big” depends on your available resources, not just the number of bytes. A 20 GB dataset is “small” on a 256 GB server but “impossible” on an 8 GB laptop. Similarly, whether disk-based computing helps depends on the ratio of data size to available RAM, not the absolute size. A 50 GB dataset might work fine in-memory if you have 128 GB RAM, but requires disk-based approaches with 32 GB RAM.\nOperation type determines feasibility. Not all operations scale the same way. Element-wise operations (like adding a constant to every value) scale linearly and can stream through data. Operations requiring the full matrix simultaneously (like PCA) are harder to scale. Operations creating large outputs (like computing all pairwise correlations) can exhaust memory even when inputs fit. Understanding your analysis pipeline’s operation types helps predict where you’ll hit limitations.\n\n\n8.2 When to Use Disk-Based Computing\nMaking the right choice about computational strategy matters for both productivity and practicality. Here’s guidance based on the challenges we’ve discussed:\n✅ Use disk-based computing when:\n\nData exceeds 20-30% of available RAM - This threshold gives enough headroom for intermediate results and operating system needs. Below this, in-memory approaches work fine. Above this, you start risking memory exhaustion during computation.\nRepeated partial access is your workflow - If you frequently access different subsets of data (different chromosomes, different time windows, different sample cohorts), HDF5’s partial I/O capabilities pay dividends. You read only what you need each time, keeping memory usage constant regardless of total data size.\nMultiple analysis types on same data - When you’ll run PCA, then regression, then association tests on the same dataset, keeping data in HDF5 format means you load it once and reuse it for all analyses. The upfront conversion cost amortizes across multiple uses.\nSharing data across platforms - If your workflow spans R, Python, and command-line tools, HDF5 provides a common format all can read efficiently. This beats converting between CSV, RData, and other formats for each tool.\n\n❌ Use traditional in-memory computing when:\n\nData comfortably fits in less than 20% of RAM - Traditional R approaches are simpler, more flexible, and better supported by the broader R ecosystem. Don’t add complexity when it’s not needed.\nOne-off analysis with simple operations - If you’re doing a quick exploratory analysis you won’t repeat, the overhead of converting to HDF5 outweighs the benefits. Load the data, compute what you need, save results, and you’re done.\nUltra-fast I/O is critical - Despite HDF5’s optimizations, RAM is always faster than disk. If your analysis involves thousands of tiny operations with random access patterns, in-memory processing wins. Disk-based computing excels at large sequential reads, not scattered tiny reads.\nYou need maximum flexibility - R’s in-memory data structures support arbitrary manipulations trivially. With disk-based data, some operations become awkward or inefficient. If your workflow involves many ad-hoc transformations and exploratory manipulations, staying in memory (if possible) maintains flexibility.\n\nThe decision isn’t always clear-cut, and many workflows benefit from a hybrid approach: use disk-based storage and computation for large matrices, but load summarized results into memory for final processing and visualization. Understanding these trade-offs helps you design efficient computational strategies for your specific needs.",
    "crumbs": [
      "Core Concepts",
      "The Big Data Problem in Genomics"
    ]
  },
  {
    "objectID": "fundamentals/big-data-problem.html#next-steps",
    "href": "fundamentals/big-data-problem.html#next-steps",
    "title": "The Big Data Problem in Genomics",
    "section": "9 Next Steps",
    "text": "9 Next Steps\nNow that you understand why traditional approaches fail with large data, you’re ready to learn about the solution:\n\nUnderstanding HDF5 → Learn how HDF5 enables disk-based data access\nBlock-Wise Computing → Understand how algorithms adapt to work with data blocks\nGetting Started Tutorial → Start working with your own data\n\n\n\n\n\n\n\n\nNoteQuestions or Feedback?\n\n\n\nIf you have questions about whether BigDataStatMeth is appropriate for your data size, or want to discuss specific use cases, please open an issue on GitHub.",
    "crumbs": [
      "Core Concepts",
      "The Big Data Problem in Genomics"
    ]
  },
  {
    "objectID": "developing-methods/cca-r-implementation.html",
    "href": "developing-methods/cca-r-implementation.html",
    "title": "CCA Implementation in R",
    "section": "",
    "text": "This document shows how to implement Canonical Correlation Analysis (CCA) from scratch using BigDataStatMeth’s R API. Rather than using a pre-built CCA function, you’ll see how to combine BigDataStatMeth’s building blocks—QR decomposition, matrix operations, SVD—to create a complete statistical method.\nThink of this as a blueprint for developing your own statistical methods. CCA serves as the example, but the patterns you’ll learn apply to any algorithm: partition data into blocks, apply operations iteratively, combine results hierarchically. This is how you extend BigDataStatMeth beyond its built-in functions to implement novel statistical methods for big data.\n\n\nBy the end of this document, you will:\n\nUnderstand CCA’s mathematical algorithm and why block-wise processing helps\nImplement QR decomposition by blocks for large matrices\nCombine intermediate QR results hierarchically\nCompute canonical correlations from Q matrices\nExtract canonical coefficients and variates\nStructure complex multi-step algorithms in R\nManage intermediate results in HDF5 files\nCreate reusable functions for new statistical methods",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in R"
    ]
  },
  {
    "objectID": "developing-methods/cca-r-implementation.html#overview",
    "href": "developing-methods/cca-r-implementation.html#overview",
    "title": "CCA Implementation in R",
    "section": "",
    "text": "This document shows how to implement Canonical Correlation Analysis (CCA) from scratch using BigDataStatMeth’s R API. Rather than using a pre-built CCA function, you’ll see how to combine BigDataStatMeth’s building blocks—QR decomposition, matrix operations, SVD—to create a complete statistical method.\nThink of this as a blueprint for developing your own statistical methods. CCA serves as the example, but the patterns you’ll learn apply to any algorithm: partition data into blocks, apply operations iteratively, combine results hierarchically. This is how you extend BigDataStatMeth beyond its built-in functions to implement novel statistical methods for big data.\n\n\nBy the end of this document, you will:\n\nUnderstand CCA’s mathematical algorithm and why block-wise processing helps\nImplement QR decomposition by blocks for large matrices\nCombine intermediate QR results hierarchically\nCompute canonical correlations from Q matrices\nExtract canonical coefficients and variates\nStructure complex multi-step algorithms in R\nManage intermediate results in HDF5 files\nCreate reusable functions for new statistical methods",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in R"
    ]
  },
  {
    "objectID": "developing-methods/cca-r-implementation.html#cca-algorithm-overview",
    "href": "developing-methods/cca-r-implementation.html#cca-algorithm-overview",
    "title": "CCA Implementation in R",
    "section": "2 CCA Algorithm Overview",
    "text": "2 CCA Algorithm Overview\n\n2.1 Mathematical Foundation\nCanonical Correlation Analysis finds linear combinations of features in X and Y that maximize correlation:\n\n\\max_{\\alpha, \\beta} \\text{cor}(X\\alpha, Y\\beta)\n\nThe standard algorithm:\n\nCompute QR decompositions: X = Q_X R_X, Y = Q_Y R_Y\nCompute cross-product: M = Q_X^T Q_Y\nSVD of cross-product: M = U D V^T\nSolve for canonical coefficients: \\alpha = R_X^{-1} U, \\beta = R_Y^{-1} V\nCompute canonical variates: s_X = X\\alpha, s_Y = Y\\beta\n\n\n\n2.2 Why Block-Wise Processing?\nFor large matrices (e.g., 50,000 samples × 500,000 genomic features), computing QR decomposition directly requires massive RAM. Block-wise processing:\n\nPartitions X into m blocks by rows\nComputes QR on each block separately (manageable size)\nCombines block QR results hierarchically\nFinal result identical to full QR, but memory-friendly",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in R"
    ]
  },
  {
    "objectID": "developing-methods/cca-r-implementation.html#step-1-qr-decomposition-by-blocks",
    "href": "developing-methods/cca-r-implementation.html#step-1-qr-decomposition-by-blocks",
    "title": "CCA Implementation in R",
    "section": "3 Step 1: QR Decomposition by Blocks",
    "text": "3 Step 1: QR Decomposition by Blocks\n\n3.1 The getQRbyBlocks Function\nThis is the core building block. It computes QR decomposition of a matrix stored in HDF5 by processing it in blocks:\n\nlibrary(BigDataStatMeth)\nlibrary(rhdf5)\n\ngetQRbyBlocks &lt;- function(strdataset, file, m, center, scale, bcols, overwrt) {\n  \n  # Parse dataset path (group/dataset)\n  strgroup &lt;- gsub(\"/.?$\", \"\", strdataset)\n  strdataset &lt;- gsub(\"^.*/\", \"\", strdataset)\n  \n  # Step 1: Normalize the dataset\n  # Centering and scaling ensure numerical stability\n  bdNormalize_hdf5(\n    filename = file,\n    group = strgroup,\n    dataset = strdataset,\n    bcenter = center,\n    bscale = scale,\n    overwrite = overwrt\n  )\n  \n  # Step 2: Split matrix into m blocks\n  # Process by rows (samples) - each block gets ~n/m rows\n  bdSplit_matrix_hdf5(\n    filename = file,\n    group = paste0(\"NORMALIZED/\", strgroup),\n    dataset = strdataset,\n    outgroup = paste0(\"Step1/\", strdataset, \"rows\"),\n    nblocks = m,\n    bycols = bcols,  # FALSE = split by rows\n    overwrite = overwrt\n  )\n  \n  # Step 3: Apply QR to each block independently\n  blocks &lt;- bdgetDatasetsList_hdf5(file, paste0(\"Step1/\", strdataset, \"rows\"))\n  \n  bdapply_Function_hdf5(\n    filename = file,\n    group = paste0(\"Step1/\", strdataset, \"rows\"),\n    datasets = blocks,\n    outgroup = paste0(\"Step2/\", strdataset, \"rows\"),\n    func = \"QR\",  # Apply QR decomposition\n    overwrite = overwrt\n  )\n  \n  # Step 4: Merge R matrices from all blocks\n  # QR gives Q (orthogonal) and R (upper triangular)\n  # We need to combine all R matrices\n  blocks_qr &lt;- bdgetDatasetsList_hdf5(file, paste0(\"Step2/\", strdataset, \"rows\"))\n  \n  # Filter to get only R matrices (not Q)\n  r_matrices &lt;- blocks_qr[which(blocks_qr %like% \".R\")]\n  \n  bdBind_hdf5_datasets(\n    filename = file,\n    group = paste0(\"Step2/\", strdataset, \"rows\"),\n    datasets = r_matrices,\n    outgroup = \"Step3/merged\",\n    outdataset = paste0(strdataset, \"Rt\"),\n    func = \"bindRows\",  # Stack R matrices vertically\n    overwrite = overwrt\n  )\n  \n  # Step 5: Final QR on merged R matrices\n  # This gives us the overall R matrix\n  bdapply_Function_hdf5(\n    filename = file,\n    group = \"Step3/merged\",\n    datasets = paste0(strdataset, \"Rt\"),\n    outgroup = \"Step3/Final_QR\",\n    func = \"QR\",\n    overwrite = overwrt\n  )\n  \n  # Step 6: Split final Q for block-wise multiplication\n  bdSplit_matrix_hdf5(\n    filename = file,\n    group = \"Step3/Final_QR\",\n    dataset = paste0(strdataset, \"Rt.Q\"),\n    outgroup = \"Step4/splitted\",\n    nblocks = m,\n    bycols = bcols,\n    overwrite = overwrt\n  )\n  \n  # Step 7: Multiply original Q blocks by final Q blocks\n  # This reconstructs the full Q matrix in blocks\n  tmp &lt;- bdgetDatasetsList_hdf5(file, \"Step4/splitted\")\n  Rt_Q_divide &lt;- tmp[which(tmp %like% paste0(strdataset, \"Rt.Q\"))]\n  \n  q_matrices &lt;- blocks_qr[which(blocks_qr %like% \".Q\")]\n  \n  bdapply_Function_hdf5(\n    filename = file,\n    group = paste0(\"Step2/\", strdataset, \"rows\"),\n    datasets = q_matrices,\n    outgroup = \"Step5\",\n    func = \"blockmult\",\n    b_group = \"Step4/splitted\",\n    b_datasets = Rt_Q_divide,\n    overwrite = TRUE\n  )\n  \n  # Step 8: Combine all Q blocks into final Q matrix\n  blocks_Q &lt;- bdgetDatasetsList_hdf5(file, \"Step5\")\n  \n  bdBind_hdf5_datasets(\n    filename = file,\n    group = \"Step5\",\n    datasets = blocks_Q[which(blocks_Q %like% paste0(strdataset, \".\"))],\n    outgroup = \"Step6\",\n    outdataset = paste0(strdataset, \"Q\"),\n    func = \"bindRows\",\n    overwrite = overwrt\n  )\n  \n  cat(\"✓ QR decomposition complete for\", strdataset, \"\\n\")\n}\n\n\n\n\n\n\n\nNoteWhy This Multi-Step Process?\n\n\n\nEach block gets QR: X_i = Q_i R_i\nStacking all R_i and taking QR again gives: [R_1; R_2; ...; R_m] = \\tilde{Q} \\tilde{R}\nMultiplying: Q = [Q_1; Q_2; ...; Q_m] \\times \\tilde{Q} gives the final Q\nThis hierarchical approach is mathematically equivalent to QR on the full matrix but processes data in memory-friendly chunks.",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in R"
    ]
  },
  {
    "objectID": "developing-methods/cca-r-implementation.html#step-2-main-cca-function",
    "href": "developing-methods/cca-r-implementation.html#step-2-main-cca-function",
    "title": "CCA Implementation in R",
    "section": "4 Step 2: Main CCA Function",
    "text": "4 Step 2: Main CCA Function\nNow we combine everything into the complete CCA implementation:\n\nbdCCA_hdf5 &lt;- function(filename, X, Y, m = 10,\n                       bcenter = TRUE, bscale = FALSE, bycols = FALSE,\n                       overwriteResults = FALSE, keepInteResults = FALSE) {\n  \n  matrices &lt;- c(X, Y)\n  \n  # Step 1-6: Compute QR for both X and Y\n  # This is the computationally intensive part\n  sapply(\n    matrices,\n    getQRbyBlocks,\n    file = filename,\n    m = m,\n    center = bcenter,\n    scale = bscale,\n    bcols = bycols,\n    overwrt = overwriteResults\n  )\n  \n  cat(\"✓ QR decompositions complete for both matrices\\n\")\n  \n  # Step 7: Compute cross-product of Q matrices\n  # This finds the canonical space\n  res &lt;- bdCrossprod_hdf5(\n    filename = filename,\n    group = \"Step6\",\n    A = \"XQ\",\n    groupB = \"Step6\",\n    B = \"YQ\",\n    outdataset = \"CrossProd_XQ_YQ\",\n    outgroup = \"Step7\",\n    overwrite = TRUE\n  )\n  \n  cat(\"✓ Cross-product computed\\n\")\n  \n  # Step 8: SVD of cross-product\n  # Singular values = canonical correlations\n  # U, V = canonical directions\n  res &lt;- bdSVD_hdf5(\n    file = filename,\n    group = \"Step7\",\n    dataset = \"CrossProd_XQ_YQ\",\n    bcenter = FALSE,  # Already centered\n    bscale = FALSE,\n    k = 16,  # Process in blocks\n    q = 2,   # Two-level hierarchy\n    threads = 3,\n    overwrite = TRUE\n  )\n  \n  cat(\"✓ SVD computed - canonical correlations extracted\\n\")\n  \n  # Get dimensions for coefficient computation\n  res &lt;- sapply(matrices, bdgetDim_hdf5, filename = filename)\n  \n  # Step 9: Compute and save canonical coefficients\n  writeCCAComponents_hdf5(filename, res[2, X], res[2, Y])\n  \n  cat(\"✓ Canonical coefficients and variates saved\\n\")\n  \n  # Cleanup intermediate results if requested\n  if (keepInteResults == FALSE) {\n    sapply(paste0(\"Step\", 1:7), function(x) {\n      invisible(bdRemove_hdf5_element(filename, element = x))\n      cat(\"  \", x, \"removed\\n\")\n    })\n    cat(\"✓ Intermediate results cleaned up\\n\")\n  }\n  \n  return(list(\n    filename = filename,\n    results_group = \"Results\"\n  ))\n}",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in R"
    ]
  },
  {
    "objectID": "developing-methods/cca-r-implementation.html#step-3-computing-canonical-coefficients",
    "href": "developing-methods/cca-r-implementation.html#step-3-computing-canonical-coefficients",
    "title": "CCA Implementation in R",
    "section": "5 Step 3: Computing Canonical Coefficients",
    "text": "5 Step 3: Computing Canonical Coefficients\nThe final step converts SVD results into interpretable canonical components:\n\nwriteCCAComponents_hdf5 &lt;- function(filename, ncolsX, ncolsY) {\n  \n  if (!file.exists(filename)) {\n    stop(\"ERROR - File does not exist\")\n  }\n  \n  # Read all necessary components from HDF5\n  h5f &lt;- H5Fopen(filename)\n  \n  # Get first ncolsX × ncolsX and ncolsY × ncolsY from Q matrices\n  XQ &lt;- h5f$Step6$XQ[1:ncolsX, 1:ncolsX]\n  YQ &lt;- h5f$Step6$YQ[1:ncolsY, 1:ncolsY]\n  \n  # Get R matrices from final QR\n  XR &lt;- h5f$Step3$Final_QR$XRt.R\n  YR &lt;- h5f$Step3$Final_QR$YRt.R\n  \n  # Get SVD results (canonical correlations and directions)\n  d &lt;- h5f$SVD$CrossProd_XQ_YQ$d\n  u &lt;- h5f$SVD$CrossProd_XQ_YQ$u\n  v &lt;- h5f$SVD$CrossProd_XQ_YQ$v\n  \n  # Get centering values\n  xcenter &lt;- h5f$NORMALIZED$data$mean.X\n  ycenter &lt;- h5f$NORMALIZED$data$mean.Y\n  \n  # Get feature names\n  x_names &lt;- h5f$data$.X_dimnames$`2`\n  y_names &lt;- h5f$data$.Y_dimnames$`2`\n  \n  h5closeAll()\n  \n  # Reconstruct compact QR representation\n  # Q and R are stored separately, combine them\n  XR[lower.tri(XR, diag = FALSE)] &lt;- 0  # R is upper triangular\n  XQ[upper.tri(XQ, diag = TRUE)] &lt;- 0   # Q lower part\n  XQR &lt;- XR + XQ\n  \n  YR[lower.tri(YR, diag = FALSE)] &lt;- 0\n  YQ[upper.tri(YQ, diag = TRUE)] &lt;- 0\n  YQR &lt;- YR + YQ\n  \n  # Solve for canonical coefficients\n  # X canonical coefficients: solve XQR * xcoef = u\n  xcoef &lt;- bdSolve(XQR, u)\n  \n  # Y canonical coefficients: solve YQR * ycoef = v\n  ycoef &lt;- bdSolve(YQR, v)\n  \n  # Add feature names\n  rownames(xcoef) &lt;- as.matrix(x_names)\n  rownames(ycoef) &lt;- as.matrix(y_names)\n  \n  # Save canonical coefficients to HDF5\n  bdCreate_hdf5_matrix(\n    filename,\n    object = xcoef,\n    group = \"Results\",\n    dataset = \"xcoef\",\n    overwriteDataset = TRUE\n  )\n  \n  bdCreate_hdf5_matrix(\n    filename = filename,\n    object = ycoef,\n    group = \"Results\",\n    dataset = \"ycoef\",\n    overwriteDataset = TRUE\n  )\n  \n  # Save canonical correlations\n  bdCreate_hdf5_matrix(\n    filename,\n    object = as.matrix(diag(d)),\n    group = \"Results\",\n    dataset = \"cor\",\n    overwriteDataset = TRUE\n  )\n  \n  # Save centering values (needed for new data projection)\n  bdCreate_hdf5_matrix(\n    filename,\n    object = xcenter,\n    group = \"Results\",\n    dataset = \"xcenter\",\n    overwriteDataset = TRUE\n  )\n  \n  bdCreate_hdf5_matrix(\n    filename,\n    object = ycenter,\n    group = \"Results\",\n    dataset = \"ycenter\",\n    overwriteDataset = TRUE\n  )\n  \n  # Compute canonical variates (scores) for samples\n  # X scores = X * xcoef\n  bdblockmult_hdf5(\n    filename = filename,\n    group = \"data\",\n    A = \"X\",\n    B = \"xcoef\",\n    groupB = \"Results\",\n    outgroup = \"Results\",\n    outdataset = \"xscores\",\n    overwrite = TRUE\n  )\n  \n  # Y scores = Y * ycoef\n  bdblockmult_hdf5(\n    filename,\n    group = \"data\",\n    A = \"Y\",\n    B = \"ycoef\",\n    groupB = \"Results\",\n    outgroup = \"Results\",\n    outdataset = \"yscores\",\n    overwrite = TRUE\n  )\n  \n  cat(\"✓ All CCA components saved to /Results/\\n\")\n}\n\n\n\n\n\n\n\nImportantUnderstanding the Coefficients\n\n\n\n\nxcoef, ycoef: Feature loadings - which features contribute to each canonical variate\ncor: Canonical correlations - strength of relationship for each component\nxscores, yscores: Sample scores on canonical variates\nxcenter, ycenter: Means used for centering (needed for projecting new data)\n\nThese components together provide complete CCA results interpretable just like standard CCA output.",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in R"
    ]
  },
  {
    "objectID": "developing-methods/cca-r-implementation.html#example-usage",
    "href": "developing-methods/cca-r-implementation.html#example-usage",
    "title": "CCA Implementation in R",
    "section": "6 Example Usage",
    "text": "6 Example Usage\nHere’s how to use the implemented CCA function:\n\nlibrary(BigDataStatMeth)\nlibrary(rhdf5)\n\n# Create example multi-omic data\nset.seed(123)\nn_samples &lt;- 500\nn_genes_x &lt;- 3000\nn_genes_y &lt;- 2000\n\n# Genomic data (X)\nX &lt;- matrix(rnorm(n_samples * n_genes_x), n_samples, n_genes_x)\nrownames(X) &lt;- paste0(\"Sample_\", 1:n_samples)\ncolnames(X) &lt;- paste0(\"Gene_X_\", 1:n_genes_x)\n\n# Expression data (Y)\nY &lt;- matrix(rnorm(n_samples * n_genes_y), n_samples, n_genes_y)\nrownames(Y) &lt;- paste0(\"Sample_\", 1:n_samples)\ncolnames(Y) &lt;- paste0(\"Gene_Y_\", 1:n_genes_y)\n\n# Create HDF5 file\ncca_file &lt;- \"example_cca.hdf5\"\n\nbdCreate_hdf5_matrix(cca_file, X, \"data\", \"X\", overwriteFile = TRUE)\nbdCreate_hdf5_matrix(cca_file, Y, \"data\", \"Y\", overwriteFile = FALSE)\n\n# Run CCA\nresult &lt;- bdCCA_hdf5(\n  filename = cca_file,\n  X = \"data/X\",\n  Y = \"data/Y\",\n  m = 4,              # 4 blocks for QR\n  bcenter = TRUE,     # Center data\n  bscale = TRUE,      # Scale data\n  overwriteResults = TRUE,\n  keepInteResults = FALSE  # Remove intermediate steps\n)\n\n# Examine results\nh5ls(cca_file)\n\n# Extract canonical correlations\nh5f &lt;- H5Fopen(cca_file)\ncanonical_cors &lt;- diag(h5f$Results$cor)\nxcoef &lt;- h5f$Results$xcoef\nycoef &lt;- h5f$Results$ycoef\nH5Fclose(h5f)\n\ncat(\"\\nCanonical correlations:\\n\")\nprint(canonical_cors[1:5])\n\ncat(\"\\nTop features for first canonical variate:\\n\")\ncat(\"X features:\\n\")\nprint(head(sort(abs(xcoef[, 1]), decreasing = TRUE)))\ncat(\"Y features:\\n\")\nprint(head(sort(abs(ycoef[, 1]), decreasing = TRUE)))",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in R"
    ]
  },
  {
    "objectID": "developing-methods/cca-r-implementation.html#interactive-exercise",
    "href": "developing-methods/cca-r-implementation.html#interactive-exercise",
    "title": "CCA Implementation in R",
    "section": "7 Interactive Exercise",
    "text": "7 Interactive Exercise\n\n7.1 Practice: Modifying the CCA Implementation\nUnderstanding how the algorithm works helps you adapt it for your needs or create entirely new methods.\n\n# Exercise 1: Change block size\n# Try m = 2, 4, 8, 16\n# How does memory usage change?\n# How does computation time change?\n# Do results differ?\n\n# Exercise 2: Skip normalization\n# Set bcenter = FALSE, bscale = FALSE\n# How do canonical correlations change?\n# Why does this matter?\n\n# Exercise 3: Keep intermediate results\n# Set keepInteResults = TRUE\n# Explore Step1 through Step7 in HDF5 file\n# What does each step contain?\n# How does data transform through the pipeline?\n\n# Exercise 4: Add progress reporting\n# Modify getQRbyBlocks to print progress\n# Track time for each major step\n# Identify bottlenecks\n\n\n\n\n\n\n\nTipReflection Questions\n\n\n\n1. Algorithm Understanding: - Why QR decomposition instead of direct SVD on X and Y? - What would break if you skipped the hierarchical QR merge? - How does block size affect accuracy vs. efficiency?\n2. Design Decisions: - Why store intermediate results in HDF5 instead of memory? - keepInteResults = TRUE vs. FALSE - when use each? - Could you implement this with fewer steps? Trade-offs?\n3. Extending to New Methods: - What if you wanted sparse CCA? Which step changes? - How would you implement regularized CCA? - Could you use this pattern for PLS regression?\n4. Practical Considerations: - Your data has missing values. Where in the pipeline do you handle them? - X has 100,000 features, Y has 50,000. What breaks? - How do you validate results match standard CCA?\n5. Performance Optimization: - Which step is slowest? QR blocks? SVD? Coefficient solving? - Could you parallelize any steps? - When does the C++ version become worth the effort?\nThink about how you’d adapt this pattern for your own statistical method. The structure—partition, process blocks, combine hierarchically—applies broadly beyond CCA.",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in R"
    ]
  },
  {
    "objectID": "developing-methods/cca-r-implementation.html#key-takeaways",
    "href": "developing-methods/cca-r-implementation.html#key-takeaways",
    "title": "CCA Implementation in R",
    "section": "8 Key Takeaways",
    "text": "8 Key Takeaways\nLet’s consolidate what you’ve learned about implementing CCA using BigDataStatMeth’s R API.\n\n8.1 Essential Concepts\nBlock-wise algorithms decompose complex operations into manageable steps. Rather than computing QR decomposition on a 50,000 × 500,000 matrix (requiring hundreds of GB RAM), we partition into blocks, compute QR on each (requiring only GB), then combine results hierarchically. This pattern—partition, process, combine—applies to virtually any matrix algorithm. Understanding this pattern lets you implement statistical methods that would otherwise be impossible on standard hardware.\nHDF5 files serve as persistent working memory. Each step saves results to HDF5: Step1 has split matrices, Step2 has block QRs, Step3 has merged results. This isn’t just storage—it enables debugging (inspect intermediate results), resuming (restart from any step), and memory management (only current block in RAM). Without HDF5’s structure, you’d need everything in memory simultaneously or manage dozens of temporary files manually.\nHierarchical combination preserves mathematical correctness. Computing QR on blocks, stacking R matrices, taking QR of the stack, and multiplying back through Q matrices gives identical results to single-shot QR on the full matrix. The mathematics works because QR decomposition composes correctly. Not all algorithms have this property—you can’t naively block-wise any operation and expect correct results. Understanding which operations compose correctly is essential for designing block-wise algorithms.\nNormalization affects numerical stability more than correctness. Centering and scaling aren’t just preprocessing niceties for CCA—they prevent numerical precision loss when working with disparate scales. Genomic copy number (±2) and expression (0-100,000) in the same analysis without scaling produces correlations driven by scale, not biology. For block-wise processing on HDF5, normalization happens once upfront and stores normalized data for all subsequent operations, avoiding repeated normalization overhead.\nIntermediate result management is a design decision. keepInteResults = FALSE removes Step1-Step7 after completion, saving disk space. keepInteResults = TRUE preserves them for inspection, debugging, or reusing with different final steps. For production pipelines, clean up. For method development, keep everything until you’re certain it works. The choice affects both disk usage and your ability to troubleshoot problems.\nCode structure mirrors algorithm structure. getQRbyBlocks handles one matrix’s QR decomposition. bdCCA_hdf5 calls it twice (for X and Y), computes cross-product, performs SVD, and extracts coefficients. writeCCAComponents_hdf5 handles result extraction and formatting. This modular structure makes code understandable and reusable—getQRbyBlocks could be used for methods beyond CCA. Clear function boundaries with specific responsibilities enable building complex methods from simple components.\n\n\n8.2 When to Implement Methods in R\nUnderstanding when R implementation suffices versus when C++ becomes necessary guides efficient development.\n✅ Implement in R when:\n\nPrototyping new methods - R’s interactive nature enables rapid testing. Write the algorithm, run it on small data, iterate until correct. Once working, optimize or port to C++ if needed. Starting in R saves development time even if you eventually rewrite in C++.\nCombining existing BigDataStatMeth functions - If your method uses bdSVD_hdf5, bdCrossprod_hdf5, bdSolve, etc., R glue code suffices. The heavy lifting happens in C++ (BigDataStatMeth internals), R just orchestrates. CCA is this case—all computational work happens in compiled code, R just sequences operations.\nMethod will be used occasionally - If you run CCA once per project, R performance is fine. Minutes vs. seconds doesn’t matter when you run it monthly. Save development effort for methods you’ll run thousands of times.\nYour team works primarily in R - Keeping implementation in R enables others to understand, modify, and extend your code without C++ expertise. Collaboration and maintainability sometimes outweigh raw performance.\nIntermediate result inspection matters - R makes it trivial to peek at any step: load data from HDF5, examine dimensions, plot distributions. This transparency helps verify correctness and debug problems. C++ implementations bury these details in compiled code.\n\n✅ Consider C++ implementation when:\n\nPerformance is critical - If your method runs hundreds of times daily, saving 50% computation time through C++ pays for development effort. Profile first—identify if performance matters before investing in optimization.\nCustom algorithms, not BigDataStatMeth combinations - If you’re implementing novel linear algebra (not using existing bdXXX functions), C++ gives you control over algorithms, memory layout, and parallelization. R becomes a bottleneck for truly custom computation.\nProduction deployment required - Software going into production pipelines benefits from C++’s speed and static typing. Development takes longer but runtime is faster and more predictable.\nYou need fine-grained parallelization - R’s parallelization works at function level. C++ with OpenMP or threading libraries enables parallelizing within matrix operations, achieving better speedups for large-scale data.\n\n❌ Don’t reimplement in C++ when:\n\nR version works fine - “Working” beats “optimal” when deadlines loom. If R implementation meets your needs, moving to C++ wastes time that could go toward science.\nThe bottleneck isn’t computation - If your analysis spends 90% of time reading files or waiting for database queries, optimizing computation achieves nothing. Profile to find true bottlenecks before optimizing.\nYou’re still figuring out the algorithm - Algorithm design in C++ is painful—compile, run, crash, debug, repeat. R’s interactivity (run line-by-line, inspect variables, iterate quickly) is invaluable during method development. Get it working in R, then optimize if needed.\n\nThe key principle: R for development and prototyping, C++ for production and performance-critical code. Many methods never need C++ because R orchestration of C++ building blocks (BigDataStatMeth functions) provides adequate performance.",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in R"
    ]
  },
  {
    "objectID": "developing-methods/cca-r-implementation.html#next-steps",
    "href": "developing-methods/cca-r-implementation.html#next-steps",
    "title": "CCA Implementation in R",
    "section": "9 Next Steps",
    "text": "9 Next Steps\nExplore the C++ implementation:\n\nCCA Implementation in C++ - Same algorithm, different language\nCompare R and C++ approaches\nUnderstand when each makes sense\n\nApply this pattern to new methods:\n\nImplement PLS (Partial Least Squares) using similar structure\nCreate sparse CCA with L1 penalties\nDevelop custom dimensionality reduction methods\n\nExtend CCA functionality:\n\nAdd cross-validation for component selection\nImplement permutation testing for significance\nCreate visualization functions for results\nBuild prediction functions for new samples",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in R"
    ]
  },
  {
    "objectID": "developing-methods/cca-r-implementation.html#cleanup",
    "href": "developing-methods/cca-r-implementation.html#cleanup",
    "title": "CCA Implementation in R",
    "section": "10 Cleanup",
    "text": "10 Cleanup\n\n# Close any open HDF5 connections\nh5closeAll()\n\n# Note: Keep example_cca.hdf5 for testing and learning\n# It contains complete CCA results you can explore",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in R"
    ]
  },
  {
    "objectID": "appendix/r_functions_table.html",
    "href": "appendix/r_functions_table.html",
    "title": "R Functions Reference Table",
    "section": "",
    "text": "Complete list of R functions in BigDataStatMeth for manual API matching."
  },
  {
    "objectID": "appendix/r_functions_table.html#overview",
    "href": "appendix/r_functions_table.html#overview",
    "title": "R Functions Reference Table",
    "section": "",
    "text": "Complete list of R functions in BigDataStatMeth for manual API matching."
  },
  {
    "objectID": "appendix/r_functions_table.html#r-functions",
    "href": "appendix/r_functions_table.html#r-functions",
    "title": "R Functions Reference Table",
    "section": "2 R Functions",
    "text": "2 R Functions\n\n\n\n\n\n\n\n\n\n\n\nFunction Name\nCategory\nData Type\nDescription\nUsage\nLink\n\n\n\n\nbd_wproduct\nlinear_algebra\nIn-memory\nCompute weighted operations using a diagonal weight from w: Inputs may be base numeric matrices .\nbd_wproduct(X, w, op)\n../api-reference/r/linear_algebra/bd_wproduct.qmd\n\n\nbdapply_Function_hdf5\nhdf5_statistics\nHDF5\n—\nbdapply_Function_hdf5(filename, group, datasets, outgroup, func, b_group = NULL, b_datasets = NULL, overwrite = FALSE, transp_dataset = FALSE, transp_bdataset = FALSE, fullMatrix = FALSE, byrows = FALSE, threads = 2L)\n../api-reference/r/hdf5_statistics/bdapply_Function_hdf5.qmd\n\n\nbdBind_hdf5_datasets\nhdf5_io_management\nHDF5\n—\nbdBind_hdf5_datasets(filename, group, datasets, outgroup, outdataset, func, overwrite = FALSE)\n../api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.qmd\n\n\nbdblockMult\nother\nIn-memory\nPerforms efficient matrix multiplication using block-based algorithms. The function supports various input combinations (matrix-matrix, matrix-vector, vector-vector) and provides options for parallel processing and block-based computation.\nbdblockMult(A, B, block_size = NULL, paral = NULL, byBlocks = TRUE, threads = NULL)\n../api-reference/r/other/bdblockMult.qmd\n\n\nbdblockmult_hdf5\nblockwise_ops\nHDF5\n—\nbdblockmult_hdf5(filename, group, A, B, groupB = NULL, transpose_A = NULL, transpose_B = NULL, block_size = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)\n../api-reference/r/blockwise_ops/bdblockmult_hdf5.qmd\n\n\nbdblockmult_sparse_hdf5\nblockwise_ops\nHDF5\n—\nbdblockmult_sparse_hdf5(filename, group, A, B, groupB = NULL, block_size = NULL, mixblock_size = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)\n../api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.qmd\n\n\nbdblockSubstract\nblockwise_ops\nIn-memory\nPerforms efficient matrix subtraction using block-based algorithms. The function supports various input combinations (matrix-matrix, matrix-vector, vector-vector) and provides options for parallel processing and block-based computation.\nbdblockSubstract(A, B, block_size = NULL, paral = NULL, byBlocks = TRUE, threads = NULL)\n../api-reference/r/blockwise_ops/bdblockSubstract.qmd\n\n\nbdblockSubstract_hdf5\nblockwise_ops\nHDF5\n—\nbdblockSubstract_hdf5(filename, group, A, B, groupB = NULL, block_size = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)\n../api-reference/r/blockwise_ops/bdblockSubstract_hdf5.qmd\n\n\nbdblockSum\nblockwise_ops\nIn-memory\nPerforms efficient matrix addition using block-based algorithms. The function supports various input combinations (matrix-matrix, matrix-vector, vector-vector) and provides options for parallel processing and block-based computation.\nbdblockSum(A, B, block_size = NULL, paral = NULL, byBlocks = TRUE, threads = NULL)\n../api-reference/r/blockwise_ops/bdblockSum.qmd\n\n\nbdblockSum_hdf5\nblockwise_ops\nHDF5\n—\nbdblockSum_hdf5(filename, group, A, B, groupB = NULL, block_size = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)\n../api-reference/r/blockwise_ops/bdblockSum_hdf5.qmd\n\n\nbdCheckMatrix_hdf5\nhdf5_io_management\nHDF5\nChecks whether a matrix stored in HDF5 format is suitable for eigenvalue decomposition using Spectra. The function verifies that the matrix is square and optionally checks for symmetry to recommend the best solver type.\nbdCheckMatrix_hdf5(filename, group = NULL, dataset = NULL, check_symmetry = NULL, tolerance = NULL, sample_size = NULL)\n../api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.qmd\n\n\nbdCholesky_hdf5\nhdf5_algebra\nHDF5\nComputes the Cholesky decomposition of a symmetric positive-definite matrix stored in an HDF5 file. The Cholesky decomposition factors a matrix A into the product A = LL’ where L is a lower triangular matrix.\nbdCholesky_hdf5(filename, group, dataset, outdataset, outgroup = NULL, fullMatrix = NULL, overwrite = NULL, threads = NULL, elementsBlock = 1000000L)\n../api-reference/r/hdf5_algebra/bdCholesky_hdf5.qmd\n\n\nbdcomputeMatrixVector_hdf5\nblockwise_ops\nHDF5\nPerforms element-wise operations between a matrix and a vector stored in HDF5 format. The function supports addition, subtraction, multiplication, division and power operations, with options for row-wise or column-wise application and parallel processing.\nbdcomputeMatrixVector_hdf5(filename, group, dataset, vectorgroup, vectordataset, outdataset, func, outgroup = NULL, byrows = NULL, paral = NULL, threads = NULL, overwrite = FALSE)\n../api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.qmd\n\n\nbdCorr_hdf5\nhdf5_statistics\nHDF5\nThis function computes Pearson or Spearman correlation matrix for matrices stored in HDF5 format. It automatically detects whether to compute: It automatically selects between direct computation for small matrices and block-wise processing for large matrices to optimize memory usage and performance. Correlation types supported: For omics data analysis: \nbdCorr_hdf5(filename_x, group_x, dataset_x, filename_y = \"\", group_y = \"\", dataset_y = \"\", trans_x = FALSE, trans_y = FALSE, method = \"pearson\", use_complete_obs = TRUE, compute_pvalues = TRUE, block_size = 1000L, overwrite = FALSE, output_filename = \"\", output_group = \"\", output_dataset_corr = \"\", output_dataset_pval = \"\", threads = -1L)\n../api-reference/r/hdf5_statistics/bdCorr_hdf5.qmd\n\n\nbdCorr_matrix\nlinear_algebra\nIn-memory\nCompute Pearson or Spearman correlation matrix for matrices that fit in memory. This function automatically detects whether to compute: \nbdCorr_matrix(X, Y = NULL, trans_x = NULL, trans_y = NULL, method = NULL, use_complete_obs = NULL, compute_pvalues = NULL, threads = NULL)\n../api-reference/r/linear_algebra/bdCorr_matrix.qmd\n\n\nbdCreate_diagonal_hdf5\nhdf5_algebra\nHDF5\nCreates a diagonal matrix or vector directly in an HDF5 file using block-wise processing to minimize memory usage. This unified function replaces separate diagonal and identity matrix creation functions, providing flexible diagonal creation with automatic parameter detection.\nbdCreate_diagonal_hdf5(filename, group, dataset, size = NULL, scalar = 1.0, diagonal_values = NULL, output_type = \"matrix\", block_size = 0L, compression = 6L, overwriteFile = NULL, overwriteDataset = NULL, threads = NULL)\n../api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.qmd\n\n\nbdCreate_hdf5_emptyDataset\nhdf5_io_management\nHDF5\nCreates an HDF5 dataset of size nrows × ncols inside group with name dataset, without writing data (allocation only). Honors file/dataset overwrite flags and supports unlimited datasets.\nbdCreate_hdf5_emptyDataset(filename, group, dataset, nrows = 0L, ncols = 0L, overwriteFile = NULL, overwriteDataset = NULL, unlimited = NULL, datatype = NULL)\n../api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.qmd\n\n\nbdCreate_hdf5_group\nhdf5_io_management\nHDF5\nCreate a (nested) group inside an HDF5 file. The operation is idempotent: if the group already exists, no error is raised.\nbdCreate_hdf5_group(filename, group)\n../api-reference/r/hdf5_io_management/bdCreate_hdf5_group.qmd\n\n\nbdCreate_hdf5_matrix\nhdf5_io_management\nHDF5\n—\nbdCreate_hdf5_matrix(filename, object, group = NULL, dataset = NULL, transp = NULL, overwriteFile = NULL, overwriteDataset = NULL, unlimited = NULL)\n../api-reference/r/hdf5_io_management/bdCreate_hdf5_matrix.qmd\n\n\nbdCrossprod\nlinear_algebra\nIn-memory\nComputes matrix cross-products efficiently using block-based algorithms and optional parallel processing. Supports both single-matrix (X’X) and two-matrix (X’Y) cross-products.\nbdCrossprod(A, B = NULL, transposed = NULL, block_size = NULL, paral = NULL, threads = NULL)\n../api-reference/r/linear_algebra/bdCrossprod.qmd\n\n\nbdCrossprod_hdf5\nhdf5_algebra\nHDF5\n—\nbdCrossprod_hdf5(filename, group, A, B = NULL, groupB = NULL, block_size = NULL, mixblock_size = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)\n../api-reference/r/hdf5_algebra/bdCrossprod_hdf5.qmd\n\n\nbdDiag_add_hdf5\nhdf5_algebra\nHDF5\nPerforms optimized diagonal addition between two datasets stored in HDF5 format. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct operation) and uses the most efficient approach.\nbdDiag_add_hdf5(filename, group, A, B, groupB = NULL, target = NULL, outgroup = NULL, outdataset = NULL, paral = NULL, threads = NULL, overwrite = NULL)\n../api-reference/r/hdf5_algebra/bdDiag_add_hdf5.qmd\n\n\nbdDiag_divide_hdf5\nhdf5_algebra\nHDF5\nPerforms optimized diagonal division between two datasets stored in HDF5 format. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct operation) and uses the most efficient approach. This function is ~50-250x faster than traditional matrix operations for diagonal computations.\nbdDiag_divide_hdf5(filename, group, A, B, groupB = NULL, target = NULL, outgroup = NULL, outdataset = NULL, paral = NULL, threads = NULL, overwrite = NULL)\n../api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.qmd\n\n\nbdDiag_multiply_hdf5\nhdf5_algebra\nHDF5\nPerforms optimized diagonal multiplication between two datasets stored in HDF5 format. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct operation) and uses the most efficient approach. This function performs element-wise multiplication and is ~50-250x faster than traditional matrix operations.\nbdDiag_multiply_hdf5(filename, group, A, B, groupB = NULL, target = NULL, outgroup = NULL, outdataset = NULL, paral = NULL, threads = NULL, overwrite = NULL)\n../api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.qmd\n\n\nbdDiag_scalar_hdf5\nhdf5_algebra\nHDF5\nPerforms optimized scalar operations on diagonal elements of matrices or vectors stored in HDF5 format. Automatically detects whether input is a matrix (extracts diagonal) or vector (direct operation) and applies the specified scalar operation.\nbdDiag_scalar_hdf5(filename, group, dataset, scalar, operation, target = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)\n../api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.qmd\n\n\nbdDiag_subtract_hdf5\nhdf5_algebra\nHDF5\nPerforms optimized diagonal subtraction between two datasets stored in HDF5 format. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct operation) and uses the most efficient approach. This function is ~50-250x faster than traditional matrix operations for diagonal computations.\nbdDiag_subtract_hdf5(filename, group, A, B, groupB = NULL, target = NULL, outgroup = NULL, outdataset = NULL, paral = NULL, threads = NULL, overwrite = NULL)\n../api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.qmd\n\n\nbdEigen_hdf5\nhdf5_algebra\nHDF5\nComputes the eigenvalue decomposition of a large matrix stored in an HDF5 file using the Spectra library. This provides consistent results with the RSpectra package and can handle both symmetric and non-symmetric matrices.\nbdEigen_hdf5(filename, group = NULL, dataset = NULL, k = NULL, which = NULL, ncv = NULL, bcenter = NULL, bscale = NULL, tolerance = NULL, max_iter = NULL, compute_vectors = NULL, overwrite = NULL, threads = NULL)\n../api-reference/r/hdf5_algebra/bdEigen_hdf5.qmd\n\n\nbdgetDatasetsList_hdf5\nhdf5_io_management\nHDF5\nRetrieves a list of all datasets within a specified HDF5 group, with optional filtering by prefix or suffix.\nbdgetDatasetsList_hdf5(filename, group, prefix = NULL)\n../api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.qmd\n\n\nbdgetDiagonal_hdf5\nhdf5_algebra\nHDF5\nRetrieves the diagonal elements from a matrix stored in an HDF5 file.\nbdgetDiagonal_hdf5(filename, group, dataset)\n../api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.qmd\n\n\nbdgetDim_hdf5\nhdf5_io_management\nHDF5\nRetrieves the dimensions (number of rows and columns) of a dataset stored in an HDF5 file.\nbdgetDim_hdf5(filename, dataset)\n../api-reference/r/hdf5_io_management/bdgetDim_hdf5.qmd\n\n\nbdgetSDandMean_hdf5\nhdf5_statistics\nHDF5\nComputes standard deviation and/or mean statistics for a matrix stored in HDF5 format, with support for row-wise or column-wise computations.\nbdgetSDandMean_hdf5(filename, group, dataset, outgroup = NULL, outdataset = NULL, sd = NULL, mean = NULL, byrows = NULL, onmemory = NULL, wsize = NULL, overwrite = FALSE)\n../api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.qmd\n\n\nbdImportData_hdf5\nhdf5_io_management\nHDF5\n—\nbdImportData_hdf5(...)\n../api-reference/r/hdf5_io_management/bdImportData_hdf5.qmd\n\n\nbdImportTextFile_hdf5\nhdf5_io_management\nHDF5\nConverts a text file (e.g., CSV, TSV) to HDF5 format, providing efficient storage and access capabilities.\nbdImportTextFile_hdf5(filename, outputfile, outGroup, outDataset, sep = NULL, header = FALSE, rownames = FALSE, overwrite = FALSE, paral = NULL, threads = NULL, overwriteFile = NULL)\n../api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.qmd\n\n\nbdImputeSNPs_hdf5\nhdf5_statistics\nHDF5\nPerforms imputation of missing values in SNP (Single Nucleotide Polymorphism) data stored in HDF5 format.\nbdImputeSNPs_hdf5(filename, group, dataset, outgroup = NULL, outdataset = NULL, bycols = TRUE, paral = NULL, threads = NULL, overwrite = NULL)\n../api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.qmd\n\n\nbdInvCholesky_hdf5\nhdf5_algebra\nHDF5\nComputes the inverse of a symmetric positive-definite matrix stored in an HDF5 file using the Cholesky decomposition method. This approach is more efficient and numerically stable than general matrix inversion methods for symmetric positive-definite matrices.\nbdInvCholesky_hdf5(filename, group, dataset, outdataset, outgroup = NULL, fullMatrix = NULL, overwrite = NULL, threads = 2L, elementsBlock = 1000000L)\n../api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.qmd\n\n\nbdIsLocked_hdf5\nhdf5_io_management\nHDF5\nUses HDF5 file locking to check if filename can be opened in read/write mode. If opening fails under locking, the file is treated as “in use” and TRUE is returned. Non-existent files return FALSE.\nbdIsLocked_hdf5(filename)\n../api-reference/r/hdf5_io_management/bdIsLocked_hdf5.qmd\n\n\nbdmove_hdf5_dataset\nhdf5_io_management\nHDF5\nMoves an HDF5 dataset from one location to another within the same HDF5 file. This function automatically handles moving associated rownames and colnames datasets, creates parent groups if needed, and updates all internal references.\nbdmove_hdf5_dataset(filename, source_path, dest_path, overwrite = FALSE)\n../api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.qmd\n\n\nbdNormalize_hdf5\nhdf5_statistics\nHDF5\n—\nbdNormalize_hdf5(filename, group, dataset, bcenter = NULL, bscale = NULL, byrows = NULL, wsize = NULL, overwrite = FALSE)\n../api-reference/r/hdf5_statistics/bdNormalize_hdf5.qmd\n\n\nbdPCA_hdf5\nhdf5_algebra\nHDF5\nPerforms Principal Component Analysis (PCA) on a large matrix stored in an HDF5 file. PCA reduces the dimensionality of the data while preserving as much variance as possible. The implementation uses SVD internally for efficient and numerically stable computation.\nbdPCA_hdf5(filename, group, dataset, ncomponents = 0L, bcenter = FALSE, bscale = FALSE, k = 2L, q = 1L, rankthreshold = 0.0, SVDgroup = NULL, overwrite = FALSE, method = NULL, threads = NULL)\n../api-reference/r/hdf5_algebra/bdPCA_hdf5.qmd\n\n\nbdpseudoinv\nlinear_algebra\nIn-memory\nComputes the Moore-Penrose pseudoinverse of a matrix using SVD decomposition. This implementation handles both square and rectangular matrices, and provides numerically stable results even for singular or near-singular matrices.\nbdpseudoinv(X, threads = NULL)\n../api-reference/r/linear_algebra/bdpseudoinv.qmd\n\n\nbdpseudoinv_hdf5\nhdf5_algebra\nHDF5\nComputes the Moore-Penrose pseudoinverse of a matrix stored in HDF5 format. The implementation is designed for large matrices, using block-based processing and efficient I/O operations.\nbdpseudoinv_hdf5(filename, group, dataset, outgroup = NULL, outdataset = NULL, overwrite = NULL, threads = NULL)\n../api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.qmd\n\n\nbdQR\nlinear_algebra\nIn-memory\nComputes the QR decomposition (also called QR factorization) of a matrix A into a product A = QR where Q is an orthogonal matrix and R is an upper triangular matrix. This function operates on in-memory matrices.\nbdQR(X, thin = NULL, block_size = NULL, threads = NULL)\n../api-reference/r/linear_algebra/bdQR.qmd\n\n\nbdQR_hdf5\nhdf5_algebra\nHDF5\nComputes the QR decomposition of a matrix stored in an HDF5 file, factoring it into a product A = QR where Q is an orthogonal matrix and R is an upper triangular matrix. Results are stored back in the HDF5 file.\nbdQR_hdf5(filename, group, dataset, outgroup = NULL, outdataset = NULL, thin = NULL, block_size = NULL, overwrite = NULL, threads = NULL)\n../api-reference/r/hdf5_algebra/bdQR_hdf5.qmd\n\n\nbdReduce_hdf5_dataset\nhdf5_io_management\nHDF5\nReduces multiple datasets within an HDF5 group using arithmetic operations (addition or subtraction).\nbdReduce_hdf5_dataset(filename, group, reducefunction, outgroup = NULL, outdataset = NULL, overwrite = FALSE, remove = FALSE)\n../api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.qmd\n\n\nbdRemove_hdf5_element\nhdf5_io_management\nHDF5\nRemoves specified groups or datasets from an HDF5 file.\nbdRemove_hdf5_element(filename, elements)\n../api-reference/r/hdf5_io_management/bdRemove_hdf5_element.qmd\n\n\nbdRemovelowdata_hdf5\nhdf5_statistics\nHDF5\nRemoves SNPs (Single Nucleotide Polymorphisms) with low representation from genomic data stored in HDF5 format.\nbdRemovelowdata_hdf5(filename, group, dataset, outgroup, outdataset, pcent, bycols, overwrite = NULL)\n../api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.qmd\n\n\nbdRemoveMAF_hdf5\nhdf5_statistics\nHDF5\nFilters SNPs (Single Nucleotide Polymorphisms) based on Minor Allele Frequency (MAF) in genomic data stored in HDF5 format.\nbdRemoveMAF_hdf5(filename, group, dataset, outgroup, outdataset, maf, bycols, blocksize, overwrite = NULL)\n../api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.qmd\n\n\nbdScalarwproduct\nlinear_algebra\nIn-memory\nMultiplies a numeric matrix A by a scalar weight w, returning . The input must be a base R numeric matrix (or convertible to one).\nbdScalarwproduct(A, w)\n../api-reference/r/linear_algebra/bdScalarwproduct.qmd\n\n\nbdSolve\nlinear_algebra\nIn-memory\nSolves the linear system AX = B where A is an N-by-N matrix and X and B are N-by-NRHS matrices. The function automatically detects if A is symmetric and uses the appropriate solver.\nbdSolve(A, B)\n../api-reference/r/linear_algebra/bdSolve.qmd\n\n\nbdSolve_hdf5\nhdf5_algebra\nHDF5\nSolves the linear system AX = B where matrices A and B are stored in HDF5 format. The solution X is written back to the HDF5 file.\nbdSolve_hdf5(filename, groupA, datasetA, groupB, datasetB, outgroup = NULL, outdataset = NULL, overwrite = NULL)\n../api-reference/r/hdf5_algebra/bdSolve_hdf5.qmd\n\n\nbdSort_hdf5_dataset\nhdf5_io_management\nHDF5\nSorts a dataset in an HDF5 file based on a predefined ordering specified through a list of sorting blocks.\nbdSort_hdf5_dataset(filename, group, dataset, outdataset, blockedSortlist, func, outgroup = NULL, overwrite = FALSE)\n../api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.qmd\n\n\nbdSplit_matrix_hdf5\nhdf5_io_management\nHDF5\nSplits a large dataset in an HDF5 file into smaller submatrices, with support for both row-wise and column-wise splitting.\nbdSplit_matrix_hdf5(filename, group, dataset, outgroup = NULL, outdataset = NULL, nblocks = NULL, blocksize = NULL, bycols = TRUE, overwrite = FALSE)\n../api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.qmd\n\n\nbdsubset_hdf5_dataset\nhdf5_io_management\nHDF5\nCreates a new HDF5 dataset containing only the specified rows or columns from an existing dataset. This operation is memory efficient as it uses HDF5’s hyperslab selection for direct disk-to-disk copying without loading the entire dataset into memory.\nbdsubset_hdf5_dataset(filename, dataset_path, indices, select_rows = TRUE, new_group = \"\", new_name = \"\", overwrite = FALSE)\n../api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.qmd\n\n\nbdSVD_hdf5\nhdf5_algebra\nHDF5\nComputes the Singular Value Decomposition (SVD) of a large matrix stored in an HDF5 file. The SVD decomposes a matrix A into a product A = UDV’ where U and V are orthogonal matrices and D is a diagonal matrix containing the singular values.\nbdSVD_hdf5(filename, group = NULL, dataset = NULL, k = 2L, q = 1L, bcenter = TRUE, bscale = TRUE, rankthreshold = 0.0, overwrite = NULL, method = NULL, threads = NULL)\n../api-reference/r/hdf5_algebra/bdSVD_hdf5.qmd\n\n\nbdtCrossprod\nlinear_algebra\nIn-memory\nComputes matrix transposed cross-products efficiently using block-based algorithms and optional parallel processing. Supports both single-matrix (XX’) and two-matrix (XY’) transposed cross-products.\nbdtCrossprod(A, B = NULL, transposed = NULL, block_size = NULL, paral = NULL, threads = NULL)\n../api-reference/r/linear_algebra/bdtCrossprod.qmd\n\n\nbdtCrossprod_hdf5\nhdf5_algebra\nHDF5\n—\nbdtCrossprod_hdf5(filename, group, A, B = NULL, groupB = NULL, block_size = NULL, mixblock_size = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)\n../api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.qmd\n\n\nbdWrite_hdf5_dimnames\nhdf5_io_management\nHDF5\nWrite row and/or column names metadata for an existing dataset in an HDF5 file. Empty vectors skip the corresponding dimnames.\nbdWrite_hdf5_dimnames(filename, group, dataset, rownames, colnames)\n../api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.qmd\n\n\nbdWriteDiagonal_hdf5\nhdf5_algebra\nHDF5\nUpdates the diagonal elements of a matrix stored in an HDF5 file.\nbdWriteDiagonal_hdf5(diagonal, filename, group, dataset)\n../api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.qmd\n\n\nbdWriteOppsiteTriangularMatrix_hdf5\nhdf5_algebra\nHDF5\nCreates a symmetric matrix by mirroring values from one triangular part to the other in an HDF5-stored matrix. This function modifies the matrix in-place, either copying the upper triangular values to the lower triangular part or vice versa.\nbdWriteOppsiteTriangularMatrix_hdf5(filename, group, dataset, copytolower = NULL, elementsBlock = 1000000L)\n../api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.qmd"
  },
  {
    "objectID": "api-reference/r/other/bdblockMult.html",
    "href": "api-reference/r/other/bdblockMult.html",
    "title": "bdblockMult",
    "section": "",
    "text": "OTHER",
    "crumbs": [
      "Other Functions",
      "bdblockMult"
    ]
  },
  {
    "objectID": "api-reference/r/other/bdblockMult.html#description",
    "href": "api-reference/r/other/bdblockMult.html#description",
    "title": "bdblockMult",
    "section": "1 Description",
    "text": "1 Description\nPerforms efficient matrix multiplication using block-based algorithms. The function supports various input combinations (matrix-matrix, matrix-vector, vector-vector) and provides options for parallel processing and block-based computation.",
    "crumbs": [
      "Other Functions",
      "bdblockMult"
    ]
  },
  {
    "objectID": "api-reference/r/other/bdblockMult.html#usage",
    "href": "api-reference/r/other/bdblockMult.html#usage",
    "title": "bdblockMult",
    "section": "2 Usage",
    "text": "2 Usage\nbdblockMult(A, B, block_size = NULL, paral = NULL, byBlocks = TRUE, threads = NULL)",
    "crumbs": [
      "Other Functions",
      "bdblockMult"
    ]
  },
  {
    "objectID": "api-reference/r/other/bdblockMult.html#arguments",
    "href": "api-reference/r/other/bdblockMult.html#arguments",
    "title": "bdblockMult",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nA\nMatrix or vector. First input operand.\n\n\nB\nMatrix or vector. Second input operand.\n\n\nblock_size\nInteger. Block size for computation. If NULL, uses maximum allowed block size.\n\n\nparal\nLogical. If TRUE, enables parallel computation. Default is FALSE.\n\n\nbyBlocks\nLogical. If TRUE (default), forces block-based computation for large matrices. Can be set to FALSE to disable blocking.\n\n\nthreads\nInteger. Number of threads for parallel computation. If NULL, uses half of available threads or maximum allowed threads.",
    "crumbs": [
      "Other Functions",
      "bdblockMult"
    ]
  },
  {
    "objectID": "api-reference/r/other/bdblockMult.html#value",
    "href": "api-reference/r/other/bdblockMult.html#value",
    "title": "bdblockMult",
    "section": "4 Value",
    "text": "4 Value\n\nMatrix or vector containing the result of A * B.",
    "crumbs": [
      "Other Functions",
      "bdblockMult"
    ]
  },
  {
    "objectID": "api-reference/r/other/bdblockMult.html#details",
    "href": "api-reference/r/other/bdblockMult.html#details",
    "title": "bdblockMult",
    "section": "5 Details",
    "text": "5 Details\nThis function implements block-based matrix multiplication algorithms optimized for cache efficiency and memory usage. Key features: - Input combinations supported: - Matrix-matrix multiplication - Matrix-vector multiplication (both left and right) - Vector-vector multiplication - Performance optimizations: - Block-based computation for cache efficiency - Parallel processing for large matrices - Automatic block size selection - Memory-efficient implementation\nThe function automatically selects the appropriate multiplication method based on input types and sizes. For large matrices (&gt;2.25e+08 elements), block-based computation is used by default.",
    "crumbs": [
      "Other Functions",
      "bdblockMult"
    ]
  },
  {
    "objectID": "api-reference/r/other/bdblockMult.html#examples",
    "href": "api-reference/r/other/bdblockMult.html#examples",
    "title": "bdblockMult",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Matrix-matrix multiplication\nN &lt;- 2500\nM &lt;- 400\nnc &lt;- 4\n\nset.seed(555)\nmat &lt;- matrix(rnorm(N*M, mean=0, sd=10), N, M)\n\n# Parallel block multiplication\nresult &lt;- bdblockMult(mat, mat,\n                      paral = TRUE,\n                      threads = nc)\n\n# Matrix-vector multiplication\nvec &lt;- rnorm(M)\nresult_mv &lt;- bdblockMult(mat, vec,\n                         paral = TRUE,\n                         threads = nc)",
    "crumbs": [
      "Other Functions",
      "bdblockMult"
    ]
  },
  {
    "objectID": "api-reference/r/other/bdblockMult.html#see-also",
    "href": "api-reference/r/other/bdblockMult.html#see-also",
    "title": "bdblockMult",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdblockSum for block-based matrix addition\nbdblockSubstract for block-based matrix subtraction",
    "crumbs": [
      "Other Functions",
      "bdblockMult"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdtCrossprod.html",
    "href": "api-reference/r/linear_algebra/bdtCrossprod.html",
    "title": "bdtCrossprod",
    "section": "",
    "text": "LINEAR_ALGEBRA",
    "crumbs": [
      "Linear Algebra",
      "bdtCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdtCrossprod.html#description",
    "href": "api-reference/r/linear_algebra/bdtCrossprod.html#description",
    "title": "bdtCrossprod",
    "section": "1 Description",
    "text": "1 Description\nComputes matrix transposed cross-products efficiently using block-based algorithms and optional parallel processing. Supports both single-matrix (XX’) and two-matrix (XY’) transposed cross-products.",
    "crumbs": [
      "Linear Algebra",
      "bdtCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdtCrossprod.html#usage",
    "href": "api-reference/r/linear_algebra/bdtCrossprod.html#usage",
    "title": "bdtCrossprod",
    "section": "2 Usage",
    "text": "2 Usage\nbdtCrossprod(A, B = NULL, transposed = NULL, block_size = NULL, paral = NULL, threads = NULL)",
    "crumbs": [
      "Linear Algebra",
      "bdtCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdtCrossprod.html#arguments",
    "href": "api-reference/r/linear_algebra/bdtCrossprod.html#arguments",
    "title": "bdtCrossprod",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nA\nNumeric matrix. First input matrix.\n\n\nB\nOptional numeric matrix. If provided, computes XY’ instead of XX’.\n\n\ntransposed\nLogical. If TRUE, uses transposed input matrix.\n\n\nblock_size\nInteger. Block size for computation. If NULL, uses optimal block size based on matrix dimensions and cache size.\n\n\nparal\nLogical. If TRUE, enables parallel computation.\n\n\nthreads\nInteger. Number of threads for parallel computation. If NULL, uses all available threads.",
    "crumbs": [
      "Linear Algebra",
      "bdtCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdtCrossprod.html#value",
    "href": "api-reference/r/linear_algebra/bdtCrossprod.html#value",
    "title": "bdtCrossprod",
    "section": "4 Value",
    "text": "4 Value\n\nNumeric matrix containing the transposed cross-product result.",
    "crumbs": [
      "Linear Algebra",
      "bdtCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdtCrossprod.html#details",
    "href": "api-reference/r/linear_algebra/bdtCrossprod.html#details",
    "title": "bdtCrossprod",
    "section": "5 Details",
    "text": "5 Details\nThis function implements efficient transposed cross-product computation using block-based algorithms optimized for cache efficiency and memory usage. Key features: - Operation modes: - Single matrix: Computes XX’ - Two matrices: Computes XY’ - Performance optimizations: - Block-based computation for cache efficiency - Parallel processing for large matrices - Automatic block size selection - Memory-efficient implementation\nThe function automatically selects optimal computation strategies based on input size and available resources. For large matrices, block-based computation is used to improve cache utilization.",
    "crumbs": [
      "Linear Algebra",
      "bdtCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdtCrossprod.html#examples",
    "href": "api-reference/r/linear_algebra/bdtCrossprod.html#examples",
    "title": "bdtCrossprod",
    "section": "6 Examples",
    "text": "6 Examples\n\nlibrary(BigDataStatMeth)\n\n# Single matrix transposed cross-product\nn &lt;- 100\np &lt;- 60\nX &lt;- matrix(rnorm(n*p), nrow=n, ncol=p)\nres &lt;- bdtCrossprod(X)\n\n# Verify against base R\nall.equal(tcrossprod(X), res)\n\n# Two-matrix transposed cross-product\nn &lt;- 100\np &lt;- 100\nY &lt;- matrix(rnorm(n*p), nrow=n)\nres &lt;- bdtCrossprod(X, Y)\n\n# Parallel computation\nres_par &lt;- bdtCrossprod(X, Y,\n                        paral = TRUE,\n                        threads = 4)",
    "crumbs": [
      "Linear Algebra",
      "bdtCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdtCrossprod.html#see-also",
    "href": "api-reference/r/linear_algebra/bdtCrossprod.html#see-also",
    "title": "bdtCrossprod",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdCrossprod for standard cross-product\nbdblockMult for block-based matrix multiplication",
    "crumbs": [
      "Linear Algebra",
      "bdtCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bd_wproduct.html",
    "href": "api-reference/r/linear_algebra/bd_wproduct.html",
    "title": "bd_wproduct",
    "section": "",
    "text": "LINEAR_ALGEBRA",
    "crumbs": [
      "Linear Algebra",
      "bd_wproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bd_wproduct.html#description",
    "href": "api-reference/r/linear_algebra/bd_wproduct.html#description",
    "title": "bd_wproduct",
    "section": "1 Description",
    "text": "1 Description\nCompute weighted operations using a diagonal weight from w: Inputs may be base numeric matrices .",
    "crumbs": [
      "Linear Algebra",
      "bd_wproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bd_wproduct.html#usage",
    "href": "api-reference/r/linear_algebra/bd_wproduct.html#usage",
    "title": "bd_wproduct",
    "section": "2 Usage",
    "text": "2 Usage\nbd_wproduct(X, w, op)",
    "crumbs": [
      "Linear Algebra",
      "bd_wproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bd_wproduct.html#arguments",
    "href": "api-reference/r/linear_algebra/bd_wproduct.html#arguments",
    "title": "bd_wproduct",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nX\nNumeric matrix (n x p).\n\n\nw\nNumeric weight vector (length or ), or a 1D matrix coerced to a vector.\n\n\nop\nCharacter string (case-insensitive): one of , , , .",
    "crumbs": [
      "Linear Algebra",
      "bd_wproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bd_wproduct.html#value",
    "href": "api-reference/r/linear_algebra/bd_wproduct.html#value",
    "title": "bd_wproduct",
    "section": "4 Value",
    "text": "4 Value\n\nNumeric matrix with dimensions depending on : for , for , and for /.",
    "crumbs": [
      "Linear Algebra",
      "bd_wproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bd_wproduct.html#details",
    "href": "api-reference/r/linear_algebra/bd_wproduct.html#details",
    "title": "bd_wproduct",
    "section": "5 Details",
    "text": "5 Details\nw is interpreted as the diagonal of a weight matrix; its required length depends on the operation: rows for \"xtwx\" and \"wx\", columns for \"xwxt\" and \"xw\".",
    "crumbs": [
      "Linear Algebra",
      "bd_wproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bd_wproduct.html#examples",
    "href": "api-reference/r/linear_algebra/bd_wproduct.html#examples",
    "title": "bd_wproduct",
    "section": "6 Examples",
    "text": "6 Examples\n\nset.seed(1)\nn &lt;- 10; p &lt;- 5\nX &lt;- matrix(rnorm(n * p), n, p)\nu &lt;- runif(n); w &lt;- u * (1 - u)\nbd_wproduct(X, w, \"xtwx\")  # p x p\nbd_wproduct(X, w, \"wx\")    # n x p (row scaling)\n\nv &lt;- runif(p)\nbd_wproduct(X, v, \"xw\")    # n x p (col scaling)\nbd_wproduct(X, v, \"xwxt\")  # n x n",
    "crumbs": [
      "Linear Algebra",
      "bd_wproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdScalarwproduct.html",
    "href": "api-reference/r/linear_algebra/bdScalarwproduct.html",
    "title": "bdScalarwproduct",
    "section": "",
    "text": "LINEAR_ALGEBRA",
    "crumbs": [
      "Linear Algebra",
      "bdScalarwproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdScalarwproduct.html#description",
    "href": "api-reference/r/linear_algebra/bdScalarwproduct.html#description",
    "title": "bdScalarwproduct",
    "section": "1 Description",
    "text": "1 Description\nMultiplies a numeric matrix A by a scalar weight w, returning . The input must be a base R numeric matrix (or convertible to one).",
    "crumbs": [
      "Linear Algebra",
      "bdScalarwproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdScalarwproduct.html#usage",
    "href": "api-reference/r/linear_algebra/bdScalarwproduct.html#usage",
    "title": "bdScalarwproduct",
    "section": "2 Usage",
    "text": "2 Usage\nbdScalarwproduct(A, w)",
    "crumbs": [
      "Linear Algebra",
      "bdScalarwproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdScalarwproduct.html#arguments",
    "href": "api-reference/r/linear_algebra/bdScalarwproduct.html#arguments",
    "title": "bdScalarwproduct",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nA\nNumeric matrix (or object convertible to a dense numeric matrix).\n\n\nw\nNumeric scalar weight.",
    "crumbs": [
      "Linear Algebra",
      "bdScalarwproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdScalarwproduct.html#value",
    "href": "api-reference/r/linear_algebra/bdScalarwproduct.html#value",
    "title": "bdScalarwproduct",
    "section": "4 Value",
    "text": "4 Value\n\nA numeric matrix with the same dimensions as .",
    "crumbs": [
      "Linear Algebra",
      "bdScalarwproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdScalarwproduct.html#examples",
    "href": "api-reference/r/linear_algebra/bdScalarwproduct.html#examples",
    "title": "bdScalarwproduct",
    "section": "5 Examples",
    "text": "5 Examples\n\nset.seed(1234)\nn &lt;- 5; p &lt;- 3\nX &lt;- matrix(rnorm(n * p), n, p)\nw &lt;- 0.75\nbdScalarwproduct(X, w)",
    "crumbs": [
      "Linear Algebra",
      "bdScalarwproduct"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCrossprod.html",
    "href": "api-reference/r/linear_algebra/bdCrossprod.html",
    "title": "bdCrossprod",
    "section": "",
    "text": "LINEAR_ALGEBRA",
    "crumbs": [
      "Linear Algebra",
      "bdCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCrossprod.html#description",
    "href": "api-reference/r/linear_algebra/bdCrossprod.html#description",
    "title": "bdCrossprod",
    "section": "1 Description",
    "text": "1 Description\nComputes matrix cross-products efficiently using block-based algorithms and optional parallel processing. Supports both single-matrix (X’X) and two-matrix (X’Y) cross-products.",
    "crumbs": [
      "Linear Algebra",
      "bdCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCrossprod.html#usage",
    "href": "api-reference/r/linear_algebra/bdCrossprod.html#usage",
    "title": "bdCrossprod",
    "section": "2 Usage",
    "text": "2 Usage\nbdCrossprod(A, B = NULL, transposed = NULL, block_size = NULL, paral = NULL, threads = NULL)",
    "crumbs": [
      "Linear Algebra",
      "bdCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCrossprod.html#arguments",
    "href": "api-reference/r/linear_algebra/bdCrossprod.html#arguments",
    "title": "bdCrossprod",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nA\nNumeric matrix. First input matrix.\n\n\nB\nOptional numeric matrix. If provided, computes A’B instead of A’A.\n\n\ntransposed\nLogical. If TRUE, uses transposed input matrix.\n\n\nblock_size\nInteger. Block size for computation. If NULL, uses optimal block size based on matrix dimensions and cache size.\n\n\nparal\nLogical. If TRUE, enables parallel computation.\n\n\nthreads\nInteger. Number of threads for parallel computation. If NULL, uses all available threads.",
    "crumbs": [
      "Linear Algebra",
      "bdCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCrossprod.html#value",
    "href": "api-reference/r/linear_algebra/bdCrossprod.html#value",
    "title": "bdCrossprod",
    "section": "4 Value",
    "text": "4 Value\n\nNumeric matrix containing the cross-product result.",
    "crumbs": [
      "Linear Algebra",
      "bdCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCrossprod.html#details",
    "href": "api-reference/r/linear_algebra/bdCrossprod.html#details",
    "title": "bdCrossprod",
    "section": "5 Details",
    "text": "5 Details\nThis function implements efficient cross-product computation using block-based algorithms optimized for cache efficiency and memory usage. Key features: - Operation modes: - Single matrix: Computes X’X - Two matrices: Computes X’Y - Performance optimizations: - Block-based computation for cache efficiency - Parallel processing for large matrices - Automatic block size selection - Memory-efficient implementation\nThe function automatically selects optimal computation strategies based on input size and available resources. For large matrices, block-based computation is used to improve cache utilization.",
    "crumbs": [
      "Linear Algebra",
      "bdCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCrossprod.html#examples",
    "href": "api-reference/r/linear_algebra/bdCrossprod.html#examples",
    "title": "bdCrossprod",
    "section": "6 Examples",
    "text": "6 Examples\n\nlibrary(BigDataStatMeth)\n\n# Single matrix cross-product\nn &lt;- 100\np &lt;- 60\nX &lt;- matrix(rnorm(n*p), nrow=n, ncol=p)\nres &lt;- bdCrossprod(X)\n\n# Verify against base R\nall.equal(crossprod(X), res)\n\n# Two-matrix cross-product\nn &lt;- 100\np &lt;- 100\nY &lt;- matrix(rnorm(n*p), nrow=n)\nres &lt;- bdCrossprod(X, Y)\n\n# Parallel computation\nres_par &lt;- bdCrossprod(X, Y,\n                       paral = TRUE,\n                       threads = 4)",
    "crumbs": [
      "Linear Algebra",
      "bdCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCrossprod.html#see-also",
    "href": "api-reference/r/linear_algebra/bdCrossprod.html#see-also",
    "title": "bdCrossprod",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdtCrossprod for transposed cross-product\nbdblockMult for block-based matrix multiplication",
    "crumbs": [
      "Linear Algebra",
      "bdCrossprod"
    ]
  },
  {
    "objectID": "api-reference/r/index.html",
    "href": "api-reference/r/index.html",
    "title": "R Functions Reference",
    "section": "",
    "text": "Complete reference for all R functions in BigDataStatMeth package.",
    "crumbs": [
      "R Functions Reference"
    ]
  },
  {
    "objectID": "api-reference/r/index.html#overview",
    "href": "api-reference/r/index.html#overview",
    "title": "R Functions Reference",
    "section": "",
    "text": "Complete reference for all R functions in BigDataStatMeth package.",
    "crumbs": [
      "R Functions Reference"
    ]
  },
  {
    "objectID": "api-reference/r/index.html#categories",
    "href": "api-reference/r/index.html#categories",
    "title": "R Functions Reference",
    "section": "2 Categories",
    "text": "2 Categories\n\n2.1 Linear Algebra\nLinear algebra operations that run in memory (RAM).\n\n\n2.2 HDF5 Algebra\nAlgebra and matrix factorizations over HDF5 datasets (PCA, SVD, QR, Cholesky, eigen, solve, pseudoinverse).\n\n\n2.3 Block-wise Operations\nLow-level block-wise primitives (multiplication, sums, differences, vector–matrix) for scaling and performance.\n\n\n2.4 HDF5 Statistics\nDescriptive statistics, normalization, correlation, and domain-specific preprocessing over HDF5.\n\n\n2.5 HDF5 I/O & Management\nCreation, import, layout and management of HDF5 datasets and groups (I/O, metadata, subsetting, binding, moving, reducing).\n\n\n2.6 Other Functions\nAdditional utility functions and methods.",
    "crumbs": [
      "R Functions Reference"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html",
    "href": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html",
    "title": "bdgetSDandMean_hdf5",
    "section": "",
    "text": "HDF5_STATISTICS",
    "crumbs": [
      "HDF5 Statistics",
      "bdgetSDandMean_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#description",
    "href": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#description",
    "title": "bdgetSDandMean_hdf5",
    "section": "1 Description",
    "text": "1 Description\nComputes standard deviation and/or mean statistics for a matrix stored in HDF5 format, with support for row-wise or column-wise computations.",
    "crumbs": [
      "HDF5 Statistics",
      "bdgetSDandMean_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#usage",
    "href": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#usage",
    "title": "bdgetSDandMean_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdgetSDandMean_hdf5(filename, group, dataset, outgroup = NULL, outdataset = NULL, sd = NULL, mean = NULL, byrows = NULL, onmemory = NULL, wsize = NULL, overwrite = FALSE)",
    "crumbs": [
      "HDF5 Statistics",
      "bdgetSDandMean_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#arguments",
    "title": "bdgetSDandMean_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ngroup\nCharacter string. Path to the group containing the dataset.\n\n\ndataset\nCharacter string. Name of the dataset to analyze.\n\n\nsd\nLogical (optional). Whether to compute sd. Default is TRUE.\n\n\noutgroup\nCharacter string, custom output group name (default: mean_sd)\n\n\noutdataset\nCharacter string, custom correlation dataset name (default: mean.dataset_original_name and sd.dataset_original_name)\n\n\nmean\nLogical (optional). Whether to compute mean. Default is TRUE.\n\n\nbyrows\nLogical (optional). Whether to compute by rows (TRUE) or columns (FALSE). Default is FALSE.\n\n\nwsize\nInteger (optional). Block size for processing. Default is 1000.\n\n\nonmemory\nlogical (default = FALSE). If TRUE, results are kept in memory and returned as a matrix; nothing is written to disk. If FALSE, results are written to disk.\n\n\noverwrite\nLogical (optional). Whether to overwrite existing results. Default is FALSE.",
    "crumbs": [
      "HDF5 Statistics",
      "bdgetSDandMean_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#value",
    "href": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#value",
    "title": "bdgetSDandMean_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nDepending on the parameter:\n\nIf onmemory = TRUE: List with components: \\itemize{\n\\code{mean\nIf onmemory = FALSE: List with components: \\itemize{\n\\code{fn",
    "crumbs": [
      "HDF5 Statistics",
      "bdgetSDandMean_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#details",
    "href": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#details",
    "title": "bdgetSDandMean_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides efficient statistical computation capabilities with: - Computation options: - Standard deviation computation - Mean computation - Row-wise or column-wise processing - Processing features: - Block-based computation - Memory-efficient processing - Configurable block size - Implementation features: - Safe HDF5 file operations - Memory-efficient implementation - Comprehensive error handling\nResults are stored in a new group ‘mean_sd’ within the HDF5 file.",
    "crumbs": [
      "HDF5 Statistics",
      "bdgetSDandMean_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#examples",
    "href": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#examples",
    "title": "bdgetSDandMean_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test matrices\nset.seed(123)\nY &lt;- matrix(rnorm(100), 10, 10)\nX &lt;- matrix(rnorm(10), 10, 1)\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", Y, \"data\", \"matrix1\",\n                     overwriteFile = TRUE)\nbdCreate_hdf5_matrix(\"test.hdf5\", X, \"data\", \"vector1\",\n                     overwriteFile = FALSE)\n\n# Compute statistics\nbdgetSDandMean_hdf5(\n  filename = \"test.hdf5\",\n  group = \"data\",\n  dataset = \"matrix1\",\n  sd = TRUE,\n  mean = TRUE,\n  byrows = TRUE,\n  wsize = 500\n)\n\n# Cleanup\nif (file.exists(\"test.hdf5\")) {\n  file.remove(\"test.hdf5\")\n}",
    "crumbs": [
      "HDF5 Statistics",
      "bdgetSDandMean_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_statistics/bdgetSDandMean_hdf5.html#see-also",
    "title": "bdgetSDandMean_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 Statistics",
      "bdgetSDandMean_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html",
    "href": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html",
    "title": "bdRemovelowdata_hdf5",
    "section": "",
    "text": "HDF5_STATISTICS",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemovelowdata_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#description",
    "href": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#description",
    "title": "bdRemovelowdata_hdf5",
    "section": "1 Description",
    "text": "1 Description\nRemoves SNPs (Single Nucleotide Polymorphisms) with low representation from genomic data stored in HDF5 format.",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemovelowdata_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#usage",
    "href": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#usage",
    "title": "bdRemovelowdata_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdRemovelowdata_hdf5(filename, group, dataset, outgroup, outdataset, pcent, bycols, overwrite = NULL)",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemovelowdata_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#arguments",
    "title": "bdRemovelowdata_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ngroup\nCharacter string. Path to the group containing input dataset.\n\n\ndataset\nCharacter string. Name of the dataset to filter.\n\n\noutgroup\nCharacter string. Output group path for filtered data.\n\n\noutdataset\nCharacter string. Output dataset name for filtered data.\n\n\npcent\nNumeric (optional). Threshold percentage for removal (0-1). Default is 0.5. SNPs with representation below this threshold are removed.\n\n\nbycols\nLogical (optional). Whether to filter by columns (TRUE) or rows (FALSE). Default is TRUE.\n\n\noverwrite\nLogical (optional). Whether to overwrite existing dataset. Default is FALSE.",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemovelowdata_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#value",
    "href": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#value",
    "title": "bdRemovelowdata_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the filtered dataset (group/dataset)\nnremoved: Integer with the number of rows/columns removed due to low data quality",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemovelowdata_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#details",
    "href": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#details",
    "title": "bdRemovelowdata_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides efficient filtering capabilities for genomic data with support for: - Filtering options: - Row-wise or column-wise filtering - Configurable threshold percentage - Flexible output location - Implementation features: - Memory-efficient processing - Safe file operations - Comprehensive error handling - Progress reporting\nThe function supports both in-place modification and creation of new datasets.",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemovelowdata_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#examples",
    "href": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#examples",
    "title": "bdRemovelowdata_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test SNP data with missing values\nsnps &lt;- matrix(sample(c(0, 1, 2, NA), 100, replace = TRUE,\n                     prob = c(0.3, 0.3, 0.3, 0.1)), 10, 10)\n\n# Save to HDF5\nfn &lt;- \"snp_data.hdf5\"\nbdCreate_hdf5_matrix(fn, snps, \"genotype\", \"raw_snps\",\n                     overwriteFile = TRUE)\n\n# Remove SNPs with low representation\nbdRemovelowdata_hdf5(\n  filename = fn,\n  group = \"genotype\",\n  dataset = \"raw_snps\",\n  outgroup = \"genotype_filtered\",\n  outdataset = \"filtered_snps\",\n  pcent = 0.3,\n  bycols = TRUE\n)\n\n# Cleanup\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemovelowdata_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_statistics/bdRemovelowdata_hdf5.html#see-also",
    "title": "bdRemovelowdata_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdImputeSNPs_hdf5 for imputing missing SNP values\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemovelowdata_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdNormalize_hdf5.html",
    "href": "api-reference/r/hdf5_statistics/bdNormalize_hdf5.html",
    "title": "bdNormalize_hdf5",
    "section": "",
    "text": "HDF5_STATISTICS",
    "crumbs": [
      "HDF5 Statistics",
      "bdNormalize_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdNormalize_hdf5.html#usage",
    "href": "api-reference/r/hdf5_statistics/bdNormalize_hdf5.html#usage",
    "title": "bdNormalize_hdf5",
    "section": "1 Usage",
    "text": "1 Usage\nbdNormalize_hdf5(filename, group, dataset, bcenter = NULL, bscale = NULL, byrows = NULL, wsize = NULL, overwrite = FALSE)",
    "crumbs": [
      "HDF5 Statistics",
      "bdNormalize_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdNormalize_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_statistics/bdNormalize_hdf5.html#arguments",
    "title": "bdNormalize_hdf5",
    "section": "2 Arguments",
    "text": "2 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString indicating the HDF5 file path\n\n\ngroup\nString specifying the group containing the dataset\n\n\ndataset\nString specifying the dataset name to normalize\n\n\nbcenter\nOptional boolean indicating whether to center the data. If TRUE (default), subtracts mean from each column/row\n\n\nbscale\nOptional boolean indicating whether to scale the data. If TRUE (default), divides by standard deviation\n\n\nbyrows\nOptional boolean indicating whether to operate by rows. If TRUE, processes row-wise; if FALSE (default), column-wise\n\n\nwsize\nOptional integer specifying the block size for processing. Default is 1000\n\n\noverwrite\nOptional boolean indicating whether to overwrite existing datasets. Default is false",
    "crumbs": [
      "HDF5 Statistics",
      "bdNormalize_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdNormalize_hdf5.html#value",
    "href": "api-reference/r/hdf5_statistics/bdNormalize_hdf5.html#value",
    "title": "bdNormalize_hdf5",
    "section": "3 Value",
    "text": "3 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string. Path to the HDF5 file containing the results\nds: Character string. Full dataset path to the normalized data, stored under “NORMALIZED/\\[group\\]/\\[dataset\\]”\nmean: Character string. Dataset path to the column means used for centering, stored under “NORMALIZED/\\[group\\]/mean.\\[dataset\\]”\nsd: Character string. Dataset path to the standard deviations used for scaling, stored under “NORMALIZED/\\[group\\]/sd.\\[dataset\\]”",
    "crumbs": [
      "HDF5 Statistics",
      "bdNormalize_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdNormalize_hdf5.html#details",
    "href": "api-reference/r/hdf5_statistics/bdNormalize_hdf5.html#details",
    "title": "bdNormalize_hdf5",
    "section": "4 Details",
    "text": "4 Details\nThe function implements block-wise normalization through:\nStatistical computations: - Mean calculation (for centering) - Standard deviation calculation (for scaling) - Efficient block-wise updates\nMemory efficiency: - Block-wise data processing - Minimal temporary storage - Proper resource cleanup\nProcessing options: - Row-wise or column-wise operations - Flexible block size selection - Optional centering and scaling\nError handling: - Input validation - Resource management - Exception handling",
    "crumbs": [
      "HDF5 Statistics",
      "bdNormalize_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdNormalize_hdf5.html#examples",
    "href": "api-reference/r/hdf5_statistics/bdNormalize_hdf5.html#examples",
    "title": "bdNormalize_hdf5",
    "section": "5 Examples",
    "text": "5 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test data\ndata &lt;- matrix(rnorm(1000*100), 1000, 100)\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", data, \"data\", \"matrix\",\n                     overwriteFile = TRUE)\n\n# Normalize data\nbdNormalize_hdf5(\"test.hdf5\", \"data\", \"matrix\",\n                 bcenter = TRUE,\n                 bscale = TRUE,\n                 wsize = 1000)",
    "crumbs": [
      "HDF5 Statistics",
      "bdNormalize_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdCorr_hdf5.html",
    "href": "api-reference/r/hdf5_statistics/bdCorr_hdf5.html",
    "title": "bdCorr_hdf5",
    "section": "",
    "text": "HDF5_STATISTICS",
    "crumbs": [
      "HDF5 Statistics",
      "bdCorr_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdCorr_hdf5.html#description",
    "href": "api-reference/r/hdf5_statistics/bdCorr_hdf5.html#description",
    "title": "bdCorr_hdf5",
    "section": "1 Description",
    "text": "1 Description\nThis function computes Pearson or Spearman correlation matrix for matrices stored in HDF5 format. It automatically detects whether to compute: It automatically selects between direct computation for small matrices and block-wise processing for large matrices to optimize memory usage and performance.\nCorrelation types supported: \nFor omics data analysis:",
    "crumbs": [
      "HDF5 Statistics",
      "bdCorr_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdCorr_hdf5.html#usage",
    "href": "api-reference/r/hdf5_statistics/bdCorr_hdf5.html#usage",
    "title": "bdCorr_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdCorr_hdf5(filename_x, group_x, dataset_x, filename_y = \"\", group_y = \"\", dataset_y = \"\", trans_x = FALSE, trans_y = FALSE, method = \"pearson\", use_complete_obs = TRUE, compute_pvalues = TRUE, block_size = 1000L, overwrite = FALSE, output_filename = \"\", output_group = \"\", output_dataset_corr = \"\", output_dataset_pval = \"\", threads = -1L)",
    "crumbs": [
      "HDF5 Statistics",
      "bdCorr_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdCorr_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_statistics/bdCorr_hdf5.html#arguments",
    "title": "bdCorr_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename_x\nCharacter string with the path to the HDF5 file containing matrix X\n\n\ngroup_x\nCharacter string indicating the group containing matrix X\n\n\ndataset_x\nCharacter string indicating the dataset name of matrix X\n\n\nfilename_y\nCharacter string with the path to the HDF5 file containing matrix Y (optional, default: ““)\n\n\ngroup_y\nCharacter string indicating the group containing matrix Y (optional, default: ““)\n\n\ndataset_y\nCharacter string indicating the dataset name of matrix Y (optional, default: ““)\n\n\ntrans_x\nLogical, whether to transpose matrix X (default: FALSE)\n\n\ntrans_y\nLogical, whether to transpose matrix Y (default: FALSE, ignored for single matrix)\n\n\nmethod\nCharacter string indicating correlation method (“pearson” or “spearman”, default: “pearson”)\n\n\nuse_complete_obs\nLogical, whether to use only complete observations (default: TRUE)\n\n\ncompute_pvalues\nLogical, whether to compute p-values for correlations (default: TRUE)\n\n\nblock_size\nInteger, block size for large matrix processing (default: 1000)\n\n\noverwrite\nLogical, whether to overwrite existing results (default: FALSE)\n\n\noutput_filename\nCharacter string, output HDF5 file (default: same as filename_x)\n\n\noutput_group\nCharacter string, custom output group name (default: auto-generated)\n\n\noutput_dataset_corr\nCharacter string, custom correlation dataset name (default: “correlation”)\n\n\noutput_dataset_pval\nCharacter string, custom p-values dataset name (default: “pvalues”)\n\n\nthreads\nInteger, number of threads for parallel computation (optional, default: auto)",
    "crumbs": [
      "HDF5 Statistics",
      "bdCorr_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdCorr_hdf5.html#value",
    "href": "api-reference/r/hdf5_statistics/bdCorr_hdf5.html#value",
    "title": "bdCorr_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the correlation matrix (group/dataset)",
    "crumbs": [
      "HDF5 Statistics",
      "bdCorr_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdCorr_hdf5.html#examples",
    "href": "api-reference/r/hdf5_statistics/bdCorr_hdf5.html#examples",
    "title": "bdCorr_hdf5",
    "section": "5 Examples",
    "text": "5 Examples\n\n\nCode\n# Backward compatible - existing code works unchanged\nresult_original &lt;- bdCorr_hdf5(\"data.h5\", \"expression\", \"genes\")\n\n# New transpose functionality\n# Gene-gene correlations (variables)\ngene_corr &lt;- bdCorr_hdf5(\"omics.h5\", \"expression\", \"genes\", trans_x = FALSE)\n\n# Sample-sample correlations (individuals) \nsample_corr &lt;- bdCorr_hdf5(\"omics.h5\", \"expression\", \"genes\", trans_x = TRUE)\n\n# Cross-correlation: genes vs methylation sites (variables vs variables)\ncross_vars &lt;- bdCorr_hdf5(\"omics.h5\", \"expression\", \"genes\", \n                         \"omics.h5\", \"methylation\", \"cpg_sites\",\n                         trans_x = FALSE, trans_y = FALSE)\n\n# Cross-correlation: samples vs methylation sites (samples vs variables)\nsamples_vs_cpg &lt;- bdCorr_hdf5(\"omics.h5\", \"expression\", \"genes\",\n                             \"omics.h5\", \"methylation\", \"cpg_sites\", \n                             trans_x = TRUE, trans_y = FALSE)",
    "crumbs": [
      "HDF5 Statistics",
      "bdCorr_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html",
    "href": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html",
    "title": "bdsubset_hdf5_dataset",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdsubset_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html#description",
    "href": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html#description",
    "title": "bdsubset_hdf5_dataset",
    "section": "1 Description",
    "text": "1 Description\nCreates a new HDF5 dataset containing only the specified rows or columns from an existing dataset. This operation is memory efficient as it uses HDF5’s hyperslab selection for direct disk-to-disk copying without loading the entire dataset into memory.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdsubset_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html#usage",
    "title": "bdsubset_hdf5_dataset",
    "section": "2 Usage",
    "text": "2 Usage\nbdsubset_hdf5_dataset(filename, dataset_path, indices, select_rows = TRUE, new_group = \"\", new_name = \"\", overwrite = FALSE)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdsubset_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html#arguments",
    "title": "bdsubset_hdf5_dataset",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file\n\n\ndataset_path\nCharacter string. Path to the source dataset (e.g., “/group1/dataset1”)\n\n\nindices\nInteger vector. Row or column indices to include (1-based, as per R convention)\n\n\nselect_rows\nLogical. If TRUE, selects rows; if FALSE, selects columns (default: TRUE)\n\n\nnew_group\nCharacter string. Target group for the new dataset (default: same as source)\n\n\nnew_name\nCharacter string. Name for the new dataset (default: original_name + “_subset”)\n\n\noverwrite\nLogical. Whether to overwrite destination if it exists (default: FALSE)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdsubset_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html#value",
    "href": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html#value",
    "title": "bdsubset_hdf5_dataset",
    "section": "4 Value",
    "text": "4 Value\n\nLogical. TRUE on success, FALSE on failure",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdsubset_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html#details",
    "href": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html#details",
    "title": "bdsubset_hdf5_dataset",
    "section": "5 Details",
    "text": "5 Details\nThis function provides an efficient way to create subsets of large HDF5 datasets without loading all data into memory. It uses HDF5’s native hyperslab selection mechanism for optimal performance with big data.\nKey features:",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdsubset_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdsubset_hdf5_dataset.html#examples",
    "title": "bdsubset_hdf5_dataset",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\n# Select specific rows (e.g., rows 1, 3, 5, 10-15)\nsuccess &lt;- bdsubset_dataset(\"data.h5\", \n                           dataset_path = \"/matrix/data\",\n                           indices = c(1, 3, 5, 10:15),\n                           select_rows = TRUE,\n                           new_name = \"selected_rows\")\n\n# Select specific columns\nsuccess &lt;- bdsubset_dataset(\"data.h5\",\n                           dataset_path = \"/matrix/data\", \n                           indices = c(2, 4, 6:10),\n                           select_rows = FALSE,\n                           new_group = \"/filtered\",\n                           new_name = \"selected_cols\")\n\n# Create subset in different group\nsuccess &lt;- bdsubset_dataset(\"data.h5\",\n                           dataset_path = \"/raw_data/matrix\",\n                           indices = 1:100,  # First 100 rows\n                           select_rows = TRUE,\n                           new_group = \"/processed\",\n                           new_name = \"top_100_rows\")\n\n# Extract specific samples for analysis\ninteresting_samples &lt;- c(15, 23, 45, 67, 89, 123)\nsuccess &lt;- bdsubset_dataset(\"data.h5\",\n                           dataset_path = \"/experiments/results\",\n                           indices = interesting_samples,\n                           select_rows = TRUE,\n                           new_name = \"analysis_subset\")",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdsubset_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html",
    "href": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html",
    "title": "bdgetDim_hdf5",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDim_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#description",
    "href": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#description",
    "title": "bdgetDim_hdf5",
    "section": "1 Description",
    "text": "1 Description\nRetrieves the dimensions (number of rows and columns) of a dataset stored in an HDF5 file.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDim_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#usage",
    "title": "bdgetDim_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdgetDim_hdf5(filename, dataset)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDim_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#arguments",
    "title": "bdgetDim_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ndataset\nCharacter string. Full path to the dataset within the HDF5 file (e.g., “group/subgroup/dataset”).",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDim_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#value",
    "href": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#value",
    "title": "bdgetDim_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nInteger vector of length 2 containing: - [1] Number of rows - [2] Number of columns",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDim_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#details",
    "href": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#details",
    "title": "bdgetDim_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides efficient access to dataset dimensions in HDF5 files. Key features: - Dimension information: - Number of rows - Number of columns - Implementation features: - Safe HDF5 file operations - Memory-efficient implementation - Comprehensive error handling - Read-only access to files\nThe function opens the HDF5 file in read-only mode to ensure data safety.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDim_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#examples",
    "title": "bdgetDim_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create a test HDF5 file\nfn &lt;- \"test.hdf5\"\nX &lt;- matrix(rnorm(100), 10, 10)\n\n# Save matrix to HDF5\nbdCreate_hdf5_matrix(fn, X, \"data\", \"matrix1\",\n                     overwriteFile = TRUE)\n\n# Get dimensions\ndims &lt;- bdgetDim_hdf5(fn, \"data/matrix1\")\nprint(paste(\"Rows:\", dims[1]))\nprint(paste(\"Columns:\", dims[2]))\n\n# Cleanup\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDim_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_io_management/bdgetDim_hdf5.html#see-also",
    "title": "bdgetDim_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdgetDatasetsList_hdf5 for listing available datasets\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDim_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html",
    "href": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html",
    "title": "bdWrite_hdf5_dimnames",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdWrite_hdf5_dimnames"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html#description",
    "href": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html#description",
    "title": "bdWrite_hdf5_dimnames",
    "section": "1 Description",
    "text": "1 Description\nWrite row and/or column names metadata for an existing dataset in an HDF5 file. Empty vectors skip the corresponding dimnames.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdWrite_hdf5_dimnames"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html#usage",
    "title": "bdWrite_hdf5_dimnames",
    "section": "2 Usage",
    "text": "2 Usage\nbdWrite_hdf5_dimnames(filename, group, dataset, rownames, colnames)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdWrite_hdf5_dimnames"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html#arguments",
    "title": "bdWrite_hdf5_dimnames",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ngroup\nCharacter string. Group containing the dataset.\n\n\ndataset\nCharacter string. Dataset name inside .\n\n\nrownames\nCharacter vector of row names. Use to skip writing row names. If provided, length must equal nrow.\n\n\ncolnames\nCharacter vector of column names. Use to skip writing column names. If provided, length must equal ncol.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdWrite_hdf5_dimnames"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html#value",
    "href": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html#value",
    "title": "bdWrite_hdf5_dimnames",
    "section": "4 Value",
    "text": "4 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string with the HDF5 filename\ndsrows: Character string with the full dataset path to the row names, stored as “.\\code{dataset\ndscols: Character string with the full dataset path to the column names, stored as “.\\code{dataset",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdWrite_hdf5_dimnames"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html#details",
    "href": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html#details",
    "title": "bdWrite_hdf5_dimnames",
    "section": "5 Details",
    "text": "5 Details\nThe dataset group/dataset must already exist. When non-empty, rownames and colnames lengths are validated against the dataset dimensions.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdWrite_hdf5_dimnames"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdWrite_hdf5_dimnames.html#examples",
    "title": "bdWrite_hdf5_dimnames",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nbdWrite_hdf5_dimnames(\n  filename = \"test.h5\",\n  group = \"MGCCA_IN\",\n  dataset = \"X\",\n  rownames = paste0(\"r\", seq_len(100)),\n  colnames = paste0(\"c\", seq_len(50))\n)\n\n# Skip column names:\nbdWrite_hdf5_dimnames(\"test.h5\", \"MGCCA_IN\", \"X\",\n                      rownames = paste0(\"r\", 1:100),\n                      colnames = character(0))",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdWrite_hdf5_dimnames"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html",
    "href": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html",
    "title": "bdSort_hdf5_dataset",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSort_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#description",
    "href": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#description",
    "title": "bdSort_hdf5_dataset",
    "section": "1 Description",
    "text": "1 Description\nSorts a dataset in an HDF5 file based on a predefined ordering specified through a list of sorting blocks.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSort_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#usage",
    "title": "bdSort_hdf5_dataset",
    "section": "2 Usage",
    "text": "2 Usage\nbdSort_hdf5_dataset(filename, group, dataset, outdataset, blockedSortlist, func, outgroup = NULL, overwrite = FALSE)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSort_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#arguments",
    "title": "bdSort_hdf5_dataset",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ngroup\nCharacter string. Path to the group containing input dataset.\n\n\ndataset\nCharacter string. Name of the dataset to sort.\n\n\noutdataset\nCharacter string. Name for the sorted dataset.\n\n\nblockedSortlist\nList of data frames. Each data frame specifies the sorting order for a block of elements. See Details for structure.\n\n\nfunc\nCharacter string. Function to apply: - “sortRows” for row-wise sorting - “sortCols” for column-wise sorting\n\n\noutgroup\nCharacter string (optional). Output group path. If NULL, uses input group.\n\n\noverwrite\nLogical (optional). Whether to overwrite existing dataset. Default is FALSE.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSort_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#value",
    "href": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#value",
    "title": "bdSort_hdf5_dataset",
    "section": "4 Value",
    "text": "4 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the sorted dataset (group/dataset)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSort_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#details",
    "href": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#details",
    "title": "bdSort_hdf5_dataset",
    "section": "5 Details",
    "text": "5 Details\nThis function provides efficient dataset sorting capabilities with: - Sorting options: - Row-wise sorting - Column-wise sorting - Block-based processing - Implementation features: - Memory-efficient processing - Block-based operations - Safe file operations - Progress reporting\nThe sorting order is specified through a list of data frames, where each data frame represents a block of elements to be sorted. Each data frame must contain: - Row names (current identifiers) - chr (new identifiers) - order (current positions) - newOrder (target positions)\nExample sorting blocks structure:\nBlock 1 (maintaining order): chr order newOrder Diagonal TCGA-OR-A5J1 TCGA-OR-A5J1 1 1 1 TCGA-OR-A5J2 TCGA-OR-A5J2 2 2 1 TCGA-OR-A5J3 TCGA-OR-A5J3 3 3 1 TCGA-OR-A5J4 TCGA-OR-A5J4 4 4 1\nBlock 2 (reordering with new identifiers): chr order newOrder Diagonal TCGA-OR-A5J5 TCGA-OR-A5JA 10 5 1 TCGA-OR-A5J6 TCGA-OR-A5JB 11 6 1 TCGA-OR-A5J7 TCGA-OR-A5JC 12 7 0 TCGA-OR-A5J8 TCGA-OR-A5JD 13 8 1\nBlock 3 (reordering with identifier swaps): chr order newOrder Diagonal TCGA-OR-A5J9 TCGA-OR-A5J5 5 9 1 TCGA-OR-A5JA TCGA-OR-A5J6 6 10 1 TCGA-OR-A5JB TCGA-OR-A5J7 7 11 1 TCGA-OR-A5JC TCGA-OR-A5J8 8 12 1 TCGA-OR-A5JD TCGA-OR-A5J9 9 13 0\nIn this example: - Block 1 maintains the original order - Block 2 assigns new identifiers (A5JA-D) to elements - Block 3 swaps identifiers between elements - The Diagonal column indicates whether the element is on the diagonal (1) or not (0)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSort_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#examples",
    "title": "bdSort_hdf5_dataset",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test data\ndata &lt;- matrix(rnorm(100), 10, 10)\nrownames(data) &lt;- paste0(\"TCGA-OR-A5J\", 1:10)\n\n# Save to HDF5\nfn &lt;- \"test.hdf5\"\nbdCreate_hdf5_matrix(fn, data, \"data\", \"matrix1\",\n                     overwriteFile = TRUE)\n\n# Create sorting blocks\nblock1 &lt;- data.frame(\n  chr = paste0(\"TCGA-OR-A5J\", c(2,1,3,4)),\n  order = 1:4,\n  newOrder = c(2,1,3,4),\n  row.names = paste0(\"TCGA-OR-A5J\", 1:4)\n)\n\nblock2 &lt;- data.frame(\n  chr = paste0(\"TCGA-OR-A5J\", c(6,5,8,7)),\n  order = 5:8,\n  newOrder = c(6,5,8,7),\n  row.names = paste0(\"TCGA-OR-A5J\", 5:8)\n)\n\n# Sort dataset\nbdSort_hdf5_dataset(\n  filename = fn,\n  group = \"data\",\n  dataset = \"matrix1\",\n  outdataset = \"matrix1_sorted\",\n  blockedSortlist = list(block1, block2),\n  func = \"sortRows\"\n)\n\n# Cleanup\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSort_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#see-also",
    "href": "api-reference/r/hdf5_io_management/bdSort_hdf5_dataset.html#see-also",
    "title": "bdSort_hdf5_dataset",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSort_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html",
    "href": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html",
    "title": "bdReduce_hdf5_dataset",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdReduce_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#description",
    "href": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#description",
    "title": "bdReduce_hdf5_dataset",
    "section": "1 Description",
    "text": "1 Description\nReduces multiple datasets within an HDF5 group using arithmetic operations (addition or subtraction).",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdReduce_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#usage",
    "title": "bdReduce_hdf5_dataset",
    "section": "2 Usage",
    "text": "2 Usage\nbdReduce_hdf5_dataset(filename, group, reducefunction, outgroup = NULL, outdataset = NULL, overwrite = FALSE, remove = FALSE)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdReduce_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#arguments",
    "title": "bdReduce_hdf5_dataset",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ngroup\nCharacter string. Path to the group containing datasets.\n\n\nreducefunction\nCharacter. Operation to apply, either “+” or “-”.\n\n\noutgroup\nCharacter string (optional). Output group path. If NULL, uses input group.\n\n\noutdataset\nCharacter string (optional). Output dataset name. If NULL, uses input group name.\n\n\noverwrite\nLogical (optional). Whether to overwrite existing dataset. Default is FALSE.\n\n\nremove\nLogical (optional). Whether to remove source datasets after reduction. Default is FALSE.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdReduce_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#value",
    "href": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#value",
    "title": "bdReduce_hdf5_dataset",
    "section": "4 Value",
    "text": "4 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the reduced dataset (group/dataset)\nfunc: Character string with the reduction function applied",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdReduce_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#details",
    "href": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#details",
    "title": "bdReduce_hdf5_dataset",
    "section": "5 Details",
    "text": "5 Details\nThis function provides efficient dataset reduction capabilities with: - Operation options: - Addition of datasets - Subtraction of datasets - Output options: - Custom output location - Configurable dataset name - Overwrite protection - Implementation features: - Memory-efficient processing - Safe file operations - Optional source cleanup - Comprehensive error handling\nThe function processes datasets efficiently while maintaining data integrity.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdReduce_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#examples",
    "title": "bdReduce_hdf5_dataset",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test matrices\nX1 &lt;- matrix(1:100, 10, 10)\nX2 &lt;- matrix(101:200, 10, 10)\nX3 &lt;- matrix(201:300, 10, 10)\n\n# Save to HDF5\nfn &lt;- \"test.hdf5\"\nbdCreate_hdf5_matrix(fn, X1, \"data\", \"matrix1\",\n                     overwriteFile = TRUE)\nbdCreate_hdf5_matrix(fn, X2, \"data\", \"matrix2\",\n                     overwriteFile = FALSE)\nbdCreate_hdf5_matrix(fn, X3, \"data\", \"matrix3\",\n                     overwriteFile = FALSE)\n\n# Reduce datasets by addition\nbdReduce_hdf5_dataset(\n  filename = fn,\n  group = \"data\",\n  reducefunction = \"+\",\n  outgroup = \"results\",\n  outdataset = \"sum_matrix\",\n  overwrite = TRUE\n)\n\n# Cleanup\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdReduce_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#see-also",
    "href": "api-reference/r/hdf5_io_management/bdReduce_hdf5_dataset.html#see-also",
    "title": "bdReduce_hdf5_dataset",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdReduce_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html",
    "href": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html",
    "title": "bdImportTextFile_hdf5",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportTextFile_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#description",
    "href": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#description",
    "title": "bdImportTextFile_hdf5",
    "section": "1 Description",
    "text": "1 Description\nConverts a text file (e.g., CSV, TSV) to HDF5 format, providing efficient storage and access capabilities.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportTextFile_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#usage",
    "title": "bdImportTextFile_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdImportTextFile_hdf5(filename, outputfile, outGroup, outDataset, sep = NULL, header = FALSE, rownames = FALSE, overwrite = FALSE, paral = NULL, threads = NULL, overwriteFile = NULL)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportTextFile_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#arguments",
    "title": "bdImportTextFile_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the input text file.\n\n\noutputfile\nCharacter string. Path to the output HDF5 file.\n\n\noutGroup\nCharacter string. Name of the group to create in HDF5 file.\n\n\noutDataset\nCharacter string. Name of the dataset to create.\n\n\nsep\nCharacter string (optional). Field separator, default is “\\t”.\n\n\nheader\nLogical (optional). Whether first row contains column names.\n\n\nrownames\nLogical (optional). Whether first column contains row names.\n\n\noverwrite\nLogical (optional). Whether to overwrite existing dataset.\n\n\nparal\nLogical (optional). Whether to use parallel processing.\n\n\nthreads\nInteger (optional). Number of threads for parallel processing.\n\n\noverwriteFile\nLogical (optional). Whether to overwrite existing HDF5 file.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportTextFile_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#value",
    "href": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#value",
    "title": "bdImportTextFile_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the imported data (group/dataset)\nds_rows: Character string with the full dataset path to the row names\nds_cols: Character string with the full dataset path to the column names",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportTextFile_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#details",
    "href": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#details",
    "title": "bdImportTextFile_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides flexible text file import capabilities with support for: - Input format options: - Custom field separators - Header row handling - Row names handling - Processing options: - Parallel processing - Memory-efficient import - Configurable thread count - File handling: - Safe file operations - Overwrite protection - Comprehensive error handling\nThe function supports parallel processing for large files and provides memory-efficient import capabilities.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportTextFile_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#examples",
    "title": "bdImportTextFile_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create a test CSV file\ndata &lt;- matrix(rnorm(100), 10, 10)\nwrite.csv(data, \"test.csv\", row.names = FALSE)\n\n# Import to HDF5\nbdImportTextFile_hdf5(\n  filename = \"test.csv\",\n  outputfile = \"output.hdf5\",\n  outGroup = \"data\",\n  outDataset = \"matrix1\",\n  sep = \",\",\n  header = TRUE,\n  overwriteFile = TRUE\n)\n\n# Cleanup\nunlink(c(\"test.csv\", \"output.hdf5\"))",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportTextFile_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_io_management/bdImportTextFile_hdf5.html#see-also",
    "title": "bdImportTextFile_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdCreate_hdf5_matrix for creating HDF5 matrices directly",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportTextFile_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_matrix.html",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_matrix.html",
    "title": "bdCreate_hdf5_matrix",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_matrix"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_matrix.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_matrix.html#usage",
    "title": "bdCreate_hdf5_matrix",
    "section": "1 Usage",
    "text": "1 Usage\nbdCreate_hdf5_matrix(filename, object, group = NULL, dataset = NULL, transp = NULL, overwriteFile = NULL, overwriteDataset = NULL, unlimited = NULL)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_matrix"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_matrix.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_matrix.html#arguments",
    "title": "bdCreate_hdf5_matrix",
    "section": "2 Arguments",
    "text": "2 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nobject\nnumerical data matrix\n\n\ntransp\nboolean, if trans=true matrix is stored transposed in hdf5 file",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_matrix"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_matrix.html#value",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_matrix.html#value",
    "title": "bdCreate_hdf5_matrix",
    "section": "3 Value",
    "text": "3 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the created matrix (group/dataset)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_matrix"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_matrix.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_matrix.html#examples",
    "title": "bdCreate_hdf5_matrix",
    "section": "4 Examples",
    "text": "4 Examples\n\nmatA &lt;- matrix(c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15), nrow = 3, byrow = TRUE)\nbdCreate_hdf5_matrix(filename = \"test_temp.hdf5\", \n                    object = matA, group = \"datasets\", \n                    dataset = \"datasetA\", transp = FALSE, \n                    overwriteFile = TRUE, \n                    overwriteDataset = TRUE,\n                    unlimited = FALSE)\n\n# Remove file (used as example)\n  if (file.exists(\"test_temp.hdf5\")) {\n    # Delete file if it exist\n    file.remove(\"test_temp.hdf5\")\n  }",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_matrix"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.html",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.html",
    "title": "bdCreate_hdf5_emptyDataset",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_emptyDataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.html#description",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.html#description",
    "title": "bdCreate_hdf5_emptyDataset",
    "section": "1 Description",
    "text": "1 Description\nCreates an HDF5 dataset of size nrows × ncols inside group with name dataset, without writing data (allocation only). Honors file/dataset overwrite flags and supports unlimited datasets.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_emptyDataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.html#usage",
    "title": "bdCreate_hdf5_emptyDataset",
    "section": "2 Usage",
    "text": "2 Usage\nbdCreate_hdf5_emptyDataset(filename, group, dataset, nrows = 0L, ncols = 0L, overwriteFile = NULL, overwriteDataset = NULL, unlimited = NULL, datatype = NULL)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_emptyDataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.html#arguments",
    "title": "bdCreate_hdf5_emptyDataset",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter. Path to the HDF5 file.\n\n\ngroup\nCharacter. Group path.\n\n\ndataset\nCharacter. Dataset name.\n\n\nnrows\nInteger (&gt;= 1). Number of rows.\n\n\nncols\nInteger (&gt;= 1). Number of columns.\n\n\noverwriteFile\nLogical. If , allow file recreate default value .\n\n\noverwriteDataset\nLogical. If , replace dataset default value .\n\n\nunlimited\nLogical. If , create unlimited dataset default value .\n\n\ndatatype\nCharacter. Element type (e.g., “real”).",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_emptyDataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.html#value",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.html#value",
    "title": "bdCreate_hdf5_emptyDataset",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the empty dataset (group/dataset)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_emptyDataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_emptyDataset.html#examples",
    "title": "bdCreate_hdf5_emptyDataset",
    "section": "5 Examples",
    "text": "5 Examples\n\n\nCode\nbdCreate_hdf5_emptyDataset(\"test.h5\", \"MGCCA_IN\", \"X\", 1000, 500,\n                          overwriteFile = FALSE,\n                          overwriteDataset = TRUE,\n                          unlimited = FALSE,\n                          datatype = \"real\")",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_emptyDataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.html",
    "href": "api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.html",
    "title": "bdBind_hdf5_datasets",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdBind_hdf5_datasets"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.html#usage",
    "title": "bdBind_hdf5_datasets",
    "section": "1 Usage",
    "text": "1 Usage\nbdBind_hdf5_datasets(filename, group, datasets, outgroup, outdataset, func, overwrite = FALSE)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdBind_hdf5_datasets"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.html#arguments",
    "title": "bdBind_hdf5_datasets",
    "section": "2 Arguments",
    "text": "2 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter array indicating the name of the file to create\n\n\ngroup\nCharacter array indicating the input group containing the datasets\n\n\ndatasets\nCharacter array specifying the input datasets to bind\n\n\noutgroup\nCharacter array indicating the output group for the merged dataset. If NULL, output is stored in the same input group\n\n\noutdataset\nCharacter array specifying the name for the new merged dataset\n\n\nfunc\nCharacter array specifying the binding operation: - “bindRows”: Merge datasets by rows (vertical stacking) - “bindCols”: Merge datasets by columns (horizontal joining) - “bindRowsbyIndex”: Merge datasets by rows using an index\n\n\noverwrite\nBoolean indicating whether to overwrite existing datasets. Defaults to false",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdBind_hdf5_datasets"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.html#value",
    "href": "api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.html#value",
    "title": "bdBind_hdf5_datasets",
    "section": "3 Value",
    "text": "3 Value\n\nA list containing the location of the combined dataset:\n\nfn: Character string. Path to the HDF5 file containing the result\nds: Character string. Full dataset path to the bound/combined dataset within the HDF5 file",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdBind_hdf5_datasets"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.html#details",
    "href": "api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.html#details",
    "title": "bdBind_hdf5_datasets",
    "section": "4 Details",
    "text": "4 Details\nThe function performs dimension validation before binding: - For row binding: All datasets must have the same number of columns - For column binding: All datasets must have the same number of rows\nMemory efficiency is achieved through: - Block-wise reading and writing - Minimal data copying - Proper resource cleanup",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdBind_hdf5_datasets"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdBind_hdf5_datasets.html#examples",
    "title": "bdBind_hdf5_datasets",
    "section": "5 Examples",
    "text": "5 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test matrices\na &lt;- matrix(1:12, 4, 3)\nb &lt;- matrix(13:24, 4, 3)\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", a, \"data\", \"A\")\nbdCreate_hdf5_matrix(\"test.hdf5\", b, \"data\", \"B\")\n\n# Bind by rows\nbdBind_hdf5_datasets(\"test.hdf5\", \"data\", \n                     c(\"A\", \"B\"),\n                     \"results\", \"combined\",\n                     \"bindRows\")",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdBind_hdf5_datasets"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.html",
    "title": "bdtCrossprod_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdtCrossprod_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.html#usage",
    "title": "bdtCrossprod_hdf5",
    "section": "1 Usage",
    "text": "1 Usage\nbdtCrossprod_hdf5(filename, group, A, B = NULL, groupB = NULL, block_size = NULL, mixblock_size = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdtCrossprod_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.html#arguments",
    "title": "bdtCrossprod_hdf5",
    "section": "2 Arguments",
    "text": "2 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString indicating the HDF5 file path\n\n\ngroup\nString indicating the input group containing matrix A\n\n\nA\nString specifying the dataset name for matrix A\n\n\nB\nOptional string specifying dataset name for matrix B. If NULL, performs A * A^t\n\n\ngroupB\nOptional string indicating group containing matrix B. If NULL, uses same group as A\n\n\nblock_size\nOptional integer specifying the block size for processing. Default is automatically determined based on matrix dimensions\n\n\nmixblock_size\nOptional integer for memory block size in parallel processing\n\n\nparal\nOptional boolean indicating whether to use parallel processing. Default is false\n\n\nthreads\nOptional integer specifying number of threads for parallel processing. If NULL, uses maximum available threads\n\n\noutgroup\nOptional string specifying output group. Default is “OUTPUT”\n\n\noutdataset\nOptional string specifying output dataset name. Default is “tCrossProd_A_x_B”\n\n\noverwrite\nOptional boolean indicating whether to overwrite existing datasets. Default is false",
    "crumbs": [
      "HDF5 Algebra",
      "bdtCrossprod_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.html#value",
    "title": "bdtCrossprod_hdf5",
    "section": "3 Value",
    "text": "3 Value\n\nA list containing the location of the transposed crossproduct result:\n\nfn: Character string. Path to the HDF5 file containing the result\nds: Character string. Full dataset path to the transposed crossproduct result (A %% t(A) or A %% t(B)) within the HDF5 file",
    "crumbs": [
      "HDF5 Algebra",
      "bdtCrossprod_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.html#details",
    "title": "bdtCrossprod_hdf5",
    "section": "4 Details",
    "text": "4 Details\nThe function implements block-wise matrix multiplication to handle large matrices efficiently. Block size is automatically optimized based on: - Available memory - Matrix dimensions - Whether parallel processing is enabled\nFor parallel processing: - Uses OpenMP for thread management - Implements cache-friendly block operations - Provides automatic thread count optimization\nMemory efficiency is achieved through: - Block-wise reading and writing - Minimal temporary storage - Proper resource cleanup\nMathematical operations: - For single matrix A: computes A * A^t - For two matrices A, B: computes A * B^t - Optimized for numerical stability",
    "crumbs": [
      "HDF5 Algebra",
      "bdtCrossprod_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdtCrossprod_hdf5.html#examples",
    "title": "bdtCrossprod_hdf5",
    "section": "5 Examples",
    "text": "5 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\nlibrary(rhdf5)\n\n# Create test matrix\nN &lt;- 1000\nM &lt;- 1000\nset.seed(555)\na &lt;- matrix(rnorm(N*M), N, M)\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", a, \"INPUT\", \"A\",\n                     overwriteFile = TRUE)\n\n# Compute transposed cross product\nbdtCrossprod_hdf5(\"test.hdf5\", \"INPUT\", \"A\",\n                  outgroup = \"OUTPUT\",\n                  outdataset = \"result\",\n                  block_size = 1024,\n                  paral = TRUE,\n                  threads = 4)",
    "crumbs": [
      "HDF5 Algebra",
      "bdtCrossprod_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html",
    "title": "bdgetDiagonal_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdgetDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#description",
    "title": "bdgetDiagonal_hdf5",
    "section": "1 Description",
    "text": "1 Description\nRetrieves the diagonal elements from a matrix stored in an HDF5 file.",
    "crumbs": [
      "HDF5 Algebra",
      "bdgetDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#usage",
    "title": "bdgetDiagonal_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdgetDiagonal_hdf5(filename, group, dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdgetDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#arguments",
    "title": "bdgetDiagonal_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ngroup\nCharacter string. Path to the group containing the dataset.\n\n\ndataset\nCharacter string. Name of the dataset.",
    "crumbs": [
      "HDF5 Algebra",
      "bdgetDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#value",
    "title": "bdgetDiagonal_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nNumeric vector containing diagonal elements.",
    "crumbs": [
      "HDF5 Algebra",
      "bdgetDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#details",
    "title": "bdgetDiagonal_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides efficient access to matrix diagonal elements with: - Access features: - Direct diagonal access - Memory-efficient retrieval - Support for large matrices - Implementation features: - Safe HDF5 file operations - Memory-efficient implementation - Comprehensive error handling - Read-only access to files\nThe function opens the HDF5 file in read-only mode to ensure data safety.",
    "crumbs": [
      "HDF5 Algebra",
      "bdgetDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#examples",
    "title": "bdgetDiagonal_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test matrix\nX &lt;- matrix(rnorm(100), 10, 10)\ndiag(X) &lt;- 0.5\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", X, \"data\", \"matrix1\",\n                     overwriteFile = TRUE)\n\n# Get diagonal\ndiag_elements &lt;- bdgetDiagonal_hdf5(\"test.hdf5\", \"data\", \"matrix1\")\nprint(diag_elements)\n\n# Cleanup\nif (file.exists(\"test.hdf5\")) {\n  file.remove(\"test.hdf5\")\n}",
    "crumbs": [
      "HDF5 Algebra",
      "bdgetDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_algebra/bdgetDiagonal_hdf5.html#see-also",
    "title": "bdgetDiagonal_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdWriteDiagonal_hdf5 for writing diagonal elements\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 Algebra",
      "bdgetDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html",
    "title": "bdWriteDiagonal_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#description",
    "title": "bdWriteDiagonal_hdf5",
    "section": "1 Description",
    "text": "1 Description\nUpdates the diagonal elements of a matrix stored in an HDF5 file.",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#usage",
    "title": "bdWriteDiagonal_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdWriteDiagonal_hdf5(diagonal, filename, group, dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#arguments",
    "title": "bdWriteDiagonal_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\ndiagonal\nNumeric vector. New diagonal elements to write.\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ngroup\nCharacter string. Path to the group containing the dataset.\n\n\ndataset\nCharacter string. Name of the dataset to modify.",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#value",
    "title": "bdWriteDiagonal_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the diagonal elements written (group/dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#details",
    "title": "bdWriteDiagonal_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides efficient diagonal modification capabilities with: - Write features: - Direct diagonal access - Type checking and validation - Support for large matrices - Implementation features: - Safe HDF5 file operations - Memory-efficient implementation - Comprehensive error handling - Type conversion support\nThe function validates input types and dimensions before modification.",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#examples",
    "title": "bdWriteDiagonal_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test matrix\nX &lt;- matrix(rnorm(100), 10, 10)\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", X, \"data\", \"matrix1\",\n                     overwriteFile = TRUE)\n\n# Create new diagonal\nnew_diag &lt;- seq(1, 10)\n\n# Update diagonal\nbdWriteDiagonal_hdf5(new_diag, \"test.hdf5\", \"data\", \"matrix1\")\n\n# Verify\ndiag_elements &lt;- bdgetDiagonal_hdf5(\"test.hdf5\", \"data\", \"matrix1\")\nprint(diag_elements)\n\n# Cleanup\nif (file.exists(\"test.hdf5\")) {\n  file.remove(\"test.hdf5\")\n}",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_algebra/bdWriteDiagonal_hdf5.html#see-also",
    "title": "bdWriteDiagonal_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdgetDiagonal_hdf5 for reading diagonal elements\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteDiagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html",
    "title": "bdSVD_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#description",
    "title": "bdSVD_hdf5",
    "section": "1 Description",
    "text": "1 Description\nComputes the Singular Value Decomposition (SVD) of a large matrix stored in an HDF5 file. The SVD decomposes a matrix A into a product A = UDV’ where U and V are orthogonal matrices and D is a diagonal matrix containing the singular values.",
    "crumbs": [
      "HDF5 Algebra",
      "bdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#usage",
    "title": "bdSVD_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdSVD_hdf5(filename, group = NULL, dataset = NULL, k = 2L, q = 1L, bcenter = TRUE, bscale = TRUE, rankthreshold = 0.0, overwrite = NULL, method = NULL, threads = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#arguments",
    "title": "bdSVD_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file containing the input matrix.\n\n\ngroup\nCharacter string. Path to the group containing the input dataset.\n\n\ndataset\nCharacter string. Name of the input dataset to decompose.\n\n\nk\nInteger. Number of local SVDs to concatenate at each level (default = 2). Controls the trade-off between memory usage and computation speed.\n\n\nq\nInteger. Number of levels for SVD computation (default = 1). Higher values can improve accuracy but increase computation time.\n\n\nbcenter\nLogical. If TRUE (default), centers the data by subtracting column means.\n\n\nbscale\nLogical. If TRUE (default), scales the centered columns by their standard deviations or root mean square.\n\n\nrankthreshold\nNumeric. Threshold for determining matrix rank (default = 0). Must be between 0 and 0.1. Used to approximate rank for nearly singular matrices.\n\n\noverwrite\nLogical. If TRUE, allows overwriting existing results.\n\n\nmethod\nCharacter string. Computation method: * “auto”: Automatically selects between “full” and “blocks” based on matrix size * “blocks”: Uses block-based computation (recommended for large matrices) * “full”: Performs direct computation without partitioning\n\n\nthreads\nInteger. Number of threads for parallel computation.",
    "crumbs": [
      "HDF5 Algebra",
      "bdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#value",
    "title": "bdSVD_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nA list with the following elements:\n\nfn: Path to the HDF5 file\nds_d: Path to the dataset containing singular values\nds_u: Path to the dataset containing left singular vectors\nds_v: Path to the dataset containing right singular vectors",
    "crumbs": [
      "HDF5 Algebra",
      "bdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#details",
    "title": "bdSVD_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function implements a block-based SVD algorithm suitable for large matrices that may not fit in memory. Key features include: - Automatic method selection based on matrix size - Block-based computation for large matrices - Data centering and scaling options - Parallel processing support - Rank approximation through threshold - Memory-efficient incremental algorithm\nThe implementation uses an incremental algorithm with two key parameters: - k: number of local SVDs to concatenate at each level - q: number of levels in the computation",
    "crumbs": [
      "HDF5 Algebra",
      "bdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#examples",
    "title": "bdSVD_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\n# Create a sample large matrix in HDF5\n\nlibrary(BigDataStatMeth)\nlibrary(rhdf5)\n\n# Create a sample large matrix in HDF5\nA &lt;- matrix(rnorm(10000), 1000, 10)\n\nfn &lt;- \"test_temp.hdf5\"\nbdCreate_hdf5_matrix(filename = fn, object = A, group = \"data\", dataset = \"matrix\")\n\n# Compute SVD with default parameters\nres &lt;- bdSVD_hdf5(fn, \"data\", \"matrix\")\n\n# Compute SVD with custom parameters\nres &lt;- bdSVD_hdf5(fn, \"data\", \"matrix\",\n           k = 4, q = 2,\n           bcenter = TRUE, bscale = TRUE,\n           method = \"blocks\",\n           threads = 4)\n\n# list contents\nh5ls(res$fn)\n\n# Extract the result from HDF5 (d)\nresult_d_hdf5 &lt;- h5read(res$fn, res$ds_d)\nresult_d_hdf5\n\n# Compute the same SVD in R\nresult_d_r &lt;- svd(A)$d\nresult_d_r\n\n# Compare both results (should be TRUE)\nall.equal(result_d_hdf5, result_d_r)\n\n# Remove file\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "HDF5 Algebra",
      "bdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_algebra/bdSVD_hdf5.html#see-also",
    "title": "bdSVD_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdPCA_hdf5 for Principal Component Analysis\nbdQR_hdf5 for QR decomposition",
    "crumbs": [
      "HDF5 Algebra",
      "bdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html",
    "title": "bdPCA_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdPCA_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#description",
    "title": "bdPCA_hdf5",
    "section": "1 Description",
    "text": "1 Description\nPerforms Principal Component Analysis (PCA) on a large matrix stored in an HDF5 file. PCA reduces the dimensionality of the data while preserving as much variance as possible. The implementation uses SVD internally for efficient and numerically stable computation.",
    "crumbs": [
      "HDF5 Algebra",
      "bdPCA_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#usage",
    "title": "bdPCA_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdPCA_hdf5(filename, group, dataset, ncomponents = 0L, bcenter = FALSE, bscale = FALSE, k = 2L, q = 1L, rankthreshold = 0.0, SVDgroup = NULL, overwrite = FALSE, method = NULL, threads = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdPCA_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#arguments",
    "title": "bdPCA_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file containing the input matrix.\n\n\ngroup\nCharacter string. Path to the group containing the input dataset.\n\n\ndataset\nCharacter string. Name of the input dataset to analyze.\n\n\nncomponents\nInteger. Number of principal components to compute (default = 0, which computes all components).\n\n\nbcenter\nLogical. If TRUE, centers the data by subtracting column means. Default is FALSE.\n\n\nbscale\nLogical. If TRUE, scales the centered columns by their standard deviations (if centered) or root mean square. Default is FALSE.\n\n\nk\nInteger. Number of local SVDs to concatenate at each level (default = 2). Controls memory usage in block computation.\n\n\nq\nInteger. Number of levels for SVD computation (default = 1). Higher values can improve accuracy but increase computation time.\n\n\nrankthreshold\nNumeric. Threshold for determining matrix rank (default = 0). Must be between 0 and 0.1.\n\n\nSVDgroup\nCharacter string. Group name where intermediate SVD results are stored. If SVD was previously computed, results will be reused from this group.\n\n\noverwrite\nLogical. If TRUE, forces recomputation of SVD even if results exist.\n\n\nmethod\nCharacter string. Computation method: * “auto”: Automatically selects method based on matrix size * “blocks”: Uses block-based computation (for large matrices) * “full”: Performs direct computation (for smaller matrices)\n\n\nthreads\nInteger. Number of threads for parallel computation.",
    "crumbs": [
      "HDF5 Algebra",
      "bdPCA_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#value",
    "title": "bdPCA_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nA list containing the paths to the PCA results stored in the HDF5 file:\n\nfn: Character string. Path to the HDF5 file containing the results\nlambda: Character string. Dataset path to eigenvalues \\eqn{\nvariance: Character string. Dataset path to variance explained by each PC\ncumvar: Character string. Dataset path to cumulative variance explained\nvar.coord: Character string. Dataset path to variable coordinates on the PCs\nvar.cos2: Character string. Dataset path to squared cosines (quality of representation) for variables\nind.dist: Character string. Dataset path to distances of individuals from the origin\ncomponents: Character string. Dataset path to principal components (rotated data)\nind.coord: Character string. Dataset path to individual coordinates on the PCs\nind.cos2: Character string. Dataset path to squared cosines (quality of representation) for individuals\nind.contrib: Character string. Dataset path to contributions of individuals to each PC All results are written to the HDF5 file in the group ‘PCA/dataset’.",
    "crumbs": [
      "HDF5 Algebra",
      "bdPCA_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#details",
    "title": "bdPCA_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function implements a scalable PCA algorithm suitable for large matrices that may not fit in memory. Key features include: - Automatic method selection based on matrix size - Block-based computation for large matrices - Optional data preprocessing (centering and scaling) - Parallel processing support - Memory-efficient incremental algorithm - Reuse of existing SVD results\nThe implementation uses SVD internally and supports two computation methods: - Full decomposition: Suitable for matrices that fit in memory - Block-based decomposition: For large matrices, uses an incremental algorithm",
    "crumbs": [
      "HDF5 Algebra",
      "bdPCA_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#examples",
    "title": "bdPCA_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\n# Create a sample large matrix in HDF5\nlibrary(rhdf5)\nX &lt;- matrix(rnorm(10000), 1000, 10)\nh5createFile(\"data.h5\")\nh5write(X, \"data.h5\", \"data/matrix\")\n\n# Basic PCA with default parameters\nbdPCA_hdf5(\"data.h5\", \"data\", \"matrix\")\n\n# PCA with preprocessing and specific number of components\nbdPCA_hdf5(\"data.h5\", \"data\", \"matrix\",\n           ncomponents = 3,\n           bcenter = TRUE, bscale = TRUE,\n           method = \"blocks\",\n           threads = 4)",
    "crumbs": [
      "HDF5 Algebra",
      "bdPCA_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_algebra/bdPCA_hdf5.html#see-also",
    "title": "bdPCA_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdSVD_hdf5 for the underlying SVD computation\nbdNormalize_hdf5 for data preprocessing options",
    "crumbs": [
      "HDF5 Algebra",
      "bdPCA_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html",
    "title": "bdEigen_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#description",
    "title": "bdEigen_hdf5",
    "section": "1 Description",
    "text": "1 Description\nComputes the eigenvalue decomposition of a large matrix stored in an HDF5 file using the Spectra library. This provides consistent results with the RSpectra package and can handle both symmetric and non-symmetric matrices.",
    "crumbs": [
      "HDF5 Algebra",
      "bdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#usage",
    "title": "bdEigen_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdEigen_hdf5(filename, group = NULL, dataset = NULL, k = NULL, which = NULL, ncv = NULL, bcenter = NULL, bscale = NULL, tolerance = NULL, max_iter = NULL, compute_vectors = NULL, overwrite = NULL, threads = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#arguments",
    "title": "bdEigen_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file containing the input matrix.\n\n\ngroup\nCharacter string. Path to the group containing the input dataset.\n\n\ndataset\nCharacter string. Name of the input dataset to decompose.\n\n\nk\nInteger. Number of eigenvalues to compute (default = 6, following Spectra convention).\n\n\nwhich\nCharacter string. Which eigenvalues to compute (default = “LM”): * “LM”: Largest magnitude * “SM”: Smallest magnitude * “LR”: Largest real part (non-symmetric matrices) * “SR”: Smallest real part (non-symmetric matrices) * “LI”: Largest imaginary part (non-symmetric matrices) * “SI”: Smallest imaginary part (non-symmetric matrices) * “LA”: Largest algebraic (symmetric matrices) * “SA”: Smallest algebraic (symmetric matrices)\n\n\nncv\nInteger. Number of Arnoldi vectors (default = 0, auto-selected as max(2*k+1, 20)).\n\n\nbcenter\nLogical. If TRUE, centers the data by subtracting column means (default = FALSE).\n\n\nbscale\nLogical. If TRUE, scales the centered columns by their standard deviations (default = FALSE).\n\n\ntolerance\nNumeric. Convergence tolerance for Spectra algorithms (default = 1e-10).\n\n\nmax_iter\nInteger. Maximum number of iterations for Spectra algorithms (default = 1000).\n\n\ncompute_vectors\nLogical. If TRUE (default), computes both eigenvalues and eigenvectors.\n\n\noverwrite\nLogical. If TRUE, allows overwriting existing results (default = FALSE).\n\n\nthreads\nInteger. Number of threads for parallel computation (default = NULL, uses available cores).",
    "crumbs": [
      "HDF5 Algebra",
      "bdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#value",
    "title": "bdEigen_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\nvalues: Character string with the full dataset path to the eigenvalues (real part) (group/dataset)\nvectors: Character string with the full dataset path to the eigenvectors (real part) (group/dataset)\nvalues_imag: Character string with the full dataset path to the eigenvalues (imaginary part), or NULL if all eigenvalues are real\nvectors_imag: Character string with the full dataset path to the eigenvectors (imaginary part), or NULL if all eigenvectors are real\nis_symmetric: Logical indicating whether the matrix was detected as symmetric",
    "crumbs": [
      "HDF5 Algebra",
      "bdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#details",
    "title": "bdEigen_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function uses the Spectra library (same as RSpectra) for eigenvalue computation, ensuring consistent results. Key features include: - Automatic detection of symmetric vs non-symmetric matrices - Support for both real and complex eigenvalues/eigenvectors - Memory-efficient block-based processing for large matrices - Parallel processing support - Various eigenvalue selection criteria - Consistent interface with RSpectra::eigs()\nThe implementation automatically: - Detects matrix symmetry and uses appropriate solver (SymEigsSolver vs GenEigsSolver) - Handles complex eigenvalues for non-symmetric matrices - Saves imaginary parts separately when non-zero - Provides the same results as RSpectra::eigs() function",
    "crumbs": [
      "HDF5 Algebra",
      "bdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#examples",
    "title": "bdEigen_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\nlibrary(rhdf5)\nlibrary(RSpectra)\n\n# Create a sample matrix (can be non-symmetric)\nset.seed(123)\nA &lt;- matrix(rnorm(2500), 50, 50)\n\nfn &lt;- \"test_eigen.hdf5\"\nbdCreate_hdf5_matrix_file(filename = fn, object = A, group = \"data\", dataset = \"matrix\")\n\n# Compute eigendecomposition with BigDataStatMeth\nres &lt;- bdEigen_hdf5(fn, \"data\", \"matrix\", k = 6, which = \"LM\")\n\n# Compare with RSpectra (should give same results)\nrspectra_result &lt;- eigs(A, k = 6, which = \"LM\")\n\n# Extract results from HDF5\neigenvals_bd &lt;- h5read(res$fn, res$values)\neigenvecs_bd &lt;- h5read(res$fn, res$vectors)\n\n# Compare eigenvalues (should be identical)\nall.equal(eigenvals_bd, Re(rspectra_result$values), tolerance = 1e-12)\n\n# For non-symmetric matrices, check imaginary parts\nif (!is.null(res$values_imag)) {\n  eigenvals_imag &lt;- h5read(res$fn, res$values_imag)\n  all.equal(eigenvals_imag, Im(rspectra_result$values), tolerance = 1e-12)\n}\n\n# Remove file\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "HDF5 Algebra",
      "bdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_algebra/bdEigen_hdf5.html#see-also",
    "title": "bdEigen_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdSVD_hdf5 for Singular Value Decomposition\nbdPCA_hdf5 for Principal Component Analysis\nRSpectra::eigs for the R equivalent function",
    "crumbs": [
      "HDF5 Algebra",
      "bdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html",
    "title": "bdDiag_scalar_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_scalar_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html#description",
    "title": "bdDiag_scalar_hdf5",
    "section": "1 Description",
    "text": "1 Description\nPerforms optimized scalar operations on diagonal elements of matrices or vectors stored in HDF5 format. Automatically detects whether input is a matrix (extracts diagonal) or vector (direct operation) and applies the specified scalar operation.",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_scalar_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html#usage",
    "title": "bdDiag_scalar_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdDiag_scalar_hdf5(filename, group, dataset, scalar, operation, target = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_scalar_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html#arguments",
    "title": "bdDiag_scalar_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString. Path to the HDF5 file containing the dataset.\n\n\ngroup\nString. Group path containing the input dataset.\n\n\ndataset\nString. Name of the input dataset (matrix or vector).\n\n\nscalar\nNumeric. Scalar value for the operation.\n\n\noperation\nString. Operation to perform: “add”, “subtract”, “multiply”, “divide”.\n\n\ntarget\nOptional string. Where to write result: “input” or “new” (default: “new”).\n\n\nparal\nOptional logical. Whether to use parallel processing (default: FALSE).\n\n\nthreads\nOptional integer. Number of threads for parallel processing.\n\n\noutgroup\nOptional string. Output group path (only used if target=“new”).\n\n\noutdataset\nOptional string. Output dataset name (only used if target=“new”).\n\n\noverwrite\nOptional logical. Whether to overwrite existing datasets (default: FALSE).",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_scalar_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html#value",
    "title": "bdDiag_scalar_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\ngr: Character string with the HDF5 group\nds: Character string with the full dataset path (group/dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_scalar_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html#details",
    "title": "bdDiag_scalar_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides flexible scalar operations on diagonals: - Supported operations: - “+”: diagonal[i] + scalar - “-”: diagonal[i] - scalar\n- “”: diagonal[i]  scalar -”/“: diagonal[i] / scalar -”pow”: diagonal[i] ^ scalar - Input types: - Matrix input: Extracts diagonal automatically - Vector input: Operates directly (most efficient) - Target options: - “input”: Modifies original dataset in-place - “new”: Creates new dataset with result",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_scalar_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdDiag_scalar_hdf5.html#examples",
    "title": "bdDiag_scalar_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test matrix\nA &lt;- matrix(rnorm(100), 10, 10)\nbdCreate_hdf5_matrix(\"test.h5\", A, \"data\", \"matrix_A\", overwriteFile = TRUE)\n\n# Add scalar to diagonal (creates new dataset)\nresult &lt;- bdDiag_scalar_hdf5(\"test.h5\", \"data\", \"matrix_A\",\n                            scalar = 5.0, operation = \"+\",\n                            target = \"new\", outdataset = \"diag_plus_5\")\n\n# Multiply diagonal in-place\nresult2 &lt;- bdDiag_scalar_hdf5(\"test.h5\", \"data\", \"matrix_A\", \n                             scalar = 2.0, operation = \"*\",\n                             target = \"input\")",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_scalar_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html",
    "title": "bdDiag_divide_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html#description",
    "title": "bdDiag_divide_hdf5",
    "section": "1 Description",
    "text": "1 Description\nPerforms optimized diagonal division between two datasets stored in HDF5 format. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct operation) and uses the most efficient approach. This function is ~50-250x faster than traditional matrix operations for diagonal computations.",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html#usage",
    "title": "bdDiag_divide_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdDiag_divide_hdf5(filename, group, A, B, groupB = NULL, target = NULL, outgroup = NULL, outdataset = NULL, paral = NULL, threads = NULL, overwrite = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html#arguments",
    "title": "bdDiag_divide_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString. Path to the HDF5 file containing the datasets.\n\n\ngroup\nString. Group path containing the first dataset (A, dividend).\n\n\nA\nString. Name of the first dataset (dividend).\n\n\nB\nString. Name of the second dataset (divisor).\n\n\ngroupB\nOptional string. Group path containing dataset B. If NULL, uses same group as A.\n\n\ntarget\nOptional string. Where to write result: “A”, “B”, or “new” (default: “new”).\n\n\noutgroup\nOptional string. Output group path. Default is “OUTPUT”.\n\n\noutdataset\nOptional string. Output dataset name. Default is “A_/_B” with .diag suffix if appropriate.\n\n\nparal\nOptional logical. Whether to use parallel processing. Default is FALSE.\n\n\nthreads\nOptional integer. Number of threads for parallel processing. If NULL, uses maximum available threads.\n\n\noverwrite\nOptional logical. Whether to overwrite existing datasets. Default is FALSE.",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html#value",
    "title": "bdDiag_divide_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the diagonal division result (group/dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html#details",
    "title": "bdDiag_divide_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides flexible diagonal division with automatic optimization: - Operation modes: - Matrix / Matrix: Extract diagonals → vector division → save as vector - Matrix / Vector: Extract diagonal → vector division → save as vector\n- Vector / Vector: Direct vector division (most efficient) - Performance features: - Uses optimized vector operations for maximum efficiency - Automatic type detection and dimension validation - Memory-efficient processing for large datasets - Parallel processing support for improved performance - Mathematical properties: - Element-wise division: result[i] = A[i] / B[i] - Division by zero results in infinity (IEEE 754 standard) - Handles special cases: ±inf, NaN, and subnormal numbers - Order matters: .",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdDiag_divide_hdf5.html#examples",
    "title": "bdDiag_divide_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test matrices\nN &lt;- 1000\nset.seed(123)\nA &lt;- matrix(rnorm(N*N), N, N)\nB &lt;- matrix(rnorm(N*N, mean=1), N, N)  # Avoid division by zero\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", A, \"data\", \"matrixA\",\n                     overwriteFile = TRUE)\nbdCreate_hdf5_matrix(\"test.hdf5\", B, \"data\", \"matrixB\",\n                     overwriteFile = FALSE)\n\n# Divide diagonals\nresult &lt;- bdDiag_divide_hdf5(\"test.hdf5\", \"data\", \"matrixA\", \"matrixB\",\n                            outgroup = \"results\",\n                            outdataset = \"diagonal_ratio\",\n                            paral = TRUE)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCrossprod_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdCrossprod_hdf5.html",
    "title": "bdCrossprod_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdCrossprod_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCrossprod_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdCrossprod_hdf5.html#usage",
    "title": "bdCrossprod_hdf5",
    "section": "1 Usage",
    "text": "1 Usage\nbdCrossprod_hdf5(filename, group, A, B = NULL, groupB = NULL, block_size = NULL, mixblock_size = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdCrossprod_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCrossprod_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdCrossprod_hdf5.html#arguments",
    "title": "bdCrossprod_hdf5",
    "section": "2 Arguments",
    "text": "2 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString indicating the HDF5 file path\n\n\ngroup\nString indicating the input group containing matrix A\n\n\nA\nString specifying the dataset name for matrix A\n\n\nB\nOptional string specifying dataset name for matrix B. If NULL, performs A^t * A\n\n\ngroupB\nOptional string indicating group containing matrix B. If NULL, uses same group as A\n\n\nblock_size\nOptional integer specifying the block size for processing. Default is automatically determined based on matrix dimensions\n\n\nmixblock_size\nOptional integer for memory block size in parallel processing\n\n\nparal\nOptional boolean indicating whether to use parallel processing. Default is false\n\n\nthreads\nOptional integer specifying number of threads for parallel processing. If NULL, uses maximum available threads\n\n\noutgroup\nOptional string specifying output group. Default is “OUTPUT”\n\n\noutdataset\nOptional string specifying output dataset name. Default is “CrossProd_A_x_B”\n\n\noverwrite\nOptional boolean indicating whether to overwrite existing datasets. Default is false",
    "crumbs": [
      "HDF5 Algebra",
      "bdCrossprod_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCrossprod_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdCrossprod_hdf5.html#value",
    "title": "bdCrossprod_hdf5",
    "section": "3 Value",
    "text": "3 Value\n\nA list containing the location of the crossproduct result:\n\nfn: Character string. Path to the HDF5 file containing the result\nds: Character string. Full dataset path to the crossproduct result (t(A) %% A or t(A) %% B) within the HDF5 file",
    "crumbs": [
      "HDF5 Algebra",
      "bdCrossprod_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCrossprod_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdCrossprod_hdf5.html#details",
    "title": "bdCrossprod_hdf5",
    "section": "4 Details",
    "text": "4 Details\nThe function implements block-wise matrix multiplication to handle large matrices efficiently. Block size is automatically optimized based on: - Available memory - Matrix dimensions - Whether parallel processing is enabled\nFor parallel processing: - Uses OpenMP for thread management - Implements cache-friendly block operations - Provides automatic thread count optimization\nMemory efficiency is achieved through: - Block-wise reading and writing - Minimal temporary storage - Proper resource cleanup",
    "crumbs": [
      "HDF5 Algebra",
      "bdCrossprod_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCrossprod_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdCrossprod_hdf5.html#examples",
    "title": "bdCrossprod_hdf5",
    "section": "5 Examples",
    "text": "5 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n  library(rhdf5)\n  \n  # Create test matrix\n  N = 1000\n  M = 1000\n  set.seed(555)\n  a &lt;- matrix(rnorm(N*M), N, M)\n  \n  # Save to HDF5\n  bdCreate_hdf5_matrix(\"test.hdf5\", a, \"INPUT\", \"A\", overwriteFile = TRUE)\n  \n  # Compute cross product\n  bdCrossprod_hdf5(\"test.hdf5\", \"INPUT\", \"A\", \n                   outgroup = \"OUTPUT\",\n                   outdataset = \"result\",\n                   block_size = 1024,\n                   paral = TRUE,\n                   threads = 4)",
    "crumbs": [
      "HDF5 Algebra",
      "bdCrossprod_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html",
    "title": "bdCholesky_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#description",
    "title": "bdCholesky_hdf5",
    "section": "1 Description",
    "text": "1 Description\nComputes the Cholesky decomposition of a symmetric positive-definite matrix stored in an HDF5 file. The Cholesky decomposition factors a matrix A into the product A = LL’ where L is a lower triangular matrix.",
    "crumbs": [
      "HDF5 Algebra",
      "bdCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#usage",
    "title": "bdCholesky_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdCholesky_hdf5(filename, group, dataset, outdataset, outgroup = NULL, fullMatrix = NULL, overwrite = NULL, threads = NULL, elementsBlock = 1000000L)",
    "crumbs": [
      "HDF5 Algebra",
      "bdCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#arguments",
    "title": "bdCholesky_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file containing the input matrix.\n\n\ngroup\nCharacter string. Path to the group containing the input dataset.\n\n\ndataset\nCharacter string. Name of the input dataset to decompose.\n\n\noutdataset\nCharacter string. Name for the output dataset.\n\n\noutgroup\nCharacter string. Optional output group path. If not provided, results are stored in the input group.\n\n\nfullMatrix\nLogical. If TRUE, stores the complete matrix. If FALSE (default), stores only the lower triangular part to save space.\n\n\noverwrite\nLogical. If TRUE, allows overwriting existing results.\n\n\nthreads\nInteger. Number of threads for parallel computation.\n\n\nelementsBlock\nInteger. Maximum number of elements to process in each block (default = 100,000). For matrices larger than 5000x5000, automatically adjusted to number of rows or columns * 2.",
    "crumbs": [
      "HDF5 Algebra",
      "bdCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#value",
    "title": "bdCholesky_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nA list containing the location of the Cholesky decomposition result:\n\nfn: Character string. Path to the HDF5 file containing the result\nds: Character string. Full dataset path to the Cholesky decomposition result within the HDF5 file",
    "crumbs": [
      "HDF5 Algebra",
      "bdCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#details",
    "title": "bdCholesky_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThe Cholesky decomposition is a specialized factorization for symmetric positive-definite matrices that provides several advantages: - More efficient than LU decomposition for symmetric positive-definite matrices - Numerically stable - Useful for solving linear systems and computing matrix inverses - Important in statistical computing (e.g., for sampling from multivariate normal distributions)\nThis implementation features: - Block-based computation for large matrices - Optional storage formats (full or triangular) - Parallel processing support - Memory-efficient block algorithm\nMathematical Details: For a symmetric positive-definite matrix A, the decomposition A = LL’ has the following properties: - L is lower triangular - L has positive diagonal elements - L is unique\nThe elements of L are computed using:",
    "crumbs": [
      "HDF5 Algebra",
      "bdCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#examples",
    "title": "bdCholesky_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(rhdf5)\n\n# Create a symmetric positive-definite matrix\nset.seed(1234)\nX &lt;- matrix(rnorm(100), 10, 10)\nA &lt;- crossprod(X)  # A = X'X is symmetric positive-definite\n    \n# Save to HDF5\nh5createFile(\"matrix.h5\")\nh5write(A, \"matrix.h5\", \"data/matrix\")\n        \n# Compute Cholesky decomposition\nbdCholesky_hdf5(\"matrix.h5\", \"data\", \"matrix\",\n                outdataset = \"chol\",\n                outgroup = \"decompositions\",\n                fullMatrix = FALSE)\n       \n# Verify the decomposition\nL &lt;- h5read(\"matrix.h5\", \"decompositions/chol\")\nmax(abs(A - L %*% t(L)))  # Should be very small",
    "crumbs": [
      "HDF5 Algebra",
      "bdCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_algebra/bdCholesky_hdf5.html#see-also",
    "title": "bdCholesky_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdInvCholesky_hdf5 for computing inverse using Cholesky decomposition\nbdSolve_hdf5 for solving linear systems",
    "crumbs": [
      "HDF5 Algebra",
      "bdCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html",
    "href": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html",
    "title": "bdcomputeMatrixVector_hdf5",
    "section": "",
    "text": "BLOCKWISE_OPS",
    "crumbs": [
      "Block-wise Operations",
      "bdcomputeMatrixVector_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#description",
    "href": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#description",
    "title": "bdcomputeMatrixVector_hdf5",
    "section": "1 Description",
    "text": "1 Description\nPerforms element-wise operations between a matrix and a vector stored in HDF5 format. The function supports addition, subtraction, multiplication, division and power operations, with options for row-wise or column-wise application and parallel processing.",
    "crumbs": [
      "Block-wise Operations",
      "bdcomputeMatrixVector_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#usage",
    "href": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#usage",
    "title": "bdcomputeMatrixVector_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdcomputeMatrixVector_hdf5(filename, group, dataset, vectorgroup, vectordataset, outdataset, func, outgroup = NULL, byrows = NULL, paral = NULL, threads = NULL, overwrite = FALSE)",
    "crumbs": [
      "Block-wise Operations",
      "bdcomputeMatrixVector_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#arguments",
    "href": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#arguments",
    "title": "bdcomputeMatrixVector_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString. Path to the HDF5 file containing the datasets.\n\n\ngroup\nString. Path to the group containing the matrix dataset.\n\n\ndataset\nString. Name of the matrix dataset.\n\n\nvectorgroup\nString. Path to the group containing the vector dataset.\n\n\nvectordataset\nString. Name of the vector dataset.\n\n\noutdataset\nString. Name for the output dataset.\n\n\nfunc\nString. Operation to perform: “+”, “-”, “*“,”/“, or”pow”.\n\n\noutgroup\nOptional string. Output group path. If not provided, results are stored in the same group as the input matrix.\n\n\nbyrows\nLogical. If TRUE, applies operation by rows. If FALSE (default), applies operation by columns.\n\n\nparal\nLogical. If TRUE, enables parallel processing.\n\n\nthreads\nInteger. Number of threads for parallel processing. Ignored if paral is FALSE.\n\n\noverwrite\nLogical. If TRUE, allows overwriting existing datasets.",
    "crumbs": [
      "Block-wise Operations",
      "bdcomputeMatrixVector_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#value",
    "href": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#value",
    "title": "bdcomputeMatrixVector_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\ngr: Character string with the HDF5 group\nds: Character string with the full dataset path (group/dataset)",
    "crumbs": [
      "Block-wise Operations",
      "bdcomputeMatrixVector_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#details",
    "href": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#details",
    "title": "bdcomputeMatrixVector_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides a flexible interface for performing element-wise operations between matrices and vectors stored in HDF5 format. It supports: - Four basic operations: - Addition (+): Adds vector elements to matrix rows/columns - Subtraction (-): Subtracts vector elements from matrix rows/columns - Multiplication (*): Multiplies matrix rows/columns by vector elements - Division (/): Divides matrix rows/columns by vector elements - Power (pow): power matrix rows/columns by vector elements - Processing options: - Row-wise or column-wise operations - Parallel processing for improved performance - Configurable thread count for parallel execution - Memory-efficient processing for large datasets\nThe function performs extensive validation: - Checks matrix and vector dimensions for compatibility - Validates operation type - Verifies HDF5 file and dataset accessibility - Ensures proper data structures (matrix vs. vector)",
    "crumbs": [
      "Block-wise Operations",
      "bdcomputeMatrixVector_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#examples",
    "href": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#examples",
    "title": "bdcomputeMatrixVector_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\nlibrary(BigDataStatMeth)\n    \n# Create test data\nset.seed(123)\nY &lt;- matrix(rnorm(100), 10, 10)\nX &lt;- matrix(rnorm(10), 10, 1)\n        \n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", Y, \"data\", \"Y\",\n                     overwriteFile = TRUE,\n                     overwriteDataset = FALSE,\n                     unlimited = FALSE)\nbdCreate_hdf5_matrix(\"test.hdf5\", X, \"data\", \"X\",\n                     overwriteFile = FALSE,\n                     overwriteDataset = FALSE,\n                     unlimited = FALSE)\n            \n# Multiply matrix rows by vector\nbdcomputeMatrixVector_hdf5(\"test.hdf5\",\n                           group = \"data\",\n                           dataset = \"Y\",\n                           vectorgroup = \"data\",\n                           vectordataset = \"X\",\n                           outdataset = \"ProdComputed\",\n                           func = \"*\",\n                           byrows = TRUE,\n                           overwrite = TRUE)\n    \n# Subtract vector from matrix rows\nbdcomputeMatrixVector_hdf5(\"test.hdf5\",\n                           group = \"data\",\n                           dataset = \"Y\",\n                           vectorgroup = \"data\",\n                           vectordataset = \"X\",\n                           outdataset = \"SubsComputed\",\n                           func = \"-\",\n                           byrows = TRUE,\n                           overwrite = TRUE)\n    \n# Subtract vector from matrix columns\nbdcomputeMatrixVector_hdf5(\"test.hdf5\",\n                           group = \"data\",\n                           dataset = \"Y\",\n                           vectorgroup = \"data\",\n                           vectordataset = \"X\",\n                           outdataset = \"SubsComputed\",\n                           func = \"-\",\n                           byrows = FALSE,\n                           overwrite = TRUE)\n                           \n# Cleanup\nif (file.exists(\"test.hdf5\")) {\n  file.remove(\"test.hdf5\")\n}",
    "crumbs": [
      "Block-wise Operations",
      "bdcomputeMatrixVector_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#see-also",
    "href": "api-reference/r/blockwise_ops/bdcomputeMatrixVector_hdf5.html#see-also",
    "title": "bdcomputeMatrixVector_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "Block-wise Operations",
      "bdcomputeMatrixVector_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockmult_hdf5.html",
    "href": "api-reference/r/blockwise_ops/bdblockmult_hdf5.html",
    "title": "bdblockmult_hdf5",
    "section": "",
    "text": "BLOCKWISE_OPS",
    "crumbs": [
      "Block-wise Operations",
      "bdblockmult_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockmult_hdf5.html#usage",
    "href": "api-reference/r/blockwise_ops/bdblockmult_hdf5.html#usage",
    "title": "bdblockmult_hdf5",
    "section": "1 Usage",
    "text": "1 Usage\nbdblockmult_hdf5(filename, group, A, B, groupB = NULL, transpose_A = NULL, transpose_B = NULL, block_size = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)",
    "crumbs": [
      "Block-wise Operations",
      "bdblockmult_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockmult_hdf5.html#arguments",
    "href": "api-reference/r/blockwise_ops/bdblockmult_hdf5.html#arguments",
    "title": "bdblockmult_hdf5",
    "section": "2 Arguments",
    "text": "2 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nstring specifying the path to the HDF5 file\n\n\ngroup\nstring specifying the group within the HDF5 file containing matrix A.\n\n\nA\nstring specifying the dataset name for matrix A. the data matrix to be used in calculus\n\n\nB\nstring specifying the dataset name for matrix B.\n\n\ngroupB\nstring, (optional), An optional string specifying the group for matrix B. Defaults to the value of group if not provided.\n\n\ntranspose_A\nWhether to transpose matrix A\n\n\ntranspose_B\nWhether to transpose matrix B\n\n\nblock_size\ninteger (optional), an optional parameter specifying the block size for processing the matrices. If not provided, a default block size is used. The block size should be chosen based on the available memory and the size of the matrices\n\n\nparal\nboolean (optional), an optional parameter to enable parallel computation. Defaults to FALSE. Set paral = true to force parallel execution\n\n\nthreads\ninteger (optional), an optional parameter specifying the number of threads to use if paral = TRUE. Ignored if paral = FALSE.\n\n\noutgroup\nstring (optional), An optional parameter specifying the group where the output matrix will be stored. If NULL, the output will be stored in the default group “OUTPUT”.\n\n\noutdataset\nstring (optional), An optional parameter specifying the dataset name for the output matrix. If NULL, the default name will be constructed as the name of dataset A concatenated with x and the name of dataset B.\n\n\noverwrite\nlogical (optional), An optional parameter to indicate whether existing results in the HDF5 file should be overwritten. Defaults to FALSE. If FALSE and the dataset already exists, an error will be displayed, and no calculations will be performed. If TRUE and a dataset with the same name as specified in outdataset already exists, it will be overwritten.",
    "crumbs": [
      "Block-wise Operations",
      "bdblockmult_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockmult_hdf5.html#value",
    "href": "api-reference/r/blockwise_ops/bdblockmult_hdf5.html#value",
    "title": "bdblockmult_hdf5",
    "section": "3 Value",
    "text": "3 Value\n\nA list containing the location of the matrix multiplication result:\n\nfn: Character string. Path to the HDF5 file containing the result\nds: Character string. Full dataset path to the A*B multiplication result within the HDF5 file",
    "crumbs": [
      "Block-wise Operations",
      "bdblockmult_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockmult_hdf5.html#details",
    "href": "api-reference/r/blockwise_ops/bdblockmult_hdf5.html#details",
    "title": "bdblockmult_hdf5",
    "section": "4 Details",
    "text": "4 Details\n\nThe function bdblockmult_hdf5() is efficient for both matrices that cannot fit into memory (by processing in blocks) and matrices that can be fully loaded into memory, as it optimizes computations based on available resources.\nEnsure that the dimensions of A and B matrices are compatible for matrix multiplication.\nThe block size should be chosen based on the available memory and the size of the matrices.\nIf bparal = true, number of concurrent threads in parallelization. If paral = TRUE and threads = NULL then threads is set to a half of a maximum number of available threads ## Examples\n\n\nlibrary(\"BigDataStatMeth\")\nlibrary(\"rhdf5\")\n\nN = 1000; M = 1000\n\nset.seed(555)\na &lt;- matrix( rnorm( N*M, mean=0, sd=1), N, M) \nb &lt;- matrix( rnorm( N*M, mean=0, sd=1), M, N) \n\nfn &lt;- \"test_temp.hdf5\"\nbdCreate_hdf5_matrix(filename = fn, \n                     object = a, group = \"groupA\", \n                     dataset = \"datasetA\",\n                     transp = FALSE,\n                     overwriteFile = TRUE, \n                     overwriteDataset = FALSE, \n                     unlimited = FALSE)\n                     \nbdCreate_hdf5_matrix(filename = fn, \n                     object = t(b), \n                     group = \"groupA\", \n                     dataset = \"datasetB\",\n                     transp = FALSE,\n                     overwriteFile = FALSE, \n                     overwriteDataset = TRUE, \n                     unlimited = FALSE)\n                     \n# Multiply two matrix\nres &lt;- bdblockmult_hdf5(filename = fn, group = \"groupA\", \n    A = \"datasetA\", B = \"datasetB\", outgroup = \"results\", \n    outdataset = \"res\", overwrite = TRUE ) \n \n# list contents\nh5ls(fn)\n\n# Extract the result from HDF5\nresult_hdf5 &lt;- h5read(res$fn, res$ds)[1:3, 1:5]\nresult_hdf5\n\n# Compute the same multiplication in R\nresult_r &lt;- (a %*% b)[1:3, 1:5]\nresult_r\n\n# Compare both results (should be TRUE)\nall.equal(result_hdf5, result_r)\n\n# Remove file\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "Block-wise Operations",
      "bdblockmult_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum.html",
    "href": "api-reference/r/blockwise_ops/bdblockSum.html",
    "title": "bdblockSum",
    "section": "",
    "text": "BLOCKWISE_OPS",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum.html#description",
    "href": "api-reference/r/blockwise_ops/bdblockSum.html#description",
    "title": "bdblockSum",
    "section": "1 Description",
    "text": "1 Description\nPerforms efficient matrix addition using block-based algorithms. The function supports various input combinations (matrix-matrix, matrix-vector, vector-vector) and provides options for parallel processing and block-based computation.",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum.html#usage",
    "href": "api-reference/r/blockwise_ops/bdblockSum.html#usage",
    "title": "bdblockSum",
    "section": "2 Usage",
    "text": "2 Usage\nbdblockSum(A, B, block_size = NULL, paral = NULL, byBlocks = TRUE, threads = NULL)",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum.html#arguments",
    "href": "api-reference/r/blockwise_ops/bdblockSum.html#arguments",
    "title": "bdblockSum",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nA\nMatrix or vector. First input operand.\n\n\nB\nMatrix or vector. Second input operand.\n\n\nblock_size\nInteger. Block size for computation. If NULL, uses maximum allowed block size.\n\n\nparal\nLogical. If TRUE, enables parallel computation. Default is FALSE.\n\n\nbyBlocks\nLogical. If TRUE (default), forces block-based computation for large matrices. Can be set to FALSE to disable blocking.\n\n\nthreads\nInteger. Number of threads for parallel computation. If NULL, uses half of available threads.",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum.html#value",
    "href": "api-reference/r/blockwise_ops/bdblockSum.html#value",
    "title": "bdblockSum",
    "section": "4 Value",
    "text": "4 Value\n\nMatrix or vector containing the result of A + B.",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum.html#details",
    "href": "api-reference/r/blockwise_ops/bdblockSum.html#details",
    "title": "bdblockSum",
    "section": "5 Details",
    "text": "5 Details\nThis function implements block-based matrix addition algorithms optimized for cache efficiency and memory usage. Key features: - Input combinations supported: - Matrix-matrix addition - Matrix-vector addition (both left and right) - Vector-vector addition - Performance optimizations: - Block-based computation for cache efficiency - Parallel processing for large matrices - Automatic method selection based on input size - Memory-efficient implementation\nThe function automatically selects the appropriate addition method based on input types and sizes. For large matrices (&gt;2.25e+08 elements), block-based computation is used by default.",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum.html#examples",
    "href": "api-reference/r/blockwise_ops/bdblockSum.html#examples",
    "title": "bdblockSum",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Matrix-matrix addition\nN &lt;- 2500\nM &lt;- 400\nnc &lt;- 4\n\nset.seed(555)\nmat1 &lt;- matrix(rnorm(N*M, mean=0, sd=10), N, M)\nmat2 &lt;- matrix(rnorm(N*M, mean=0, sd=10), N, M)\n\n# Parallel block addition\nresult &lt;- bdblockSum(mat1, mat2,\n                     paral = TRUE,\n                     threads = nc)\n\n# Matrix-vector addition\nvec &lt;- rnorm(M)\nresult_mv &lt;- bdblockSum(mat1, vec,\n                        paral = TRUE,\n                        threads = nc)",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum.html#see-also",
    "href": "api-reference/r/blockwise_ops/bdblockSum.html#see-also",
    "title": "bdblockSum",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdblockSubstract for block-based matrix subtraction\nbdblockMult for block-based matrix multiplication",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract.html",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract.html",
    "title": "bdblockSubstract",
    "section": "",
    "text": "BLOCKWISE_OPS",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract.html#description",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract.html#description",
    "title": "bdblockSubstract",
    "section": "1 Description",
    "text": "1 Description\nPerforms efficient matrix subtraction using block-based algorithms. The function supports various input combinations (matrix-matrix, matrix-vector, vector-vector) and provides options for parallel processing and block-based computation.",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract.html#usage",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract.html#usage",
    "title": "bdblockSubstract",
    "section": "2 Usage",
    "text": "2 Usage\nbdblockSubstract(A, B, block_size = NULL, paral = NULL, byBlocks = TRUE, threads = NULL)",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract.html#arguments",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract.html#arguments",
    "title": "bdblockSubstract",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nA\nMatrix or vector. First input operand.\n\n\nB\nMatrix or vector. Second input operand.\n\n\nblock_size\nInteger. Block size for computation. If NULL, uses maximum allowed block size.\n\n\nparal\nLogical. If TRUE, enables parallel computation. Default is FALSE.\n\n\nbyBlocks\nLogical. If TRUE (default), forces block-based computation for large matrices. Can be set to FALSE to disable blocking.\n\n\nthreads\nInteger. Number of threads for parallel computation. If NULL, uses half of available threads.",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract.html#value",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract.html#value",
    "title": "bdblockSubstract",
    "section": "4 Value",
    "text": "4 Value\n\nMatrix or vector containing the result of A - B.",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract.html#details",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract.html#details",
    "title": "bdblockSubstract",
    "section": "5 Details",
    "text": "5 Details\nThis function implements block-based matrix subtraction algorithms optimized for cache efficiency and memory usage. Key features: - Input combinations supported: - Matrix-matrix subtraction - Matrix-vector subtraction (both left and right) - Vector-vector subtraction - Performance optimizations: - Block-based computation for cache efficiency - Parallel processing for large matrices - Automatic method selection based on input size - Memory-efficient implementation\nThe function automatically selects the appropriate subtraction method based on input types and sizes. For large matrices (&gt;2.25e+08 elements), block-based computation is used by default.",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract.html#examples",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract.html#examples",
    "title": "bdblockSubstract",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Matrix-matrix subtraction\nN &lt;- 2500\nM &lt;- 400\nnc &lt;- 4\n\nset.seed(555)\nmat1 &lt;- matrix(rnorm(N*M, mean=0, sd=10), N, M)\nmat2 &lt;- matrix(rnorm(N*M, mean=0, sd=10), N, M)\n\n# Parallel block subtraction\nresult &lt;- bdblockSubstract(mat1, mat2,\n                          paral = TRUE,\n                          threads = nc)\n\n# Matrix-vector subtraction\nvec &lt;- rnorm(M)\nresult_mv &lt;- bdblockSubstract(mat1, vec,\n                             paral = TRUE,\n                             threads = nc)",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract.html#see-also",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract.html#see-also",
    "title": "bdblockSubstract",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdblockSum for block-based matrix addition\nbdblockMult for block-based matrix multiplication",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract"
    ]
  },
  {
    "objectID": "api-reference/index.html",
    "href": "api-reference/index.html",
    "title": "API Reference",
    "section": "",
    "text": "NoteComplete Technical Documentation\n\n\n\nThe API reference provides comprehensive documentation for every function, class, and method in BigDataStatMeth. This documentation is auto-generated from source code to ensure accuracy."
  },
  {
    "objectID": "api-reference/index.html#two-interfaces-same-power",
    "href": "api-reference/index.html#two-interfaces-same-power",
    "title": "API Reference",
    "section": "1 Two Interfaces, Same Power",
    "text": "1 Two Interfaces, Same Power\nBigDataStatMeth provides the same statistical capabilities through two interfaces, each optimized for its audience. Choose the one that fits your workflow."
  },
  {
    "objectID": "api-reference/index.html#choose-your-interface",
    "href": "api-reference/index.html#choose-your-interface",
    "title": "API Reference",
    "section": "2 Choose Your Interface",
    "text": "2 Choose Your Interface\n\n\n\n\n\n\n\n\nTipR Functions\n\n\n\nHigh-level statistical computing\nR functions that follow familiar conventions. Automatic error checking and memory management let you focus on statistical methodology.\nFunction categories:\n\nLinear Algebra\nHDF5 Algebra\nBlock-wise Operations\nHDF5 Statistics\nHDF5 I/O & Management\nOther Functions\n\nBest for: Data analysis, rapid prototyping, R workflows\nBrowse R functions →\n\n\n\n\n\n\n\n\n\n\nImportantC++ API\n\n\n\nDirect access to implementations\nHeader-only library with explicit control over memory and resources. Same capabilities as R with low-level access.\nAPI categories:\n\nClasses\nFunctions\n\nBest for: Performance-critical code, system integration, maximum control\nBrowse C++ API →"
  },
  {
    "objectID": "api-reference/index.html#using-this-reference",
    "href": "api-reference/index.html#using-this-reference",
    "title": "API Reference",
    "section": "3 Using This Reference",
    "text": "3 Using This Reference\n\n\n\n\n\n\nWarningDesigned for Lookup\n\n\n\nThis reference is designed for targeted searches, not sequential reading. When you need a specific operation, find the relevant function or class, understand its interface, and apply it.\nFor R users: Browse by category, study examples, adapt code to your needs.\nFor C++ users: Navigate to specific classes or functions. Pay attention to memory management requirements."
  },
  {
    "objectID": "api-reference/index.html#beyond-the-reference",
    "href": "api-reference/index.html#beyond-the-reference",
    "title": "API Reference",
    "section": "4 Beyond the Reference",
    "text": "4 Beyond the Reference\n\n\n\n\n\n\n\n\nTipLearn Concepts\n\n\n\nFundamentals\nUnderstand HDF5 storage, block-wise computing, and linear algebra foundations.\n\n\n\n\n\n\n\n\n\n\nTipStructured Learning\n\n\n\nTutorials\nProgressive examples from basics to advanced topics.\n\n\n\n\n\n\n\n\n\n\nTipReal Applications\n\n\n\nWorkflows\nComplete analyses solving real research problems.\n\n\n\n\nThe API reference is the technical specification. Other sections provide the context you need to apply these functions effectively."
  },
  {
    "objectID": "api-reference/cpp/functions/xwxt.html",
    "href": "api-reference/cpp/functions/xwxt.html",
    "title": "xwxt",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::xwxt(const Eigen::MatrixXd &X, const Eigen::MatrixXd &w)",
    "crumbs": [
      "Functions",
      "xwxt"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xwxt.html#signature",
    "href": "api-reference/cpp/functions/xwxt.html#signature",
    "title": "xwxt",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::xwxt(const Eigen::MatrixXd &X, const Eigen::MatrixXd &w)",
    "crumbs": [
      "Functions",
      "xwxt"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xwxt.html#description",
    "href": "api-reference/cpp/functions/xwxt.html#description",
    "title": "xwxt",
    "section": "2 Description",
    "text": "2 Description\nCompute weighted cross-product XwX’.",
    "crumbs": [
      "Functions",
      "xwxt"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xwxt.html#parameters",
    "href": "api-reference/cpp/functions/xwxt.html#parameters",
    "title": "xwxt",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (const Eigen::MatrixXd &): Input matrix\nw (const Eigen::MatrixXd &): Weight matrix",
    "crumbs": [
      "Functions",
      "xwxt"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xwxt.html#returns",
    "href": "api-reference/cpp/functions/xwxt.html#returns",
    "title": "xwxt",
    "section": "4 Returns",
    "text": "4 Returns\nWeighted cross-product XwX’",
    "crumbs": [
      "Functions",
      "xwxt"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xwxt.html#details",
    "href": "api-reference/cpp/functions/xwxt.html#details",
    "title": "xwxt",
    "section": "5 Details",
    "text": "5 Details\nComputes the weighted cross-product of a matrix with its transpose, where w is a diagonal weight matrix.",
    "crumbs": [
      "Functions",
      "xwxt"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xwxt.html#source-code",
    "href": "api-reference/cpp/functions/xwxt.html#source-code",
    "title": "xwxt",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memOptimizedProducts.hpp • Lines 212-218\ninline Eigen::MatrixXd xwxt(const Eigen::MatrixXd& X, const Eigen::MatrixXd& w) \n{\n    const int n(X.rows());\n    Eigen::MatrixXd XwXt(Eigen::MatrixXd(n, n).setZero().\n                             selfadjointView&lt;Eigen::Lower&gt;().rankUpdate(X * w.array().sqrt().matrix().asDiagonal()));\n    return (XwXt);\n}",
    "crumbs": [
      "Functions",
      "xwxt"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xwxt.html#usage-example",
    "href": "api-reference/cpp/functions/xwxt.html#usage-example",
    "title": "xwxt",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = xwxt(...);",
    "crumbs": [
      "Functions",
      "xwxt"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/writeDiagonalFromVector.html",
    "href": "api-reference/cpp/functions/writeDiagonalFromVector.html",
    "title": "writeDiagonalFromVector",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::writeDiagonalFromVector(BigDataStatMeth::hdf5Dataset *dsVector, BigDataStatMeth::hdf5Dataset *dsMatrix)",
    "crumbs": [
      "Functions",
      "writeDiagonalFromVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/writeDiagonalFromVector.html#signature",
    "href": "api-reference/cpp/functions/writeDiagonalFromVector.html#signature",
    "title": "writeDiagonalFromVector",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::writeDiagonalFromVector(BigDataStatMeth::hdf5Dataset *dsVector, BigDataStatMeth::hdf5Dataset *dsMatrix)",
    "crumbs": [
      "Functions",
      "writeDiagonalFromVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/writeDiagonalFromVector.html#description",
    "href": "api-reference/cpp/functions/writeDiagonalFromVector.html#description",
    "title": "writeDiagonalFromVector",
    "section": "2 Description",
    "text": "2 Description\nWrite diagonal vector to matrix diagonal.",
    "crumbs": [
      "Functions",
      "writeDiagonalFromVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/writeDiagonalFromVector.html#parameters",
    "href": "api-reference/cpp/functions/writeDiagonalFromVector.html#parameters",
    "title": "writeDiagonalFromVector",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsVector (BigDataStatMeth::hdf5Dataset *): Input vector dataset containing diagonal values (1×N or N×1)\ndsMatrix (BigDataStatMeth::hdf5Dataset *): Target matrix dataset (must exist and be square N×N)",
    "crumbs": [
      "Functions",
      "writeDiagonalFromVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/writeDiagonalFromVector.html#details",
    "href": "api-reference/cpp/functions/writeDiagonalFromVector.html#details",
    "title": "writeDiagonalFromVector",
    "section": "4 Details",
    "text": "4 Details\nTakes a vector dataset and writes its values to the diagonal of a matrix. Uses existing setDiagonalMatrix() function for optimized diagonal writing. Matrix must already exist and be square. Vector must match matrix diagonal size.",
    "crumbs": [
      "Functions",
      "writeDiagonalFromVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/writeDiagonalFromVector.html#call-graph",
    "href": "api-reference/cpp/functions/writeDiagonalFromVector.html#call-graph",
    "title": "writeDiagonalFromVector",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "writeDiagonalFromVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/writeDiagonalFromVector.html#source-code",
    "href": "api-reference/cpp/functions/writeDiagonalFromVector.html#source-code",
    "title": "writeDiagonalFromVector",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5DiagonalMethods.hpp • Lines 188-228\ninline void writeDiagonalFromVector(BigDataStatMeth::hdf5Dataset* dsVector,\n                                            BigDataStatMeth::hdf5Dataset* dsMatrix)\n        {\n            try {\n                if (dsMatrix-&gt;nrows() != dsMatrix-&gt;ncols()) {\n                    Rf_error(\"writeDiagonalFromVector: Matrix must be square\");\n                    return;\n                }\n                \n                hsize_t vector_size = validateVectorDataset(dsVector);\n                hsize_t matrix_size = dsMatrix-&gt;nrows();\n                \n                if (vector_size == 0) {\n                    Rf_error(\"writeDiagonalFromVector: Input is not a valid vector\");\n                    return;\n                }\n                \n                if (vector_size != matrix_size) {\n                    Rf_error(\"writeDiagonalFromVector: Vector size (%llu) must match matrix diagonal size (%llu)\", \n                             vector_size, matrix_size);\n                    return;\n                }\n                \n                // Read vector data\n                std::vector&lt;hsize_t&gt; stride = {1, 1}, block = {1, 1};\n                std::vector&lt;double&gt; vector_data(vector_size);\n                \n                if (dsVector-&gt;nrows() == 1) {\n                    dsVector-&gt;readDatasetBlock({0, 0}, {1, vector_size}, stride, block, vector_data.data());\n                } else {\n                    dsVector-&gt;readDatasetBlock({0, 0}, {vector_size, 1}, stride, block, vector_data.data());\n                }\n                \n                // Write using existing function\n                Rcpp::NumericVector diagonal_values = Rcpp::wrap(vector_data);\n                setDiagonalMatrix(dsMatrix, diagonal_values);\n                \n            } catch(std::exception& ex) {\n                Rf_error(\"Error in writeDiagonalFromVector: %s\", ex.what());\n            }\n        }",
    "crumbs": [
      "Functions",
      "writeDiagonalFromVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/writeDiagonalFromVector.html#usage-example",
    "href": "api-reference/cpp/functions/writeDiagonalFromVector.html#usage-example",
    "title": "writeDiagonalFromVector",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = writeDiagonalFromVector(...);",
    "crumbs": [
      "Functions",
      "writeDiagonalFromVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX_parallel.html",
    "href": "api-reference/cpp/functions/wdX_parallel.html",
    "title": "wdX_parallel",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::wdX_parallel(const Eigen::MatrixXd &X, const Eigen::VectorXd &w, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "wdX_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX_parallel.html#signature",
    "href": "api-reference/cpp/functions/wdX_parallel.html#signature",
    "title": "wdX_parallel",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::wdX_parallel(const Eigen::MatrixXd &X, const Eigen::VectorXd &w, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "wdX_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX_parallel.html#description",
    "href": "api-reference/cpp/functions/wdX_parallel.html#description",
    "title": "wdX_parallel",
    "section": "2 Description",
    "text": "2 Description\nCompute parallel diagonal-matrix product wX.",
    "crumbs": [
      "Functions",
      "wdX_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX_parallel.html#parameters",
    "href": "api-reference/cpp/functions/wdX_parallel.html#parameters",
    "title": "wdX_parallel",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (const Eigen::MatrixXd &): Input matrix\nw (const Eigen::VectorXd &): Vector representing diagonal matrix\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel computation",
    "crumbs": [
      "Functions",
      "wdX_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX_parallel.html#returns",
    "href": "api-reference/cpp/functions/wdX_parallel.html#returns",
    "title": "wdX_parallel",
    "section": "4 Returns",
    "text": "4 Returns\nDiagonal-matrix product wX",
    "crumbs": [
      "Functions",
      "wdX_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX_parallel.html#details",
    "href": "api-reference/cpp/functions/wdX_parallel.html#details",
    "title": "wdX_parallel",
    "section": "5 Details",
    "text": "5 Details\nParallel implementation of diagonal-matrix product computation.",
    "crumbs": [
      "Functions",
      "wdX_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX_parallel.html#source-code",
    "href": "api-reference/cpp/functions/wdX_parallel.html#source-code",
    "title": "wdX_parallel",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memOptimizedProducts.hpp • Lines 312-344\ninline Eigen::MatrixXd wdX_parallel(const Eigen::MatrixXd& X, const Eigen::VectorXd& w, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n{\n    int n = X.cols();\n    // unsigned int ithreads;\n    Eigen::MatrixXd C = Eigen::MatrixXd::Zero(X.rows(),n);\n    \n    // ithreads = get_number_threads(threads, R_NilValue);\n    \n    // if(threads.isNotNull()) {\n    //     if (Rcpp::as&lt;int&gt; (threads) &lt;= std::thread::hardware_concurrency()){\n    //         ithreads = Rcpp::as&lt;int&gt; (threads);\n    //     } else {\n    //         ithreads = getDTthreads(0, true);\n    //         //.11-04-2022.// ithreads = std::thread::hardware_concurrency()/2;}\n    //     }\n    // } else {\n    //     ithreads = getDTthreads(0, true);\n    //     //.11-04-2022.// ithreads = std::thread::hardware_concurrency()/2;\n    // }\n    \n    //.OpenMP.// omp_set_num_threads(ithreads);\n    \n    //.OpenMP.//#pragma omp parallel shared(X, w, C) \n    #pragma omp parallel num_threads( get_number_threads(threads, R_NilValue) ) shared(X, w, C) \n    {\n        #pragma omp for schedule (dynamic)\n        for (int i=0; i&lt;n; i++)\n        {\n            C.row(i) = w(i)*X.row(i);\n        }\n    }\n    return(C);\n}",
    "crumbs": [
      "Functions",
      "wdX_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX_parallel.html#usage-example",
    "href": "api-reference/cpp/functions/wdX_parallel.html#usage-example",
    "title": "wdX_parallel",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = wdX_parallel(...);",
    "crumbs": [
      "Functions",
      "wdX_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wX.html",
    "href": "api-reference/cpp/functions/wX.html",
    "title": "wX",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::wX(const Eigen::MatrixXd &X, const Eigen::MatrixXd &w)",
    "crumbs": [
      "Functions",
      "wX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wX.html#signature",
    "href": "api-reference/cpp/functions/wX.html#signature",
    "title": "wX",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::wX(const Eigen::MatrixXd &X, const Eigen::MatrixXd &w)",
    "crumbs": [
      "Functions",
      "wX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wX.html#description",
    "href": "api-reference/cpp/functions/wX.html#description",
    "title": "wX",
    "section": "2 Description",
    "text": "2 Description\nCompute diagonal-matrix product wX.",
    "crumbs": [
      "Functions",
      "wX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wX.html#parameters",
    "href": "api-reference/cpp/functions/wX.html#parameters",
    "title": "wX",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (const Eigen::MatrixXd &): Input matrix\nw (const Eigen::MatrixXd &): Matrix representing diagonal matrix",
    "crumbs": [
      "Functions",
      "wX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wX.html#returns",
    "href": "api-reference/cpp/functions/wX.html#returns",
    "title": "wX",
    "section": "4 Returns",
    "text": "4 Returns\nDiagonal-matrix product wX",
    "crumbs": [
      "Functions",
      "wX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wX.html#details",
    "href": "api-reference/cpp/functions/wX.html#details",
    "title": "wX",
    "section": "5 Details",
    "text": "5 Details\nComputes the product of a diagonal matrix with a matrix.",
    "crumbs": [
      "Functions",
      "wX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wX.html#call-graph",
    "href": "api-reference/cpp/functions/wX.html#call-graph",
    "title": "wX",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "wX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wX.html#source-code",
    "href": "api-reference/cpp/functions/wX.html#source-code",
    "title": "wX",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memOptimizedProducts.hpp • Lines 240-244\ninline Eigen::MatrixXd wX(const Eigen::MatrixXd& X, const Eigen::MatrixXd& w) \n{\n    Eigen::MatrixXd wX = w.array().matrix().asDiagonal()*X;\n    return (wX);\n}",
    "crumbs": [
      "Functions",
      "wX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wX.html#usage-example",
    "href": "api-reference/cpp/functions/wX.html#usage-example",
    "title": "wX",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = wX(...);",
    "crumbs": [
      "Functions",
      "wX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVector.html",
    "href": "api-reference/cpp/functions/validateVector.html",
    "title": "validateVector",
    "section": "",
    "text": "hsize_t BigDataStatMeth::validateVector(BigDataStatMeth::hdf5Dataset *ds)",
    "crumbs": [
      "Functions",
      "validateVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVector.html#signature",
    "href": "api-reference/cpp/functions/validateVector.html#signature",
    "title": "validateVector",
    "section": "",
    "text": "hsize_t BigDataStatMeth::validateVector(BigDataStatMeth::hdf5Dataset *ds)",
    "crumbs": [
      "Functions",
      "validateVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVector.html#description",
    "href": "api-reference/cpp/functions/validateVector.html#description",
    "title": "validateVector",
    "section": "2 Description",
    "text": "2 Description\nValidates that dataset is a vector and returns its size.",
    "crumbs": [
      "Functions",
      "validateVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVector.html#parameters",
    "href": "api-reference/cpp/functions/validateVector.html#parameters",
    "title": "validateVector",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nds (BigDataStatMeth::hdf5Dataset *): HDF5 dataset to validate",
    "crumbs": [
      "Functions",
      "validateVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVector.html#returns",
    "href": "api-reference/cpp/functions/validateVector.html#returns",
    "title": "validateVector",
    "section": "4 Returns",
    "text": "4 Returns\nVector size if valid, 0 if not a vector",
    "crumbs": [
      "Functions",
      "validateVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVector.html#details",
    "href": "api-reference/cpp/functions/validateVector.html#details",
    "title": "validateVector",
    "section": "5 Details",
    "text": "5 Details\nChecks if dataset has vector dimensions (1×N or N×1) and returns the vector length",
    "crumbs": [
      "Functions",
      "validateVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVector.html#call-graph",
    "href": "api-reference/cpp/functions/validateVector.html#call-graph",
    "title": "validateVector",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "validateVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVector.html#source-code",
    "href": "api-reference/cpp/functions/validateVector.html#source-code",
    "title": "validateVector",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectorOperations.hpp • Lines 67-79\ninline hsize_t validateVector(BigDataStatMeth::hdf5Dataset* ds) {\n        hsize_t rows = ds-&gt;nrows();\n        hsize_t cols = ds-&gt;ncols();\n        \n        if (rows == 1 && cols &gt; 1) {\n            return cols;  // Row vector\n        } else if (cols == 1 && rows &gt; 1) {\n            return rows;  // Column vector\n        } else if (rows == 1 && cols == 1) {\n            return 1;     // Scalar\n        }\n        return 0;  // Not a vector\n    }",
    "crumbs": [
      "Functions",
      "validateVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVector.html#usage-example",
    "href": "api-reference/cpp/functions/validateVector.html#usage-example",
    "title": "validateVector",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = validateVector(...);",
    "crumbs": [
      "Functions",
      "validateVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/transpose.html",
    "href": "api-reference/cpp/functions/transpose.html",
    "title": "transpose",
    "section": "",
    "text": "matrix BigDataStatMeth::transpose(const matrix &M)",
    "crumbs": [
      "Functions",
      "transpose"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/transpose.html#signature",
    "href": "api-reference/cpp/functions/transpose.html#signature",
    "title": "transpose",
    "section": "",
    "text": "matrix BigDataStatMeth::transpose(const matrix &M)",
    "crumbs": [
      "Functions",
      "transpose"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/transpose.html#description",
    "href": "api-reference/cpp/functions/transpose.html#description",
    "title": "transpose",
    "section": "2 Description",
    "text": "2 Description\nTransposes a 2D matrix.",
    "crumbs": [
      "Functions",
      "transpose"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/transpose.html#parameters",
    "href": "api-reference/cpp/functions/transpose.html#parameters",
    "title": "transpose",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nM (const matrix &): Input matrix to be transposed",
    "crumbs": [
      "Functions",
      "transpose"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/transpose.html#returns",
    "href": "api-reference/cpp/functions/transpose.html#returns",
    "title": "transpose",
    "section": "4 Returns",
    "text": "4 Returns\nmatrix Transposed matrix where rows become columns and vice versa",
    "crumbs": [
      "Functions",
      "transpose"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/transpose.html#details",
    "href": "api-reference/cpp/functions/transpose.html#details",
    "title": "transpose",
    "section": "5 Details",
    "text": "5 Details\nMInput matrix to be transposed matrix Transposed matrix where rows become columns and vice versaTime complexity: O(rows * cols) Space complexity: O(rows * cols) for the new matrix",
    "crumbs": [
      "Functions",
      "transpose"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/transpose.html#call-graph",
    "href": "api-reference/cpp/functions/transpose.html#call-graph",
    "title": "transpose",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "transpose"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/transpose.html#source-code",
    "href": "api-reference/cpp/functions/transpose.html#source-code",
    "title": "transpose",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5ImportFiles.hpp • Lines 42-55\ninline matrix transpose( const matrix &M )\n    {\n        int rows = M.size();\n        int cols = M[0].size();\n        \n        matrix T( cols, std::vector&lt;double&gt;( rows ) );\n        \n        for ( int i = 0; i &lt; rows; i++ )\n        {\n            for ( int j = 0; j &lt; cols; j++ ) T[j][i] = M[i][j];\n        }\n        \n        return T;\n    }",
    "crumbs": [
      "Functions",
      "transpose"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/transpose.html#usage-example",
    "href": "api-reference/cpp/functions/transpose.html#usage-example",
    "title": "transpose",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = transpose(...);",
    "crumbs": [
      "Functions",
      "transpose"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/t_distribution_cdf.html",
    "href": "api-reference/cpp/functions/t_distribution_cdf.html",
    "title": "t_distribution_cdf",
    "section": "",
    "text": "double BigDataStatMeth::t_distribution_cdf(double t, double df)",
    "crumbs": [
      "Functions",
      "t_distribution_cdf"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/t_distribution_cdf.html#signature",
    "href": "api-reference/cpp/functions/t_distribution_cdf.html#signature",
    "title": "t_distribution_cdf",
    "section": "",
    "text": "double BigDataStatMeth::t_distribution_cdf(double t, double df)",
    "crumbs": [
      "Functions",
      "t_distribution_cdf"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/t_distribution_cdf.html#description",
    "href": "api-reference/cpp/functions/t_distribution_cdf.html#description",
    "title": "t_distribution_cdf",
    "section": "2 Description",
    "text": "2 Description\nCompute p-value for correlation coefficient using t-distribution.",
    "crumbs": [
      "Functions",
      "t_distribution_cdf"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/t_distribution_cdf.html#parameters",
    "href": "api-reference/cpp/functions/t_distribution_cdf.html#parameters",
    "title": "t_distribution_cdf",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nr (``): Correlation coefficient [-1, 1]\nn (``): Sample size (number of observations)\nt (double): T-statistic value\ndf (double): Degrees of freedom (ν)",
    "crumbs": [
      "Functions",
      "t_distribution_cdf"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/t_distribution_cdf.html#returns",
    "href": "api-reference/cpp/functions/t_distribution_cdf.html#returns",
    "title": "t_distribution_cdf",
    "section": "4 Returns",
    "text": "4 Returns\nTwo-tailed p-value [0, 1], or NaN if invalid input",
    "crumbs": [
      "Functions",
      "t_distribution_cdf"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/t_distribution_cdf.html#details",
    "href": "api-reference/cpp/functions/t_distribution_cdf.html#details",
    "title": "t_distribution_cdf",
    "section": "5 Details",
    "text": "5 Details\nComputes the two-tailed p-value for a correlation coefficient using the t-statistic: t = r * sqrt((n-2)/(1-r²))",
    "crumbs": [
      "Functions",
      "t_distribution_cdf"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/t_distribution_cdf.html#caller-graph",
    "href": "api-reference/cpp/functions/t_distribution_cdf.html#caller-graph",
    "title": "t_distribution_cdf",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "t_distribution_cdf"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/t_distribution_cdf.html#source-code",
    "href": "api-reference/cpp/functions/t_distribution_cdf.html#source-code",
    "title": "t_distribution_cdf",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixCorrelation.hpp • Lines 236-338\ninline double t_distribution_cdf(double t, double df) {\n        if (df &lt;= 0) return std::numeric_limits&lt;double&gt;::quiet_NaN();\n        if (std::isinf(t)) return (t &gt; 0) ? 1.0 : 0.0;\n        if (t == 0) return 0.5;\n        \n        // For large df, use normal approximation\n        if (df &gt; 100) {\n            // Standard normal CDF approximation\n            double z = t;\n            return 0.5 * (1.0 + std::erf(z / std::sqrt(2.0)));\n        }\n        \n        // Use the relationship: t-CDF = 0.5 + sign(t) * I_x(1/2, df/2) / 2\n        // where x = t²/(t² + df) and I is the regularized incomplete beta function\n        double x = t * t / (t * t + df);\n        \n        // Compute incomplete beta function I_x(1/2, df/2) using continued fraction\n        // This is equivalent to R's implementation\n        double a = 0.5;\n        double b = df / 2.0;\n        \n        // For small x, use series expansion\n        if (x &lt; (a + 1.0) / (a + b + 2.0)) {\n            // Use continued fraction for B(x; a, b)\n            double bt = std::exp(std::lgamma(a + b) - std::lgamma(a) - std::lgamma(b) + \n                                 a * std::log(x) + b * std::log(1.0 - x));\n            \n            // Continued fraction approximation\n            double qab = a + b;\n            double qap = a + 1.0;\n            double qam = a - 1.0;\n            double c = 1.0;\n            double d = 1.0 - qab * x / qap;\n            \n            if (std::abs(d) &lt; 1e-30) d = 1e-30;\n            d = 1.0 / d;\n            double h = d;\n            \n            for (int m = 1; m &lt;= 100; ++m) {\n                int m2 = 2 * m;\n                double aa = m * (b - m) * x / ((qam + m2) * (a + m2));\n                d = 1.0 + aa * d;\n                if (std::abs(d) &lt; 1e-30) d = 1e-30;\n                c = 1.0 + aa / c;\n                if (std::abs(c) &lt; 1e-30) c = 1e-30;\n                d = 1.0 / d;\n                h *= d * c;\n                \n                aa = -(a + m) * (qab + m) * x / ((a + m2) * (qap + m2));\n                d = 1.0 + aa * d;\n                if (std::abs(d) &lt; 1e-30) d = 1e-30;\n                c = 1.0 + aa / c;\n                if (std::abs(c) &lt; 1e-30) c = 1e-30;\n                d = 1.0 / d;\n                double del = d * c;\n                h *= del;\n                \n                if (std::abs(del - 1.0) &lt; 1e-12) break;\n            }\n            \n            double beta_incomplete = bt * h / a;\n            return 0.5 + (t &gt; 0 ? 1 : -1) * beta_incomplete / 2.0;\n            \n        } else {\n            // Use the symmetry relation\n            double bt = std::exp(std::lgamma(a + b) - std::lgamma(a) - std::lgamma(b) + \n                                 b * std::log(1.0 - x) + a * std::log(x));\n            \n            // Continued fraction for the complementary case\n            double qab = a + b;\n            double c = 1.0;\n            double d = 1.0 - qab * (1.0 - x) / (b + 1.0);\n            \n            if (std::abs(d) &lt; 1e-30) d = 1e-30;\n            d = 1.0 / d;\n            double h = d;\n            \n            for (int m = 1; m &lt;= 100; ++m) {\n                int m2 = 2 * m;\n                double aa = m * (a - m) * (1.0 - x) / ((b - 1.0 + m2) * (b + m2));\n                d = 1.0 + aa * d;\n                if (std::abs(d) &lt; 1e-30) d = 1e-30;\n                c = 1.0 + aa / c;\n                if (std::abs(c) &lt; 1e-30) c = 1e-30;\n                d = 1.0 / d;\n                h *= d * c;\n                \n                aa = -(b + m) * (qab + m) * (1.0 - x) / ((b + m2) * (b + 1.0 + m2));\n                d = 1.0 + aa * d;\n                if (std::abs(d) &lt; 1e-30) d = 1e-30;\n                c = 1.0 + aa / c;\n                if (std::abs(c) &lt; 1e-30) c = 1e-30;\n                d = 1.0 / d;\n                double del = d * c;\n                h *= del;\n                \n                if (std::abs(del - 1.0) &lt; 1e-12) break;\n            }\n            \n            double beta_incomplete = 1.0 - bt * h / b;\n            return 0.5 + (t &gt; 0 ? 1 : -1) * beta_incomplete / 2.0;\n        }\n    }",
    "crumbs": [
      "Functions",
      "t_distribution_cdf"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/t_distribution_cdf.html#usage-example",
    "href": "api-reference/cpp/functions/t_distribution_cdf.html#usage-example",
    "title": "t_distribution_cdf",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = t_distribution_cdf(...);",
    "crumbs": [
      "Functions",
      "t_distribution_cdf"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/spearman_correlation.html",
    "href": "api-reference/cpp/functions/spearman_correlation.html",
    "title": "spearman_correlation",
    "section": "",
    "text": "double BigDataStatMeth::spearman_correlation(const Eigen::VectorXd &x, const Eigen::VectorXd &y, bool use_complete_obs=true)",
    "crumbs": [
      "Functions",
      "spearman_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/spearman_correlation.html#signature",
    "href": "api-reference/cpp/functions/spearman_correlation.html#signature",
    "title": "spearman_correlation",
    "section": "",
    "text": "double BigDataStatMeth::spearman_correlation(const Eigen::VectorXd &x, const Eigen::VectorXd &y, bool use_complete_obs=true)",
    "crumbs": [
      "Functions",
      "spearman_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/spearman_correlation.html#parameters",
    "href": "api-reference/cpp/functions/spearman_correlation.html#parameters",
    "title": "spearman_correlation",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nx (const Eigen::VectorXd &)\ny (const Eigen::VectorXd &)\nuse_complete_obs (bool)",
    "crumbs": [
      "Functions",
      "spearman_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/spearman_correlation.html#returns",
    "href": "api-reference/cpp/functions/spearman_correlation.html#returns",
    "title": "spearman_correlation",
    "section": "3 Returns",
    "text": "3 Returns\nType: double",
    "crumbs": [
      "Functions",
      "spearman_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/spearman_correlation.html#call-graph",
    "href": "api-reference/cpp/functions/spearman_correlation.html#call-graph",
    "title": "spearman_correlation",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "spearman_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/spearman_correlation.html#source-code",
    "href": "api-reference/cpp/functions/spearman_correlation.html#source-code",
    "title": "spearman_correlation",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixCorrelation.hpp • Lines 591-642\ninline double spearman_correlation(const Eigen::VectorXd& x, \n                                        const Eigen::VectorXd& y,\n                                        bool use_complete_obs = true) \n    {\n        \n        if (x.size() != y.size()) return (std::numeric_limits&lt;double&gt;::quiet_NaN());\n        \n        std::vector&lt;double&gt; x_valid, y_valid;\n        int n = x.size();\n        \n        if (use_complete_obs) {\n            x_valid.reserve(n);\n            y_valid.reserve(n);\n            \n            for (int i = 0; i &lt; n; ++i) {\n                if (std::isfinite(x(i)) && std::isfinite(y(i))) {\n                    x_valid.push_back(x(i));\n                    y_valid.push_back(y(i));\n                }\n            }\n        } else {\n            x_valid.assign(x.data(), x.data() + n);\n            y_valid.assign(y.data(), y.data() + n);\n        }\n        \n        if (x_valid.size() &lt; 3) return (std::numeric_limits&lt;double&gt;::quiet_NaN());\n        \n        // Efficient ranking using indices\n        auto rank_vector = [](const std::vector&lt;double&gt;& data) -&gt; Eigen::VectorXd {\n            int n = data.size();\n            std::vector&lt;std::pair&lt;double, int&gt;&gt; indexed_data(n);\n            \n            for (int i = 0; i &lt; n; ++i) {\n                indexed_data[i] = {data[i], i};\n            }\n            \n            std::sort(indexed_data.begin(), indexed_data.end());\n            \n            Eigen::VectorXd ranks(n);\n            for (int i = 0; i &lt; n; ++i) {\n                ranks[indexed_data[i].second] = i + 1;\n            }\n            \n            return ranks;\n        };\n        \n        Eigen::VectorXd rank_x = rank_vector(x_valid);\n        Eigen::VectorXd rank_y = rank_vector(y_valid);\n        \n        // Compute Pearson correlation of ranks\n        return pearson_correlation(rank_x, rank_y, false);\n    }",
    "crumbs": [
      "Functions",
      "spearman_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/spearman_correlation.html#usage-example",
    "href": "api-reference/cpp/functions/spearman_correlation.html#usage-example",
    "title": "spearman_correlation",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = spearman_correlation(...);",
    "crumbs": [
      "Functions",
      "spearman_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setLowerTriangularMatrix.html",
    "href": "api-reference/cpp/functions/setLowerTriangularMatrix.html",
    "title": "setLowerTriangularMatrix",
    "section": "",
    "text": "void BigDataStatMeth::setLowerTriangularMatrix(BigDataStatMeth::hdf5Dataset *dsMat, hsize_t dElementsBlock)",
    "crumbs": [
      "Functions",
      "setLowerTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setLowerTriangularMatrix.html#signature",
    "href": "api-reference/cpp/functions/setLowerTriangularMatrix.html#signature",
    "title": "setLowerTriangularMatrix",
    "section": "",
    "text": "void BigDataStatMeth::setLowerTriangularMatrix(BigDataStatMeth::hdf5Dataset *dsMat, hsize_t dElementsBlock)",
    "crumbs": [
      "Functions",
      "setLowerTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setLowerTriangularMatrix.html#description",
    "href": "api-reference/cpp/functions/setLowerTriangularMatrix.html#description",
    "title": "setLowerTriangularMatrix",
    "section": "2 Description",
    "text": "2 Description\nSet the lower triangular matrix using block-based approach.",
    "crumbs": [
      "Functions",
      "setLowerTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setLowerTriangularMatrix.html#parameters",
    "href": "api-reference/cpp/functions/setLowerTriangularMatrix.html#parameters",
    "title": "setLowerTriangularMatrix",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsMat (BigDataStatMeth::hdf5Dataset *): The dataset to set the lower triangular matrix of\ndElementsBlock (hsize_t): The block size to use for the matrix",
    "crumbs": [
      "Functions",
      "setLowerTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setLowerTriangularMatrix.html#details",
    "href": "api-reference/cpp/functions/setLowerTriangularMatrix.html#details",
    "title": "setLowerTriangularMatrix",
    "section": "4 Details",
    "text": "4 Details\nReads blocks, modifies them in memory to create lower triangular structure, and writes complete modified blocks instead of individual elements",
    "crumbs": [
      "Functions",
      "setLowerTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setLowerTriangularMatrix.html#call-graph",
    "href": "api-reference/cpp/functions/setLowerTriangularMatrix.html#call-graph",
    "title": "setLowerTriangularMatrix",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "setLowerTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setLowerTriangularMatrix.html#source-code",
    "href": "api-reference/cpp/functions/setLowerTriangularMatrix.html#source-code",
    "title": "setLowerTriangularMatrix",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixTriangular.hpp • Lines 145-228\ninline void setLowerTriangularMatrix( BigDataStatMeth::hdf5Dataset* dsMat, hsize_t dElementsBlock)\n    {\n        \n        try {\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1},\n                block = {1, 1};\n            \n            hsize_t readedRows = 0,\n                rowstoRead,\n                minimumBlockSize;\n            \n            // Optimized block size calculation\n            minimumBlockSize = std::max(static_cast&lt;hsize_t&gt;(1024), \n                                        std::min(dElementsBlock, dsMat-&gt;nrows()));\n            \n            while ( readedRows &lt; dsMat-&gt;nrows() ) {\n                \n                rowstoRead = ( -2 * readedRows - 1 + std::sqrt( pow(2*readedRows, 2) - 4 * readedRows + 8 * minimumBlockSize + 1) ) / 2;\n                \n                if( readedRows + rowstoRead &gt; dsMat-&gt;nrows()) {\n                    rowstoRead = dsMat-&gt;nrows() - readedRows;\n                }\n                \n                // Read square block from diagonal position\n                std::vector&lt;hsize_t&gt; offset = {readedRows, readedRows};\n                std::vector&lt;hsize_t&gt; count = {rowstoRead, rowstoRead};\n                \n                // Read the current square block\n                std::vector&lt;double&gt; block_data(rowstoRead * rowstoRead);\n                dsMat-&gt;readDatasetBlock(offset, count, stride, block, block_data.data());\n                \n                // Map to Eigen matrix for easier manipulation\n                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; \n                    block_matrix(block_data.data(), rowstoRead, rowstoRead);\n                \n                // Create lower triangular: copy upper triangle to lower triangle\n                for (hsize_t i = 0; i &lt; rowstoRead; i++) {\n                    for (hsize_t j = i + 1; j &lt; rowstoRead; j++) {\n                        block_matrix(j, i) = block_matrix(i, j);  // Copy upper to lower\n                    }\n                }\n                \n                // Write the complete modified block back - SINGLE WRITE OPERATION\n                dsMat-&gt;writeDatasetBlock(Rcpp::wrap(block_matrix), offset, count, stride, block, false);\n                \n                // Handle rectangular regions outside the diagonal blocks\n                if (readedRows + rowstoRead &lt; dsMat-&gt;nrows()) {\n                    // Write upper-right rectangular region (above diagonal block)\n                    hsize_t remaining_cols = dsMat-&gt;nrows() - readedRows - rowstoRead;\n                    std::vector&lt;hsize_t&gt; rect_offset = {readedRows, readedRows + rowstoRead};\n                    std::vector&lt;hsize_t&gt; rect_count = {rowstoRead, remaining_cols};\n                    \n                    // Read upper-right block (source for transpose)\n                    std::vector&lt;double&gt; rect_data(rowstoRead * remaining_cols);\n                    dsMat-&gt;readDatasetBlock(rect_offset, rect_count, stride, block, rect_data.data());\n                    \n                    Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; \n                        rect_matrix(rect_data.data(), rowstoRead, remaining_cols);\n                    \n                    // Write transposed data to lower-left position\n                    std::vector&lt;hsize_t&gt; lower_offset = {readedRows + rowstoRead, readedRows};\n                    std::vector&lt;hsize_t&gt; lower_count = {remaining_cols, rowstoRead};\n                    \n                    Eigen::MatrixXd transposed = rect_matrix.transpose();\n                    dsMat-&gt;writeDatasetBlock(Rcpp::wrap(transposed), lower_offset, lower_count, stride, block, false);\n                }\n                \n                readedRows = readedRows + rowstoRead; \n            }\n        }\n        catch( H5::FileIException& error ) {\n            Rcpp::Rcout&lt;&lt;\"c++ exception setLowerTriangularMatrix (File IException)\";\n            return void();\n        } catch( H5::DataSetIException& error ) {\n            Rcpp::Rcout &lt;&lt; \"c++ exception setLowerTriangularMatrix (DataSet IException)\";\n            return void();\n        } catch(std::exception& ex) {\n            Rcpp::Rcout &lt;&lt; \"c++ exception setLowerTriangularMatrix: \" &lt;&lt; ex.what();\n            return void();\n        }\n        \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "setLowerTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setLowerTriangularMatrix.html#usage-example",
    "href": "api-reference/cpp/functions/setLowerTriangularMatrix.html#usage-example",
    "title": "setLowerTriangularMatrix",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = setLowerTriangularMatrix(...);",
    "crumbs": [
      "Functions",
      "setLowerTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/scalarOperation.html",
    "href": "api-reference/cpp/functions/scalarOperation.html",
    "title": "scalarOperation",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::scalarOperation(BigDataStatMeth::hdf5Dataset *dsInput, BigDataStatMeth::hdf5Dataset *dsResult, double scalar, int operation, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "scalarOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/scalarOperation.html#signature",
    "href": "api-reference/cpp/functions/scalarOperation.html#signature",
    "title": "scalarOperation",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::scalarOperation(BigDataStatMeth::hdf5Dataset *dsInput, BigDataStatMeth::hdf5Dataset *dsResult, double scalar, int operation, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "scalarOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/scalarOperation.html#description",
    "href": "api-reference/cpp/functions/scalarOperation.html#description",
    "title": "scalarOperation",
    "section": "2 Description",
    "text": "2 Description\nPerform scalar operations on diagonal elements.",
    "crumbs": [
      "Functions",
      "scalarOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/scalarOperation.html#parameters",
    "href": "api-reference/cpp/functions/scalarOperation.html#parameters",
    "title": "scalarOperation",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsInput (BigDataStatMeth::hdf5Dataset *): Input dataset (matrix or vector)\ndsResult (BigDataStatMeth::hdf5Dataset *): Result dataset (only used if target=“new”)\nscalar (double): Scalar value for operation\noperation (int): Operation type: 0=add, 1=subtract, 2=multiply, 3=divide\ntarget (std::string): Where to write result: “input”, “new”\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads",
    "crumbs": [
      "Functions",
      "scalarOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/scalarOperation.html#details",
    "href": "api-reference/cpp/functions/scalarOperation.html#details",
    "title": "scalarOperation",
    "section": "4 Details",
    "text": "4 Details\nApplies scalar operations (add, subtract, multiply, divide) to diagonal elements. Automatically detects if input is matrix (extracts diagonal) or vector (direct).",
    "crumbs": [
      "Functions",
      "scalarOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/scalarOperation.html#call-graph",
    "href": "api-reference/cpp/functions/scalarOperation.html#call-graph",
    "title": "scalarOperation",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "scalarOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/scalarOperation.html#source-code",
    "href": "api-reference/cpp/functions/scalarOperation.html#source-code",
    "title": "scalarOperation",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5DiagonalMethods.hpp • Lines 579-691\ninline void scalarOperation(BigDataStatMeth::hdf5Dataset* dsInput,\n                                    BigDataStatMeth::hdf5Dataset* dsResult,\n                                    double scalar,\n                                    int operation,\n                                    std::string target = \"new\",\n                                    bool bparal = false,\n                                    Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n        {\n            BigDataStatMeth::hdf5Dataset* tempInput = nullptr;\n            BigDataStatMeth::hdf5Dataset* tempResult = nullptr;\n            \n            try {\n                bool isVectorInput = isDiagonalVector(dsInput);\n                BigDataStatMeth::hdf5Dataset* finalInput = dsInput;\n                \n                // Extract diagonal from input if it's a matrix\n                if (!isVectorInput) {\n                    if (dsInput-&gt;nrows() != dsInput-&gt;ncols()) {\n                        Rf_error(\"Input matrix must be square for diagonal operations\");\n                        return;\n                    }\n                    std::string tempNameInput = dsInput-&gt;getDatasetName() + \"_temp_scalar_input\";\n                    tempInput = new BigDataStatMeth::hdf5Dataset(dsInput-&gt;getFileptr(), dsInput-&gt;getGroup(), tempNameInput, true);\n                    extractDiagonalToVector(dsInput, tempInput);\n                    finalInput = tempInput;\n                }\n                \n                // Validate input\n                hsize_t sizeInput = validateVectorDataset(finalInput);\n                if (sizeInput == 0) {\n                    Rf_error(\"Invalid input dimensions for scalar operation\");\n                    cleanup_temp_datasets(tempInput, nullptr);\n                    return;\n                }\n                \n                // Determine target for operation result\n                BigDataStatMeth::hdf5Dataset* operationTarget = nullptr;\n                \n                if (target == \"new\") {\n                    operationTarget = dsResult;\n                    // CREAR DATASET EXPLÍCITAMENTE siguiendo patrón vectorial\n                    // operationTarget-&gt;createDataset(1, sizeInput, \"real\");  // Vector 1×N\n                    operationTarget-&gt;createDataset( sizeInput, 1, \"real\");  // Vector 1×N\n                } else if (target == \"input\") {\n                    if (isVectorInput) {\n                        operationTarget = dsInput;  // Usar vector input directamente\n                    } else {\n                        // Para matriz, crear temp y después escribir diagonal\n                        std::string tempNameResult = dsInput-&gt;getDatasetName() + \"_temp_scalar_result\";\n                        tempResult = new BigDataStatMeth::hdf5Dataset(dsInput-&gt;getFileptr(), dsInput-&gt;getGroup(), tempNameResult, true);\n                        // tempResult-&gt;createDataset(1, sizeInput, \"real\");\n                        tempResult-&gt;createDataset(sizeInput, 1, \"real\");  \n                        operationTarget = tempResult;\n                    }\n                }\n                \n                // Read input vector data\n                std::vector&lt;hsize_t&gt; stride = {1, 1}, block = {1, 1};\n                std::vector&lt;double&gt; input_data(sizeInput);\n                \n                if (finalInput-&gt;nrows() == 1) {\n                    finalInput-&gt;readDatasetBlock({0, 0}, {1, sizeInput}, stride, block, input_data.data());\n                } else {\n                    finalInput-&gt;readDatasetBlock({0, 0}, {sizeInput, 1}, stride, block, input_data.data());\n                }\n                \n                // Apply scalar operation\n                if (bparal && sizeInput &gt; 10000) {\n                    #pragma omp parallel num_threads(get_threads(bparal, threads))\n                    {\n                    #pragma omp for schedule(static)\n                        for (hsize_t i = 0; i &lt; sizeInput; ++i) {\n                            switch (operation) {\n                            case 0: input_data[i] += scalar; break;  // add\n                            case 1: input_data[i] -= scalar; break;  // subtract\n                            case 2: input_data[i] *= scalar; break;  // multiply\n                            case 3: input_data[i] /= scalar; break;  // divide\n                            case 4: input_data[i] = std::pow(input_data[i], scalar); break;  // power\n                            default: Rf_error(\"Unknown scalar operation: %d\", operation);\n                            }\n                        }\n                    }\n                } else {\n                    for (hsize_t i = 0; i &lt; sizeInput; ++i) {\n                        switch (operation) {\n                        case 0: input_data[i] += scalar; break;\n                        case 1: input_data[i] -= scalar; break;\n                        case 2: input_data[i] *= scalar; break;\n                        case 3: input_data[i] /= scalar; break;\n                        case 4: input_data[i] = std::pow(input_data[i], scalar); break;\n                        default: Rf_error(\"Unknown scalar operation: %d\", operation);\n                        }\n                    }\n                }\n                \n                // Write result\n                operationTarget-&gt;writeDatasetBlock(input_data, {0, 0}, {1, sizeInput}, stride, block);\n                \n                // Write result back to matrix diagonal if needed\n                if (target == \"input\" && !isVectorInput) {\n                    writeDiagonalFromVector(tempResult, dsInput);\n                }\n                \n                // Cleanup\n                cleanup_temp_datasets(tempInput, nullptr);\n                if (tempResult) { delete tempResult; tempResult = nullptr; }\n                \n            } catch(std::exception& ex) {\n                cleanup_temp_datasets(tempInput, nullptr);\n                if (tempResult) { delete tempResult; tempResult = nullptr; }\n                Rf_error(\"Error in scalarOperation: %s\", ex.what());\n            }\n        }",
    "crumbs": [
      "Functions",
      "scalarOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/scalarOperation.html#usage-example",
    "href": "api-reference/cpp/functions/scalarOperation.html#usage-example",
    "title": "scalarOperation",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = scalarOperation(...);",
    "crumbs": [
      "Functions",
      "scalarOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/remove_elements.html",
    "href": "api-reference/cpp/functions/remove_elements.html",
    "title": "remove_elements",
    "section": "",
    "text": "bool BigDataStatMeth::remove_elements(H5::H5File *file, H5std_string element)",
    "crumbs": [
      "Functions",
      "remove_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/remove_elements.html#signature",
    "href": "api-reference/cpp/functions/remove_elements.html#signature",
    "title": "remove_elements",
    "section": "",
    "text": "bool BigDataStatMeth::remove_elements(H5::H5File *file, H5std_string element)",
    "crumbs": [
      "Functions",
      "remove_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/remove_elements.html#description",
    "href": "api-reference/cpp/functions/remove_elements.html#description",
    "title": "remove_elements",
    "section": "2 Description",
    "text": "2 Description\nRemoves a single element from an HDF5 file.",
    "crumbs": [
      "Functions",
      "remove_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/remove_elements.html#parameters",
    "href": "api-reference/cpp/functions/remove_elements.html#parameters",
    "title": "remove_elements",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfile (H5::H5File *): Pointer to HDF5 file\nelement (H5std_string): Full path to element to remove",
    "crumbs": [
      "Functions",
      "remove_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/remove_elements.html#returns",
    "href": "api-reference/cpp/functions/remove_elements.html#returns",
    "title": "remove_elements",
    "section": "4 Returns",
    "text": "4 Returns\nbool True if removal successful, false otherwise",
    "crumbs": [
      "Functions",
      "remove_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/remove_elements.html#details",
    "href": "api-reference/cpp/functions/remove_elements.html#details",
    "title": "remove_elements",
    "section": "5 Details",
    "text": "5 Details\nfilePointer to HDF5 file elementFull path to element to remove bool True if removal successful, false otherwiseH5::FileIExceptionon file operation errors H5::GroupIExceptionon group operation errors H5::DataSetIExceptionon dataset operation errors H5::DataSpaceIExceptionon dataspace operation errorsIf element is not found, returns false",
    "crumbs": [
      "Functions",
      "remove_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/remove_elements.html#caller-graph",
    "href": "api-reference/cpp/functions/remove_elements.html#caller-graph",
    "title": "remove_elements",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "remove_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/remove_elements.html#source-code",
    "href": "api-reference/cpp/functions/remove_elements.html#source-code",
    "title": "remove_elements",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5Utilities.hpp • Lines 183-221\ninline bool remove_elements(H5::H5File* file, H5std_string element)\n    {\n        \n        bool bremok = true;\n        \n        try\n        {\n            H5::Exception::dontPrint();\n            \n            // H5std_string elementtoremove = element;\n            \n            int result = H5Ldelete(file-&gt;getId(), element.data(), H5P_DEFAULT);  \n            if(result&lt;0) {\n                Rcpp::Rcout&lt;&lt;\"\\n Error removing : \"&lt;&lt;element&lt;&lt;\"\\n\";\n                bremok = false;\n            } \n            \n        } catch(H5::FileIException& error) { // catch failure caused by the H5File operations\n            Rcpp::Rcerr&lt;&lt;\"c++ exception remove_HDF5_multiple_elements_ptr (File IException)\" &lt;&lt; std::endl;\n            return(bremok);\n        } catch(H5::GroupIException& error) { // catch failure caused by the Group operations\n            Rcpp::Rcerr&lt;&lt;\"c++ exception remove_HDF5_multiple_elements_ptr (Group IException)\" &lt;&lt; std::endl;\n            return(bremok);\n        } catch(H5::DataSetIException& error) { // catch failure caused by the DataSet operations\n            Rcpp::Rcerr&lt;&lt;\"c++ exception remove_HDF5_multiple_elements_ptr (DataSet IException)\" &lt;&lt; std::endl;\n            return(bremok);\n        } catch(H5::DataSpaceIException& error) { // catch failure caused by the DataSpace operations\n            Rcpp::Rcerr&lt;&lt;\"c++ exception remove_HDF5_multiple_elements_ptr (DataSpace IException)\" &lt;&lt; std::endl;\n            return(bremok);\n        } catch(std::exception &ex) {\n            Rcpp::Rcerr &lt;&lt; \"c++ exception remove_HDF5_multiple_elements_ptr: \" &lt;&lt; ex.what();\n            return(bremok);\n        }  catch (...) {\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception remove_HDF5_multiple_elements_ptr (unknown reason)\";\n            return(bremok);\n        }\n        \n        return(bremok);\n    }",
    "crumbs": [
      "Functions",
      "remove_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/remove_elements.html#usage-example",
    "href": "api-reference/cpp/functions/remove_elements.html#usage-example",
    "title": "remove_elements",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = remove_elements(...);",
    "crumbs": [
      "Functions",
      "remove_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeColumn.html",
    "href": "api-reference/cpp/functions/removeColumn.html",
    "title": "removeColumn",
    "section": "",
    "text": "void BigDataStatMeth::removeColumn(Eigen::MatrixXd &matrix, unsigned int colToRemove)",
    "crumbs": [
      "Functions",
      "removeColumn"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeColumn.html#signature",
    "href": "api-reference/cpp/functions/removeColumn.html#signature",
    "title": "removeColumn",
    "section": "",
    "text": "void BigDataStatMeth::removeColumn(Eigen::MatrixXd &matrix, unsigned int colToRemove)",
    "crumbs": [
      "Functions",
      "removeColumn"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeColumn.html#description",
    "href": "api-reference/cpp/functions/removeColumn.html#description",
    "title": "removeColumn",
    "section": "2 Description",
    "text": "2 Description\nRemoves a column from an Eigen matrix.",
    "crumbs": [
      "Functions",
      "removeColumn"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeColumn.html#parameters",
    "href": "api-reference/cpp/functions/removeColumn.html#parameters",
    "title": "removeColumn",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nmatrix (Eigen::MatrixXd &): Reference to matrix to modify\ncolToRemove (unsigned int): Index of column to remove",
    "crumbs": [
      "Functions",
      "removeColumn"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeColumn.html#details",
    "href": "api-reference/cpp/functions/removeColumn.html#details",
    "title": "removeColumn",
    "section": "4 Details",
    "text": "4 Details\nmatrixReference to matrix to modify colToRemoveIndex of column to remove Implementation details:Shifts remaining columns leftResizes matrix to remove last column",
    "crumbs": [
      "Functions",
      "removeColumn"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeColumn.html#caller-graph",
    "href": "api-reference/cpp/functions/removeColumn.html#caller-graph",
    "title": "removeColumn",
    "section": "5 Caller Graph",
    "text": "5 Caller Graph",
    "crumbs": [
      "Functions",
      "removeColumn"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeColumn.html#source-code",
    "href": "api-reference/cpp/functions/removeColumn.html#source-code",
    "title": "removeColumn",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5Utilities.hpp • Lines 369-378\ninline void removeColumn(Eigen::MatrixXd& matrix, unsigned int colToRemove)\n    {\n        unsigned int numRows = matrix.rows();\n        unsigned int numCols = matrix.cols()-1;\n        \n        if( colToRemove &lt; numCols )\n            matrix.block(0,colToRemove,numRows,numCols-colToRemove) = matrix.rightCols(numCols-colToRemove).eval();\n        \n        matrix.conservativeResize(numRows,numCols);\n    }",
    "crumbs": [
      "Functions",
      "removeColumn"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeColumn.html#usage-example",
    "href": "api-reference/cpp/functions/removeColumn.html#usage-example",
    "title": "removeColumn",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = removeColumn(...);",
    "crumbs": [
      "Functions",
      "removeColumn"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/powerDiagonals.html",
    "href": "api-reference/cpp/functions/powerDiagonals.html",
    "title": "powerDiagonals",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::powerDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "powerDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/powerDiagonals.html#signature",
    "href": "api-reference/cpp/functions/powerDiagonals.html#signature",
    "title": "powerDiagonals",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::powerDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "powerDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/powerDiagonals.html#description",
    "href": "api-reference/cpp/functions/powerDiagonals.html#description",
    "title": "powerDiagonals",
    "section": "2 Description",
    "text": "2 Description\nDivide diagonal elements from two matrices or vectors.",
    "crumbs": [
      "Functions",
      "powerDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/powerDiagonals.html#parameters",
    "href": "api-reference/cpp/functions/powerDiagonals.html#parameters",
    "title": "powerDiagonals",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input dataset (dividend)\ndsB (BigDataStatMeth::hdf5Dataset *): Second input dataset (divisor)\ndsResult (BigDataStatMeth::hdf5Dataset *): Result dataset (will be created)\ntarget (std::string): Where to write result: “A”, “B”, or “new”\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "powerDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/powerDiagonals.html#details",
    "href": "api-reference/cpp/functions/powerDiagonals.html#details",
    "title": "powerDiagonals",
    "section": "4 Details",
    "text": "4 Details\nPerforms optimized diagonal power C_diag = A_diag ^ B_diag. Same optimization strategy as addDiagonals but for element-wise division.",
    "crumbs": [
      "Functions",
      "powerDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/powerDiagonals.html#call-graph",
    "href": "api-reference/cpp/functions/powerDiagonals.html#call-graph",
    "title": "powerDiagonals",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "powerDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/powerDiagonals.html#source-code",
    "href": "api-reference/cpp/functions/powerDiagonals.html#source-code",
    "title": "powerDiagonals",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5DiagonalMethods.hpp • Lines 545-564\ninline void powerDiagonals(BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB,\n                                    BigDataStatMeth::hdf5Dataset* dsResult, std::string target = \"new\",\n                                    bool bparal = false, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n        {\n            try {\n                bool isVectorA = isDiagonalVector(dsA);\n                bool isVectorB = isDiagonalVector(dsB);\n                \n                if (isVectorA && isVectorB && (target == \"A\" || target == \"B\")) {\n                    BigDataStatMeth::hdf5Dataset* targetDataset = (target == \"A\") ? dsA : dsB;\n                    Rcpp_vector_power_hdf5(dsA, dsB, targetDataset, bparal, threads);\n                } else if (isVectorA && isVectorB && target == \"new\") {\n                    Rcpp_vector_power_hdf5(dsA, dsB, dsResult, bparal, threads);\n                } else {\n                    performMatrixDiagonalOperation(dsA, dsB, dsResult, 3, target, bparal, threads);\n                }\n            } catch(std::exception& ex) {\n                Rf_error(\"Error in powerDiagonals: %s\", ex.what());\n            }\n        }",
    "crumbs": [
      "Functions",
      "powerDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/powerDiagonals.html#usage-example",
    "href": "api-reference/cpp/functions/powerDiagonals.html#usage-example",
    "title": "powerDiagonals",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = powerDiagonals(...);",
    "crumbs": [
      "Functions",
      "powerDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pearson_correlation.html",
    "href": "api-reference/cpp/functions/pearson_correlation.html",
    "title": "pearson_correlation",
    "section": "",
    "text": "double BigDataStatMeth::pearson_correlation(const Eigen::VectorXd &x, const Eigen::VectorXd &y, bool use_complete_obs=true)",
    "crumbs": [
      "Functions",
      "pearson_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pearson_correlation.html#signature",
    "href": "api-reference/cpp/functions/pearson_correlation.html#signature",
    "title": "pearson_correlation",
    "section": "",
    "text": "double BigDataStatMeth::pearson_correlation(const Eigen::VectorXd &x, const Eigen::VectorXd &y, bool use_complete_obs=true)",
    "crumbs": [
      "Functions",
      "pearson_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pearson_correlation.html#parameters",
    "href": "api-reference/cpp/functions/pearson_correlation.html#parameters",
    "title": "pearson_correlation",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nx (const Eigen::VectorXd &)\ny (const Eigen::VectorXd &)\nuse_complete_obs (bool)",
    "crumbs": [
      "Functions",
      "pearson_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pearson_correlation.html#returns",
    "href": "api-reference/cpp/functions/pearson_correlation.html#returns",
    "title": "pearson_correlation",
    "section": "3 Returns",
    "text": "3 Returns\nType: double",
    "crumbs": [
      "Functions",
      "pearson_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pearson_correlation.html#caller-graph",
    "href": "api-reference/cpp/functions/pearson_correlation.html#caller-graph",
    "title": "pearson_correlation",
    "section": "4 Caller Graph",
    "text": "4 Caller Graph",
    "crumbs": [
      "Functions",
      "pearson_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pearson_correlation.html#source-code",
    "href": "api-reference/cpp/functions/pearson_correlation.html#source-code",
    "title": "pearson_correlation",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixCorrelation.hpp • Lines 481-542\ninline double pearson_correlation(const Eigen::VectorXd& x, \n                                      const Eigen::VectorXd& y,\n                                      bool use_complete_obs = true) \n    {\n        \n        if (x.size() != y.size()) return (std::numeric_limits&lt;double&gt;::quiet_NaN());\n        \n        int n = x.size();\n        int valid_n = n;\n        double correlation;\n        \n        if (use_complete_obs) {\n            // Count valid pairs and compute in single pass\n            double sum_x = 0.0, sum_y = 0.0, sum_xy = 0.0, sum_x2 = 0.0, sum_y2 = 0.0;\n            int valid_n = 0;\n            \n            for (int i = 0; i &lt; n; ++i) {\n                if (std::isfinite(x(i)) && std::isfinite(y(i))) {\n                    double xi = x(i), yi = y(i);\n                    sum_x += xi;\n                    sum_y += yi;\n                    sum_xy += xi * yi;\n                    sum_x2 += xi * xi;\n                    sum_y2 += yi * yi;\n                    valid_n++;\n                }\n            }\n            \n            if (valid_n &lt; 3) return (std::numeric_limits&lt;double&gt;::quiet_NaN());\n            \n            double mean_x = sum_x / valid_n;\n            double mean_y = sum_y / valid_n;\n            \n            double numerator = sum_xy - valid_n * mean_x * mean_y;\n            double denom_x = sum_x2 - valid_n * mean_x * mean_x;\n            double denom_y = sum_y2 - valid_n * mean_y * mean_y;\n            \n            double denom = std::sqrt(denom_x * denom_y);\n            correlation = (denom &lt; 1e-14) ? std::numeric_limits&lt;double&gt;::quiet_NaN() : numerator / denom;\n            \n        } else {\n            // Direct computation assuming no missing values\n            if (valid_n &lt; 3) return (std::numeric_limits&lt;double&gt;::quiet_NaN());\n            \n            double mean_x = x.mean();\n            double mean_y = y.mean();\n            \n            double numerator = 0.0, denom_x = 0.0, denom_y = 0.0;\n            for (int i = 0; i &lt; n; ++i) {\n                double dx = x(i) - mean_x;\n                double dy = y(i) - mean_y;\n                numerator += dx * dy;\n                denom_x += dx * dx;\n                denom_y += dy * dy;\n            }\n            \n            double denom = std::sqrt(denom_x * denom_y);\n            correlation =  (denom &lt; 1e-14) ? std::numeric_limits&lt;double&gt;::quiet_NaN() : numerator / denom;\n        }\n        \n        return correlation;\n    }",
    "crumbs": [
      "Functions",
      "pearson_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pearson_correlation.html#usage-example",
    "href": "api-reference/cpp/functions/pearson_correlation.html#usage-example",
    "title": "pearson_correlation",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = pearson_correlation(...);",
    "crumbs": [
      "Functions",
      "pearson_correlation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/mygetenv.html",
    "href": "api-reference/cpp/functions/mygetenv.html",
    "title": "mygetenv",
    "section": "",
    "text": "static const char * mygetenv(const char *name, const char *unset)",
    "crumbs": [
      "Functions",
      "mygetenv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/mygetenv.html#signature",
    "href": "api-reference/cpp/functions/mygetenv.html#signature",
    "title": "mygetenv",
    "section": "",
    "text": "static const char * mygetenv(const char *name, const char *unset)",
    "crumbs": [
      "Functions",
      "mygetenv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/mygetenv.html#description",
    "href": "api-reference/cpp/functions/mygetenv.html#description",
    "title": "mygetenv",
    "section": "2 Description",
    "text": "2 Description\nSafe environment variable retrieval with default.",
    "crumbs": [
      "Functions",
      "mygetenv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/mygetenv.html#parameters",
    "href": "api-reference/cpp/functions/mygetenv.html#parameters",
    "title": "mygetenv",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nname (const char *): Environment variable name\nunset (const char *): Default value if not found",
    "crumbs": [
      "Functions",
      "mygetenv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/mygetenv.html#returns",
    "href": "api-reference/cpp/functions/mygetenv.html#returns",
    "title": "mygetenv",
    "section": "4 Returns",
    "text": "4 Returns\nconst char* Value or default string",
    "crumbs": [
      "Functions",
      "mygetenv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/mygetenv.html#details",
    "href": "api-reference/cpp/functions/mygetenv.html#details",
    "title": "mygetenv",
    "section": "5 Details",
    "text": "5 Details\nnameEnvironment variable name unsetDefault value if not found const char* Value or default string",
    "crumbs": [
      "Functions",
      "mygetenv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/mygetenv.html#call-graph",
    "href": "api-reference/cpp/functions/mygetenv.html#call-graph",
    "title": "mygetenv",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "mygetenv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/mygetenv.html#source-code",
    "href": "api-reference/cpp/functions/mygetenv.html#source-code",
    "title": "mygetenv",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/openme-utils.hpp • Lines 197-200\nstatic const char *mygetenv(const char *name, const char *unset) {\n    const char *ans = getenv(name);\n    return (ans==NULL || ans[0]=='\\0') ? unset : ans;\n}",
    "crumbs": [
      "Functions",
      "mygetenv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/mygetenv.html#usage-example",
    "href": "api-reference/cpp/functions/mygetenv.html#usage-example",
    "title": "mygetenv",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = mygetenv(...);",
    "crumbs": [
      "Functions",
      "mygetenv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplicationSparse.html",
    "href": "api-reference/cpp/functions/multiplicationSparse.html",
    "title": "multiplicationSparse",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::multiplicationSparse(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, hsize_t mem_block_size, bool bparal, bool browmajor, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "multiplicationSparse"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplicationSparse.html#signature",
    "href": "api-reference/cpp/functions/multiplicationSparse.html#signature",
    "title": "multiplicationSparse",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::multiplicationSparse(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, hsize_t mem_block_size, bool bparal, bool browmajor, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "multiplicationSparse"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplicationSparse.html#description",
    "href": "api-reference/cpp/functions/multiplicationSparse.html#description",
    "title": "multiplicationSparse",
    "section": "2 Description",
    "text": "2 Description\nSparse matrix multiplication for HDF5 matrices.",
    "crumbs": [
      "Functions",
      "multiplicationSparse"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplicationSparse.html#parameters",
    "href": "api-reference/cpp/functions/multiplicationSparse.html#parameters",
    "title": "multiplicationSparse",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input matrix dataset\ndsB (BigDataStatMeth::hdf5Dataset *): Second input matrix dataset\ndsC (BigDataStatMeth::hdf5Dataset *): Output matrix dataset\nhdf5_block (hsize_t): Block size for HDF5 I/O operations\nmem_block_size (hsize_t): Block size for in-memory operations\nbparal (bool): Whether to use parallel processing\nbrowmajor (bool): Whether matrices are stored in row-major order\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "multiplicationSparse"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplicationSparse.html#returns",
    "href": "api-reference/cpp/functions/multiplicationSparse.html#returns",
    "title": "multiplicationSparse",
    "section": "4 Returns",
    "text": "4 Returns\nType: BigDataStatMeth::hdf5Dataset *",
    "crumbs": [
      "Functions",
      "multiplicationSparse"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplicationSparse.html#details",
    "href": "api-reference/cpp/functions/multiplicationSparse.html#details",
    "title": "multiplicationSparse",
    "section": "5 Details",
    "text": "5 Details\nPerforms sparse matrix multiplication C = A * B where A, B, and C are HDF5 datasets, with optimizations for sparse data structures.",
    "crumbs": [
      "Functions",
      "multiplicationSparse"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplicationSparse.html#call-graph",
    "href": "api-reference/cpp/functions/multiplicationSparse.html#call-graph",
    "title": "multiplicationSparse",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "multiplicationSparse"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplicationSparse.html#source-code",
    "href": "api-reference/cpp/functions/multiplicationSparse.html#source-code",
    "title": "multiplicationSparse",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/multiplicationSparse.hpp • Lines 66-156\ninline BigDataStatMeth::hdf5Dataset* multiplicationSparse( BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, BigDataStatMeth::hdf5Dataset* dsC,\n                                                                  hsize_t hdf5_block, hsize_t mem_block_size, bool bparal, bool browmajor, Rcpp::Nullable&lt;int&gt; threads  = R_NilValue) \n{\n\n        try {\n            \n            hsize_t K = dsA-&gt;nrows();\n            hsize_t N = dsA-&gt;ncols();\n            \n            hsize_t M = dsB-&gt;nrows();\n            // hsize_t L = dsB-&gt;ncols();\n            \n            if( dsA-&gt;nrows() == dsB-&gt;ncols())\n            {\n                \n                hsize_t isize = hdf5_block + 1,\n                        ksize = hdf5_block + 1,\n                        jsize = hdf5_block + 1;\n                \n                std::vector&lt;hsize_t&gt; stride = {1, 1};\n                std::vector&lt;hsize_t&gt; block = {1, 1};\n                \n                dsC-&gt;createDataset( M, N, \"real\"); \n                \n                for (hsize_t ii = 0; ii &lt; N; ii += hdf5_block)\n                {\n                    \n                    if( ii + hdf5_block &gt; N ) isize = N - ii;\n                    // Això haurien de ser files i no per columnes\n                    for (hsize_t jj = 0; jj &lt; M; jj += hdf5_block)\n                    {\n                        \n                        if( jj + hdf5_block &gt; M) jsize = M - jj;\n                        \n                        for(hsize_t kk = 0; kk &lt; K; kk += hdf5_block)\n                        {\n                            \n                            if( kk + hdf5_block &gt; K ) ksize = K - kk;\n                            \n                            \n                            hsize_t iRowsA = std::min(hdf5_block,ksize),\n                                    iColsA = std::min(hdf5_block,isize),\n                                    iRowsB = std::min(hdf5_block,jsize),\n                                    iColsB = std::min(hdf5_block,ksize);\n                            \n                            std::vector&lt;double&gt; vdA( iRowsA * iColsA ); \n                            dsA-&gt;readDatasetBlock( {kk, ii}, {iRowsA, iColsA}, stride, block, vdA.data() );\n                            Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; A (vdA.data(), iRowsA, iColsA );\n                            \n                            std::vector&lt;double&gt; vdB( iRowsB * iColsB ); \n                            dsB-&gt;readDatasetBlock( {jj, kk}, {iRowsB, iColsB}, stride, block, vdB.data() );\n                            Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; B (vdB.data(), iRowsB, iColsB );\n                            \n                            std::vector&lt;double&gt; vdC( iRowsB * iColsA ); \n                            dsC-&gt;readDatasetBlock( {jj, ii}, {iRowsB, iColsA}, stride, block, vdC.data() );\n                            Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; C (vdC.data(), iRowsB, iColsA );\n                            \n                            // if( bparal == false) {\n                            //     C = C + B * A;\n                            // } else {\n                            //     C = C + Bblock_matrix_mul_parallel(B, A, mem_block_size, threads);\n                            // }\n                            \n                            C = C + (B.sparseView() * A.sparseView()).toDense() ;\n                            \n                            std::vector&lt;hsize_t&gt; offset = {jj,ii};\n                            std::vector&lt;hsize_t&gt; count = {iRowsB, iColsA};\n                            \n                            dsC-&gt;writeDatasetBlock(Rcpp::wrap(C), offset, count, stride, block, true);\n                            \n                            if( kk + hdf5_block &gt; K ) ksize = hdf5_block + 1;\n                        }\n                        \n                        if( jj + hdf5_block &gt; M ) jsize = hdf5_block + 1;\n                    }\n                    \n                    if( ii + hdf5_block &gt; N ) isize = hdf5_block + 1;\n                }\n                \n            }else {\n                throw std::range_error(\"multiplicationSparse error: non-conformable arguments\");\n            }\n            \n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception multiplicationSparse: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n            return(dsC);\n        }\n        \n        return(dsC);\n\n    }",
    "crumbs": [
      "Functions",
      "multiplicationSparse"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplicationSparse.html#usage-example",
    "href": "api-reference/cpp/functions/multiplicationSparse.html#usage-example",
    "title": "multiplicationSparse",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = multiplicationSparse(...);",
    "crumbs": [
      "Functions",
      "multiplicationSparse"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/join_datasets.html",
    "href": "api-reference/cpp/functions/join_datasets.html",
    "title": "join_datasets",
    "section": "",
    "text": "int BigDataStatMeth::join_datasets(T *dsJoined, std::string strsubgroup, Rcpp::StringVector strinput, bool bremoveJoined, bool byCols)",
    "crumbs": [
      "Functions",
      "join_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/join_datasets.html#signature",
    "href": "api-reference/cpp/functions/join_datasets.html#signature",
    "title": "join_datasets",
    "section": "",
    "text": "int BigDataStatMeth::join_datasets(T *dsJoined, std::string strsubgroup, Rcpp::StringVector strinput, bool bremoveJoined, bool byCols)",
    "crumbs": [
      "Functions",
      "join_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/join_datasets.html#description",
    "href": "api-reference/cpp/functions/join_datasets.html#description",
    "title": "join_datasets",
    "section": "2 Description",
    "text": "2 Description\nJoins multiple HDF5 datasets into a single dataset within the same group.",
    "crumbs": [
      "Functions",
      "join_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/join_datasets.html#parameters",
    "href": "api-reference/cpp/functions/join_datasets.html#parameters",
    "title": "join_datasets",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsJoined (T *): Pointer to the output dataset where joined data will be stored\nstrsubgroup (std::string): Subgroup path where the datasets are located\nstrinput (Rcpp::StringVector): Vector of input dataset names to join\nbremoveJoined (bool): Flag to remove original datasets after joining\nbyCols (bool): Flag indicating whether to join by columns",
    "crumbs": [
      "Functions",
      "join_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/join_datasets.html#returns",
    "href": "api-reference/cpp/functions/join_datasets.html#returns",
    "title": "join_datasets",
    "section": "4 Returns",
    "text": "4 Returns\nint Returns 0 on success, -1 on failure",
    "crumbs": [
      "Functions",
      "join_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/join_datasets.html#details",
    "href": "api-reference/cpp/functions/join_datasets.html#details",
    "title": "join_datasets",
    "section": "5 Details",
    "text": "5 Details\nTDataset type (must be either hdf5Dataset* or hdf5DatasetInternal*) dsJoinedPointer to the output dataset where joined data will be stored strsubgroupSubgroup path where the datasets are located strinputVector of input dataset names to join bremoveJoinedFlag to remove original datasets after joining byColsFlag indicating whether to join by columnsint Returns 0 on success, -1 on failureH5::FileIExceptionon file operation errors H5::DataSetIExceptionon dataset operation errors H5::GroupIExceptionon group operation errors H5::DataSpaceIExceptionon dataspace operation errors H5::DataTypeIExceptionon datatype operation errors std::exceptionon general errorsThe function uses unlimited datasets to allow for dynamic growth Memory is managed efficiently using Eigen’s mapping capabilities Performance considerations:Uses block-wise reading and writing for memory efficiencyImplements Eigen for fast matrix operationsAutomatically extends dataset size as needed",
    "crumbs": [
      "Functions",
      "join_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/join_datasets.html#call-graph",
    "href": "api-reference/cpp/functions/join_datasets.html#call-graph",
    "title": "join_datasets",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "join_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/join_datasets.html#source-code",
    "href": "api-reference/cpp/functions/join_datasets.html#source-code",
    "title": "join_datasets",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5Methods.hpp • Lines 72-178\nint join_datasets ( T* dsJoined, std::string strsubgroup, Rcpp::StringVector strinput, bool bremoveJoined, bool byCols )\n    {\n        static_assert(std::is_same&lt;T*, BigDataStatMeth::hdf5Dataset* &gt;::value || \n                      std::is_same&lt;T*, BigDataStatMeth::hdf5DatasetInternal* &gt;::value,\n                      \"Error - type not allowed\");\n\n        try{\n            \n            H5::Exception::dontPrint();\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1},\n                                 block = {1, 1},\n                                 offset = {0, 0},\n                                 count = {0, 0};\n            \n            std::string stroutdataset = dsJoined-&gt;getDatasetName();\n            \n            \n            \n            BigDataStatMeth::hdf5Dataset* dstoJoin = new hdf5Dataset(dsJoined-&gt;getFullPath(), strsubgroup, Rcpp::as&lt;std::string&gt;(strinput[0]), false);\n            dstoJoin-&gt;openDataset();\n            \n            hsize_t* dims_out = dstoJoin-&gt;dim();\n            \n            // Add rows and needed cols to add the merged data in the new dataset\n            dsJoined-&gt;createUnlimitedDataset( (unsigned long long)dims_out[0], (unsigned long long)dims_out[1], \"real\");\n            dsJoined-&gt;openDataset();\n\n            // Read data to merge\n            std::vector&lt;double&gt; vreadeddata( dims_out[0] * dims_out[1] ); \n            dstoJoin-&gt;readDatasetBlock( {offset[0], offset[1]}, {dims_out[0], dims_out[1]}, stride, block, vreadeddata.data() );\n            {\n                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; readedData = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vreadeddata.data(),  dims_out[0], dims_out[1] );\n                count[0] = dims_out[0]; count[1] = dims_out[1];\n                \n                // Write data to the new dataset\n                dsJoined-&gt;writeDatasetBlock( Rcpp::wrap(readedData), offset, count, stride, block, false);\n            }\n            \n            \n            delete dstoJoin; // Remove original dataset link\n            // Update offset to new position\n            offset[1] = offset[1] + dims_out[1];\n            \n            for( int i=1; i&lt;strinput.size(); i++)\n            {\n                \n                dstoJoin = new hdf5Dataset(dsJoined-&gt;getFullPath(), strsubgroup, Rcpp::as&lt;std::string&gt;(strinput[i]), false);\n                dstoJoin-&gt;openDataset();\n                dims_out = dstoJoin-&gt;dim();\n                \n                // Extend dataset before put data\n                dsJoined-&gt;extendUnlimitedDataset( (unsigned long long)0, (unsigned long long)dims_out[1] );\n                \n                // Read data to merge\n                std::vector&lt;double&gt; vreadeddata( dims_out[0] * dims_out[1] ); \n                dstoJoin-&gt;readDatasetBlock( {0, 0}, {dims_out[0], dims_out[1]}, stride, block, vreadeddata.data() );\n                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; readedData = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vreadeddata.data(),  dims_out[0], dims_out[1] );\n                \n                delete dstoJoin;\n                \n                count[0] = dims_out[0]; count[1] = dims_out[1];\n                \n                // Write data to the new dataset\n                dsJoined-&gt;writeDatasetBlock( Rcpp::wrap(readedData), offset, count, stride, block, false);\n                \n                // Update offset\n                offset[1] = offset[1] + dims_out[1];\n            }\n            \n            if(bremoveJoined == true) {\n                // Remove joined elements\n                BigDataStatMeth::remove_elements(dsJoined-&gt;getFileptr(), strsubgroup, strinput);    \n            }\n            \n            \n        } catch(H5::FileIException& error) { // catch failure caused by the H5File operations\n            checkClose_file(dsJoined);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception join_datasets (File IException)\" &lt;&lt; std::endl;\n            return -1;\n        } catch(H5::DataSetIException& error) { // catch failure caused by the DataSet operations\n            checkClose_file(dsJoined);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception join_datasets (DataSet IException)\" &lt;&lt; std::endl;\n            return -1;\n        } catch(H5::GroupIException& error) { // catch failure caused by the Group operations\n            checkClose_file(dsJoined);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception join_datasets (Group IException)\" &lt;&lt; std::endl;\n            return -1;\n        } catch(H5::DataSpaceIException& error) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsJoined);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception join_datasets (DataSpace IException)\" &lt;&lt; std::endl;\n            return -1;\n        } catch(H5::DataTypeIException& error) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsJoined);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception join_datasets (Data TypeIException)\" &lt;&lt; std::endl;\n            return -1;\n        } catch(std::exception &ex) {\n            checkClose_file(dsJoined);\n            Rcpp::Rcerr &lt;&lt; \"c++ exception join_datasets: \" &lt;&lt; ex.what();\n            return -1;\n        } catch (...) {\n            checkClose_file(dsJoined);\n            Rcpp::Rcerr&lt;&lt;\"C++ exception join_datasets (unknown reason)\";\n            return -1;\n        }\n        return(0);\n    }",
    "crumbs": [
      "Functions",
      "join_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/join_datasets.html#usage-example",
    "href": "api-reference/cpp/functions/join_datasets.html#usage-example",
    "title": "join_datasets",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = join_datasets(...);",
    "crumbs": [
      "Functions",
      "join_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isMatrixSymmetric.html",
    "href": "api-reference/cpp/functions/isMatrixSymmetric.html",
    "title": "isMatrixSymmetric",
    "section": "",
    "text": "bool BigDataStatMeth::isMatrixSymmetric(const Eigen::MatrixXd &X, int sample_size=100)",
    "crumbs": [
      "Functions",
      "isMatrixSymmetric"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isMatrixSymmetric.html#signature",
    "href": "api-reference/cpp/functions/isMatrixSymmetric.html#signature",
    "title": "isMatrixSymmetric",
    "section": "",
    "text": "bool BigDataStatMeth::isMatrixSymmetric(const Eigen::MatrixXd &X, int sample_size=100)",
    "crumbs": [
      "Functions",
      "isMatrixSymmetric"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isMatrixSymmetric.html#description",
    "href": "api-reference/cpp/functions/isMatrixSymmetric.html#description",
    "title": "isMatrixSymmetric",
    "section": "2 Description",
    "text": "2 Description\nImproved matrix symmetry detection for big-omics data.",
    "crumbs": [
      "Functions",
      "isMatrixSymmetric"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isMatrixSymmetric.html#parameters",
    "href": "api-reference/cpp/functions/isMatrixSymmetric.html#parameters",
    "title": "isMatrixSymmetric",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (const Eigen::MatrixXd &): Input matrix to check for symmetry\nsample_size (int): Number of elements to sample for symmetry check (default 100)",
    "crumbs": [
      "Functions",
      "isMatrixSymmetric"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isMatrixSymmetric.html#returns",
    "href": "api-reference/cpp/functions/isMatrixSymmetric.html#returns",
    "title": "isMatrixSymmetric",
    "section": "4 Returns",
    "text": "4 Returns\nTrue if matrix is approximately symmetric within tolerance",
    "crumbs": [
      "Functions",
      "isMatrixSymmetric"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isMatrixSymmetric.html#details",
    "href": "api-reference/cpp/functions/isMatrixSymmetric.html#details",
    "title": "isMatrixSymmetric",
    "section": "5 Details",
    "text": "5 Details\nXInput matrix to check for symmetry sample_sizeNumber of elements to sample for symmetry check (default 100) True if matrix is approximately symmetric within tolerance Optimized for large biological matrices with potential noise",
    "crumbs": [
      "Functions",
      "isMatrixSymmetric"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isMatrixSymmetric.html#caller-graph",
    "href": "api-reference/cpp/functions/isMatrixSymmetric.html#caller-graph",
    "title": "isMatrixSymmetric",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "isMatrixSymmetric"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isMatrixSymmetric.html#source-code",
    "href": "api-reference/cpp/functions/isMatrixSymmetric.html#source-code",
    "title": "isMatrixSymmetric",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixEigenDecomposition.hpp • Lines 101-142\ninline bool isMatrixSymmetric(const Eigen::MatrixXd& X, int sample_size = 100) {\n        if (X.rows() != X.cols()) return false;\n        \n        int n = X.rows();\n        if (n &lt;= 3) return true; // Too small to meaningfully check\n        \n        // For small matrices, check all elements\n        if (n &lt;= 50) {\n            double max_diff = (X - X.transpose()).cwiseAbs().maxCoeff();\n            double matrix_scale = X.cwiseAbs().maxCoeff();\n            return max_diff &lt;= 1e-12 * std::max(1.0, matrix_scale);\n        }\n        \n        // For large matrices, use optimized sampling with vectorized operations\n        sample_size = std::min(sample_size, n * n / 4);\n        \n        // Sample diagonal and off-diagonal elements efficiently\n        Eigen::VectorXd diffs(sample_size);\n        int count = 0;\n        \n        // Random sampling with good coverage\n        std::random_device rd;\n        std::mt19937 gen(rd());\n        std::uniform_int_distribution&lt;&gt; dis(0, n - 1);\n        \n        for (int s = 0; s &lt; sample_size && count &lt; sample_size; ++s) {\n            int i = dis(gen);\n            int j = dis(gen);\n            if (i != j) {\n                diffs(count++) = std::abs(X(i, j) - X(j, i));\n            }\n        }\n        \n        if (count == 0) return true;\n        \n        // Use vectorized operations to compute statistics\n        double max_diff = diffs.head(count).maxCoeff();\n        double matrix_scale = X.cwiseAbs().maxCoeff();\n        double tolerance = 1e-12 * std::max(1.0, matrix_scale);\n        \n        return max_diff &lt;= tolerance;\n    }",
    "crumbs": [
      "Functions",
      "isMatrixSymmetric"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isMatrixSymmetric.html#usage-example",
    "href": "api-reference/cpp/functions/isMatrixSymmetric.html#usage-example",
    "title": "isMatrixSymmetric",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = isMatrixSymmetric(...);",
    "crumbs": [
      "Functions",
      "isMatrixSymmetric"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/initDTthreads.html",
    "href": "api-reference/cpp/functions/initDTthreads.html",
    "title": "initDTthreads",
    "section": "",
    "text": "void initDTthreads()",
    "crumbs": [
      "Functions",
      "initDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/initDTthreads.html#signature",
    "href": "api-reference/cpp/functions/initDTthreads.html#signature",
    "title": "initDTthreads",
    "section": "",
    "text": "void initDTthreads()",
    "crumbs": [
      "Functions",
      "initDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/initDTthreads.html#description",
    "href": "api-reference/cpp/functions/initDTthreads.html#description",
    "title": "initDTthreads",
    "section": "2 Description",
    "text": "2 Description\nInitializes thread configuration based on environment and system state.",
    "crumbs": [
      "Functions",
      "initDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/initDTthreads.html#details",
    "href": "api-reference/cpp/functions/initDTthreads.html#details",
    "title": "initDTthreads",
    "section": "3 Details",
    "text": "3 Details\nThread count determination algorithm:Check R_DATATABLE_NUM_THREADSIf unset, use R_DATATABLE_NUM_PROCS_PERCENTApply system limits (num_procs, thread_limit)Honor OpenMP settingsEnsure at least one thread",
    "crumbs": [
      "Functions",
      "initDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/initDTthreads.html#call-graph",
    "href": "api-reference/cpp/functions/initDTthreads.html#call-graph",
    "title": "initDTthreads",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "initDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/initDTthreads.html#source-code",
    "href": "api-reference/cpp/functions/initDTthreads.html#source-code",
    "title": "initDTthreads",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/openme-utils.hpp • Lines 124-150\ninline void initDTthreads() {\n    // called at package startup from init.c\n    // also called by setDTthreads(threads=NULL) (default) to reread environment variables; see setDTthreads below\n    // No verbosity here in this setter. Verbosity is in getDTthreads(verbose=TRUE)\n    int ans = getIntEnv(\"R_DATATABLE_NUM_THREADS\", INT_MIN);\n    if (ans&gt;=1) {\n        ans = imin(ans, omp_get_num_procs());  // num_procs is a hard limit; user cannot achieve more. ifndef _OPENMP then myomp.h defines this to be 1\n    } else {\n        // Only when R_DATATABLE_NUM_THREADS is unset (or &lt;=0) do we use PROCS_PERCENT; #4514\n        int perc = getIntEnv(\"R_DATATABLE_NUM_PROCS_PERCENT\", 50); // use \"NUM_PROCS\" to use the same name as the OpenMP function this uses\n        // 50% of logical CPUs by default; half of 8 is 4 on laptop with 4 cores. Leaves plenty of room for other processes: #3395 & #3298\n        if (perc&lt;=1 || perc&gt;100) {\n            Rcpp::warning((\"Ignoring invalid R_DATATABLE_NUM_PROCS_PERCENT==%d. If used it must be an integer between 2 and 100. Default is 50. See ?setDTtheads.\"), perc);\n            // not allowing 1 is to catch attempts to use 1 or 1.0 to represent 100%.\n            perc = 50;\n        }\n        ans = imax(omp_get_num_procs()*perc/100, 1); // imax for when formula would result in 0.\n    }\n    ans = imin(ans, omp_get_thread_limit());  // honors OMP_THREAD_LIMIT when OpenMP started; e.g. CRAN sets this to 2. Often INT_MAX meaning unlimited/unset\n    ans = imin(ans, omp_get_max_threads());   // honors OMP_NUM_THREADS when OpenMP started, plus reflects any omp_set_* calls made since\n    // max_threads() -vs- num_procs(): https://software.intel.com/en-us/forums/intel-visual-fortran-compiler-for-windows/topic/302866\n    ans = imin(ans, getIntEnv(\"OMP_THREAD_LIMIT\", INT_MAX));  // user might expect `Sys.setenv(OMP_THREAD_LIMIT=2);setDTthreads()` to work. Satisfy this\n    ans = imin(ans, getIntEnv(\"OMP_NUM_THREADS\", INT_MAX));   //   expectation by reading them again now. OpenMP just reads them on startup (quite reasonably)\n    ans = imax(ans, 1);  // just in case omp_get_* returned &lt;=0 for any reason, or the env variables above are set &lt;=0\n    DTthreads = ans;\n    DTthrottle = imax(1, getIntEnv(\"R_DATATABLE_THROTTLE\", 1024)); // 2nd thread is used only when n&gt;1024, 3rd thread when n&gt;2048, etc\n}",
    "crumbs": [
      "Functions",
      "initDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/initDTthreads.html#usage-example",
    "href": "api-reference/cpp/functions/initDTthreads.html#usage-example",
    "title": "initDTthreads",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = initDTthreads(...);",
    "crumbs": [
      "Functions",
      "initDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imin.html",
    "href": "api-reference/cpp/functions/imin.html",
    "title": "imin",
    "section": "",
    "text": "static int imin(int a, int b)",
    "crumbs": [
      "Functions",
      "imin"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imin.html#signature",
    "href": "api-reference/cpp/functions/imin.html#signature",
    "title": "imin",
    "section": "",
    "text": "static int imin(int a, int b)",
    "crumbs": [
      "Functions",
      "imin"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imin.html#description",
    "href": "api-reference/cpp/functions/imin.html#description",
    "title": "imin",
    "section": "2 Description",
    "text": "2 Description\nMinimum of two integers.",
    "crumbs": [
      "Functions",
      "imin"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imin.html#parameters",
    "href": "api-reference/cpp/functions/imin.html#parameters",
    "title": "imin",
    "section": "3 Parameters",
    "text": "3 Parameters\n\na (int): First integer\nb (int): Second integer",
    "crumbs": [
      "Functions",
      "imin"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imin.html#returns",
    "href": "api-reference/cpp/functions/imin.html#returns",
    "title": "imin",
    "section": "4 Returns",
    "text": "4 Returns\nint Smaller value",
    "crumbs": [
      "Functions",
      "imin"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imin.html#details",
    "href": "api-reference/cpp/functions/imin.html#details",
    "title": "imin",
    "section": "5 Details",
    "text": "5 Details\naFirst integer bSecond integer int Smaller value",
    "crumbs": [
      "Functions",
      "imin"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imin.html#call-graph",
    "href": "api-reference/cpp/functions/imin.html#call-graph",
    "title": "imin",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "imin"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imin.html#source-code",
    "href": "api-reference/cpp/functions/imin.html#source-code",
    "title": "imin",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/openme-utils.hpp • Line 90\nstatic inline int imin(int a, int b) { return a &lt; b ? a : b; }",
    "crumbs": [
      "Functions",
      "imin"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imin.html#usage-example",
    "href": "api-reference/cpp/functions/imin.html#usage-example",
    "title": "imin",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = imin(...);",
    "crumbs": [
      "Functions",
      "imin"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html",
    "href": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html",
    "title": "hdf5_matrixVector_calculus",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::hdf5_matrixVector_calculus(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, int function, bool bbyrows, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "hdf5_matrixVector_calculus"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#signature",
    "href": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#signature",
    "title": "hdf5_matrixVector_calculus",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::hdf5_matrixVector_calculus(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, int function, bool bbyrows, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "hdf5_matrixVector_calculus"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#description",
    "href": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#description",
    "title": "hdf5_matrixVector_calculus",
    "section": "2 Description",
    "text": "2 Description\nVector-matrix operations for HDF5 matrices.",
    "crumbs": [
      "Functions",
      "hdf5_matrixVector_calculus"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#parameters",
    "href": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#parameters",
    "title": "hdf5_matrixVector_calculus",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): Input matrix dataset\ndsB (BigDataStatMeth::hdf5Dataset *): Input vector dataset\ndsC (BigDataStatMeth::hdf5Dataset *): Output matrix dataset\nfunction (int): Operation type (multiplication, addition, subtraction, division, power)\nbbyrows (bool): Whether to operate by rows or columns\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "hdf5_matrixVector_calculus"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#returns",
    "href": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#returns",
    "title": "hdf5_matrixVector_calculus",
    "section": "4 Returns",
    "text": "4 Returns\nType: BigDataStatMeth::hdf5Dataset *",
    "crumbs": [
      "Functions",
      "hdf5_matrixVector_calculus"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#details",
    "href": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#details",
    "title": "hdf5_matrixVector_calculus",
    "section": "5 Details",
    "text": "5 Details\nPerforms vector-matrix operations on HDF5 datasets with support for parallel processing and row/column-wise operations.",
    "crumbs": [
      "Functions",
      "hdf5_matrixVector_calculus"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#call-graph",
    "href": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#call-graph",
    "title": "hdf5_matrixVector_calculus",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "hdf5_matrixVector_calculus"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#source-code",
    "href": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#source-code",
    "title": "hdf5_matrixVector_calculus",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectormatrix.hpp • Lines 208-335\ninline BigDataStatMeth::hdf5Dataset* hdf5_matrixVector_calculus(\n        BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, \n        BigDataStatMeth::hdf5Dataset* dsC, int function, bool bbyrows, \n        bool bparal, Rcpp::Nullable&lt;int&gt; threads  = R_NilValue)\n{\n    \n    try{\n        \n        std::vector&lt;hsize_t&gt; stride = {1, 1};\n        std::vector&lt;hsize_t&gt; block = {1, 1};\n        \n        int blocksize = 0;\n        \n        // Define blocksize atending number of elements in rows and cols\n        if( bbyrows == false) {\n            if( dsA-&gt;ncols() &gt; MAXELEMSINBLOCK ) {\n                blocksize = 1;\n            } else {\n                hsize_t maxsize = std::max&lt;hsize_t&gt;(  dsA-&gt;nrows(),  dsA-&gt;ncols());\n                blocksize = std::ceil( MAXELEMSINBLOCK / maxsize);\n            }\n            \n        } else {\n            if( dsA-&gt;nrows() &gt; MAXELEMSINBLOCK) {\n                blocksize = 1;\n            } else {\n                hsize_t maxsize = std::max&lt;hsize_t&gt;( dsA-&gt;nrows(), dsA-&gt;ncols());\n                blocksize = std::ceil( MAXELEMSINBLOCK / maxsize);\n            }\n        }\n        \n        \n        dsC-&gt;createDataset( dsA-&gt;nrows(), dsA-&gt;ncols(), \"real\");    \n        \n        std::vector&lt;double&gt; vdB( dsB-&gt;nrows() * dsB-&gt;ncols());\n        dsB-&gt;readDatasetBlock( {0, 0}, { dsB-&gt;nrows(), dsB-&gt;ncols()}, stride, block, vdB.data() );\n        \n        \n        if( bbyrows == false) {\n            \n            Eigen::RowVectorXd vWeights = Eigen::Map&lt;Eigen::VectorXd, Eigen::Unaligned&gt;(vdB.data(), vdB.size());\n            \n            for(hsize_t i=0; (i * blocksize) &lt;= dsA-&gt;nrows(); i++)\n            {\n                hsize_t sizetoread = 0;\n                if((i+1)*blocksize&lt;dsA-&gt;nrows()) {\n                    sizetoread = blocksize;\n                } else {\n                    sizetoread = dsA-&gt;nrows()-(i*blocksize);\n                }\n                \n                std::vector&lt;hsize_t&gt; offset = { i*blocksize, 0};\n                std::vector&lt;hsize_t&gt; count = {sizetoread, dsA-&gt;ncols()};\n                \n                // Compute and write data\n                std::vector&lt;double&gt; vdA( count[0] * count[1]);\n                dsA-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdA.data() );\n                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; X (vdA.data(), count[0], count[1] );    \n                                \n                if( function == 0 ) {\n                    X = Rcpp_matrixVectorSum_byCol(X, vWeights); \n                } else if( function == 1 ) {\n                    X = Rcpp_matrixVectorSubstract_byCol(X, vWeights);\n                } else if( function == 2 ) {\n                    X = Rcpp_matrixVectorMultiplication_byCol(X, vWeights);\n                } else if( function == 3 ) {\n                    X = Rcpp_matrixVectorDivision_byCol(X, vWeights);\n                } else if( function == 4 ) {\n                    Rcpp_matrixVectorPower_byCol(X, vWeights);\n                }\n                \n                dsC-&gt;writeDatasetBlock(Rcpp::wrap(X.transpose()), offset, count, stride, block, false);\n            }\n        } else {\n            \n            Eigen::VectorXd vWeights = Eigen::Map&lt;Eigen::VectorXd, Eigen::Unaligned&gt;(vdB.data(), vdB.size());\n            \n            for(hsize_t i=0; i*blocksize &lt;= dsA-&gt;ncols() ; i++) {\n                hsize_t sizetoread = 0;\n                if( (i+1)*blocksize &lt; dsA-&gt;ncols() ){\n                    sizetoread = blocksize;\n                } else {\n                    sizetoread = dsA-&gt;ncols()-(i*blocksize);\n                }\n                \n                std::vector&lt;hsize_t&gt; offset = {0, i*blocksize};\n                std::vector&lt;hsize_t&gt; count = { dsA-&gt;nrows(), sizetoread};\n                \n                // Compute and write data\n                std::vector&lt;double&gt; vdA( count[0] * count[1]);\n                dsA-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdA.data() );\n                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; X (vdA.data(), count[0], count[1] );\n                \n                if( function == 0 ) {\n                    X = Rcpp_matrixVectorSum_byRow(X, vWeights); \n                } else if( function == 1 ) {\n                    X = Rcpp_matrixVectorSubstract_byRow(X, vWeights);\n                } else if( function == 2 ) {\n                    X = Rcpp_matrixVectorMultiplication_byRow(X, vWeights);\n                } else if( function == 3 ) {\n                    X = Rcpp_matrixVectorDivision_byRow(X, vWeights);\n                } else if( function == 4 ) {\n                    X = Rcpp_matrixVectorPow_byRow(X, vWeights);\n                }\n                \n                offset = {i*blocksize, 0};\n                \n                dsC-&gt;writeDatasetBlock(Rcpp::wrap(X.transpose()), offset, count, stride, block, true);\n            }\n        }\n        \n    } catch( H5::FileIException& error ) { // catch failure caused by the H5File operations\n        // error.printErrorStack();\n        checkClose_file(dsA, dsB, dsC);\n        Rcpp::Rcerr&lt;&lt; \"c++ exception hdf5_matrixVector_calculus (File IException)\" ;\n    } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n        // error.printErrorStack();\n        checkClose_file(dsA, dsB, dsC);\n        Rcpp::Rcerr&lt;&lt;\"c++ exception hdf5_matrixVector_calculus (DataSet IException)\";\n    } catch(std::exception& error) {\n        checkClose_file(dsA, dsB, dsC);\n        Rcpp::Rcout&lt;&lt; \"c++ exception vector-matrix functions: \"&lt;&lt;error.what()&lt;&lt; \" \\n\";\n        // return(dsC);\n    }\n    \n    return(dsC);\n    \n}",
    "crumbs": [
      "Functions",
      "hdf5_matrixVector_calculus"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#usage-example",
    "href": "api-reference/cpp/functions/hdf5_matrixVector_calculus.html#usage-example",
    "title": "hdf5_matrixVector_calculus",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = hdf5_matrixVector_calculus(...);",
    "crumbs": [
      "Functions",
      "hdf5_matrixVector_calculus"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_threads.html",
    "href": "api-reference/cpp/functions/get_threads.html",
    "title": "get_threads",
    "section": "",
    "text": "unsigned int BigDataStatMeth::get_threads(bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "get_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_threads.html#signature",
    "href": "api-reference/cpp/functions/get_threads.html#signature",
    "title": "get_threads",
    "section": "",
    "text": "unsigned int BigDataStatMeth::get_threads(bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "get_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_threads.html#description",
    "href": "api-reference/cpp/functions/get_threads.html#description",
    "title": "get_threads",
    "section": "2 Description",
    "text": "2 Description\nDetermines number of threads for parallel operations.",
    "crumbs": [
      "Functions",
      "get_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_threads.html#parameters",
    "href": "api-reference/cpp/functions/get_threads.html#parameters",
    "title": "get_threads",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Optional number of threads to use",
    "crumbs": [
      "Functions",
      "get_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_threads.html#returns",
    "href": "api-reference/cpp/functions/get_threads.html#returns",
    "title": "get_threads",
    "section": "4 Returns",
    "text": "4 Returns\nunsigned int Number of threads to use",
    "crumbs": [
      "Functions",
      "get_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_threads.html#details",
    "href": "api-reference/cpp/functions/get_threads.html#details",
    "title": "get_threads",
    "section": "5 Details",
    "text": "5 Details\nbparalWhether to use parallel processing threadsOptional number of threads to use unsigned int Number of threads to use Thread determination:Considers hardware concurrencyRespects user-specified thread countFalls back to single thread if parallel disabledOptimizes for system resources",
    "crumbs": [
      "Functions",
      "get_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_threads.html#call-graph",
    "href": "api-reference/cpp/functions/get_threads.html#call-graph",
    "title": "get_threads",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "get_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_threads.html#source-code",
    "href": "api-reference/cpp/functions/get_threads.html#source-code",
    "title": "get_threads",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/Utilities.hpp • Lines 541-560\ninline unsigned int get_threads(bool bparal, Rcpp::Nullable&lt;int&gt; threads = R_NilValue) \n    {\n        unsigned int ithreads = std::thread::hardware_concurrency();\n        \n        if(bparal == false) {\n            ithreads = 1;\n        } else {\n            if(threads.isNotNull()) {\n                \n                if ((unsigned)Rcpp::as&lt;int&gt; (threads) &lt;= ithreads){\n                    ithreads = Rcpp::as&lt;int&gt; (threads);\n                } \n            \n            } else {\n                ithreads =  getDTthreads(ithreads, false);\n            }    \n        }\n        \n        return(ithreads);\n    }",
    "crumbs": [
      "Functions",
      "get_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_threads.html#usage-example",
    "href": "api-reference/cpp/functions/get_threads.html#usage-example",
    "title": "get_threads",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = get_threads(...);",
    "crumbs": [
      "Functions",
      "get_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_data_as_Matrix.html",
    "href": "api-reference/cpp/functions/get_data_as_Matrix.html",
    "title": "get_data_as_Matrix",
    "section": "",
    "text": "std::vector&lt; double &gt; BigDataStatMeth::get_data_as_Matrix(std::vector&lt; std::string &gt; strBlockValues)",
    "crumbs": [
      "Functions",
      "get_data_as_Matrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_data_as_Matrix.html#signature",
    "href": "api-reference/cpp/functions/get_data_as_Matrix.html#signature",
    "title": "get_data_as_Matrix",
    "section": "",
    "text": "std::vector&lt; double &gt; BigDataStatMeth::get_data_as_Matrix(std::vector&lt; std::string &gt; strBlockValues)",
    "crumbs": [
      "Functions",
      "get_data_as_Matrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_data_as_Matrix.html#description",
    "href": "api-reference/cpp/functions/get_data_as_Matrix.html#description",
    "title": "get_data_as_Matrix",
    "section": "2 Description",
    "text": "2 Description\nConverts a vector of string values to numeric (double) values.",
    "crumbs": [
      "Functions",
      "get_data_as_Matrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_data_as_Matrix.html#parameters",
    "href": "api-reference/cpp/functions/get_data_as_Matrix.html#parameters",
    "title": "get_data_as_Matrix",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nstrBlockValues (std::vector&lt; std::string &gt;): Vector of strings to convert",
    "crumbs": [
      "Functions",
      "get_data_as_Matrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_data_as_Matrix.html#returns",
    "href": "api-reference/cpp/functions/get_data_as_Matrix.html#returns",
    "title": "get_data_as_Matrix",
    "section": "4 Returns",
    "text": "4 Returns\nstd::vector Vector of converted double values",
    "crumbs": [
      "Functions",
      "get_data_as_Matrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_data_as_Matrix.html#details",
    "href": "api-reference/cpp/functions/get_data_as_Matrix.html#details",
    "title": "get_data_as_Matrix",
    "section": "5 Details",
    "text": "5 Details\nstrBlockValuesVector of strings to convert std::vector Vector of converted double valuesRcpp::stopIf any value is not numeric or if empty fields are found Handles end-of-line characters in the last column",
    "crumbs": [
      "Functions",
      "get_data_as_Matrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_data_as_Matrix.html#caller-graph",
    "href": "api-reference/cpp/functions/get_data_as_Matrix.html#caller-graph",
    "title": "get_data_as_Matrix",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "get_data_as_Matrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_data_as_Matrix.html#source-code",
    "href": "api-reference/cpp/functions/get_data_as_Matrix.html#source-code",
    "title": "get_data_as_Matrix",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5ImportFiles.hpp • Lines 131-151\ninline std::vector&lt;double&gt; get_data_as_Matrix(std::vector&lt;std::string&gt; strBlockValues)\n    {\n        \n        std::vector&lt;double&gt; doubleVector(strBlockValues.size());\n        std::transform(strBlockValues.begin(), strBlockValues.end(), doubleVector.begin(), [](const std::string& s) mutable\n        {\n            if( is_number(s) ==1 ){\n                return (std::stod(s));\n            } else {\n                // Takes in to account the possible eol for the last column\n                if( is_number(s.substr(0,s.length()-1) ) ==1 ){\n                    return (std::stod(s));\n                } else {\n                    Rcpp::stop(\"Error: Column is not numeric. Only numeric data is allowed. Maybe there is a blank row at the end of the file or any field is empty\");\n                }\n            }\n        });\n\n        return(doubleVector);\n\n    }",
    "crumbs": [
      "Functions",
      "get_data_as_Matrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_data_as_Matrix.html#usage-example",
    "href": "api-reference/cpp/functions/get_data_as_Matrix.html#usage-example",
    "title": "get_data_as_Matrix",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = get_data_as_Matrix(...);",
    "crumbs": [
      "Functions",
      "get_data_as_Matrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_SplitData_in_vectorString.html",
    "href": "api-reference/cpp/functions/get_SplitData_in_vectorString.html",
    "title": "get_SplitData_in_vectorString",
    "section": "",
    "text": "std::vector&lt; std::string &gt; BigDataStatMeth::get_SplitData_in_vectorString(std::string line, std::regex reg_expres)",
    "crumbs": [
      "Functions",
      "get_SplitData_in_vectorString"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#signature",
    "href": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#signature",
    "title": "get_SplitData_in_vectorString",
    "section": "",
    "text": "std::vector&lt; std::string &gt; BigDataStatMeth::get_SplitData_in_vectorString(std::string line, std::regex reg_expres)",
    "crumbs": [
      "Functions",
      "get_SplitData_in_vectorString"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#description",
    "href": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#description",
    "title": "get_SplitData_in_vectorString",
    "section": "2 Description",
    "text": "2 Description\nSplits a string into fields based on a regular expression.",
    "crumbs": [
      "Functions",
      "get_SplitData_in_vectorString"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#parameters",
    "href": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#parameters",
    "title": "get_SplitData_in_vectorString",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nline (std::string): Input string to be split\nreg_expres (std::regex): Regular expression defining the splitting pattern",
    "crumbs": [
      "Functions",
      "get_SplitData_in_vectorString"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#returns",
    "href": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#returns",
    "title": "get_SplitData_in_vectorString",
    "section": "4 Returns",
    "text": "4 Returns\nstd::vector Vector containing the split substrings",
    "crumbs": [
      "Functions",
      "get_SplitData_in_vectorString"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#details",
    "href": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#details",
    "title": "get_SplitData_in_vectorString",
    "section": "5 Details",
    "text": "5 Details\nlineInput string to be split reg_expresRegular expression defining the splitting pattern std::vector Vector containing the split substringsget_data_as_Matrix() for numeric conversion of the split data",
    "crumbs": [
      "Functions",
      "get_SplitData_in_vectorString"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#caller-graph",
    "href": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#caller-graph",
    "title": "get_SplitData_in_vectorString",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "get_SplitData_in_vectorString"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#source-code",
    "href": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#source-code",
    "title": "get_SplitData_in_vectorString",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5ImportFiles.hpp • Lines 66-82\ninline std::vector&lt;std::string&gt; get_SplitData_in_vectorString(std::string line, std::regex reg_expres)\n    {\n        std::vector&lt;std::string&gt; strValues;\n\n        // Split line in columns by delim (reg_xpres)\n        std::sregex_token_iterator\n        begin(line.begin(), line.end(), reg_expres),\n        end;\n\n        // write all the words to strValues\n        std::copy(begin, end, std::back_inserter(strValues));\n        \n        // for (auto i: strValues)\n        //     std::cout &lt;&lt; i &lt;&lt; ' ';\n\n        return(strValues);\n    }",
    "crumbs": [
      "Functions",
      "get_SplitData_in_vectorString"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#usage-example",
    "href": "api-reference/cpp/functions/get_SplitData_in_vectorString.html#usage-example",
    "title": "get_SplitData_in_vectorString",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = get_SplitData_in_vectorString(...);",
    "crumbs": [
      "Functions",
      "get_SplitData_in_vectorString"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html",
    "title": "get_HDF5_mean_sd_by_row",
    "section": "",
    "text": "void BigDataStatMeth::get_HDF5_mean_sd_by_row(BigDataStatMeth::hdf5Dataset *dsA, Eigen::MatrixXd &normalize, bool bsd, bool bmean, Rcpp::Nullable&lt; int &gt; wsize)",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_row"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#signature",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#signature",
    "title": "get_HDF5_mean_sd_by_row",
    "section": "",
    "text": "void BigDataStatMeth::get_HDF5_mean_sd_by_row(BigDataStatMeth::hdf5Dataset *dsA, Eigen::MatrixXd &normalize, bool bsd, bool bmean, Rcpp::Nullable&lt; int &gt; wsize)",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_row"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#description",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#description",
    "title": "get_HDF5_mean_sd_by_row",
    "section": "2 Description",
    "text": "2 Description\nCalculate row-wise mean and standard deviation.",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_row"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#parameters",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#parameters",
    "title": "get_HDF5_mean_sd_by_row",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): Input matrix dataset\nnormalize (Eigen::MatrixXd &): Output matrix for mean and std values\nwsize (Rcpp::Nullable&lt; int &gt;): Block size for processing",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_row"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#details",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#details",
    "title": "get_HDF5_mean_sd_by_row",
    "section": "4 Details",
    "text": "4 Details\nComputes mean and standard deviation for each row of the matrix using block-based processing for memory efficiency.",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_row"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#call-graph",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#call-graph",
    "title": "get_HDF5_mean_sd_by_row",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_row"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#source-code",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#source-code",
    "title": "get_HDF5_mean_sd_by_row",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSdMean.hpp • Lines 88-161\ninline void get_HDF5_mean_sd_by_row( BigDataStatMeth::hdf5Dataset* dsA, \n                                     Eigen::MatrixXd& normalize, \n                                     bool bsd, bool bmean, \n                                     Rcpp::Nullable&lt;int&gt; wsize )\n{\n    \n    try\n    {\n        \n        hsize_t block_size = 0;\n        hsize_t* dims_out = dsA-&gt;dim();\n\n        std::vector&lt;hsize_t&gt; stride = {1, 1},\n                             block = {1, 1},\n                             offset = {0, 0},\n                             count = {0, 0};\n        \n        block_size = get_block_size(wsize, dims_out[0], dims_out[1]);\n\n        count[0] = dims_out[0];\n        if( block_size &lt; dims_out[1] ) {\n            count[1] = block_size;\n        } else{\n            count[1] = dims_out[1];\n        }\n\n        // Read data in blocks of 500 columns\n        for( hsize_t i=0; (i &lt;= floor(dims_out[1]/block_size)) || i==0 ; i++)\n        {\n            \n            // if( i&gt;0 ) {\n                \n\n            if( offset[1] + block_size &lt;= dims_out[1] ) {\n                count[1] = block_size;\n            } else {\n                count[1] = dims_out[1] - offset[1];\n            }\n            // }\n\n            std::vector&lt;double&gt; vdA( count[0] * count[1] ); \n            dsA-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdA.data() );\n            Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; X (vdA.data(), count[0], count[1] );\n\n            Eigen::RowVectorXd mean = X.colwise().mean();\n            normalize.block( 0, offset[1], 1, mean.size()) = mean;\n            \n            if(bsd){\n                Eigen::RowVectorXd sd = ((X.rowwise() - mean).array().square().colwise().sum() / (X.rows() - 1)).sqrt();\n                normalize.block( 1, offset[1], 1, sd.size()) = sd;\n            }\n            \n            \n            \n            offset[1] = offset[1] + block_size;\n\n        }\n        \n    } catch( H5::FileIException& error ) { // catch failure caused by the H5File operations\n        // error.printErrorStack();\n        checkClose_file(dsA);\n        Rf_error(\"c++ exception get_HDF5_mean_sd_by_row (File IException)\");\n    } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n        // error.printErrorStack();\n        checkClose_file(dsA);\n        Rf_error(\"c++ exception get_HDF5_mean_sd_by_row (DataSet IException)\");\n    } catch(std::exception& error) {\n        checkClose_file(dsA);\n        Rf_error(\"c++ exception get_HDF5_mean_sd_by_row function: %s\",error.what());\n    }\n    \n    return void(); \n    \n}",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_row"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#usage-example",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_row.html#usage-example",
    "title": "get_HDF5_mean_sd_by_row",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = get_HDF5_mean_sd_by_row(...);",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_row"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getVectorBlockSize.html",
    "href": "api-reference/cpp/functions/getVectorBlockSize.html",
    "title": "getVectorBlockSize",
    "section": "",
    "text": "hsize_t BigDataStatMeth::getVectorBlockSize(int maxSize)",
    "crumbs": [
      "Functions",
      "getVectorBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getVectorBlockSize.html#signature",
    "href": "api-reference/cpp/functions/getVectorBlockSize.html#signature",
    "title": "getVectorBlockSize",
    "section": "",
    "text": "hsize_t BigDataStatMeth::getVectorBlockSize(int maxSize)",
    "crumbs": [
      "Functions",
      "getVectorBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getVectorBlockSize.html#description",
    "href": "api-reference/cpp/functions/getVectorBlockSize.html#description",
    "title": "getVectorBlockSize",
    "section": "2 Description",
    "text": "2 Description\nCalculates optimal block size for vector operations.",
    "crumbs": [
      "Functions",
      "getVectorBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getVectorBlockSize.html#parameters",
    "href": "api-reference/cpp/functions/getVectorBlockSize.html#parameters",
    "title": "getVectorBlockSize",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nmaxSize (int): Maximum size of the vector",
    "crumbs": [
      "Functions",
      "getVectorBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getVectorBlockSize.html#returns",
    "href": "api-reference/cpp/functions/getVectorBlockSize.html#returns",
    "title": "getVectorBlockSize",
    "section": "4 Returns",
    "text": "4 Returns\nhsize_t Optimal block size for vector processing",
    "crumbs": [
      "Functions",
      "getVectorBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getVectorBlockSize.html#details",
    "href": "api-reference/cpp/functions/getVectorBlockSize.html#details",
    "title": "getVectorBlockSize",
    "section": "5 Details",
    "text": "5 Details\nmaxSizeMaximum size of the vector hsize_t Optimal block size for vector processing Block size optimization:Considers memory constraintsOptimizes for vector operationsEnsures efficient I/O",
    "crumbs": [
      "Functions",
      "getVectorBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getVectorBlockSize.html#source-code",
    "href": "api-reference/cpp/functions/getVectorBlockSize.html#source-code",
    "title": "getVectorBlockSize",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/Utilities.hpp • Lines 398-415\ninline hsize_t getVectorBlockSize(int maxSize) \n    {\n        hsize_t blockSize = 0;\n        \n        try\n        {\n            if( (unsigned)maxSize &gt; MAXELEMSINBLOCK) {\n                blockSize = MAXELEMSINBLOCK;\n            } else {\n                blockSize = maxSize;\n            }\n            \n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception getVectorBlockSize: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n        }\n        \n        return(blockSize);\n    }",
    "crumbs": [
      "Functions",
      "getVectorBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getVectorBlockSize.html#usage-example",
    "href": "api-reference/cpp/functions/getVectorBlockSize.html#usage-example",
    "title": "getVectorBlockSize",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getVectorBlockSize(...);",
    "crumbs": [
      "Functions",
      "getVectorBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSizetoRead.html",
    "href": "api-reference/cpp/functions/getSizetoRead.html",
    "title": "getSizetoRead",
    "section": "",
    "text": "std::vector&lt; hsize_t &gt; BigDataStatMeth::getSizetoRead(bool transp, int count, int rows, int cols)",
    "crumbs": [
      "Functions",
      "getSizetoRead"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSizetoRead.html#signature",
    "href": "api-reference/cpp/functions/getSizetoRead.html#signature",
    "title": "getSizetoRead",
    "section": "",
    "text": "std::vector&lt; hsize_t &gt; BigDataStatMeth::getSizetoRead(bool transp, int count, int rows, int cols)",
    "crumbs": [
      "Functions",
      "getSizetoRead"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSizetoRead.html#description",
    "href": "api-reference/cpp/functions/getSizetoRead.html#description",
    "title": "getSizetoRead",
    "section": "2 Description",
    "text": "2 Description\nCalculates dimensions for reading matrix blocks.",
    "crumbs": [
      "Functions",
      "getSizetoRead"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSizetoRead.html#parameters",
    "href": "api-reference/cpp/functions/getSizetoRead.html#parameters",
    "title": "getSizetoRead",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ntransp (bool): Whether to transpose dimensions\ncount (int): Number of elements to read\nrows (int): Total rows in matrix\ncols (int): Total columns in matrix",
    "crumbs": [
      "Functions",
      "getSizetoRead"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSizetoRead.html#returns",
    "href": "api-reference/cpp/functions/getSizetoRead.html#returns",
    "title": "getSizetoRead",
    "section": "4 Returns",
    "text": "4 Returns\nstd::vector Dimensions for reading",
    "crumbs": [
      "Functions",
      "getSizetoRead"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSizetoRead.html#details",
    "href": "api-reference/cpp/functions/getSizetoRead.html#details",
    "title": "getSizetoRead",
    "section": "5 Details",
    "text": "5 Details\ntranspWhether to transpose dimensions countNumber of elements to read rowsTotal rows in matrix colsTotal columns in matrix std::vector Dimensions for reading Dimension calculation:Handles transposed matricesRespects matrix boundariesOptimizes for memory usageReturns [rows_to_read, cols_to_read]",
    "crumbs": [
      "Functions",
      "getSizetoRead"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSizetoRead.html#caller-graph",
    "href": "api-reference/cpp/functions/getSizetoRead.html#caller-graph",
    "title": "getSizetoRead",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "getSizetoRead"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSizetoRead.html#source-code",
    "href": "api-reference/cpp/functions/getSizetoRead.html#source-code",
    "title": "getSizetoRead",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/Utilities.hpp • Lines 505-520\ninline std::vector&lt;hsize_t&gt; getSizetoRead(bool transp, int count, int rows, int cols)\n    {\n        std::vector&lt;hsize_t&gt; vcount = {0, 0};\n\n        if(transp == true)\n        {\n            vcount[0] = cols;\n            vcount[1] = count;\n            \n        } else {\n            vcount[0] = count;\n            vcount[1] = cols;\n        }\n        \n        return(vcount);\n    }",
    "crumbs": [
      "Functions",
      "getSizetoRead"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSizetoRead.html#usage-example",
    "href": "api-reference/cpp/functions/getSizetoRead.html#usage-example",
    "title": "getSizetoRead",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getSizetoRead(...);",
    "crumbs": [
      "Functions",
      "getSizetoRead"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimBlockSize.html",
    "href": "api-reference/cpp/functions/getOptimBlockSize.html",
    "title": "getOptimBlockSize",
    "section": "",
    "text": "size_t BigDataStatMeth::getOptimBlockSize(size_t fullSize, size_t blockSize, size_t iDesp, size_t currentSize)",
    "crumbs": [
      "Functions",
      "getOptimBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimBlockSize.html#signature",
    "href": "api-reference/cpp/functions/getOptimBlockSize.html#signature",
    "title": "getOptimBlockSize",
    "section": "",
    "text": "size_t BigDataStatMeth::getOptimBlockSize(size_t fullSize, size_t blockSize, size_t iDesp, size_t currentSize)",
    "crumbs": [
      "Functions",
      "getOptimBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimBlockSize.html#description",
    "href": "api-reference/cpp/functions/getOptimBlockSize.html#description",
    "title": "getOptimBlockSize",
    "section": "2 Description",
    "text": "2 Description\nOptimizes block size for edge cases.",
    "crumbs": [
      "Functions",
      "getOptimBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimBlockSize.html#parameters",
    "href": "api-reference/cpp/functions/getOptimBlockSize.html#parameters",
    "title": "getOptimBlockSize",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfullSize (size_t): Total size of dimension\nblockSize (size_t): Current block size\niDesp (size_t): Current displacement\ncurrentSize (size_t): Current size being processed",
    "crumbs": [
      "Functions",
      "getOptimBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimBlockSize.html#returns",
    "href": "api-reference/cpp/functions/getOptimBlockSize.html#returns",
    "title": "getOptimBlockSize",
    "section": "4 Returns",
    "text": "4 Returns\nsize_t Optimized block size",
    "crumbs": [
      "Functions",
      "getOptimBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimBlockSize.html#details",
    "href": "api-reference/cpp/functions/getOptimBlockSize.html#details",
    "title": "getOptimBlockSize",
    "section": "5 Details",
    "text": "5 Details\nfullSizeTotal size of dimension blockSizeCurrent block size iDespCurrent displacement currentSizeCurrent size being processed size_t Optimized block size Optimization strategy:Handles last block to avoid single element operationsAdjusts for matrix boundariesPrevents overflowOptimizes for performance",
    "crumbs": [
      "Functions",
      "getOptimBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimBlockSize.html#caller-graph",
    "href": "api-reference/cpp/functions/getOptimBlockSize.html#caller-graph",
    "title": "getOptimBlockSize",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "getOptimBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimBlockSize.html#source-code",
    "href": "api-reference/cpp/functions/getOptimBlockSize.html#source-code",
    "title": "getOptimBlockSize",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/Utilities.hpp • Lines 299-317\ninline size_t getOptimBlockSize( size_t fullSize, size_t blockSize, size_t iDesp, size_t currentSize ) \n    {\n        \n        try\n        {\n            if( iDesp + blockSize == fullSize - 1) {\n                currentSize = blockSize + 1;\n            } else if( iDesp + blockSize &gt; fullSize ) { \n                currentSize = fullSize - iDesp; \n            } else {\n                currentSize = blockSize;\n            }\n            \n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception getOptimBlockSize: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n        }\n        \n        return(currentSize);\n    }",
    "crumbs": [
      "Functions",
      "getOptimBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimBlockSize.html#usage-example",
    "href": "api-reference/cpp/functions/getOptimBlockSize.html#usage-example",
    "title": "getOptimBlockSize",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getOptimBlockSize(...);",
    "crumbs": [
      "Functions",
      "getOptimBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjecDataType.html",
    "href": "api-reference/cpp/functions/getObjecDataType.html",
    "title": "getObjecDataType",
    "section": "",
    "text": "std::string BigDataStatMeth::getObjecDataType(Rcpp::RObject obj)",
    "crumbs": [
      "Functions",
      "getObjecDataType"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjecDataType.html#signature",
    "href": "api-reference/cpp/functions/getObjecDataType.html#signature",
    "title": "getObjecDataType",
    "section": "",
    "text": "std::string BigDataStatMeth::getObjecDataType(Rcpp::RObject obj)",
    "crumbs": [
      "Functions",
      "getObjecDataType"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjecDataType.html#description",
    "href": "api-reference/cpp/functions/getObjecDataType.html#description",
    "title": "getObjecDataType",
    "section": "2 Description",
    "text": "2 Description\nDetermines the data type of an R object.",
    "crumbs": [
      "Functions",
      "getObjecDataType"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjecDataType.html#parameters",
    "href": "api-reference/cpp/functions/getObjecDataType.html#parameters",
    "title": "getObjecDataType",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nobj (Rcpp::RObject): R object to analyze",
    "crumbs": [
      "Functions",
      "getObjecDataType"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjecDataType.html#returns",
    "href": "api-reference/cpp/functions/getObjecDataType.html#returns",
    "title": "getObjecDataType",
    "section": "4 Returns",
    "text": "4 Returns\nstd::string Type identifier string",
    "crumbs": [
      "Functions",
      "getObjecDataType"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjecDataType.html#details",
    "href": "api-reference/cpp/functions/getObjecDataType.html#details",
    "title": "getObjecDataType",
    "section": "5 Details",
    "text": "5 Details\nobjR object to analyze std::string Type identifier string Supported types:“numeric”: Numeric vectors/matrices”int”: Integer vectors/matrices”char”: Character vectors/matrices”logic”: Logical vectors/matrices”dataframe”: Data frames”list”: Lists”S4”: S4 objects”NULL”: NULL objects”unknown”: Unrecognized types",
    "crumbs": [
      "Functions",
      "getObjecDataType"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjecDataType.html#caller-graph",
    "href": "api-reference/cpp/functions/getObjecDataType.html#caller-graph",
    "title": "getObjecDataType",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "getObjecDataType"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjecDataType.html#source-code",
    "href": "api-reference/cpp/functions/getObjecDataType.html#source-code",
    "title": "getObjecDataType",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/Utilities.hpp • Lines 129-160\ninline std::string getObjecDataType(Rcpp::RObject obj) \n    {\n        \n        std::string strtype = \"\";\n        \n        try \n        {\n            if( Rcpp::is&lt;Rcpp::NumericVector&gt;(obj) ) {\n                strtype = \"numeric\";\n            } else if( Rcpp::is&lt;Rcpp::IntegerVector&gt;(obj) ) {\n                strtype = \"int\";\n            } else if( Rcpp::is&lt;Rcpp::CharacterVector&gt;(obj) ) {\n                strtype = \"char\";\n            } else if( Rcpp::is&lt;Rcpp::LogicalVector&gt;(obj) ) {\n                strtype = \"logic\";\n            } else if( Rcpp::is&lt;Rcpp::DataFrame&gt;(obj) ) {\n                strtype = \"dataframe\";\n            } else if( Rcpp::is&lt;Rcpp::List&gt;(obj) ) {\n                strtype = \"list\";\n            } else if( obj.isS4() ) {\n                strtype = \"S4\";\n            } else if( obj.isNULL() ) {\n                strtype = \"NULL\";\n            } else {\n                strtype = \"unknown\";\n            }\n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception getObjecDataType: \"&lt;&lt; ex.what() &lt;&lt; \"\\n\";\n        }\n        \n        return(strtype);\n    }",
    "crumbs": [
      "Functions",
      "getObjecDataType"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjecDataType.html#usage-example",
    "href": "api-reference/cpp/functions/getObjecDataType.html#usage-example",
    "title": "getObjecDataType",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getObjecDataType(...);",
    "crumbs": [
      "Functions",
      "getObjecDataType"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMatrixBlockSize.html",
    "href": "api-reference/cpp/functions/getMatrixBlockSize.html",
    "title": "getMatrixBlockSize",
    "section": "",
    "text": "std::vector&lt; hsize_t &gt; BigDataStatMeth::getMatrixBlockSize(int nrows, int ncols)",
    "crumbs": [
      "Functions",
      "getMatrixBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMatrixBlockSize.html#signature",
    "href": "api-reference/cpp/functions/getMatrixBlockSize.html#signature",
    "title": "getMatrixBlockSize",
    "section": "",
    "text": "std::vector&lt; hsize_t &gt; BigDataStatMeth::getMatrixBlockSize(int nrows, int ncols)",
    "crumbs": [
      "Functions",
      "getMatrixBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMatrixBlockSize.html#description",
    "href": "api-reference/cpp/functions/getMatrixBlockSize.html#description",
    "title": "getMatrixBlockSize",
    "section": "2 Description",
    "text": "2 Description\nCalculates optimal block size for matrix operations.",
    "crumbs": [
      "Functions",
      "getMatrixBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMatrixBlockSize.html#parameters",
    "href": "api-reference/cpp/functions/getMatrixBlockSize.html#parameters",
    "title": "getMatrixBlockSize",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nnrows (int): Number of rows\nncols (int): Number of columns",
    "crumbs": [
      "Functions",
      "getMatrixBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMatrixBlockSize.html#returns",
    "href": "api-reference/cpp/functions/getMatrixBlockSize.html#returns",
    "title": "getMatrixBlockSize",
    "section": "4 Returns",
    "text": "4 Returns\nstd::vector Vector containing optimized dimensions",
    "crumbs": [
      "Functions",
      "getMatrixBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMatrixBlockSize.html#details",
    "href": "api-reference/cpp/functions/getMatrixBlockSize.html#details",
    "title": "getMatrixBlockSize",
    "section": "5 Details",
    "text": "5 Details\nnrowsNumber of rows ncolsNumber of columns std::vector Vector containing optimized dimensions Particularly useful for:Omics data with few samples and many variablesMemory-efficient processing of rectangular matricesOptimizing I/O operations",
    "crumbs": [
      "Functions",
      "getMatrixBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMatrixBlockSize.html#caller-graph",
    "href": "api-reference/cpp/functions/getMatrixBlockSize.html#caller-graph",
    "title": "getMatrixBlockSize",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "getMatrixBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMatrixBlockSize.html#source-code",
    "href": "api-reference/cpp/functions/getMatrixBlockSize.html#source-code",
    "title": "getMatrixBlockSize",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/Utilities.hpp • Lines 338-379\ninline std::vector&lt;hsize_t&gt; getMatrixBlockSize( int nrows, int ncols ) \n    {\n        size_t  maxRows = nrows,\n                maxCols = ncols;\n        \n        std::vector&lt;hsize_t&gt; blockSize = {0, 0};\n        \n        try\n        {\n            // Calculem el mínim de files\n            if( nrows &lt; ncols ) {\n                if( maxRows &lt; MAXBLOCKSIZE ){\n                    maxRows = nrows;\n                } else{\n                    maxRows = MAXBLOCKSIZE;\n                }\n                \n                maxCols = std::floor( MAXELEMSINBLOCK / maxRows );\n                if( maxCols&gt; (unsigned)ncols || maxCols + 1 == (unsigned)ncols) {\n                    maxCols = ncols;\n                }\n            } else {\n                if( maxCols &lt; MAXBLOCKSIZE ){\n                    maxCols = ncols;\n                } else{\n                    maxCols = MAXBLOCKSIZE;\n                }\n                maxRows = std::floor( MAXELEMSINBLOCK / maxCols );\n                if( maxRows&gt; (unsigned)nrows || maxRows + 1 == (unsigned)nrows) {\n                    maxRows = nrows;\n                }\n                \n            }    \n            blockSize[0] = maxRows;\n            blockSize[1] = maxCols;\n            \n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception getMatrixBlockSize: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n        }\n        \n        return(blockSize);\n    }",
    "crumbs": [
      "Functions",
      "getMatrixBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMatrixBlockSize.html#usage-example",
    "href": "api-reference/cpp/functions/getMatrixBlockSize.html#usage-example",
    "title": "getMatrixBlockSize",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getMatrixBlockSize(...);",
    "crumbs": [
      "Functions",
      "getMatrixBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getInitialPosition.html",
    "href": "api-reference/cpp/functions/getInitialPosition.html",
    "title": "getInitialPosition",
    "section": "",
    "text": "std::vector&lt; hsize_t &gt; BigDataStatMeth::getInitialPosition(bool transp, int desp)",
    "crumbs": [
      "Functions",
      "getInitialPosition"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getInitialPosition.html#signature",
    "href": "api-reference/cpp/functions/getInitialPosition.html#signature",
    "title": "getInitialPosition",
    "section": "",
    "text": "std::vector&lt; hsize_t &gt; BigDataStatMeth::getInitialPosition(bool transp, int desp)",
    "crumbs": [
      "Functions",
      "getInitialPosition"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getInitialPosition.html#description",
    "href": "api-reference/cpp/functions/getInitialPosition.html#description",
    "title": "getInitialPosition",
    "section": "2 Description",
    "text": "2 Description\nDetermines initial position for matrix operations.",
    "crumbs": [
      "Functions",
      "getInitialPosition"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getInitialPosition.html#parameters",
    "href": "api-reference/cpp/functions/getInitialPosition.html#parameters",
    "title": "getInitialPosition",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ntransp (bool): Whether to use transposed coordinates\ndesp (int): Displacement from start",
    "crumbs": [
      "Functions",
      "getInitialPosition"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getInitialPosition.html#returns",
    "href": "api-reference/cpp/functions/getInitialPosition.html#returns",
    "title": "getInitialPosition",
    "section": "4 Returns",
    "text": "4 Returns\nstd::vector Initial position coordinates",
    "crumbs": [
      "Functions",
      "getInitialPosition"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getInitialPosition.html#details",
    "href": "api-reference/cpp/functions/getInitialPosition.html#details",
    "title": "getInitialPosition",
    "section": "5 Details",
    "text": "5 Details\ntranspWhether to use transposed coordinates despDisplacement from start std::vector Initial position coordinates Position calculation:Handles both normal and transposed matricesAccounts for displacementReturns [row, col] coordinates",
    "crumbs": [
      "Functions",
      "getInitialPosition"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getInitialPosition.html#caller-graph",
    "href": "api-reference/cpp/functions/getInitialPosition.html#caller-graph",
    "title": "getInitialPosition",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "getInitialPosition"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getInitialPosition.html#source-code",
    "href": "api-reference/cpp/functions/getInitialPosition.html#source-code",
    "title": "getInitialPosition",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/Utilities.hpp • Lines 448-459\ninline std::vector&lt;hsize_t&gt; getInitialPosition(bool transp, int desp)\n    {\n        std::vector&lt;hsize_t&gt; voffset = {0, 0};\n        \n        if(transp == true) {\n            voffset[1] = desp;\n        } else {\n            voffset[0] = desp;\n        }\n        \n        return(voffset);\n    }",
    "crumbs": [
      "Functions",
      "getInitialPosition"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getInitialPosition.html#usage-example",
    "href": "api-reference/cpp/functions/getInitialPosition.html#usage-example",
    "title": "getInitialPosition",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getInitialPosition(...);",
    "crumbs": [
      "Functions",
      "getInitialPosition"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDiagonalfromMatrix.html",
    "href": "api-reference/cpp/functions/getDiagonalfromMatrix.html",
    "title": "getDiagonalfromMatrix",
    "section": "",
    "text": "Rcpp::NumericVector BigDataStatMeth::getDiagonalfromMatrix(BigDataStatMeth::hdf5Dataset *dsMat)",
    "crumbs": [
      "Functions",
      "getDiagonalfromMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDiagonalfromMatrix.html#signature",
    "href": "api-reference/cpp/functions/getDiagonalfromMatrix.html#signature",
    "title": "getDiagonalfromMatrix",
    "section": "",
    "text": "Rcpp::NumericVector BigDataStatMeth::getDiagonalfromMatrix(BigDataStatMeth::hdf5Dataset *dsMat)",
    "crumbs": [
      "Functions",
      "getDiagonalfromMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDiagonalfromMatrix.html#description",
    "href": "api-reference/cpp/functions/getDiagonalfromMatrix.html#description",
    "title": "getDiagonalfromMatrix",
    "section": "2 Description",
    "text": "2 Description\nExtracts the diagonal elements from a matrix stored in HDF5 format.",
    "crumbs": [
      "Functions",
      "getDiagonalfromMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDiagonalfromMatrix.html#parameters",
    "href": "api-reference/cpp/functions/getDiagonalfromMatrix.html#parameters",
    "title": "getDiagonalfromMatrix",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsMat (BigDataStatMeth::hdf5Dataset *): Input HDF5 dataset containing the matrix",
    "crumbs": [
      "Functions",
      "getDiagonalfromMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDiagonalfromMatrix.html#returns",
    "href": "api-reference/cpp/functions/getDiagonalfromMatrix.html#returns",
    "title": "getDiagonalfromMatrix",
    "section": "4 Returns",
    "text": "4 Returns\nRcpp::NumericVector Vector containing the diagonal elements",
    "crumbs": [
      "Functions",
      "getDiagonalfromMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDiagonalfromMatrix.html#details",
    "href": "api-reference/cpp/functions/getDiagonalfromMatrix.html#details",
    "title": "getDiagonalfromMatrix",
    "section": "5 Details",
    "text": "5 Details\ndsMatInput HDF5 dataset containing the matrix Rcpp::NumericVector Vector containing the diagonal elements Implementation details:Reads diagonal elements one at a time to minimize memory usageUses HDF5 block reading for efficient accessReturns empty vector on error",
    "crumbs": [
      "Functions",
      "getDiagonalfromMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDiagonalfromMatrix.html#call-graph",
    "href": "api-reference/cpp/functions/getDiagonalfromMatrix.html#call-graph",
    "title": "getDiagonalfromMatrix",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "getDiagonalfromMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDiagonalfromMatrix.html#source-code",
    "href": "api-reference/cpp/functions/getDiagonalfromMatrix.html#source-code",
    "title": "getDiagonalfromMatrix",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixDiagonal.hpp • Lines 46-80\ninline Rcpp::NumericVector getDiagonalfromMatrix(BigDataStatMeth::hdf5Dataset* dsMat)\n    {\n        hsize_t matrix_size = dsMat-&gt;nrows();\n        Rcpp::NumericVector diagonal(matrix_size);\n        \n        try {\n            const hsize_t DIAG_BLOCK_SIZE = 256;\n            std::vector&lt;hsize_t&gt; stride = {1, 1}, block = {1, 1};\n            \n            for (hsize_t block_start = 0; block_start &lt; matrix_size; block_start += DIAG_BLOCK_SIZE) {\n                hsize_t current_block_size = std::min(DIAG_BLOCK_SIZE, matrix_size - block_start);\n                \n                // Read square block starting from diagonal position  \n                std::vector&lt;hsize_t&gt; offset = {block_start, block_start};\n                std::vector&lt;hsize_t&gt; count = {current_block_size, current_block_size};\n                \n                std::vector&lt;double&gt; block_data(current_block_size * current_block_size);\n                dsMat-&gt;readDatasetBlock(offset, count, stride, block, block_data.data());\n                \n                // Map to Eigen for correct R/HDF5 layout handling\n                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; \n                    block_matrix(block_data.data(), current_block_size, current_block_size);\n                \n                // Use Eigen diagonal extraction - NO LOOP NEEDED\n                Eigen::Map&lt;Eigen::VectorXd&gt; diagonal_segment(REAL(diagonal) + block_start, current_block_size);\n                diagonal_segment = block_matrix.diagonal();\n            }\n            \n        } catch(std::exception& ex) {\n            Rcpp::stop (\"c++ exception getDiagonalfromMatrix: %s\", ex.what());\n            // Rcpp::Rcout &lt;&lt; \"c++ exception getDiagonalfromMatrix: \" &lt;&lt; ex.what();\n        }\n        \n        return diagonal;\n    }",
    "crumbs": [
      "Functions",
      "getDiagonalfromMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDiagonalfromMatrix.html#usage-example",
    "href": "api-reference/cpp/functions/getDiagonalfromMatrix.html#usage-example",
    "title": "getDiagonalfromMatrix",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getDiagonalfromMatrix(...);",
    "crumbs": [
      "Functions",
      "getDiagonalfromMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads.html",
    "href": "api-reference/cpp/functions/getDTthreads.html",
    "title": "getDTthreads",
    "section": "",
    "text": "int getDTthreads(const int64_t n, const bool throttle)",
    "crumbs": [
      "Functions",
      "getDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads.html#signature",
    "href": "api-reference/cpp/functions/getDTthreads.html#signature",
    "title": "getDTthreads",
    "section": "",
    "text": "int getDTthreads(const int64_t n, const bool throttle)",
    "crumbs": [
      "Functions",
      "getDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads.html#description",
    "href": "api-reference/cpp/functions/getDTthreads.html#description",
    "title": "getDTthreads",
    "section": "2 Description",
    "text": "2 Description\nDetermines number of threads for parallel operations.",
    "crumbs": [
      "Functions",
      "getDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads.html#parameters",
    "href": "api-reference/cpp/functions/getDTthreads.html#parameters",
    "title": "getDTthreads",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nn (const int64_t): Number of iterations/items to process\nthrottle (const bool): Whether to apply thread throttling",
    "crumbs": [
      "Functions",
      "getDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads.html#returns",
    "href": "api-reference/cpp/functions/getDTthreads.html#returns",
    "title": "getDTthreads",
    "section": "4 Returns",
    "text": "4 Returns\nint Number of threads to use",
    "crumbs": [
      "Functions",
      "getDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads.html#details",
    "href": "api-reference/cpp/functions/getDTthreads.html#details",
    "title": "getDTthreads",
    "section": "5 Details",
    "text": "5 Details\nnNumber of iterations/items to process throttleWhether to apply thread throttling int Number of threads to use Thread allocation strategy:With throttling: Incremental thread activation based on workloadWithout throttling: Direct allocation based on available threadsRespects global thread limits",
    "crumbs": [
      "Functions",
      "getDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads.html#call-graph",
    "href": "api-reference/cpp/functions/getDTthreads.html#call-graph",
    "title": "getDTthreads",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "getDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads.html#source-code",
    "href": "api-reference/cpp/functions/getDTthreads.html#source-code",
    "title": "getDTthreads",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/openme-utils.hpp • Lines 173-183\ninline int getDTthreads(const int64_t n, const bool throttle) {\n    \n    initDTthreads();\n    \n    // throttle==true  : a number of iterations per thread (DTthrottle) is applied before a second thread is utilized\n    // throttle==false : parallel region is already pre-chunked such as in fread; e.g. two batches intended for two threads\n    if (n&lt;1) return 1; // 0 or negative could be deliberate in calling code for edge cases where loop is not intended to run at all\n    int64_t ans = throttle ? 1+(n-1)/DTthrottle :  // 1 thread for n&lt;=1024, 2 thread for n&lt;=2048, etc\n        n;                    // don't use 20 threads for just one or two batches\n    return ans&gt;=DTthreads ? DTthreads : (int)ans;  // apply limit in static local DTthreads saved there by initDTthreads() and setDTthreads()\n}",
    "crumbs": [
      "Functions",
      "getDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads.html#usage-example",
    "href": "api-reference/cpp/functions/getDTthreads.html#usage-example",
    "title": "getDTthreads",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getDTthreads(...);",
    "crumbs": [
      "Functions",
      "getDTthreads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html",
    "title": "getBlockPositionsSizes_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::getBlockPositionsSizes_hdf5(hsize_t maxPosition, hsize_t blockSize, std::vector&lt; hsize_t &gt; &starts, std::vector&lt; hsize_t &gt; &sizes)",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#signature",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#signature",
    "title": "getBlockPositionsSizes_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::getBlockPositionsSizes_hdf5(hsize_t maxPosition, hsize_t blockSize, std::vector&lt; hsize_t &gt; &starts, std::vector&lt; hsize_t &gt; &sizes)",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#description",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#description",
    "title": "getBlockPositionsSizes_hdf5",
    "section": "2 Description",
    "text": "2 Description\nCalculate block positions and sizes for HDF5 matrix operations.",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#parameters",
    "title": "getBlockPositionsSizes_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nmaxPosition (hsize_t): Maximum position to process\nblockSize (hsize_t): Size of each block\nstarts (std::vector&lt; hsize_t &gt; &): Vector to store starting positions of blocks\nsizes (std::vector&lt; hsize_t &gt; &): Vector to store sizes of blocks",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#details",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#details",
    "title": "getBlockPositionsSizes_hdf5",
    "section": "4 Details",
    "text": "4 Details\nDetermines optimal block positions and sizes for block-based matrix operations on HDF5 datasets.",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#call-graph",
    "title": "getBlockPositionsSizes_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#source-code",
    "title": "getBlockPositionsSizes_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/multiplication 2.hpp • Lines 72-92\ninline void getBlockPositionsSizes_hdf5( hsize_t maxPosition, hsize_t blockSize, std::vector&lt;hsize_t&gt;& starts, std::vector&lt;hsize_t&gt;& sizes ){\n\n        hsize_t isize = blockSize + 1;\n\n        for (hsize_t ii = 0; ii &lt; maxPosition; ii += blockSize)\n        {\n            if( ii + blockSize &gt; maxPosition ) {\n                isize = maxPosition - ii; }\n\n            hsize_t sizetoRead = getOptimBlockSize( maxPosition, blockSize, ii, isize);\n\n            starts.push_back(ii);\n            sizes.push_back(sizetoRead);\n\n            // if( ii + blockSize &gt; maxPosition ) {\n            //     isize = blockSize + 1; }\n            if( sizetoRead &gt; blockSize ) {\n                ii = ii - blockSize + sizetoRead; }\n        }\n\n    }",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_hdf5.html#usage-example",
    "title": "getBlockPositionsSizes_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getBlockPositionsSizes_hdf5(...);",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getAvailableMemoryMB.html",
    "href": "api-reference/cpp/functions/getAvailableMemoryMB.html",
    "title": "getAvailableMemoryMB",
    "section": "",
    "text": "size_t BigDataStatMeth::getAvailableMemoryMB()",
    "crumbs": [
      "Functions",
      "getAvailableMemoryMB"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getAvailableMemoryMB.html#signature",
    "href": "api-reference/cpp/functions/getAvailableMemoryMB.html#signature",
    "title": "getAvailableMemoryMB",
    "section": "",
    "text": "size_t BigDataStatMeth::getAvailableMemoryMB()",
    "crumbs": [
      "Functions",
      "getAvailableMemoryMB"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getAvailableMemoryMB.html#description",
    "href": "api-reference/cpp/functions/getAvailableMemoryMB.html#description",
    "title": "getAvailableMemoryMB",
    "section": "2 Description",
    "text": "2 Description\nDetects available system memory using R-compatible methods.",
    "crumbs": [
      "Functions",
      "getAvailableMemoryMB"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getAvailableMemoryMB.html#returns",
    "href": "api-reference/cpp/functions/getAvailableMemoryMB.html#returns",
    "title": "getAvailableMemoryMB",
    "section": "3 Returns",
    "text": "3 Returns\nsize_t Available memory in megabytes (MB)",
    "crumbs": [
      "Functions",
      "getAvailableMemoryMB"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getAvailableMemoryMB.html#details",
    "href": "api-reference/cpp/functions/getAvailableMemoryMB.html#details",
    "title": "getAvailableMemoryMB",
    "section": "4 Details",
    "text": "4 Details\nSafely detects available memory using R’s internal functions without external dependencies. Designed for CRAN/Bioconductor compatibility across Windows, Linux, and macOS platforms.",
    "crumbs": [
      "Functions",
      "getAvailableMemoryMB"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getAvailableMemoryMB.html#caller-graph",
    "href": "api-reference/cpp/functions/getAvailableMemoryMB.html#caller-graph",
    "title": "getAvailableMemoryMB",
    "section": "5 Caller Graph",
    "text": "5 Caller Graph",
    "crumbs": [
      "Functions",
      "getAvailableMemoryMB"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getAvailableMemoryMB.html#source-code",
    "href": "api-reference/cpp/functions/getAvailableMemoryMB.html#source-code",
    "title": "getAvailableMemoryMB",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/system-utils.hpp • Lines 37-68\ninline size_t getAvailableMemoryMB() {\n    try {\n\n                \n        // Use R's internal memory functions (CRAN-safe)\n        // // Use R's internal memory functions (CRAN-safe)\n        // SEXP memCall = PROTECT(Rf_lang1(Rf_install(\"memory.size\")));\n        // SEXP memResult = PROTECT(Rf_eval(memCall, R_GlobalEnv));\n        // \n        // if (Rf_isReal(memResult) && Rf_length(memResult) &gt; 0) {\n        //     double memMB = REAL(memResult)[0];\n        //     UNPROTECT(2);\n        //     return static_cast&lt;size_t&gt;(memMB * 0.6); // Use 60% of available\n        // }\n        // UNPROTECT(2);\n        \n        Rcpp::Function memSize(\"memory.size\");\n        Rcpp::NumericVector memResult = memSize();\n        \n        if (memResult.size() &gt; 0) {\n            double memMB = memResult[0];\n            return static_cast&lt;size_t&gt;(memMB * 0.6); // Use 60% of available\n        }\n        \n\n    } catch(...) {\n        // Fallback silently - no error throwing for robustness\n    }\n    \n    // Conservative fallback for any system (safe minimum)\n    return 4000; // Assume 4GB available memory\n}",
    "crumbs": [
      "Functions",
      "getAvailableMemoryMB"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getAvailableMemoryMB.html#usage-example",
    "href": "api-reference/cpp/functions/getAvailableMemoryMB.html#usage-example",
    "title": "getAvailableMemoryMB",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getAvailableMemoryMB(...);",
    "crumbs": [
      "Functions",
      "getAvailableMemoryMB"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/extractDiagonalToVector.html",
    "href": "api-reference/cpp/functions/extractDiagonalToVector.html",
    "title": "extractDiagonalToVector",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::extractDiagonalToVector(BigDataStatMeth::hdf5Dataset *dsMatrix, BigDataStatMeth::hdf5Dataset *dsVector)",
    "crumbs": [
      "Functions",
      "extractDiagonalToVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/extractDiagonalToVector.html#signature",
    "href": "api-reference/cpp/functions/extractDiagonalToVector.html#signature",
    "title": "extractDiagonalToVector",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::extractDiagonalToVector(BigDataStatMeth::hdf5Dataset *dsMatrix, BigDataStatMeth::hdf5Dataset *dsVector)",
    "crumbs": [
      "Functions",
      "extractDiagonalToVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/extractDiagonalToVector.html#description",
    "href": "api-reference/cpp/functions/extractDiagonalToVector.html#description",
    "title": "extractDiagonalToVector",
    "section": "2 Description",
    "text": "2 Description\nExtract diagonal from matrix and save as vector dataset.",
    "crumbs": [
      "Functions",
      "extractDiagonalToVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/extractDiagonalToVector.html#parameters",
    "href": "api-reference/cpp/functions/extractDiagonalToVector.html#parameters",
    "title": "extractDiagonalToVector",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsMatrix (BigDataStatMeth::hdf5Dataset *): Input matrix dataset (must be square N×N)\ndsVector (BigDataStatMeth::hdf5Dataset *): Output vector dataset (will be created as 1×N)",
    "crumbs": [
      "Functions",
      "extractDiagonalToVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/extractDiagonalToVector.html#details",
    "href": "api-reference/cpp/functions/extractDiagonalToVector.html#details",
    "title": "extractDiagonalToVector",
    "section": "4 Details",
    "text": "4 Details\nExtracts diagonal elements from a matrix and creates a new vector dataset. Uses existing getDiagonalfromMatrix() function for optimized diagonal reading. The result vector is stored in 1×N format for compatibility with vector operations.",
    "crumbs": [
      "Functions",
      "extractDiagonalToVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/extractDiagonalToVector.html#call-graph",
    "href": "api-reference/cpp/functions/extractDiagonalToVector.html#call-graph",
    "title": "extractDiagonalToVector",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "extractDiagonalToVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/extractDiagonalToVector.html#source-code",
    "href": "api-reference/cpp/functions/extractDiagonalToVector.html#source-code",
    "title": "extractDiagonalToVector",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5DiagonalMethods.hpp • Lines 139-164\ninline void extractDiagonalToVector(BigDataStatMeth::hdf5Dataset* dsMatrix, \n                                            BigDataStatMeth::hdf5Dataset* dsVector)\n        {\n            try {\n                if (dsMatrix-&gt;nrows() != dsMatrix-&gt;ncols()) {\n                    Rf_error(\"extractDiagonalToVector: Matrix must be square\");\n                    return;\n                }\n                \n                hsize_t matrix_size = dsMatrix-&gt;nrows();\n                \n                // Extract diagonal using existing function\n                Rcpp::NumericVector diagonal = getDiagonalfromMatrix(dsMatrix);\n                \n                // Create vector dataset (1×N format)\n                dsVector-&gt;createDataset(matrix_size, 1, \"numeric\");\n                \n                // Write diagonal data\n                std::vector&lt;double&gt; diag_vector = Rcpp::as&lt;std::vector&lt;double&gt;&gt;(diagonal);\n                std::vector&lt;hsize_t&gt; stride = {1, 1}, block = {1, 1};\n                dsVector-&gt;writeDatasetBlock(diag_vector, {0, 0}, {1, matrix_size}, stride, block);\n                \n            } catch(std::exception& ex) {\n                Rf_error(\"Error in extractDiagonalToVector: %s\", ex.what());\n            }\n        }",
    "crumbs": [
      "Functions",
      "extractDiagonalToVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/extractDiagonalToVector.html#usage-example",
    "href": "api-reference/cpp/functions/extractDiagonalToVector.html#usage-example",
    "title": "extractDiagonalToVector",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = extractDiagonalToVector(...);",
    "crumbs": [
      "Functions",
      "extractDiagonalToVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/estimateOptimalBlockSize.html",
    "href": "api-reference/cpp/functions/estimateOptimalBlockSize.html",
    "title": "estimateOptimalBlockSize",
    "section": "",
    "text": "hsize_t BigDataStatMeth::estimateOptimalBlockSize(hsize_t matrix_size, double available_memory_mb=100.0)",
    "crumbs": [
      "Functions",
      "estimateOptimalBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/estimateOptimalBlockSize.html#signature",
    "href": "api-reference/cpp/functions/estimateOptimalBlockSize.html#signature",
    "title": "estimateOptimalBlockSize",
    "section": "",
    "text": "hsize_t BigDataStatMeth::estimateOptimalBlockSize(hsize_t matrix_size, double available_memory_mb=100.0)",
    "crumbs": [
      "Functions",
      "estimateOptimalBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/estimateOptimalBlockSize.html#description",
    "href": "api-reference/cpp/functions/estimateOptimalBlockSize.html#description",
    "title": "estimateOptimalBlockSize",
    "section": "2 Description",
    "text": "2 Description\nUtility function to estimate optimal block size for diagonal matrix operations.",
    "crumbs": [
      "Functions",
      "estimateOptimalBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/estimateOptimalBlockSize.html#parameters",
    "href": "api-reference/cpp/functions/estimateOptimalBlockSize.html#parameters",
    "title": "estimateOptimalBlockSize",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nmatrix_size (hsize_t): Size of the matrix\navailable_memory_mb (double): Available memory per thread in MB (default: 100MB)",
    "crumbs": [
      "Functions",
      "estimateOptimalBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/estimateOptimalBlockSize.html#returns",
    "href": "api-reference/cpp/functions/estimateOptimalBlockSize.html#returns",
    "title": "estimateOptimalBlockSize",
    "section": "4 Returns",
    "text": "4 Returns\nOptimal block size for diagonal processing",
    "crumbs": [
      "Functions",
      "estimateOptimalBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/estimateOptimalBlockSize.html#details",
    "href": "api-reference/cpp/functions/estimateOptimalBlockSize.html#details",
    "title": "estimateOptimalBlockSize",
    "section": "5 Details",
    "text": "5 Details\nCalculates optimal block size based on available memory and matrix size. This function is specifically optimized for diagonal-only processing where only diagonal blocks are read/written, not the entire matrix.",
    "crumbs": [
      "Functions",
      "estimateOptimalBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/estimateOptimalBlockSize.html#source-code",
    "href": "api-reference/cpp/functions/estimateOptimalBlockSize.html#source-code",
    "title": "estimateOptimalBlockSize",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5Diagonal.hpp • Lines 851-863\ninline hsize_t estimateOptimalBlockSize(hsize_t matrix_size, double available_memory_mb = 100.0) {\n        // Each diagonal block uses block_size^2 * 8 bytes (double precision)\n        // Keep blocks under specified memory limit per thread\n        double bytes_per_mb = 1024.0 * 1024.0;\n        double max_elements = (available_memory_mb * bytes_per_mb) / sizeof(double);\n        hsize_t max_block_size = static_cast&lt;hsize_t&gt;(std::sqrt(max_elements));\n        \n        // Ensure block size is reasonable for diagonal processing\n        hsize_t min_block = 64;   // Minimum for efficient I/O\n        hsize_t max_block = std::min(static_cast&lt;hsize_t&gt;(4096), matrix_size); // Maximum practical size\n        \n        return std::max(min_block, std::min(max_block_size, max_block));\n    }",
    "crumbs": [
      "Functions",
      "estimateOptimalBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/estimateOptimalBlockSize.html#usage-example",
    "href": "api-reference/cpp/functions/estimateOptimalBlockSize.html#usage-example",
    "title": "estimateOptimalBlockSize",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = estimateOptimalBlockSize(...);",
    "crumbs": [
      "Functions",
      "estimateOptimalBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dscal_.html",
    "href": "api-reference/cpp/functions/dscal_.html",
    "title": "dscal_",
    "section": "",
    "text": "void BigDataStatMeth::dscal_(int *, double *, double *, int *)",
    "crumbs": [
      "Functions",
      "dscal_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dscal_.html#signature",
    "href": "api-reference/cpp/functions/dscal_.html#signature",
    "title": "dscal_",
    "section": "",
    "text": "void BigDataStatMeth::dscal_(int *, double *, double *, int *)",
    "crumbs": [
      "Functions",
      "dscal_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dscal_.html#description",
    "href": "api-reference/cpp/functions/dscal_.html#description",
    "title": "dscal_",
    "section": "2 Description",
    "text": "2 Description\nLAPACK DSCAL vector scaling.",
    "crumbs": [
      "Functions",
      "dscal_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dscal_.html#details",
    "href": "api-reference/cpp/functions/dscal_.html#details",
    "title": "dscal_",
    "section": "3 Details",
    "text": "3 Details\nExternal LAPACK function for scaling a vector by a constant",
    "crumbs": [
      "Functions",
      "dscal_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dscal_.html#call-graph",
    "href": "api-reference/cpp/functions/dscal_.html#call-graph",
    "title": "dscal_",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "dscal_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dscal_.html#usage-example",
    "href": "api-reference/cpp/functions/dscal_.html#usage-example",
    "title": "dscal_",
    "section": "5 Usage Example",
    "text": "5 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = dscal_(...);",
    "crumbs": [
      "Functions",
      "dscal_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesvd_.html",
    "href": "api-reference/cpp/functions/dgesvd_.html",
    "title": "dgesvd_",
    "section": "",
    "text": "void BigDataStatMeth::dgesvd_(char *, char *, int *, int *, double *, int *, double *, double *, int *, double *, int *, double *, int *, int *)",
    "crumbs": [
      "Functions",
      "dgesvd_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesvd_.html#signature",
    "href": "api-reference/cpp/functions/dgesvd_.html#signature",
    "title": "dgesvd_",
    "section": "",
    "text": "void BigDataStatMeth::dgesvd_(char *, char *, int *, int *, double *, int *, double *, double *, int *, double *, int *, double *, int *, int *)",
    "crumbs": [
      "Functions",
      "dgesvd_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesvd_.html#description",
    "href": "api-reference/cpp/functions/dgesvd_.html#description",
    "title": "dgesvd_",
    "section": "2 Description",
    "text": "2 Description\nLAPACK DGESVD singular value decomposition.",
    "crumbs": [
      "Functions",
      "dgesvd_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesvd_.html#details",
    "href": "api-reference/cpp/functions/dgesvd_.html#details",
    "title": "dgesvd_",
    "section": "3 Details",
    "text": "3 Details\nExternal LAPACK function for computing the singular value decomposition of a real M-by-N matrix A",
    "crumbs": [
      "Functions",
      "dgesvd_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesvd_.html#call-graph",
    "href": "api-reference/cpp/functions/dgesvd_.html#call-graph",
    "title": "dgesvd_",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "dgesvd_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesvd_.html#usage-example",
    "href": "api-reference/cpp/functions/dgesvd_.html#usage-example",
    "title": "dgesvd_",
    "section": "5 Usage Example",
    "text": "5 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = dgesvd_(...);",
    "crumbs": [
      "Functions",
      "dgesvd_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesdd_.html",
    "href": "api-reference/cpp/functions/dgesdd_.html",
    "title": "dgesdd_",
    "section": "",
    "text": "void BigDataStatMeth::dgesdd_(char *, int *, int *, double *, int *, double *, double *, int *, double *, int *, double *, int *, int *, int *)",
    "crumbs": [
      "Functions",
      "dgesdd_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesdd_.html#signature",
    "href": "api-reference/cpp/functions/dgesdd_.html#signature",
    "title": "dgesdd_",
    "section": "",
    "text": "void BigDataStatMeth::dgesdd_(char *, int *, int *, double *, int *, double *, double *, int *, double *, int *, double *, int *, int *, int *)",
    "crumbs": [
      "Functions",
      "dgesdd_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesdd_.html#call-graph",
    "href": "api-reference/cpp/functions/dgesdd_.html#call-graph",
    "title": "dgesdd_",
    "section": "2 Call Graph",
    "text": "2 Call Graph",
    "crumbs": [
      "Functions",
      "dgesdd_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesdd_.html#usage-example",
    "href": "api-reference/cpp/functions/dgesdd_.html#usage-example",
    "title": "dgesdd_",
    "section": "3 Usage Example",
    "text": "3 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = dgesdd_(...);",
    "crumbs": [
      "Functions",
      "dgesdd_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cumsum.html",
    "href": "api-reference/cpp/functions/cumsum.html",
    "title": "cumsum",
    "section": "",
    "text": "Eigen::VectorXd BigDataStatMeth::cumsum(Eigen::VectorXd x)",
    "crumbs": [
      "Functions",
      "cumsum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cumsum.html#signature",
    "href": "api-reference/cpp/functions/cumsum.html#signature",
    "title": "cumsum",
    "section": "",
    "text": "Eigen::VectorXd BigDataStatMeth::cumsum(Eigen::VectorXd x)",
    "crumbs": [
      "Functions",
      "cumsum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cumsum.html#description",
    "href": "api-reference/cpp/functions/cumsum.html#description",
    "title": "cumsum",
    "section": "2 Description",
    "text": "2 Description\nCompute cumulative sum of a vector.",
    "crumbs": [
      "Functions",
      "cumsum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cumsum.html#parameters",
    "href": "api-reference/cpp/functions/cumsum.html#parameters",
    "title": "cumsum",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nx (Eigen::VectorXd): Input vector",
    "crumbs": [
      "Functions",
      "cumsum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cumsum.html#returns",
    "href": "api-reference/cpp/functions/cumsum.html#returns",
    "title": "cumsum",
    "section": "4 Returns",
    "text": "4 Returns\nVector containing cumulative sums",
    "crumbs": [
      "Functions",
      "cumsum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cumsum.html#details",
    "href": "api-reference/cpp/functions/cumsum.html#details",
    "title": "cumsum",
    "section": "5 Details",
    "text": "5 Details\nCalculates the cumulative sum (running sum) of elements in a vector. For a vector [a, b, c], returns [a, a+b, a+b+c].",
    "crumbs": [
      "Functions",
      "cumsum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cumsum.html#caller-graph",
    "href": "api-reference/cpp/functions/cumsum.html#caller-graph",
    "title": "cumsum",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "cumsum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cumsum.html#source-code",
    "href": "api-reference/cpp/functions/cumsum.html#source-code",
    "title": "cumsum",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memOtherFunctions.hpp • Lines 55-66\ninline Eigen::VectorXd cumsum(Eigen::VectorXd x)\n    {\n        // initialize an accumulator variable\n        double acc = 0;\n        // initialize the result vector\n        Eigen::VectorXd res = Eigen::VectorXd::Zero(x.size());\n        for(int i = 0; i &lt; x.size(); i++){\n            acc += x[i];\n            res[i] = acc;\n        }\n        return res;\n}",
    "crumbs": [
      "Functions",
      "cumsum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cumsum.html#usage-example",
    "href": "api-reference/cpp/functions/cumsum.html#usage-example",
    "title": "cumsum",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = cumsum(...);",
    "crumbs": [
      "Functions",
      "cumsum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/createHardLink.html",
    "href": "api-reference/cpp/functions/createHardLink.html",
    "title": "createHardLink",
    "section": "",
    "text": "void BigDataStatMeth::createHardLink(H5::H5File *file, std::string original, std::string link)",
    "crumbs": [
      "Functions",
      "createHardLink"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/createHardLink.html#signature",
    "href": "api-reference/cpp/functions/createHardLink.html#signature",
    "title": "createHardLink",
    "section": "",
    "text": "void BigDataStatMeth::createHardLink(H5::H5File *file, std::string original, std::string link)",
    "crumbs": [
      "Functions",
      "createHardLink"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/createHardLink.html#description",
    "href": "api-reference/cpp/functions/createHardLink.html#description",
    "title": "createHardLink",
    "section": "2 Description",
    "text": "2 Description\nCreates a hard link in an HDF5 file.",
    "crumbs": [
      "Functions",
      "createHardLink"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/createHardLink.html#parameters",
    "href": "api-reference/cpp/functions/createHardLink.html#parameters",
    "title": "createHardLink",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfile (H5::H5File *): Pointer to HDF5 file\noriginal (std::string): Path to original element\nlink (std::string): Path for new link",
    "crumbs": [
      "Functions",
      "createHardLink"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/createHardLink.html#details",
    "href": "api-reference/cpp/functions/createHardLink.html#details",
    "title": "createHardLink",
    "section": "4 Details",
    "text": "4 Details\nfilePointer to HDF5 file originalPath to original element linkPath for new linkH5::FileIExceptionon file operation errors H5::DataSetIExceptionon dataset operation errors H5::GroupIExceptionon group operation errors std::exceptionon other errorsThis function is not fully tested",
    "crumbs": [
      "Functions",
      "createHardLink"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/createHardLink.html#source-code",
    "href": "api-reference/cpp/functions/createHardLink.html#source-code",
    "title": "createHardLink",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5Utilities.hpp • Lines 241-276\ninline void createHardLink( H5::H5File* file, std::string original, std::string link)\n    {\n        \n        try{\n            \n            H5::Exception::dontPrint();\n            \n            const char * charOriginal = original.c_str();\n            const char * charLink = link.c_str();\n            \n            herr_t status = H5Lcreate_hard(file-&gt;getId(), charOriginal, file-&gt;getId(), charLink, H5P_DEFAULT, H5P_DEFAULT);\n            \n            if(status&lt;0) {\n                Rcpp::Rcerr&lt;&lt;\"c++ exception createHardLink (create_hard IException)\" &lt;&lt; std::endl;\n                return void();\n            }\n            \n        } catch(H5::FileIException& error) { \n            Rcpp::Rcerr&lt;&lt;\"c++ exception createHardLink (File IException)\" &lt;&lt; std::endl;\n            return void();\n        } catch(H5::DataSetIException& error) { \n            Rcpp::Rcerr&lt;&lt;\"c++ exception createHardLink (DataSet IException)\" &lt;&lt; std::endl;\n            return void();\n        } catch(H5::GroupIException& error) { \n            Rcpp::Rcerr&lt;&lt;\"c++ exception createHardLink (Group IException)\" &lt;&lt; std::endl;\n            return void();\n        } catch(std::exception &ex) {\n            Rcpp::Rcerr &lt;&lt; \"c++ exception createHardLink: \" &lt;&lt; ex.what();\n            return void();\n        }  catch (...) {\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception createHardLink (unknown reason)\";\n            return void();\n        }\n        \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "createHardLink"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/createHardLink.html#usage-example",
    "href": "api-reference/cpp/functions/createHardLink.html#usage-example",
    "title": "createHardLink",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = createHardLink(...);",
    "crumbs": [
      "Functions",
      "createHardLink"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/compute_pvalues_optimized.html",
    "href": "api-reference/cpp/functions/compute_pvalues_optimized.html",
    "title": "compute_pvalues_optimized",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::compute_pvalues_optimized(const Eigen::MatrixXd &correlation_matrix, int n_obs, bool symmetric=true)",
    "crumbs": [
      "Functions",
      "compute_pvalues_optimized"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/compute_pvalues_optimized.html#signature",
    "href": "api-reference/cpp/functions/compute_pvalues_optimized.html#signature",
    "title": "compute_pvalues_optimized",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::compute_pvalues_optimized(const Eigen::MatrixXd &correlation_matrix, int n_obs, bool symmetric=true)",
    "crumbs": [
      "Functions",
      "compute_pvalues_optimized"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/compute_pvalues_optimized.html#description",
    "href": "api-reference/cpp/functions/compute_pvalues_optimized.html#description",
    "title": "compute_pvalues_optimized",
    "section": "2 Description",
    "text": "2 Description\nOptimized p-value computation for correlation matrices using Eigen.",
    "crumbs": [
      "Functions",
      "compute_pvalues_optimized"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/compute_pvalues_optimized.html#parameters",
    "href": "api-reference/cpp/functions/compute_pvalues_optimized.html#parameters",
    "title": "compute_pvalues_optimized",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ncorrelation_matrix (const Eigen::MatrixXd &)\nn_obs (int)\nsymmetric (bool)",
    "crumbs": [
      "Functions",
      "compute_pvalues_optimized"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/compute_pvalues_optimized.html#returns",
    "href": "api-reference/cpp/functions/compute_pvalues_optimized.html#returns",
    "title": "compute_pvalues_optimized",
    "section": "4 Returns",
    "text": "4 Returns\nType: Eigen::MatrixXd",
    "crumbs": [
      "Functions",
      "compute_pvalues_optimized"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/compute_pvalues_optimized.html#details",
    "href": "api-reference/cpp/functions/compute_pvalues_optimized.html#details",
    "title": "compute_pvalues_optimized",
    "section": "5 Details",
    "text": "5 Details\nUses vectorized Eigen operations for maximum performance:cwiseAbs2() for r² computationcwiseSqrt() for square root operations cwiseQuotient() for element-wise divisionunaryExpr() for applying t-distribution CDF",
    "crumbs": [
      "Functions",
      "compute_pvalues_optimized"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/compute_pvalues_optimized.html#call-graph",
    "href": "api-reference/cpp/functions/compute_pvalues_optimized.html#call-graph",
    "title": "compute_pvalues_optimized",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "compute_pvalues_optimized"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/compute_pvalues_optimized.html#source-code",
    "href": "api-reference/cpp/functions/compute_pvalues_optimized.html#source-code",
    "title": "compute_pvalues_optimized",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixCorrelation.hpp • Lines 384-433\ninline Eigen::MatrixXd compute_pvalues_optimized(const Eigen::MatrixXd& correlation_matrix, \n                                                     int n_obs, \n                                                     bool symmetric = true) {\n        \n        if (n_obs &lt; 3) {\n            return Eigen::MatrixXd::Constant(correlation_matrix.rows(), correlation_matrix.cols(), \n                                             std::numeric_limits&lt;double&gt;::quiet_NaN());\n        }\n        \n        double df = n_obs - 2;\n        double sqrt_df = std::sqrt(df);\n        \n        // Vectorized computation using Eigen operations\n        // Step 1: Compute r² for entire matrix\n        Eigen::MatrixXd r_squared = correlation_matrix.cwiseAbs2();\n        \n        // Step 2: Compute 1 - r² and take square root\n        Eigen::MatrixXd one_minus_r_squared = \n            (Eigen::MatrixXd::Ones(r_squared.rows(), r_squared.cols()) - r_squared);\n        Eigen::MatrixXd denominator = one_minus_r_squared.cwiseSqrt();\n        \n        // Step 3: Compute t-statistics vectorized: t = |r| * sqrt(df) / sqrt(1 - r²)\n        Eigen::MatrixXd abs_correlations = correlation_matrix.cwiseAbs();\n        Eigen::MatrixXd t_stats = (abs_correlations * sqrt_df).cwiseQuotient(denominator);\n        \n        // Step 4: Handle diagonal for symmetric matrices\n        if (symmetric) {\n            t_stats.diagonal().setConstant(std::numeric_limits&lt;double&gt;::quiet_NaN());\n        }\n        \n        // Step 5: Apply t-distribution CDF and compute p-values using unaryExpr\n        Eigen::MatrixXd pvalues = t_stats.unaryExpr([df](double t_val) -&gt; double {\n            if (std::isnan(t_val) || std::isinf(t_val)) {\n                return std::numeric_limits&lt;double&gt;::quiet_NaN();\n            }\n            \n            double cdf_value = t_distribution_cdf(t_val, df);\n            double p_value = 2.0 * (1.0 - cdf_value);\n            return std::max(0.0, std::min(1.0, p_value));\n        });\n        \n        // Step 6: Ensure symmetry and set diagonal to 1.0 for symmetric case\n        if (symmetric) {\n            // Use Eigen operations for symmetry\n            pvalues = (pvalues + pvalues.transpose()) * 0.5;\n            pvalues.diagonal().setOnes();\n        }\n        \n        return pvalues;\n    }",
    "crumbs": [
      "Functions",
      "compute_pvalues_optimized"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/compute_pvalues_optimized.html#usage-example",
    "href": "api-reference/cpp/functions/compute_pvalues_optimized.html#usage-example",
    "title": "compute_pvalues_optimized",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = compute_pvalues_optimized(...);",
    "crumbs": [
      "Functions",
      "compute_pvalues_optimized"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/classify_matrix_type.html",
    "href": "api-reference/cpp/functions/classify_matrix_type.html",
    "title": "classify_matrix_type",
    "section": "",
    "text": "MatrixType BigDataStatMeth::classify_matrix_type(hsize_t N, hsize_t M, hsize_t K)",
    "crumbs": [
      "Functions",
      "classify_matrix_type"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/classify_matrix_type.html#signature",
    "href": "api-reference/cpp/functions/classify_matrix_type.html#signature",
    "title": "classify_matrix_type",
    "section": "",
    "text": "MatrixType BigDataStatMeth::classify_matrix_type(hsize_t N, hsize_t M, hsize_t K)",
    "crumbs": [
      "Functions",
      "classify_matrix_type"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/classify_matrix_type.html#description",
    "href": "api-reference/cpp/functions/classify_matrix_type.html#description",
    "title": "classify_matrix_type",
    "section": "2 Description",
    "text": "2 Description\nClassify matrix type based on dimensional characteristics.",
    "crumbs": [
      "Functions",
      "classify_matrix_type"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/classify_matrix_type.html#parameters",
    "href": "api-reference/cpp/functions/classify_matrix_type.html#parameters",
    "title": "classify_matrix_type",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nN (hsize_t): Number of rows in result matrix\nM (hsize_t): Number of columns in result matrix\nK (hsize_t): Inner dimension for matrix multiplication",
    "crumbs": [
      "Functions",
      "classify_matrix_type"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/classify_matrix_type.html#returns",
    "href": "api-reference/cpp/functions/classify_matrix_type.html#returns",
    "title": "classify_matrix_type",
    "section": "4 Returns",
    "text": "4 Returns\nMatrixType Classification result for optimization strategy selection",
    "crumbs": [
      "Functions",
      "classify_matrix_type"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/classify_matrix_type.html#details",
    "href": "api-reference/cpp/functions/classify_matrix_type.html#details",
    "title": "classify_matrix_type",
    "section": "5 Details",
    "text": "5 Details\nAnalyzes matrix dimensions to determine the optimal computational strategy. Classification considers both absolute size and aspect ratios to identify rectangular vs. square-like matrices and their relative sizes.",
    "crumbs": [
      "Functions",
      "classify_matrix_type"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/classify_matrix_type.html#caller-graph",
    "href": "api-reference/cpp/functions/classify_matrix_type.html#caller-graph",
    "title": "classify_matrix_type",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "classify_matrix_type"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/classify_matrix_type.html#source-code",
    "href": "api-reference/cpp/functions/classify_matrix_type.html#source-code",
    "title": "classify_matrix_type",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/Utilities.hpp • Lines 580-596\ninline MatrixType classify_matrix_type(hsize_t N, hsize_t M, hsize_t K) {\n        hsize_t min_dim = std::min({N, M, K});\n        hsize_t max_dim = std::max({N, M, K});\n        double ratio = (double)max_dim / min_dim;\n        \n        if (min_dim &lt; 1000 && ratio &gt; 100) {\n            return RECTANGULAR_EXTREME;\n        } else if (max_dim &lt; 10000 && ratio &lt; 5) {\n            return SQUARE_SMALL;\n        } else if (max_dim &lt; 100000 && ratio &lt; 5) {\n            return SQUARE_LARGE;  \n        } else if (ratio &lt; 5) {\n            return SQUARE_EXTREME;\n        } else {\n            return RECTANGULAR_EXTREME;  // Default for other rectangular cases\n        }\n    }",
    "crumbs": [
      "Functions",
      "classify_matrix_type"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/classify_matrix_type.html#usage-example",
    "href": "api-reference/cpp/functions/classify_matrix_type.html#usage-example",
    "title": "classify_matrix_type",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = classify_matrix_type(...);",
    "crumbs": [
      "Functions",
      "classify_matrix_type"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calculate_multiplication_blocks.html",
    "href": "api-reference/cpp/functions/calculate_multiplication_blocks.html",
    "title": "calculate_multiplication_blocks",
    "section": "",
    "text": "BlockSizes BigDataStatMeth::calculate_multiplication_blocks(hsize_t N, hsize_t M, hsize_t K)",
    "crumbs": [
      "Functions",
      "calculate_multiplication_blocks"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calculate_multiplication_blocks.html#signature",
    "href": "api-reference/cpp/functions/calculate_multiplication_blocks.html#signature",
    "title": "calculate_multiplication_blocks",
    "section": "",
    "text": "BlockSizes BigDataStatMeth::calculate_multiplication_blocks(hsize_t N, hsize_t M, hsize_t K)",
    "crumbs": [
      "Functions",
      "calculate_multiplication_blocks"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calculate_multiplication_blocks.html#description",
    "href": "api-reference/cpp/functions/calculate_multiplication_blocks.html#description",
    "title": "calculate_multiplication_blocks",
    "section": "2 Description",
    "text": "2 Description\nCalculate optimal block sizes specifically for multiplication.",
    "crumbs": [
      "Functions",
      "calculate_multiplication_blocks"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calculate_multiplication_blocks.html#parameters",
    "href": "api-reference/cpp/functions/calculate_multiplication_blocks.html#parameters",
    "title": "calculate_multiplication_blocks",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nN (hsize_t): Number of result rows (dsA-&gt;ncols)\nM (hsize_t): Number of result cols (dsB-&gt;nrows)\nK (hsize_t): Inner dimension (dsA-&gt;nrows = dsB-&gt;ncols)",
    "crumbs": [
      "Functions",
      "calculate_multiplication_blocks"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calculate_multiplication_blocks.html#returns",
    "href": "api-reference/cpp/functions/calculate_multiplication_blocks.html#returns",
    "title": "calculate_multiplication_blocks",
    "section": "4 Returns",
    "text": "4 Returns\nType: BlockSizes",
    "crumbs": [
      "Functions",
      "calculate_multiplication_blocks"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calculate_multiplication_blocks.html#details",
    "href": "api-reference/cpp/functions/calculate_multiplication_blocks.html#details",
    "title": "calculate_multiplication_blocks",
    "section": "5 Details",
    "text": "5 Details\nNNumber of result rows (dsA-&gt;ncols) MNumber of result cols (dsB-&gt;nrows) KInner dimension (dsA-&gt;nrows = dsB-&gt;ncols)",
    "crumbs": [
      "Functions",
      "calculate_multiplication_blocks"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calculate_multiplication_blocks.html#call-graph",
    "href": "api-reference/cpp/functions/calculate_multiplication_blocks.html#call-graph",
    "title": "calculate_multiplication_blocks",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "calculate_multiplication_blocks"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calculate_multiplication_blocks.html#source-code",
    "href": "api-reference/cpp/functions/calculate_multiplication_blocks.html#source-code",
    "title": "calculate_multiplication_blocks",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/multiplication.hpp • Lines 105-161\ninline BlockSizes calculate_multiplication_blocks(hsize_t N, hsize_t M, hsize_t K) {\n        const hsize_t MEMORY_BUDGET = (hsize_t)(1.5 * 1024 * 1024 * 1024);  // 1.5GB\n        const hsize_t bytes_per_element = sizeof(double);\n        const hsize_t min_block = 256;\n        \n        BlockSizes result;\n        result.type = classify_matrix_type(N, M, K);\n        \n        switch (result.type) {\n        case RECTANGULAR_EXTREME: {\n            // Para big-omics: K &gt;&gt; N,M - priorizar bloques grandes en K\n            hsize_t small_dim = std::min(N, M);\n            result.output_block = std::min(small_dim, (hsize_t)1024);\n            \n            // Maximizar bloque K con memoria restante\n            // Memory = A_block + B_block = K×(N+M) aprox\n            hsize_t remaining_budget = (hsize_t)(0.8 * MEMORY_BUDGET);\n            result.inner_block = remaining_budget / ((N + M) * bytes_per_element);\n            result.inner_block = std::min({result.inner_block, K, (hsize_t)32768});\n            break;\n        }\n            \n        case SQUARE_SMALL: {\n            hsize_t block = sqrt(MEMORY_BUDGET / (3 * bytes_per_element));\n            block = std::min({block, N/2, M/2, K/2});\n            result.inner_block = result.output_block = block;\n            break;\n        }\n            \n        case SQUARE_LARGE: {\n            hsize_t target_blocks = 2;  // 2×2 output blocks\n            result.output_block = std::max({N / target_blocks, M / target_blocks, min_block});\n            \n            hsize_t output_memory = result.output_block * result.output_block * bytes_per_element;\n            hsize_t remaining_budget = MEMORY_BUDGET - output_memory;\n            result.inner_block = remaining_budget / (2 * result.output_block * bytes_per_element);\n            result.inner_block = std::min({result.inner_block, K, (hsize_t)16384});\n            break;\n        }\n            \n        case SQUARE_EXTREME: {\n            hsize_t block = sqrt(MEMORY_BUDGET / (3 * bytes_per_element));\n            block = std::min({block, (hsize_t)8192});\n            result.inner_block = result.output_block = block;\n            break;\n        }\n        }\n        \n        // Aplicar límites y redondear\n        result.inner_block = std::max(result.inner_block, min_block);\n        result.output_block = std::max(result.output_block, min_block);\n        \n        result.inner_block = ((result.inner_block + 63) / 64) * 64;\n        result.output_block = ((result.output_block + 63) / 64) * 64;\n        \n        return result;\n    }",
    "crumbs": [
      "Functions",
      "calculate_multiplication_blocks"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calculate_multiplication_blocks.html#usage-example",
    "href": "api-reference/cpp/functions/calculate_multiplication_blocks.html#usage-example",
    "title": "calculate_multiplication_blocks",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = calculate_multiplication_blocks(...);",
    "crumbs": [
      "Functions",
      "calculate_multiplication_blocks"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdtcrossproduct.html",
    "href": "api-reference/cpp/functions/bdtcrossproduct.html",
    "title": "bdtcrossproduct",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::bdtcrossproduct(T X)",
    "crumbs": [
      "Functions",
      "bdtcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdtcrossproduct.html#signature",
    "href": "api-reference/cpp/functions/bdtcrossproduct.html#signature",
    "title": "bdtcrossproduct",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::bdtcrossproduct(T X)",
    "crumbs": [
      "Functions",
      "bdtcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdtcrossproduct.html#description",
    "href": "api-reference/cpp/functions/bdtcrossproduct.html#description",
    "title": "bdtcrossproduct",
    "section": "2 Description",
    "text": "2 Description\nCompute matrix transposed cross-product XX’.",
    "crumbs": [
      "Functions",
      "bdtcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdtcrossproduct.html#parameters",
    "href": "api-reference/cpp/functions/bdtcrossproduct.html#parameters",
    "title": "bdtcrossproduct",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (T): Input matrix",
    "crumbs": [
      "Functions",
      "bdtcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdtcrossproduct.html#returns",
    "href": "api-reference/cpp/functions/bdtcrossproduct.html#returns",
    "title": "bdtcrossproduct",
    "section": "4 Returns",
    "text": "4 Returns\nTransposed cross-product matrix XX’",
    "crumbs": [
      "Functions",
      "bdtcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdtcrossproduct.html#details",
    "href": "api-reference/cpp/functions/bdtcrossproduct.html#details",
    "title": "bdtcrossproduct",
    "section": "5 Details",
    "text": "5 Details\nEfficiently computes the transposed cross-product of a matrix (XX’) using optimized matrix operations.",
    "crumbs": [
      "Functions",
      "bdtcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdtcrossproduct.html#call-graph",
    "href": "api-reference/cpp/functions/bdtcrossproduct.html#call-graph",
    "title": "bdtcrossproduct",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "bdtcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdtcrossproduct.html#source-code",
    "href": "api-reference/cpp/functions/bdtcrossproduct.html#source-code",
    "title": "bdtcrossproduct",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memOptimizedProducts.hpp • Lines 192-207\ninline Eigen::MatrixXd bdtcrossproduct ( T X )\n{\n    \n    static_assert(std::is_same&lt;T, Eigen::MatrixXd &gt;::value || \n                  std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n                  std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value ||\n                  std::is_same&lt;T, Eigen::Transpose&lt;Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt; &gt;::value ||\n                  std::is_same&lt;T, Eigen::Transpose&lt;Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt; &gt;::value,\n                  \"Error - type not allowed\");\n    \n    Eigen::MatrixXd Xem = X;\n    \n    size_t nr(Xem.rows());\n    Eigen::MatrixXd XXt(Eigen::MatrixXd(nr, nr).setZero().selfadjointView&lt;Eigen::Lower&gt;().rankUpdate(Xem));\n    return(XXt);\n}",
    "crumbs": [
      "Functions",
      "bdtcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdtcrossproduct.html#usage-example",
    "href": "api-reference/cpp/functions/bdtcrossproduct.html#usage-example",
    "title": "bdtcrossproduct",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = bdtcrossproduct(...);",
    "crumbs": [
      "Functions",
      "bdtcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/avoid_openmp_hang_within_fork.html",
    "href": "api-reference/cpp/functions/avoid_openmp_hang_within_fork.html",
    "title": "avoid_openmp_hang_within_fork",
    "section": "",
    "text": "void avoid_openmp_hang_within_fork()",
    "crumbs": [
      "Functions",
      "avoid_openmp_hang_within_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/avoid_openmp_hang_within_fork.html#signature",
    "href": "api-reference/cpp/functions/avoid_openmp_hang_within_fork.html#signature",
    "title": "avoid_openmp_hang_within_fork",
    "section": "",
    "text": "void avoid_openmp_hang_within_fork()",
    "crumbs": [
      "Functions",
      "avoid_openmp_hang_within_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/avoid_openmp_hang_within_fork.html#call-graph",
    "href": "api-reference/cpp/functions/avoid_openmp_hang_within_fork.html#call-graph",
    "title": "avoid_openmp_hang_within_fork",
    "section": "2 Call Graph",
    "text": "2 Call Graph",
    "crumbs": [
      "Functions",
      "avoid_openmp_hang_within_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/avoid_openmp_hang_within_fork.html#source-code",
    "href": "api-reference/cpp/functions/avoid_openmp_hang_within_fork.html#source-code",
    "title": "avoid_openmp_hang_within_fork",
    "section": "3 Source Code",
    "text": "3 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/openme-utils.hpp • Lines 266-271\ninline void avoid_openmp_hang_within_fork() {\n        // Called once on loading BigDataStatMeth from init.c\n    #ifdef _OPENMP\n        pthread_atfork(&when_fork, &after_fork, NULL);\n    #endif\n    }",
    "crumbs": [
      "Functions",
      "avoid_openmp_hang_within_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/avoid_openmp_hang_within_fork.html#usage-example",
    "href": "api-reference/cpp/functions/avoid_openmp_hang_within_fork.html#usage-example",
    "title": "avoid_openmp_hang_within_fork",
    "section": "4 Usage Example",
    "text": "4 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = avoid_openmp_hang_within_fork(...);",
    "crumbs": [
      "Functions",
      "avoid_openmp_hang_within_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/addDiagonals.html",
    "href": "api-reference/cpp/functions/addDiagonals.html",
    "title": "addDiagonals",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::addDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "addDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/addDiagonals.html#signature",
    "href": "api-reference/cpp/functions/addDiagonals.html#signature",
    "title": "addDiagonals",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::addDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "addDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/addDiagonals.html#description",
    "href": "api-reference/cpp/functions/addDiagonals.html#description",
    "title": "addDiagonals",
    "section": "2 Description",
    "text": "2 Description\nAdd diagonal elements from two matrices or vectors.",
    "crumbs": [
      "Functions",
      "addDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/addDiagonals.html#parameters",
    "href": "api-reference/cpp/functions/addDiagonals.html#parameters",
    "title": "addDiagonals",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input dataset (matrix or vector)\ndsB (BigDataStatMeth::hdf5Dataset *): Second input dataset (matrix or vector)\ndsResult (BigDataStatMeth::hdf5Dataset *): Result dataset (only used if target=“new”)\ntarget (std::string): Where to write result: “A”, “B”, or “new”\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "addDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/addDiagonals.html#details",
    "href": "api-reference/cpp/functions/addDiagonals.html#details",
    "title": "addDiagonals",
    "section": "4 Details",
    "text": "4 Details\nPerforms optimized diagonal addition C_diag = A_diag + B_diag. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct vector operation). Uses vectorOperations.hpp for maximum efficiency.",
    "crumbs": [
      "Functions",
      "addDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/addDiagonals.html#call-graph",
    "href": "api-reference/cpp/functions/addDiagonals.html#call-graph",
    "title": "addDiagonals",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "addDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/addDiagonals.html#source-code",
    "href": "api-reference/cpp/functions/addDiagonals.html#source-code",
    "title": "addDiagonals",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5DiagonalMethods.hpp • Lines 386-411\ninline void addDiagonals(BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, \n                                 BigDataStatMeth::hdf5Dataset* dsResult, std::string target = \"new\",\n                                 bool bparal = false, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n        {\n            try {\n                bool isVectorA = isDiagonalVector(dsA);\n                bool isVectorB = isDiagonalVector(dsB);\n                \n                Rcpp::Rcout&lt;&lt;\"\\n Error add-1\";\n                if (isVectorA && isVectorB && (target == \"A\" || target == \"B\")) {\n                    Rcpp::Rcout&lt;&lt;\"\\n Error add-2\";\n                    BigDataStatMeth::hdf5Dataset* targetDataset = (target == \"A\") ? dsA : dsB;\n                    Rcpp::Rcout&lt;&lt;\"\\n Error add-3\";\n                    Rcpp_vector_add_hdf5(dsA, dsB, targetDataset, bparal, threads);\n                } else if (isVectorA && isVectorB && target == \"new\") {\n                    Rcpp::Rcout&lt;&lt;\"\\n Error add-4\";\n                    Rcpp_vector_add_hdf5(dsA, dsB, dsResult, bparal, threads);\n                } else {\n                    Rcpp::Rcout&lt;&lt;\"\\n Error add-5\";\n                    performMatrixDiagonalOperation(dsA, dsB, dsResult, 0, target, bparal, threads);\n                }\n                Rcpp::Rcout&lt;&lt;\"\\n Error add-6\";\n            } catch(std::exception& ex) {\n                Rf_error(\"Error in addDiagonals: %s\", ex.what());\n            }\n        }",
    "crumbs": [
      "Functions",
      "addDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/addDiagonals.html#usage-example",
    "href": "api-reference/cpp/functions/addDiagonals.html#usage-example",
    "title": "addDiagonals",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = addDiagonals(...);",
    "crumbs": [
      "Functions",
      "addDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd.html",
    "href": "api-reference/cpp/functions/Xwd.html",
    "title": "Xwd",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Xwd(const Eigen::MatrixXd &X, const Eigen::VectorXd &w)",
    "crumbs": [
      "Functions",
      "Xwd"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd.html#signature",
    "href": "api-reference/cpp/functions/Xwd.html#signature",
    "title": "Xwd",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Xwd(const Eigen::MatrixXd &X, const Eigen::VectorXd &w)",
    "crumbs": [
      "Functions",
      "Xwd"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd.html#description",
    "href": "api-reference/cpp/functions/Xwd.html#description",
    "title": "Xwd",
    "section": "2 Description",
    "text": "2 Description\nCompute matrix-diagonal product Xw.",
    "crumbs": [
      "Functions",
      "Xwd"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd.html#parameters",
    "href": "api-reference/cpp/functions/Xwd.html#parameters",
    "title": "Xwd",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (const Eigen::MatrixXd &): Input matrix\nw (const Eigen::VectorXd &): Vector representing diagonal matrix",
    "crumbs": [
      "Functions",
      "Xwd"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd.html#returns",
    "href": "api-reference/cpp/functions/Xwd.html#returns",
    "title": "Xwd",
    "section": "4 Returns",
    "text": "4 Returns\nMatrix-diagonal product Xw",
    "crumbs": [
      "Functions",
      "Xwd"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd.html#details",
    "href": "api-reference/cpp/functions/Xwd.html#details",
    "title": "Xwd",
    "section": "5 Details",
    "text": "5 Details\nImplementation of matrix-diagonal product computation.",
    "crumbs": [
      "Functions",
      "Xwd"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd.html#source-code",
    "href": "api-reference/cpp/functions/Xwd.html#source-code",
    "title": "Xwd",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memOptimizedProducts.hpp • Lines 249-258\ninline Eigen::MatrixXd Xwd(const Eigen::MatrixXd& X, const Eigen::VectorXd& w)\n{\n    int n = X.rows();\n    Eigen::MatrixXd C = Eigen::MatrixXd::Zero(n,X.cols()) ; \n    \n    for (int i=0; i&lt;n; i++) {\n        C.col(i) = X.col(i)*w(i);\n    }\n    return(C);\n}",
    "crumbs": [
      "Functions",
      "Xwd"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd.html#usage-example",
    "href": "api-reference/cpp/functions/Xwd.html#usage-example",
    "title": "Xwd",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Xwd(...);",
    "crumbs": [
      "Functions",
      "Xwd"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html",
    "href": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html",
    "title": "VectortoOrderedMap_SNP_counts",
    "section": "",
    "text": "std::map&lt; double, double &gt; BigDataStatMeth::VectortoOrderedMap_SNP_counts(Eigen::VectorXd vdata)",
    "crumbs": [
      "Functions",
      "VectortoOrderedMap_SNP_counts"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#signature",
    "href": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#signature",
    "title": "VectortoOrderedMap_SNP_counts",
    "section": "",
    "text": "std::map&lt; double, double &gt; BigDataStatMeth::VectortoOrderedMap_SNP_counts(Eigen::VectorXd vdata)",
    "crumbs": [
      "Functions",
      "VectortoOrderedMap_SNP_counts"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#description",
    "href": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#description",
    "title": "VectortoOrderedMap_SNP_counts",
    "section": "2 Description",
    "text": "2 Description\nConverts a vector to an ordered map of value frequencies.",
    "crumbs": [
      "Functions",
      "VectortoOrderedMap_SNP_counts"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#parameters",
    "href": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#parameters",
    "title": "VectortoOrderedMap_SNP_counts",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nvdata (Eigen::VectorXd): Input vector of values",
    "crumbs": [
      "Functions",
      "VectortoOrderedMap_SNP_counts"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#returns",
    "href": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#returns",
    "title": "VectortoOrderedMap_SNP_counts",
    "section": "4 Returns",
    "text": "4 Returns\nstd::map&lt;double, double&gt; Map of value-frequency pairs",
    "crumbs": [
      "Functions",
      "VectortoOrderedMap_SNP_counts"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#details",
    "href": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#details",
    "title": "VectortoOrderedMap_SNP_counts",
    "section": "5 Details",
    "text": "5 Details\nCreates a map where keys are unique values from the input vector and values are their frequencies of occurrence.",
    "crumbs": [
      "Functions",
      "VectortoOrderedMap_SNP_counts"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#caller-graph",
    "href": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#caller-graph",
    "title": "VectortoOrderedMap_SNP_counts",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "VectortoOrderedMap_SNP_counts"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#source-code",
    "href": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#source-code",
    "title": "VectortoOrderedMap_SNP_counts",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5ImputeData.hpp • Lines 101-128\ninline std::map&lt;double, double&gt; VectortoOrderedMap_SNP_counts( Eigen::VectorXd  vdata)\n    {\n        std::map&lt;double, double&gt; mapv;\n        \n        try \n        {\n            int position = 0;\n            std::vector&lt;double&gt; v(vdata.data(), vdata.data()+vdata.size());\n            \n            std::sort(v.begin(), v.end() ); // Sort vector to optimize search and count\n            \n            for (size_t i = 0; i &lt;=  *std::max_element(v.begin(), v.end()) ; ++i)  \n            {\n                double mycount = std::count(v.begin() + position, v.end(), i);\n                mapv[i] = mycount;\n                position = position + mycount;\n            }\n            \n        } catch(std::exception &ex) {\n            Rf_error( \"c++ exception VectortoOrderedMap_SNP_counts : %s\", ex.what());\n            return std::map&lt;double, double&gt;();\n        } catch(...) { \n            Rf_error(\"c++ exception VectortoOrderedMap_SNP_counts (unknown reason)\"); \n            return std::map&lt;double, double&gt;();\n        } \n        \n        return mapv;\n    }",
    "crumbs": [
      "Functions",
      "VectortoOrderedMap_SNP_counts"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#usage-example",
    "href": "api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.html#usage-example",
    "title": "VectortoOrderedMap_SNP_counts",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = VectortoOrderedMap_SNP_counts(...);",
    "crumbs": [
      "Functions",
      "VectortoOrderedMap_SNP_counts"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_lapack.html",
    "href": "api-reference/cpp/functions/RcppbdSVD_lapack.html",
    "title": "RcppbdSVD_lapack",
    "section": "",
    "text": "svdeig BigDataStatMeth::RcppbdSVD_lapack(T X, bool bcenter, bool bscale, bool complete)",
    "crumbs": [
      "Functions",
      "RcppbdSVD_lapack"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_lapack.html#signature",
    "href": "api-reference/cpp/functions/RcppbdSVD_lapack.html#signature",
    "title": "RcppbdSVD_lapack",
    "section": "",
    "text": "svdeig BigDataStatMeth::RcppbdSVD_lapack(T X, bool bcenter, bool bscale, bool complete)",
    "crumbs": [
      "Functions",
      "RcppbdSVD_lapack"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_lapack.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdSVD_lapack.html#parameters",
    "title": "RcppbdSVD_lapack",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nX (T)\nbcenter (bool)\nbscale (bool)\ncomplete (bool)",
    "crumbs": [
      "Functions",
      "RcppbdSVD_lapack"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_lapack.html#returns",
    "href": "api-reference/cpp/functions/RcppbdSVD_lapack.html#returns",
    "title": "RcppbdSVD_lapack",
    "section": "3 Returns",
    "text": "3 Returns\nType: class T",
    "crumbs": [
      "Functions",
      "RcppbdSVD_lapack"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_lapack.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdSVD_lapack.html#call-graph",
    "title": "RcppbdSVD_lapack",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdSVD_lapack"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_lapack.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdSVD_lapack.html#source-code",
    "title": "RcppbdSVD_lapack",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSvdBlock.hpp • Lines 73-129\ninline svdeig RcppbdSVD_lapack( T X, bool bcenter, bool bscale, bool complete ) {\n    \n    svdeig retsvd;\n    \n    char Schar='S';\n    char Achar='A';\n    int info = 0;\n    \n    try {\n        \n        if(bcenter == true || bscale == true) {\n            X = RcppNormalize_Data(X, bcenter, bscale, false);\n        }\n        \n        int m = X.rows();\n        int n = X.cols();\n        int lda = std::max(1,m);\n        int ldu = std::max(1,m);\n        int ldvt = std::min(m, n);\n        int k = std::min(m,n);\n        int lwork;\n        \n        if( complete == false ) {\n            lwork = 4*std::min(m,n)*std::min(m,n) + 7*std::min(m,n);\n        } else {\n            lwork = 4*std::min(m,n)*std::min(m,n) + 6*std::min(m,n) + std::max(m,n);\n        }\n        \n        Eigen::VectorXd s = Eigen::VectorXd::Zero(k);\n        Eigen::VectorXd work = Eigen::VectorXd::Zero(lwork);\n        Eigen::VectorXi iwork(8*std::min(m,n));\n        Eigen::MatrixXd u;\n        Eigen::MatrixXd vt = Eigen::MatrixXd::Zero(ldvt,n);\n        \n        if( complete == false ) {\n            u = Eigen::MatrixXd::Zero(ldu,k);\n            dgesdd_( &Schar, &m, &n, X.data(), &lda, s.data(), u.data(), &ldu, vt.data(), &ldvt, work.data(), &lwork, iwork.data(), &info);\n        } else {\n            u = Eigen::MatrixXd::Zero(ldu,m);\n            dgesdd_( &Achar, &m, &n, X.data(), &lda, s.data(), u.data(), &ldu, vt.data(), &ldvt, work.data(), &lwork, iwork.data(), &info);\n        }\n        \n        retsvd.d = s;\n        retsvd.u = u;\n        retsvd.v = vt.transpose();\n        \n    } catch(std::exception &ex) {\n        Rcpp::Rcout&lt;&lt; \"C++ exception RcppbdSVD_lapack : \"&lt;&lt; ex.what();\n        return retsvd;\n    } catch (...) {\n        Rf_error(\"C++ exception RcppbdSVD_lapack (unknown reason)\");\n        return retsvd;\n    }\n    \n    return retsvd;\n    \n}",
    "crumbs": [
      "Functions",
      "RcppbdSVD_lapack"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_lapack.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdSVD_lapack.html#usage-example",
    "title": "RcppbdSVD_lapack",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdSVD_lapack(...);",
    "crumbs": [
      "Functions",
      "RcppbdSVD_lapack"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5.html",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5.html",
    "title": "RcppbdSVD_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppbdSVD_hdf5(std::string filename, std::string strsubgroup, std::string strdataset, int k, int q, int nev, bool bcenter, bool bscale, double dthreshold, bool bforce, bool asRowMajor, Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; method=R_NilValue, Rcpp::Nullable&lt; int &gt; ithreads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#signature",
    "title": "RcppbdSVD_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppbdSVD_hdf5(std::string filename, std::string strsubgroup, std::string strdataset, int k, int q, int nev, bool bcenter, bool bscale, double dthreshold, bool bforce, bool asRowMajor, Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; method=R_NilValue, Rcpp::Nullable&lt; int &gt; ithreads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#description",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#description",
    "title": "RcppbdSVD_hdf5",
    "section": "2 Description",
    "text": "2 Description\nMain SVD computation function for HDF5 matrices.",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#parameters",
    "title": "RcppbdSVD_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfilename (std::string): Path to the HDF5 file containing input matrix\nstrsubgroup (std::string): Group path within the HDF5 file\nstrdataset (std::string): Dataset name containing the matrix\nk (int): Number of local SVDs to concatenate at each level\nq (int): Number of decomposition levels\nnev (int): Number of eigenvalues per block\nbcenter (bool): Whether to center the data before SVD\nbscale (bool): Whether to scale the data before SVD\ndthreshold (double): Numerical threshold for computations\nbforce (bool): Whether to overwrite existing results\nasRowMajor (bool): Whether to interpret matrix as row-major\nmethod (Rcpp::Nullable&lt; Rcpp::CharacterVector &gt;): Computation method (“auto”, “blocks”, “full”)\nithreads (Rcpp::Nullable&lt; int &gt;): Number of parallel threads (optional)",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#details",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#details",
    "title": "RcppbdSVD_hdf5",
    "section": "4 Details",
    "text": "4 Details\nThis is the main interface for computing SVD of matrices stored in HDF5 format. It automatically selects between direct LAPACK computation for small matrices and block-wise decomposition for large matrices.",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#call-graph",
    "title": "RcppbdSVD_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#source-code",
    "title": "RcppbdSVD_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSvd.hpp • Lines 370-475\ninline void RcppbdSVD_hdf5( std::string filename, std::string strsubgroup, std::string strdataset,  \n                                int k, int q, int nev, bool bcenter, bool bscale, double dthreshold, \n                                bool bforce, bool asRowMajor, \n                                Rcpp::Nullable&lt;Rcpp::CharacterVector&gt; method = R_NilValue,\n                                Rcpp::Nullable&lt;int&gt; ithreads = R_NilValue)\n    {\n        \n        BigDataStatMeth::hdf5Dataset* dsA = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsu = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsv = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsd = nullptr;\n        \n        try {\n            \n            std::string strMethod;\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1},\n                block = {1, 1},\n                offset = {0, 0},\n                count = {0, 0};\n            \n            std::vector&lt;std::string&gt; strMethods = {\"auto\", \"blocks\", \"full\"};\n            \n            if(method.isNull())  strMethod = \"auto\" ;\n            else    strMethod = Rcpp::as&lt;std::string&gt;(method);\n            \n            dsA = new BigDataStatMeth::hdf5Dataset(filename, strsubgroup, strdataset, false);\n            dsA-&gt;openDataset();\n            \n            if( dsA-&gt;getDatasetptr() != nullptr ) { \n                \n                // Create results folder\n                std::string stroutgroup = \"SVD/\"+ strdataset;\n                \n                std::vector&lt;hsize_t&gt; dims_out = {dsA-&gt;nrows(), dsA-&gt;ncols()};;\n                count = { dims_out[0], dims_out[1]};\n                \n                // Small matrices ==&gt; Direct SVD (lapack)\n                if( (dims_out[0] * dims_out[1] &lt; (MAXELEMSINBLOCK / 20) && strMethod == \"auto\") || strMethod == \"full\" ) {\n                    \n                    // Rcpp::Rcout&lt;&lt;\"\\nEste, aquÃ­ - 1\";\n                    Eigen::MatrixXd X;\n                    svdeig retsvd;\n                    \n                    std::vector&lt;double&gt; vdA( count[0] * count[1] ); \n                    dsA-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdA.data() );\n                    \n                    if(asRowMajor == true) {\n                        Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; X (vdA.data(), count[0], count[1] );    \n                        retsvd = RcppbdSVD_lapack(X, bcenter, bscale, false);\n                    } else {\n                        Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; X (vdA.data(), count[1], count[0] );\n                        retsvd = RcppbdSVD_lapack(X, bcenter, bscale, false);\n                    }\n                    \n                    dsu = new hdf5Dataset(filename, stroutgroup, \"u\", bforce);\n                    dsu-&gt;createDataset( retsvd.u.rows(), retsvd.u.cols(), \"real\");\n                    dsu-&gt;writeDataset( Rcpp::wrap(retsvd.u) );\n                    \n                    dsv = new hdf5Dataset(filename, stroutgroup, \"v\", bforce);\n                    dsv-&gt;createDataset( retsvd.v.rows(), retsvd.v.cols(), \"real\");\n                    dsv-&gt;writeDataset( Rcpp::wrap(retsvd.v) );\n                    \n                    dsd = new hdf5Dataset(filename, stroutgroup, \"d\", bforce);\n                    //.. COMENTAT 2025-02-06 ..// dsd-&gt;createDataset( retsvd.d.size(), 1, \"real\");\n                    dsd-&gt;createDataset( 1, retsvd.d.size(), \"real\");\n                    dsd-&gt;writeDataset( Rcpp::wrap(retsvd.d) );\n                    \n                } else {\n                    // Rcpp::Rcout&lt;&lt;\"\\nEste, aquÃ­ - 2\";\n                    dsu = new BigDataStatMeth::hdf5Dataset(filename, stroutgroup, \"u\", true);\n                    dsv = new BigDataStatMeth::hdf5Dataset(filename, stroutgroup, \"v\", true);\n                    dsd = new BigDataStatMeth::hdf5Dataset(filename, stroutgroup, \"d\", true);\n                    \n                    if( dsA-&gt;getDatasetptr() != nullptr) {\n                        RcppbdSVD_hdf5_Block( dsA, dsu, dsv, dsd, k, q, nev, bcenter, bscale, count[1], count[0], dthreshold, ithreads );\n                    }\n                    \n                } \n            } \n            \n            delete dsu; dsu = nullptr;\n            delete dsv; dsv = nullptr;\n            delete dsd; dsd = nullptr;\n            delete dsA; dsA = nullptr;\n            \n        }  catch( H5::FileIException& error ) { // catch failure caused by the H5File operations\n            checkClose_file(dsA, dsd, dsu, dsv);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppbdSVD_hdf5 (File IException)\\n\";\n            return void();\n        } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n            checkClose_file(dsA, dsd, dsu, dsv);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppbdSVD_hdf5 (DataSet IException)\\n\";\n            return void();\n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsd, dsu, dsv);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception RcppbdSVD_hdf5 \\n\"&lt;&lt; ex.what();\n            return void();\n        } catch (...) {\n            checkClose_file(dsA, dsd, dsu, dsv);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception RcppbdSVD_hdf5 (unknown reason)\";\n            return void();\n        }\n        \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5.html#usage-example",
    "title": "RcppbdSVD_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdSVD_hdf5(...);",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_spectra.html",
    "href": "api-reference/cpp/functions/RcppbdEigen_spectra.html",
    "title": "RcppbdEigen_spectra",
    "section": "",
    "text": "eigdecomp BigDataStatMeth::RcppbdEigen_spectra(const Eigen::MatrixXd &X, int k, const std::string &which=\"LM\", int ncv=0, bool bcenter=false, bool bscale=false, double tol=1e-10, int max_iter=1000)",
    "crumbs": [
      "Functions",
      "RcppbdEigen_spectra"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_spectra.html#signature",
    "href": "api-reference/cpp/functions/RcppbdEigen_spectra.html#signature",
    "title": "RcppbdEigen_spectra",
    "section": "",
    "text": "eigdecomp BigDataStatMeth::RcppbdEigen_spectra(const Eigen::MatrixXd &X, int k, const std::string &which=\"LM\", int ncv=0, bool bcenter=false, bool bscale=false, double tol=1e-10, int max_iter=1000)",
    "crumbs": [
      "Functions",
      "RcppbdEigen_spectra"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_spectra.html#description",
    "href": "api-reference/cpp/functions/RcppbdEigen_spectra.html#description",
    "title": "RcppbdEigen_spectra",
    "section": "2 Description",
    "text": "2 Description\nEigenvalue decomposition using Spectra (compatible with BigDataStatMeth SVD version)",
    "crumbs": [
      "Functions",
      "RcppbdEigen_spectra"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_spectra.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdEigen_spectra.html#parameters",
    "title": "RcppbdEigen_spectra",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (const Eigen::MatrixXd &): Input matrix\nk (int): Number of eigenvalues to compute\nwhich (const std::string &): Which eigenvalues to compute (LM, SM, LR, SR, LI, SI, LA, SA)\nncv (int): Number of Arnoldi vectors (if 0, uses auto-selection)\nbcenter (bool): Whether to center the data\nbscale (bool): Whether to scale the data\ntol (double): Convergence tolerance for Spectra\nmax_iter (int): Maximum iterations for Spectra",
    "crumbs": [
      "Functions",
      "RcppbdEigen_spectra"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_spectra.html#returns",
    "href": "api-reference/cpp/functions/RcppbdEigen_spectra.html#returns",
    "title": "RcppbdEigen_spectra",
    "section": "4 Returns",
    "text": "4 Returns\neigdecomp structure containing results",
    "crumbs": [
      "Functions",
      "RcppbdEigen_spectra"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_spectra.html#details",
    "href": "api-reference/cpp/functions/RcppbdEigen_spectra.html#details",
    "title": "RcppbdEigen_spectra",
    "section": "5 Details",
    "text": "5 Details\nUses the same Spectra version and patterns as the existing SVD implementation in BigDataStatMeth. Based on the RcppbdSVD function in matrixSvd.hpp.",
    "crumbs": [
      "Functions",
      "RcppbdEigen_spectra"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_spectra.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdEigen_spectra.html#call-graph",
    "title": "RcppbdEigen_spectra",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdEigen_spectra"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_spectra.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdEigen_spectra.html#source-code",
    "title": "RcppbdEigen_spectra",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixEigenDecomposition.hpp • Lines 179-282\ninline eigdecomp RcppbdEigen_spectra(const Eigen::MatrixXd& X, int k, const std::string& which = \"LM\", \n                                         int ncv = 0, bool bcenter = false, bool bscale = false,\n                                         double tol = 1e-10, int max_iter = 1000) {\n        \n        eigdecomp reteig;\n        Eigen::MatrixXd nX;\n        // int nconv = 0;\n        \n        try {\n            \n            int n = X.rows();\n            if (X.rows() != X.cols()) {\n                Rf_error(\"Matrix must be square for eigendecomposition\");\n                return reteig;\n            }\n            \n            // Better parameter selection following RSpectra defaults\n            std::tie(k, ncv) = validateSpectraParams(n, k, ncv);\n            \n            // Improved symmetry detection\n            reteig.is_symmetric = isMatrixSymmetric(X);\n            \n            if (reteig.is_symmetric) {\n                // Use symmetric solver - following SVD pattern\n                Eigen::MatrixXd Xcp;\n                if (bcenter == true || bscale == true) {\n                    nX = RcppNormalize_Data(X, bcenter, bscale, false);\n                    Xcp = nX;  // For symmetric case, use matrix directly\n                } else {\n                    Xcp = X;\n                }\n                \n                Spectra::DenseSymMatProd&lt;double&gt; op(Xcp);\n                Spectra::SymEigsSolver&lt;Spectra::DenseSymMatProd&lt;double&gt;&gt; eigs(op, k, ncv);\n                \n                // // Set tolerance and max iterations\n                // eigs.set_max_iter(max_iter);\n                \n                // Initialize and compute with appropriate sort rule\n                eigs.init();\n                Spectra::SortRule sort_rule = getSymmetricSortRule(which);\n                //..// nconv = eigs.compute(sort_rule, max_iter, tol);\n                (void)eigs.compute(sort_rule, max_iter, tol);\n                \n                // Retrieve results\n                if (eigs.info() == Spectra::CompInfo::Successful) {\n                    reteig.eigenvalues_real = eigs.eigenvalues();\n                    reteig.eigenvalues_imag = Eigen::VectorXd::Zero(k);\n                    reteig.eigenvectors_real = eigs.eigenvectors();\n                    reteig.eigenvectors_imag = Eigen::MatrixXd::Zero(n, k);\n                    reteig.bconv = true;\n                } else {\n                    reteig.bconv = false;\n                }\n                \n            } else {\n                // For non-symmetric matrices, use general solver\n                Eigen::MatrixXd Xcp;\n                if (bcenter == true || bscale == true) {\n                    nX = RcppNormalize_Data(X, bcenter, bscale, false);\n                    Xcp = nX;\n                } else {\n                    Xcp = X;\n                }\n                \n                Spectra::DenseGenMatProd&lt;double&gt; op(Xcp);\n                Spectra::GenEigsSolver&lt;Spectra::DenseGenMatProd&lt;double&gt;&gt; eigs(op, k, ncv);\n                \n                // // Set tolerance and max iterations\n                // eigs.set_max_iter(max_iter);\n                \n                // Initialize and compute with appropriate sort rule\n                eigs.init();\n                Spectra::SortRule sort_rule = getGeneralSortRule(which);\n                //..// nconv = eigs.compute(sort_rule, max_iter, tol);\n                (void)eigs.compute(sort_rule, max_iter, tol);\n                \n                // Retrieve results\n                if (eigs.info() == Spectra::CompInfo::Successful) {\n                    Eigen::VectorXcd eigenvals = eigs.eigenvalues();\n                    Eigen::MatrixXcd eigenvecs = eigs.eigenvectors();\n                    \n                    reteig.eigenvalues_real = eigenvals.real();\n                    reteig.eigenvalues_imag = eigenvals.imag();\n                    reteig.eigenvectors_real = eigenvecs.real();\n                    reteig.eigenvectors_imag = eigenvecs.imag();\n                    reteig.bconv = true;\n                } else {\n                    reteig.bconv = false;\n                }\n            }\n            \n            reteig.bcomputevectors = true;\n            \n        } catch(std::exception &ex) {\n            Rcpp::Rcout &lt;&lt; \"C++ exception RcppbdEigen_spectra: \" &lt;&lt; ex.what();\n            reteig.bconv = false;\n        } catch (...) {\n            Rf_error(\"C++ exception RcppbdEigen_spectra (unknown reason)\");\n            reteig.bconv = false;\n        }\n        \n        return reteig;\n    }",
    "crumbs": [
      "Functions",
      "RcppbdEigen_spectra"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_spectra.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdEigen_spectra.html#usage-example",
    "title": "RcppbdEigen_spectra",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdEigen_spectra(...);",
    "crumbs": [
      "Functions",
      "RcppbdEigen_spectra"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5.html",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5.html",
    "title": "RcppbdEigen_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppbdEigen_hdf5(std::string filename, std::string strsubgroup, std::string strdataset, int k=0, const std::string &which=\"LM\", int ncv=0, bool bcenter=false, bool bscale=false, double tolerance=1e-10, int max_iter=1000, bool compute_vectors=true, bool bforce=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#signature",
    "title": "RcppbdEigen_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppbdEigen_hdf5(std::string filename, std::string strsubgroup, std::string strdataset, int k=0, const std::string &which=\"LM\", int ncv=0, bool bcenter=false, bool bscale=false, double tolerance=1e-10, int max_iter=1000, bool compute_vectors=true, bool bforce=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#description",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#description",
    "title": "RcppbdEigen_hdf5",
    "section": "2 Description",
    "text": "2 Description\nMain eigenvalue decomposition function for HDF5 matrices using Spectra.",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#parameters",
    "title": "RcppbdEigen_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfilename (std::string): Path to HDF5 file containing the input matrix\nstrsubgroup (std::string): Group path within the HDF5 file\nstrdataset (std::string): Dataset name containing the matrix\nk (int): Number of eigenvalues to compute (0 = auto-select based on matrix size)\nwhich (const std::string &): Which eigenvalues to compute (LM, SM, LR, SR, LI, SI, LA, SA)\nncv (int): Number of Arnoldi vectors (0 = auto-select)\nbcenter (bool): Whether to center the data before decomposition\nbscale (bool): Whether to scale the data before decomposition\ntolerance (double): Convergence tolerance for Spectra algorithms\nmax_iter (int): Maximum iterations for Spectra algorithms\ncompute_vectors (bool): Whether to compute eigenvectors or just eigenvalues\nbforce (bool): Whether to overwrite existing results\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel computation",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#details",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#details",
    "title": "RcppbdEigen_hdf5",
    "section": "4 Details",
    "text": "4 Details\nThis is the main interface for eigenvalue decomposition of matrices stored in HDF5. It uses Spectra library for consistent results with RSpectra and automatically handles both symmetric and non-symmetric matrices.",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#call-graph",
    "title": "RcppbdEigen_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#source-code",
    "title": "RcppbdEigen_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixEigenDecomposition.hpp • Lines 456-568\ninline void RcppbdEigen_hdf5(std::string filename, std::string strsubgroup, std::string strdataset,\n                                 int k = 0, const std::string& which = \"LM\", int ncv = 0,\n                                 bool bcenter = false, bool bscale = false,\n                                 double tolerance = 1e-10, int max_iter = 1000, \n                                 bool compute_vectors = true, bool bforce = false,\n                                 Rcpp::Nullable&lt;int&gt; threads = R_NilValue) {\n        \n        BigDataStatMeth::hdf5Dataset* dsA = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsd = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsu = nullptr;\n        \n        try {\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1}, block = {1, 1}, offset = {0, 0}, count = {0, 0};\n            \n            dsA = new BigDataStatMeth::hdf5Dataset(filename, strsubgroup, strdataset, false);\n            dsA-&gt;openDataset();\n            \n            if (dsA-&gt;getDatasetptr() != nullptr) {\n                \n                // Create results folder following SVD pattern\n                std::string stroutgroup = \"EIGEN/\" + strdataset;\n                \n                std::vector&lt;hsize_t&gt; dims_out = {dsA-&gt;nrows(), dsA-&gt;ncols()};\n                count = {dims_out[0], dims_out[1]};\n                \n                // Check if matrix is square\n                if (dims_out[0] != dims_out[1]) {\n                    Rf_error(\"Matrix must be square for eigendecomposition\");\n                    return;\n                }\n                \n                hsize_t n = dims_out[0];\n                \n                // Use parameter validation function\n                std::tie(k, ncv) = validateSpectraParams((int)n, k, ncv);\n                \n                // Create output datasets\n                dsd = new BigDataStatMeth::hdf5Dataset(filename, stroutgroup, \"values\", bforce);\n                if (compute_vectors) {\n                    dsu = new BigDataStatMeth::hdf5Dataset(filename, stroutgroup, \"vectors\", bforce);\n                }\n                \n                // Small matrices =&gt; Direct eigendecomposition with Spectra\n                if (n * n &lt; MAXELEMSINBLOCK / 20) {\n                    \n                    Eigen::MatrixXd X;\n                    eigdecomp reteig;\n                    \n                    std::vector&lt;double&gt; vdA(count[0] * count[1]);\n                    dsA-&gt;readDatasetBlock({offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdA.data());\n                    \n                    X = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt;(\n                        vdA.data(), count[0], count[1]);\n                    \n                    reteig = RcppbdEigen_spectra(X, k, which, ncv, bcenter, bscale, tolerance, max_iter);\n                    \n                    if (!reteig.bconv) {\n                        Rf_error(\"Eigendecomposition failed to converge\");\n                        return;\n                    }\n                    \n                    // Write real eigenvalues\n                    dsd-&gt;createDataset(1, reteig.eigenvalues_real.size(), \"real\");\n                    dsd-&gt;writeDataset(Rcpp::wrap(reteig.eigenvalues_real));\n                    \n                    // Write real eigenvectors if computed\n                    if (compute_vectors && reteig.bcomputevectors) {\n                        dsu-&gt;createDataset(reteig.eigenvectors_real.rows(), reteig.eigenvectors_real.cols(), \"real\");\n                        dsu-&gt;writeDataset(Rcpp::wrap(reteig.eigenvectors_real));\n                    }\n                    \n                    // Write imaginary parts if non-zero (for non-symmetric matrices)\n                    if (!reteig.is_symmetric && reteig.eigenvalues_imag.cwiseAbs().maxCoeff() &gt; 1e-14) {\n                        \n                        BigDataStatMeth::hdf5Dataset* dsd_imag = new BigDataStatMeth::hdf5Dataset(filename, stroutgroup, \"values_imag\", bforce);\n                        dsd_imag-&gt;createDataset(1, reteig.eigenvalues_imag.size(), \"real\");\n                        dsd_imag-&gt;writeDataset(Rcpp::wrap(reteig.eigenvalues_imag));\n                        delete dsd_imag;\n                        \n                        if (compute_vectors && reteig.bcomputevectors && reteig.eigenvectors_imag.cwiseAbs().maxCoeff() &gt; 1e-14) {\n                            BigDataStatMeth::hdf5Dataset* dsu_imag = new BigDataStatMeth::hdf5Dataset(filename, stroutgroup, \"vectors_imag\", bforce);\n                            dsu_imag-&gt;createDataset(reteig.eigenvectors_imag.rows(), reteig.eigenvectors_imag.cols(), \"real\");\n                            dsu_imag-&gt;writeDataset(Rcpp::wrap(reteig.eigenvectors_imag));\n                            delete dsu_imag;\n                        }\n                    }\n                    \n                } else {\n                    // Large matrices =&gt; Block-based approach with Spectra  \n                    if (dsA-&gt;getDatasetptr() != nullptr) {\n                        RcppbdEigen_hdf5_Block(dsA, dsd, dsu, k, which, ncv, bcenter, bscale, tolerance, max_iter, compute_vectors, threads);\n                    }\n                }\n            }\n            delete dsd; dsd = nullptr;\n            if (dsu) { delete dsu; dsu = nullptr; }\n            delete dsA; dsA = nullptr;\n            \n        } catch(H5::FileIException& error) {\n            checkClose_file(dsA, dsd, dsu);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception RcppbdEigen_hdf5 (File IException)\\n\";\n        } catch(H5::DataSetIException& error) {\n            checkClose_file(dsA, dsd, dsu);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception RcppbdEigen_hdf5 (DataSet IException) \"&lt;&lt;error.getDetailMsg() &lt;&lt;\" \\n\";\n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsd, dsu);\n            Rcpp::Rcerr &lt;&lt; \"c++ exception RcppbdEigen_hdf5: \" &lt;&lt; ex.what();\n        } catch (...) {\n            checkClose_file(dsA, dsd, dsu);\n            Rcpp::Rcerr &lt;&lt; \"\\nC++ exception RcppbdEigen_hdf5 (unknown reason)\";\n        }\n    }",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5.html#usage-example",
    "title": "RcppbdEigen_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdEigen_hdf5(...);",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html",
    "title": "RcppbdCorr_matrix_cross",
    "section": "",
    "text": "corr_result BigDataStatMeth::RcppbdCorr_matrix_cross(const Eigen::MatrixXd &X, const Eigen::MatrixXd &Y, const std::string &method=\"pearson\", bool use_complete_obs=true, bool compute_pvalues=true, bool trans_x=false, bool trans_y=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html#signature",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html#signature",
    "title": "RcppbdCorr_matrix_cross",
    "section": "",
    "text": "corr_result BigDataStatMeth::RcppbdCorr_matrix_cross(const Eigen::MatrixXd &X, const Eigen::MatrixXd &Y, const std::string &method=\"pearson\", bool use_complete_obs=true, bool compute_pvalues=true, bool trans_x=false, bool trans_y=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html#parameters",
    "title": "RcppbdCorr_matrix_cross",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nX (const Eigen::MatrixXd &)\nY (const Eigen::MatrixXd &)\nmethod (const std::string &)\nuse_complete_obs (bool)\ncompute_pvalues (bool)\ntrans_x (bool)\ntrans_y (bool)\nthreads (Rcpp::Nullable&lt; int &gt;)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html#returns",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html#returns",
    "title": "RcppbdCorr_matrix_cross",
    "section": "3 Returns",
    "text": "3 Returns\nType: corr_result",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html#call-graph",
    "title": "RcppbdCorr_matrix_cross",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html#source-code",
    "title": "RcppbdCorr_matrix_cross",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixCorrelation.hpp • Lines 1044-1153\ninline corr_result RcppbdCorr_matrix_cross(const Eigen::MatrixXd& X,\n                                                         const Eigen::MatrixXd& Y,\n                                                         const std::string& method = \"pearson\",\n                                                         bool use_complete_obs = true,\n                                                         bool compute_pvalues = true,\n                                                         bool trans_x = false,\n                                                         bool trans_y = false,\n                                                         Rcpp::Nullable&lt;int&gt; threads = R_NilValue) {\n        \n        corr_result result;\n        result.trans_x = trans_x;\n        result.trans_y = trans_y;\n        \n        try {\n            \n            // Determine effective dimensions after transposition\n            int n_obs_x, n_vars_x, n_obs_y, n_vars_y;\n            \n            if (trans_x) {\n                n_obs_x = X.cols();  // Original variables become observations\n                n_vars_x = X.rows(); // Original observations become variables\n            } else {\n                n_obs_x = X.rows();  // Standard layout\n                n_vars_x = X.cols();\n            }\n            \n            if (trans_y) {\n                n_obs_y = Y.cols();  // Original variables become observations\n                n_vars_y = Y.rows(); // Original observations become variables\n            } else {\n                n_obs_y = Y.rows();  // Standard layout\n                n_vars_y = Y.cols();\n            }\n            \n            // Validate dimensions after transposition\n            if (n_obs_x != n_obs_y) {\n                Rcpp::Rcerr &lt;&lt; \"Matrices must have the same number of observations after transposition\" &lt;&lt; std::endl;\n                result.bcomputed = false;\n                return result;\n            }\n            \n            result.correlation_matrix = Eigen::MatrixXd::Zero(n_vars_x, n_vars_y);\n            result.method = method;\n            result.n_obs = n_obs_x;\n            result.n_vars_x = n_vars_x;\n            result.n_vars_y = n_vars_y;\n            result.has_pvalues = compute_pvalues; \n            \n            // Optimized threading for cross-correlation\n            \n    #ifdef _OPENMP\n            int num_threads = 1;\n            if (!threads.isNull()) {\n                num_threads = Rcpp::as&lt;int&gt;(threads);\n            } else {\n                // Scale with matrix size but be conservative\n                int total_pairs = n_vars_x * n_vars_y;\n                num_threads = (total_pairs &gt; 1000) ? std::min(6, omp_get_max_threads()) : 1;\n            }\n            num_threads = std::max(1, std::min(num_threads, omp_get_max_threads()));\n    // #endif\n    //         \n    //         // Single level parallelization with good cache locality\n    // #ifdef _OPENMP\n    #pragma omp parallel for num_threads(num_threads) schedule(dynamic, 1)\n    #endif\n            for (int i = 0; i &lt; n_vars_x; ++i) {\n                for (int j = 0; j &lt; n_vars_y; ++j) {\n                    \n                    Eigen::VectorXd vec_x, vec_y;\n                    double corr_val;//, pval;\n                    \n                    // Get vectors with logical transposition\n                    if (trans_x) {\n                        vec_x = X.row(i).transpose(); // Row becomes variable\n                    } else {\n                        vec_x = X.col(i);            // Column is variable\n                    }\n                    \n                    if (trans_y) {\n                        vec_y = Y.row(j).transpose(); // Row becomes variable\n                    } else {\n                        vec_y = Y.col(j);            // Column is variable\n                    }\n                    \n                    // double corr_val;\n                    if (method == \"spearman\") {\n                        corr_val = spearman_correlation(vec_x, vec_y, use_complete_obs);\n                    } else {\n                        corr_val = pearson_correlation(vec_x, vec_y, use_complete_obs);\n                    }\n                    \n                    result.correlation_matrix(i, j) = corr_val;\n                }\n            }\n            \n            // Compute p-values using optimized vectorized function\n            if (compute_pvalues) {\n                result.pvalues = compute_pvalues_optimized(result.correlation_matrix, n_obs_x, false);\n            }\n            \n            result.bcomputed = true;\n            \n        } catch (std::exception &ex) {\n            Rcpp::Rcerr &lt;&lt; \"C++ exception RcppbdCorr_matrix_cross: \" &lt;&lt; ex.what() &lt;&lt; std::endl;\n            result.bcomputed = false;\n        }\n        \n        return result;\n    }",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_cross.html#usage-example",
    "title": "RcppbdCorr_matrix_cross",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdCorr_matrix_cross(...);",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html",
    "title": "RcppbdCorr_hdf5_cross",
    "section": "",
    "text": "Rcpp::List BigDataStatMeth::RcppbdCorr_hdf5_cross(const std::string &filename_a, const std::string &strsubgroup_a, const std::string &strdataset_a, const std::string &filename_b, const std::string &strsubgroup_b, const std::string &strdataset_b, const std::string &method, bool use_complete_obs, bool compute_pvalues, int block_size, bool bforce, const std::string &output_filename, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_group, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_dataset_corr, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_dataset_pval, bool trans_x, bool trans_y, const Rcpp::Nullable&lt; int &gt; &threads)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#signature",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#signature",
    "title": "RcppbdCorr_hdf5_cross",
    "section": "",
    "text": "Rcpp::List BigDataStatMeth::RcppbdCorr_hdf5_cross(const std::string &filename_a, const std::string &strsubgroup_a, const std::string &strdataset_a, const std::string &filename_b, const std::string &strsubgroup_b, const std::string &strdataset_b, const std::string &method, bool use_complete_obs, bool compute_pvalues, int block_size, bool bforce, const std::string &output_filename, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_group, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_dataset_corr, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_dataset_pval, bool trans_x, bool trans_y, const Rcpp::Nullable&lt; int &gt; &threads)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#description",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#description",
    "title": "RcppbdCorr_hdf5_cross",
    "section": "2 Description",
    "text": "2 Description\nImplementation of cross-matrix correlation computation for HDF5 matrices - OPTIMIZED.",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#parameters",
    "title": "RcppbdCorr_hdf5_cross",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfilename_a (const std::string &): Path to HDF5 file containing first matrix\nstrsubgroup_a (const std::string &): Group path for first matrix\nstrdataset_a (const std::string &): Dataset name for first matrix\nfilename_b (const std::string &): Path to HDF5 file containing second matrix\nstrsubgroup_b (const std::string &): Group path for second matrix\nstrdataset_b (const std::string &): Dataset name for second matrix\nmethod (const std::string &): Correlation method (“pearson” or “spearman”)\nuse_complete_obs (bool): Whether to use only complete observations\ncompute_pvalues (bool): Whether to compute p-values\nblock_size (int): Block size for processing\nbforce (bool): Whether to overwrite existing results\noutput_filename (const std::string &): Output HDF5 file path\noutput_group (const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &): Custom output group name (nullable)\noutput_dataset_corr (const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &): Custom correlation dataset name (nullable)\noutput_dataset_pval (const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &): Custom p-values dataset name (nullable)\ntrans_x (bool): Whether to transpose first matrix\ntrans_y (bool): Whether to transpose second matrix\nthreads (const Rcpp::Nullable&lt; int &gt; &): Number of threads (nullable)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#returns",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#returns",
    "title": "RcppbdCorr_hdf5_cross",
    "section": "4 Returns",
    "text": "4 Returns\nRcpp::List containing dataset locations and computation metadata",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#details",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#details",
    "title": "RcppbdCorr_hdf5_cross",
    "section": "5 Details",
    "text": "5 Details\nOptimized implementation that computes cross-correlation matrix between two HDF5 datasets. Uses intelligent method selection and efficient memory management for optimal performance.",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#call-graph",
    "title": "RcppbdCorr_hdf5_cross",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#source-code",
    "title": "RcppbdCorr_hdf5_cross",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixCorrelation.hpp • Lines 1859-2053\ninline Rcpp::List RcppbdCorr_hdf5_cross(const std::string& filename_a, \n                                            const std::string& strsubgroup_a, \n                                            const std::string& strdataset_a,\n                                            const std::string& filename_b, \n                                            const std::string& strsubgroup_b, \n                                            const std::string& strdataset_b,\n                                            const std::string& method, \n                                            bool use_complete_obs,\n                                            bool compute_pvalues, \n                                            int block_size, \n                                            bool bforce,\n                                            const std::string& output_filename,\n                                            const Rcpp::Nullable&lt;Rcpp::CharacterVector&gt;& output_group,\n                                            const Rcpp::Nullable&lt;Rcpp::CharacterVector&gt;& output_dataset_corr,\n                                            const Rcpp::Nullable&lt;Rcpp::CharacterVector&gt;& output_dataset_pval,\n                                            bool trans_x,\n                                            bool trans_y,\n                                            const Rcpp::Nullable&lt;int&gt;& threads) {\n        \n        BigDataStatMeth::hdf5Dataset* dsA = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsB = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsCorr = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsPval = nullptr;\n        \n        try {\n            \n            // std::vector&lt;hsize_t&gt; stride = {1, 1}, block = {1, 1}, offset = {0, 0};\n            \n            // Open input datasets\n            dsA = new BigDataStatMeth::hdf5Dataset(filename_a, strsubgroup_a, strdataset_a, false);\n            dsA-&gt;openDataset();\n            \n            if (dsA-&gt;getDatasetptr() == nullptr) {\n                checkClose_file(dsA);\n                throw std::runtime_error(\"Failed to open first input dataset\");\n            }\n            \n            dsB = new BigDataStatMeth::hdf5Dataset(filename_b, strsubgroup_b, strdataset_b, false);\n            dsB-&gt;openDataset();\n            \n            if (dsB-&gt;getDatasetptr() == nullptr) {\n                checkClose_file(dsA, dsB);\n                throw std::runtime_error(\"Failed to open second input dataset\");\n            }\n            \n            // Get original dimensions\n            hsize_t n_rows_a_orig = dsA-&gt;nrows();\n            hsize_t n_cols_a_orig = dsA-&gt;ncols();\n            hsize_t n_rows_b_orig = dsB-&gt;nrows();\n            hsize_t n_cols_b_orig = dsB-&gt;ncols();\n            \n            // Determine effective dimensions after transposition\n            hsize_t n_rows_a = trans_x ? n_cols_a_orig : n_rows_a_orig;\n            hsize_t n_cols_a = trans_x ? n_rows_a_orig : n_cols_a_orig;\n            hsize_t n_rows_b = trans_y ? n_cols_b_orig : n_rows_b_orig;\n            hsize_t n_cols_b = trans_y ? n_rows_b_orig : n_cols_b_orig;\n            \n            // Validate dimensions\n            if (n_rows_a != n_rows_b) {\n                checkClose_file(dsA, dsB);\n                throw std::runtime_error(\"Matrices must have same number of observations after transposition\");\n            }\n            \n            // Determine output group and dataset names\n            std::string stroutgroup;\n            std::string corr_dataset_name;\n            std::string pval_dataset_name;\n            \n            \n            if (output_group.isNull()) {\n                std::string trans_suffix = \"\";\n                if (trans_x && !trans_y) trans_suffix = \"_TX\";\n                else if (!trans_x && trans_y) trans_suffix = \"_TY\";\n                else if (trans_x && trans_y) trans_suffix = \"_TXTY\";\n                \n                stroutgroup = \"CORR\" + trans_suffix + \"/\" + strdataset_a + \"_vs_\" + strdataset_b;\n            } else {\n                Rcpp::CharacterVector group_vec = Rcpp::as&lt;Rcpp::CharacterVector&gt;(output_group);\n                stroutgroup = Rcpp::as&lt;std::string&gt;(group_vec[0]);\n            }\n            \n            if (output_dataset_corr.isNull()) {\n                corr_dataset_name = \"correlation\";\n            } else {\n                Rcpp::CharacterVector corr_vec = Rcpp::as&lt;Rcpp::CharacterVector&gt;(output_dataset_corr);\n                corr_dataset_name = Rcpp::as&lt;std::string&gt;(corr_vec[0]);\n            }\n            \n            if (output_dataset_pval.isNull()) {\n                pval_dataset_name = \"pvalues\";\n            } else {\n                Rcpp::CharacterVector pval_vec = Rcpp::as&lt;Rcpp::CharacterVector&gt;(output_dataset_pval);\n                pval_dataset_name = Rcpp::as&lt;std::string&gt;(pval_vec[0]);\n            }\n            \n            // Create output datasets\n            try {\n                dsCorr = new BigDataStatMeth::hdf5Dataset(output_filename, stroutgroup, corr_dataset_name, bforce);\n                if (compute_pvalues) {\n                    dsPval = new BigDataStatMeth::hdf5Dataset(output_filename, stroutgroup, pval_dataset_name, bforce);\n                }\n            } catch (const std::exception& e) {\n                Rcpp::Rcerr &lt;&lt; \"Error creating output datasets: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n                checkClose_file(dsA, dsB, dsCorr, dsPval);\n                return R_NilValue;\n            }\n            \n            // Automatic method selection based on total matrix size\n            const hsize_t DIRECT_COMPUTATION_THRESHOLD = MAXELEMSINBLOCK / 4;\n            hsize_t total_elements = std::max(n_rows_a_orig * n_cols_a_orig, n_rows_b_orig * n_cols_b_orig);\n            \n            if (total_elements &lt; DIRECT_COMPUTATION_THRESHOLD) {\n                \n                // Direct computation for smaller matrices\n                std::vector&lt;double&gt; vdA(n_rows_a_orig * n_cols_a_orig);\n                std::vector&lt;double&gt; vdB(n_rows_b_orig * n_cols_b_orig);\n                \n                std::vector&lt;hsize_t&gt; stride = {1, 1}, block = {1, 1};\n                dsA-&gt;readDatasetBlock({0, 0}, {n_rows_a_orig, n_cols_a_orig}, stride, block, vdA.data());\n                dsB-&gt;readDatasetBlock({0, 0}, {n_rows_b_orig, n_cols_b_orig}, stride, block, vdB.data());\n                \n                Eigen::MatrixXd X = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt;(\n                    vdA.data(), n_rows_a_orig, n_cols_a_orig);\n                Eigen::MatrixXd Y = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt;(\n                    vdB.data(), n_rows_b_orig, n_cols_b_orig);\n                \n                corr_result result = RcppbdCorr_matrix_cross(X, Y, method, use_complete_obs, compute_pvalues, trans_x, trans_y, threads);\n                \n                if (!result.bcomputed) {\n                    checkClose_file(dsA, dsB, dsCorr, dsPval);\n                    throw std::runtime_error(\"Cross-correlation computation failed\");\n                }\n                \n                // Write results\n                if (dsCorr-&gt;getDatasetptr() == nullptr) {\n                    dsCorr-&gt;createDataset(result.correlation_matrix.rows(), result.correlation_matrix.cols(), \"real\");\n                }\n                dsCorr-&gt;writeDataset(Rcpp::wrap(result.correlation_matrix));\n                \n                if (compute_pvalues && result.has_pvalues && dsPval && result.pvalues.rows() &gt; 0 && result.pvalues.cols() &gt; 0) {\n                    if (dsPval-&gt;getDatasetptr() == nullptr) {\n                        dsPval-&gt;createDataset(result.pvalues.rows(), result.pvalues.cols(), \"real\");\n                    }\n                    dsPval-&gt;writeDataset(Rcpp::wrap(result.pvalues));\n                }\n                \n            } else {\n                \n                // Block-wise computation for large matrices\n                if (dsA-&gt;getDatasetptr() != nullptr && dsB-&gt;getDatasetptr() != nullptr) {\n                    RcppbdCorr_hdf5_Block_cross(dsA, dsB, dsCorr, dsPval, method, use_complete_obs, \n                                                compute_pvalues, block_size, trans_x, trans_y, threads);\n                }\n            }\n            \n            // Clean up datasets\n            delete dsA; dsA = nullptr;\n            delete dsB; dsB = nullptr;\n            delete dsCorr; dsCorr = nullptr;\n            delete dsPval; dsPval = nullptr;\n            \n            // Return comprehensive result list\n            return Rcpp::List::create(\n                Rcpp::Named(\"filename\") = output_filename,\n                Rcpp::Named(\"group\") = stroutgroup,\n                Rcpp::Named(\"correlation\") = corr_dataset_name,\n                Rcpp::Named(\"method\") = method,\n                Rcpp::Named(\"correlation_type\") = \"cross\",\n                Rcpp::Named(\"trans_x\") = trans_x,\n                Rcpp::Named(\"trans_y\") = trans_y,\n                Rcpp::Named(\"n_variables_x\") = (int)n_cols_a,\n                Rcpp::Named(\"n_variables_y\") = (int)n_cols_b,\n                Rcpp::Named(\"n_observations\") = (int)n_rows_a,\n                Rcpp::Named(\"pvalues\") = compute_pvalues ? pval_dataset_name : \"\",\n                Rcpp::Named(\"has_pvalues\") = compute_pvalues\n            );\n            \n        } catch(H5::FileIException& error) {\n            checkClose_file(dsA, dsB, dsCorr, dsPval);\n            Rcpp::Rcerr &lt;&lt; \"\\nC++ exception RcppbdCorr_hdf5_cross (File IException): \" &lt;&lt; error.getDetailMsg() &lt;&lt; std::endl;\n            return R_NilValue;\n        } catch(H5::DataSetIException& error) {\n            checkClose_file(dsA, dsB, dsCorr, dsPval);\n            Rcpp::Rcerr &lt;&lt; \"\\nC++ exception RcppbdCorr_hdf5_cross (DataSet IException): \" &lt;&lt; error.getDetailMsg() &lt;&lt; std::endl;\n            return R_NilValue;\n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsB, dsCorr, dsPval);\n            Rcpp::Rcerr &lt;&lt; \"C++ exception RcppbdCorr_hdf5_cross: \" &lt;&lt; ex.what() &lt;&lt; std::endl;\n            return R_NilValue;\n        } catch (...) {\n            checkClose_file(dsA, dsB, dsCorr, dsPval);\n            Rcpp::Rcerr &lt;&lt; \"\\nC++ exception RcppbdCorr_hdf5_cross (unknown reason)\" &lt;&lt; std::endl;\n            return R_NilValue;\n        }\n    }",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_cross.html#usage-example",
    "title": "RcppbdCorr_hdf5_cross",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdCorr_hdf5_cross(...);",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html",
    "title": "RcppbdCorr_hdf5_Block_cross",
    "section": "",
    "text": "void BigDataStatMeth::RcppbdCorr_hdf5_Block_cross(T *dsA, T *dsB, BigDataStatMeth::hdf5Dataset *dsCorr, BigDataStatMeth::hdf5Dataset *dsPval, const std::string &method=\"pearson\", bool use_complete_obs=true, bool compute_pvalues=false, int block_size=1000, bool trans_x=false, bool trans_y=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#signature",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#signature",
    "title": "RcppbdCorr_hdf5_Block_cross",
    "section": "",
    "text": "void BigDataStatMeth::RcppbdCorr_hdf5_Block_cross(T *dsA, T *dsB, BigDataStatMeth::hdf5Dataset *dsCorr, BigDataStatMeth::hdf5Dataset *dsPval, const std::string &method=\"pearson\", bool use_complete_obs=true, bool compute_pvalues=false, int block_size=1000, bool trans_x=false, bool trans_y=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#description",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#description",
    "title": "RcppbdCorr_hdf5_Block_cross",
    "section": "2 Description",
    "text": "2 Description\nEnhanced HDF5 cross-correlation with integrated transpose support for big-omics.",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#parameters",
    "title": "RcppbdCorr_hdf5_Block_cross",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (T *): First input HDF5 dataset\ndsB (T *): Second input HDF5 dataset\ndsCorr (BigDataStatMeth::hdf5Dataset *): Output HDF5 dataset for correlation matrix\ndsPval (BigDataStatMeth::hdf5Dataset *): Output HDF5 dataset for p-values (optional)\nmethod (const std::string &): Correlation method (“pearson” or “spearman”)\nuse_complete_obs (bool): Whether to use only complete observations\ncompute_pvalues (bool): Whether to compute p-values\nblock_size (int): Block size for processing\ntrans_x (bool): Whether to transpose first matrix\ntrans_y (bool): Whether to transpose second matrix\nthreads (Rcpp::Nullable&lt; int &gt;): Number of OpenMP threads",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#returns",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#returns",
    "title": "RcppbdCorr_hdf5_Block_cross",
    "section": "4 Returns",
    "text": "4 Returns\nType: class T",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#details",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#details",
    "title": "RcppbdCorr_hdf5_Block_cross",
    "section": "5 Details",
    "text": "5 Details\nEfficient implementation for cross-correlation between two HDF5-stored matrices with full transpose support while maintaining all block-wise optimizations critical for big-omics data. Uses modified I/O patterns and includes automatic mathematical optimization for cor(t(X), t(Y)) == cor(X,Y) case.",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#call-graph",
    "title": "RcppbdCorr_hdf5_Block_cross",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#source-code",
    "title": "RcppbdCorr_hdf5_Block_cross",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixCorrelation.hpp • Lines 1598-1826\ninline void RcppbdCorr_hdf5_Block_cross(T* dsA, T* dsB,\n                                            BigDataStatMeth::hdf5Dataset* dsCorr,\n                                            BigDataStatMeth::hdf5Dataset* dsPval,\n                                            const std::string& method = \"pearson\",\n                                            bool use_complete_obs = true,\n                                            bool compute_pvalues = false,\n                                            int block_size = 1000,\n                                            bool trans_x = false,\n                                            bool trans_y = false,\n                                            Rcpp::Nullable&lt;int&gt; threads = R_NilValue) {\n        \n        static_assert(std::is_same&lt;T*, BigDataStatMeth::hdf5Dataset*&gt;::value ||\n                      std::is_same&lt;T*, BigDataStatMeth::hdf5DatasetInternal*&gt;::value,\n                      \"Error - type not allowed\");\n        \n        try {\n            std::vector&lt;hsize_t&gt; stride = {1, 1}, block = {1, 1};\n            \n            // CRITICAL FIX: R→HDF5 data is implicitly transposed for both matrices\n            // HDF5 dimensions are NOT the \"real\" data dimensions from R\n            hsize_t n_rows_a_hdf5 = dsA-&gt;nrows();   // Variables in HDF5 for matrix A\n            hsize_t n_cols_a_hdf5 = dsA-&gt;ncols();   // Observations in HDF5 for matrix A\n            hsize_t n_rows_b_hdf5 = dsB-&gt;nrows();   // Variables in HDF5 for matrix B\n            hsize_t n_cols_b_hdf5 = dsB-&gt;ncols();   // Observations in HDF5 for matrix B\n            \n            // Real dimensions (as they were in R before saving to HDF5):\n            hsize_t n_obs_a_real = n_cols_a_hdf5;     // Real observations in A (samples)\n            hsize_t n_vars_a_real = n_rows_a_hdf5;    // Real variables in A (genes/features)\n            hsize_t n_obs_b_real = n_cols_b_hdf5;     // Real observations in B (samples)\n            hsize_t n_vars_b_real = n_rows_b_hdf5;    // Real variables in B (genes/features)\n            \n            // Apply user transpose logic to REAL dimensions\n            hsize_t n_rows_a = trans_x ? n_vars_a_real : n_obs_a_real;\n            hsize_t n_cols_a = trans_x ? n_obs_a_real : n_vars_a_real;\n            hsize_t n_rows_b = trans_y ? n_vars_b_real : n_obs_b_real;\n            hsize_t n_cols_b = trans_y ? n_obs_b_real : n_vars_b_real;\n            \n            // Validate dimensions\n            if (n_rows_a != n_rows_b) {\n                checkClose_file(dsA, dsB, dsCorr, dsPval);\n                throw std::runtime_error(\"Matrices must have same number of observations after transposition\");\n            }\n            \n            // hsize_t n_rows = n_rows_a;\n            \n            // Strategy selection\n            const hsize_t MEMORY_LIMIT = 250000; // Conservative for cross-correlation\n            \n            if (n_rows_a_hdf5 * n_cols_a_hdf5 &lt; MEMORY_LIMIT && n_rows_b_hdf5 * n_cols_b_hdf5 &lt; MEMORY_LIMIT) {\n                \n                // Memory-resident computation\n                std::vector&lt;double&gt; matrix_a_data(n_rows_a_hdf5 * n_cols_a_hdf5);\n                std::vector&lt;double&gt; matrix_b_data(n_rows_b_hdf5 * n_cols_b_hdf5);\n                \n                dsA-&gt;readDatasetBlock({0, 0}, {n_rows_a_hdf5, n_cols_a_hdf5}, stride, block, matrix_a_data.data());\n                dsB-&gt;readDatasetBlock({0, 0}, {n_rows_b_hdf5, n_cols_b_hdf5}, stride, block, matrix_b_data.data());\n                \n                // CRITICAL FIX: Create Eigen matrices accounting for R→HDF5 transposition\n                // Use OPTIMIZED strategy: NO explicit transpose, invert user logic instead\n                Eigen::MatrixXd X = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt;(\n                    matrix_a_data.data(), n_rows_a_hdf5, n_cols_a_hdf5);\n                Eigen::MatrixXd Y = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt;(\n                    matrix_b_data.data(), n_rows_b_hdf5, n_cols_b_hdf5);\n                \n                // PERFORMANCE OPTIMIZATION: Instead of transposing matrices, invert user intention\n                // Data in HDF5 is transposed compared to R, so invert transpose flags\n                bool effective_trans_x = !trans_x;\n                bool effective_trans_y = !trans_y;\n                \n                corr_result result = RcppbdCorr_matrix_cross(X, Y, method, use_complete_obs, compute_pvalues, effective_trans_x, effective_trans_y, threads);\n                \n                if (!result.bcomputed) {\n                    checkClose_file(dsA, dsB, dsCorr, dsPval);\n                    throw std::runtime_error(\"Cross-correlation computation failed\");\n                }\n                \n                dsCorr-&gt;createDataset(result.correlation_matrix.rows(), result.correlation_matrix.cols(), \"real\");\n                dsCorr-&gt;writeDataset(Rcpp::wrap(result.correlation_matrix));\n                \n                if (compute_pvalues && result.has_pvalues && dsPval && result.pvalues.rows() &gt; 0 && result.pvalues.cols() &gt; 0) {\n                    dsPval-&gt;createDataset(result.pvalues.rows(), result.pvalues.cols(), \"real\");\n                    dsPval-&gt;writeDataset(Rcpp::wrap(result.pvalues));\n                }\n                \n                \n            } else {\n                \n                // BIG-OMICS: Block-wise processing with intelligent batching\n                Eigen::MatrixXd corr_matrix = Eigen::MatrixXd::Zero(n_cols_a, n_cols_b);\n                \n                // Intelligent threading and batching for big-omics\n                \n                int batch_size_a = 1;\n                int batch_size_b = 1;\n                \n        #ifdef _OPENMP\n                int num_threads = 1;\n                if (!threads.isNull()) {\n                    num_threads = Rcpp::as&lt;int&gt;(threads);\n                } else {\n                    hsize_t total_correlations = n_cols_a * n_cols_b;\n                    \n                    if (total_correlations &lt; 10000) {\n                        num_threads = 1;\n                        batch_size_a = 1;\n                        batch_size_b = 1;\n                    } else if (total_correlations &lt; 1000000) {\n                        num_threads = std::min(4, omp_get_max_threads());\n                        batch_size_a = std::min((hsize_t)20, n_cols_a);\n                        batch_size_b = std::min((hsize_t)50, n_cols_b);\n                    } else {\n                        // Very large matrices: aggressive batching\n                        num_threads = std::min(8, omp_get_max_threads());\n                        batch_size_a = std::min((hsize_t)50, n_cols_a);\n                        batch_size_b = std::min((hsize_t)100, n_cols_b);\n                    }\n                }\n                num_threads = std::max(1, std::min(num_threads, omp_get_max_threads()));\n        // #endif\n                \n                // Process A in batches\n        // #ifdef _OPENMP\n        #pragma omp parallel for num_threads(num_threads) schedule(dynamic, 1)\n        #endif\n                for (hsize_t i_start = 0; i_start &lt; n_cols_a; i_start += batch_size_a) {\n                    hsize_t i_end = std::min(i_start + batch_size_a, n_cols_a);\n                    hsize_t batch_cols_a = i_end - i_start;\n                    \n                    // Read batch from A\n                    std::vector&lt;double&gt; batch_a_data(n_rows_a * batch_cols_a);\n                    \n                    // CRITICAL FIX: Transpose-aware I/O patterns for HDF5 data (R→HDF5 transposition)\n                    if (trans_x) {\n                        // User wants correlation between samples from A\n                        // In HDF5: samples are columns, so read columns as batch\n                        for (hsize_t b_idx = 0; b_idx &lt; batch_cols_a; ++b_idx) {\n                            hsize_t actual_i = i_start + b_idx;\n                            std::vector&lt;double&gt; col_data(n_rows_a);\n                            dsA-&gt;readDatasetBlock({0, actual_i}, {n_rows_a_hdf5, 1}, stride, block, col_data.data());\n                            std::copy(col_data.begin(), col_data.end(), \n                                      batch_a_data.begin() + b_idx * n_rows_a);\n                        }\n                    } else {\n                        // User wants correlation between variables from A (default)\n                        // In HDF5: variables are rows, so read rows as batch\n                        for (hsize_t b_idx = 0; b_idx &lt; batch_cols_a; ++b_idx) {\n                            hsize_t actual_i = i_start + b_idx;\n                            std::vector&lt;double&gt; row_data(n_rows_a);\n                            dsA-&gt;readDatasetBlock({actual_i, 0}, {1, n_cols_a_hdf5}, stride, block, row_data.data());\n                            std::copy(row_data.begin(), row_data.end(), \n                                      batch_a_data.begin() + b_idx * n_rows_a);\n                        }\n                    }\n                    \n                    // Process B in batches for each A batch\n                    for (hsize_t j_start = 0; j_start &lt; n_cols_b; j_start += batch_size_b) {\n                        hsize_t j_end = std::min(j_start + batch_size_b, n_cols_b);\n                        hsize_t batch_cols_b = j_end - j_start;\n                        \n                        // Read batch from B\n                        std::vector&lt;double&gt; batch_b_data(n_rows_b * batch_cols_b);\n                        \n                        // FINAL CORRECTION: Swap logic based on test results\n                        if (trans_y) {\n                            // User wants correlation with samples from B\n                            // Read columns as batch (this currently gives correct result)\n                            for (hsize_t b_idx = 0; b_idx &lt; batch_cols_b; ++b_idx) {\n                                hsize_t actual_j = j_start + b_idx;\n                                std::vector&lt;double&gt; col_data(n_rows_b);\n                                dsB-&gt;readDatasetBlock({0, actual_j}, {n_rows_b_hdf5, 1}, stride, block, col_data.data());\n                                std::copy(col_data.begin(), col_data.end(), \n                                          batch_b_data.begin() + b_idx * n_rows_b);\n                            }\n                        } else {\n                            // User wants correlation with variables from B (default - same as cor(ds))\n                            // Read rows as batch (swap to make this give cor(ds) result)\n                            for (hsize_t b_idx = 0; b_idx &lt; batch_cols_b; ++b_idx) {\n                                hsize_t actual_j = j_start + b_idx;\n                                std::vector&lt;double&gt; row_data(n_rows_b);\n                                dsB-&gt;readDatasetBlock({actual_j, 0}, {1, n_cols_b_hdf5}, stride, block, row_data.data());\n                                std::copy(row_data.begin(), row_data.end(), \n                                          batch_b_data.begin() + b_idx * n_rows_b);\n                            }\n                        }\n                        \n                        // Compute correlations for entire batch block\n                        for (hsize_t i_idx = 0; i_idx &lt; batch_cols_a; ++i_idx) {\n                            hsize_t i = i_start + i_idx;\n                            \n                            Eigen::VectorXd vec_a = Eigen::Map&lt;Eigen::VectorXd&gt;(\n                                batch_a_data.data() + i_idx * n_rows_a, n_rows_a);\n                            \n                            for (hsize_t j_idx = 0; j_idx &lt; batch_cols_b; ++j_idx) {\n                                hsize_t j = j_start + j_idx;\n                                \n                                Eigen::VectorXd vec_b = Eigen::Map&lt;Eigen::VectorXd&gt;(\n                                    batch_b_data.data() + j_idx * n_rows_a, n_rows_a);  // Use n_rows_a since dimensions must match\n                                \n                                double corr_val;\n                                if (method == \"spearman\") {\n                                    corr_val = spearman_correlation(vec_a, vec_b, use_complete_obs);\n                                } else {\n                                    corr_val = pearson_correlation(vec_a, vec_b, use_complete_obs);\n                                }\n                                \n                                corr_matrix(i, j) = corr_val;\n                            }\n                        }\n                    }\n                }\n                \n                // Write correlation matrix\n                dsCorr-&gt;createDataset(corr_matrix.rows(), corr_matrix.cols(), \"real\");\n                dsCorr-&gt;writeDataset(Rcpp::wrap(corr_matrix));\n                \n                // VECTORIZED P-VALUES: Compute on complete correlation matrix\n                if (compute_pvalues && dsPval) {\n                    Eigen::MatrixXd pvalues_matrix = compute_pvalues_optimized(corr_matrix, n_rows_a, false);  // Use n_rows_a (effective observations)\n                    dsPval-&gt;createDataset(pvalues_matrix.rows(), pvalues_matrix.cols(), \"real\");\n                    dsPval-&gt;writeDataset(Rcpp::wrap(pvalues_matrix));\n                }\n            }\n            \n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsB, dsCorr, dsPval);\n            Rcpp::Rcerr &lt;&lt; \"C++ exception RcppbdCorr_hdf5_Block_cross: \" &lt;&lt; ex.what() &lt;&lt; std::endl;\n            throw;\n        }\n    }",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.html#usage-example",
    "title": "RcppbdCorr_hdf5_Block_cross",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdCorr_hdf5_Block_cross(...);",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_cross"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html",
    "title": "Rcpp_vector_subtract_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_subtract_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#signature",
    "title": "Rcpp_vector_subtract_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_subtract_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#description",
    "title": "Rcpp_vector_subtract_hdf5",
    "section": "2 Description",
    "text": "2 Description\nPure vector subtraction for HDF5 vectors.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#parameters",
    "title": "Rcpp_vector_subtract_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input vector dataset (minuend)\ndsB (BigDataStatMeth::hdf5Dataset *): Second input vector dataset (subtrahend)\ndsC (BigDataStatMeth::hdf5Dataset *): Output vector dataset\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#returns",
    "title": "Rcpp_vector_subtract_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nPointer to result dataset",
    "crumbs": [
      "Functions",
      "Rcpp_vector_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#details",
    "title": "Rcpp_vector_subtract_hdf5",
    "section": "5 Details",
    "text": "5 Details\nPerforms optimized element-wise subtraction C = A - B where A, B, and C are HDF5 vector datasets. Uses same optimization strategy as vector addition.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#call-graph",
    "title": "Rcpp_vector_subtract_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_vector_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#source-code",
    "title": "Rcpp_vector_subtract_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectorOperations.hpp • Lines 189-249\ninline BigDataStatMeth::hdf5Dataset* Rcpp_vector_subtract_hdf5(\n            BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, BigDataStatMeth::hdf5Dataset* dsC,\n            bool bparal, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n    {\n        try {\n            hsize_t sizeA = validateVector(dsA);\n            hsize_t sizeB = validateVector(dsB);\n            \n            if (sizeA == 0 || sizeB == 0) {\n                Rcpp::Rcout &lt;&lt; \"vector subtract error: inputs are not vectors\\n\";\n                return dsC;\n            }\n            \n            if (sizeA != sizeB) {\n                Rcpp::Rcout &lt;&lt; \"vector subtract error: non-conformable vector dimensions\\n\";\n                return dsC;\n            }\n            \n            hsize_t rowsA = dsA-&gt;nrows();\n            hsize_t colsA = dsA-&gt;ncols();\n            dsC-&gt;createDataset(colsA, rowsA, \"real\");\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1};\n            std::vector&lt;hsize_t&gt; block = {1, 1};\n            \n            std::vector&lt;double&gt; vdA(sizeA);\n            std::vector&lt;double&gt; vdB(sizeB);\n            \n            dsA-&gt;readDatasetBlock({0, 0}, {rowsA, colsA}, stride, block, vdA.data());\n            dsB-&gt;readDatasetBlock({0, 0}, {dsB-&gt;nrows(), dsB-&gt;ncols()}, stride, block, vdB.data());\n            \n            if (bparal && sizeA &gt; 10000) {\n    #pragma omp parallel num_threads(get_threads(bparal, threads))\n    {\n    #pragma omp for schedule(static)\n        for (hsize_t i = 0; i &lt; sizeA; ++i) {\n            vdA[i] -= vdB[i];\n        }\n    }\n            } else {\n                std::transform(vdA.begin(), vdA.end(), vdB.begin(), vdA.begin(), std::minus&lt;double&gt;());\n            }\n            \n            dsC-&gt;writeDatasetBlock(vdA, {0, 0}, {rowsA, colsA}, stride, block);\n            \n        } catch(H5::FileIException& error) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_subtract_hdf5 (File IException)\";\n            // return dsC;\n        } catch(H5::DataSetIException& error) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_subtract_hdf5 (DataSet IException)\";\n            // return dsC;\n        } catch(std::exception& ex) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_subtract_hdf5: \" &lt;&lt; ex.what();\n            // return dsC;\n        }\n        \n        return dsC;\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_vector_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.html#usage-example",
    "title": "Rcpp_vector_subtract_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_vector_subtract_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_vector_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html",
    "title": "Rcpp_vector_power_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_power_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_power_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#signature",
    "title": "Rcpp_vector_power_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_power_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_power_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#description",
    "title": "Rcpp_vector_power_hdf5",
    "section": "2 Description",
    "text": "2 Description\nPure vector division for HDF5 vectors.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_power_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#parameters",
    "title": "Rcpp_vector_power_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input vector dataset (dividend)\ndsB (BigDataStatMeth::hdf5Dataset *): Second input vector dataset (divisor)\ndsC (BigDataStatMeth::hdf5Dataset *): Output vector dataset\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_power_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#returns",
    "title": "Rcpp_vector_power_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nPointer to result dataset",
    "crumbs": [
      "Functions",
      "Rcpp_vector_power_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#details",
    "title": "Rcpp_vector_power_hdf5",
    "section": "5 Details",
    "text": "5 Details\nPerforms optimized element-wise power C = A ^ B where A, B, and C are HDF5 vector datasets. Includes division by zero protection.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_power_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#call-graph",
    "title": "Rcpp_vector_power_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_vector_power_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#source-code",
    "title": "Rcpp_vector_power_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectorOperations.hpp • Lines 417-477\ninline BigDataStatMeth::hdf5Dataset* Rcpp_vector_power_hdf5(\n            BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, BigDataStatMeth::hdf5Dataset* dsC,\n            bool bparal, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n    {\n        try {\n            hsize_t sizeA = validateVector(dsA);\n            hsize_t sizeB = validateVector(dsB);\n            \n            if (sizeA == 0 || sizeB == 0) {\n                Rcpp::Rcout &lt;&lt; \"vector power error: inputs are not vectors\\n\";\n                return dsC;\n            }\n            \n            if (sizeA != sizeB) {\n                Rcpp::Rcout &lt;&lt; \"vector power error: non-conformable vector dimensions\\n\";\n                return dsC;\n            }\n            \n            hsize_t rowsA = dsA-&gt;nrows();\n            hsize_t colsA = dsA-&gt;ncols();\n            dsC-&gt;createDataset(colsA, rowsA, \"real\");\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1};\n            std::vector&lt;hsize_t&gt; block = {1, 1};\n            \n            std::vector&lt;double&gt; vdA(sizeA);\n            std::vector&lt;double&gt; vdB(sizeB);\n            \n            dsA-&gt;readDatasetBlock({0, 0}, {rowsA, colsA}, stride, block, vdA.data());\n            dsB-&gt;readDatasetBlock({0, 0}, {dsB-&gt;nrows(), dsB-&gt;ncols()}, stride, block, vdB.data());\n            \n            if (bparal && sizeA &gt; 10000) {\n                #pragma omp parallel num_threads(get_threads(bparal, threads))\n                {\n                #pragma omp for schedule(static)\n                    for (hsize_t i = 0; i &lt; sizeA; ++i) {\n                        vdA[i] = std::pow(vdA[i], vdB[i]);\n                    }\n                }\n            } else {\n                std::transform(vdA.begin(), vdA.end(), vdB.begin(), vdA.begin(), std::divides&lt;double&gt;());\n            }\n            \n            dsC-&gt;writeDatasetBlock(vdA, {0, 0}, {rowsA, colsA}, stride, block);\n            \n        } catch(H5::FileIException& error) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_power_hdf5 (File IException)\";\n            // return dsC;\n        } catch(H5::DataSetIException& error) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_power_hdf5 (DataSet IException)\";\n            // return dsC;\n        } catch(std::exception& ex) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_power_hdf5: \" &lt;&lt; ex.what();\n            // return dsC;\n        }\n        \n        return dsC;\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_vector_power_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_vector_power_hdf5.html#usage-example",
    "title": "Rcpp_vector_power_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_vector_power_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_vector_power_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_mult.html",
    "href": "api-reference/cpp/functions/Rcpp_vector_mult.html",
    "title": "Rcpp_vector_mult",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_vector_mult(T A, T B)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_mult.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_vector_mult.html#signature",
    "title": "Rcpp_vector_mult",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_vector_mult(T A, T B)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_mult.html#description",
    "href": "api-reference/cpp/functions/Rcpp_vector_mult.html#description",
    "title": "Rcpp_vector_mult",
    "section": "2 Description",
    "text": "2 Description\nVector multiplication (dot product)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_mult.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_vector_mult.html#parameters",
    "title": "Rcpp_vector_mult",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (T): First input vector\nB (T): Second input vector",
    "crumbs": [
      "Functions",
      "Rcpp_vector_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_mult.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_vector_mult.html#returns",
    "title": "Rcpp_vector_mult",
    "section": "4 Returns",
    "text": "4 Returns\nScalar result of vector dot product",
    "crumbs": [
      "Functions",
      "Rcpp_vector_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_mult.html#details",
    "href": "api-reference/cpp/functions/Rcpp_vector_mult.html#details",
    "title": "Rcpp_vector_mult",
    "section": "5 Details",
    "text": "5 Details\nComputes the dot product of two vectors with dimension validation.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_mult.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_vector_mult.html#source-code",
    "title": "Rcpp_vector_mult",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memMultiplication.hpp • Lines 356-374\ninline Rcpp::RObject Rcpp_vector_mult ( T  A, T  B)\n    {\n        \n        Rcpp::NumericVector v = Rcpp::as&lt;Rcpp::NumericVector&gt;(A);\n        Rcpp::NumericVector v2 = Rcpp::as&lt;Rcpp::NumericVector&gt;(B);\n        \n        if(v.size() == v2.size()) {\n            Rcpp::NumericVector C = Rcpp::no_init( v.size());\n            \n            std::transform (v.begin(), v.end(), v2.begin(), C.begin(), std::multiplies&lt;double&gt;());\n            \n            C.attr(\"dim\") = Rcpp::Dimension( C.size(), 1); \n            \n            return(C);\n        }\n        \n        return(R_NilValue);\n        \n    }",
    "crumbs": [
      "Functions",
      "Rcpp_vector_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_mult.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_vector_mult.html#usage-example",
    "title": "Rcpp_vector_mult",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_vector_mult(...);",
    "crumbs": [
      "Functions",
      "Rcpp_vector_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html",
    "title": "Rcpp_vector_add_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_add_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#signature",
    "title": "Rcpp_vector_add_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_add_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#description",
    "title": "Rcpp_vector_add_hdf5",
    "section": "2 Description",
    "text": "2 Description\nPure vector addition for HDF5 vectors.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#parameters",
    "title": "Rcpp_vector_add_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input vector dataset\ndsB (BigDataStatMeth::hdf5Dataset *): Second input vector dataset\ndsC (BigDataStatMeth::hdf5Dataset *): Output vector dataset\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#returns",
    "title": "Rcpp_vector_add_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nPointer to result dataset",
    "crumbs": [
      "Functions",
      "Rcpp_vector_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#details",
    "title": "Rcpp_vector_add_hdf5",
    "section": "5 Details",
    "text": "5 Details\nPerforms optimized element-wise addition C = A + B where A, B, and C are HDF5 vector datasets. ~100x more efficient than matrix-vector approaches by avoiding unnecessary block processing and vector duplication.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#call-graph",
    "title": "Rcpp_vector_add_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_vector_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#source-code",
    "title": "Rcpp_vector_add_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectorOperations.hpp • Lines 103-175\ninline BigDataStatMeth::hdf5Dataset* Rcpp_vector_add_hdf5(\n            BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, BigDataStatMeth::hdf5Dataset* dsC,\n            bool bparal, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n    {\n        try {\n            // Validate inputs are vectors\n            hsize_t sizeA = validateVector(dsA);\n            hsize_t sizeB = validateVector(dsB);\n            \n            if (sizeA == 0 || sizeB == 0) {\n                Rcpp::Rcout &lt;&lt; \"vector add error: inputs are not vectors\\n\";\n                return dsC;\n            }\n            \n            if (sizeA != sizeB) {\n                Rcpp::Rcout &lt;&lt; \"vector add error: non-conformable vector dimensions\\n\";\n                return dsC;\n            }\n            \n            // Create output vector with same dimensions as input\n            hsize_t rowsA = dsA-&gt;nrows();\n            hsize_t colsA = dsA-&gt;ncols();\n            dsC-&gt;createDataset(colsA, rowsA, \"real\");\n            \n            // Direct vector I/O - no block processing needed\n            std::vector&lt;hsize_t&gt; stride = {1, 1};\n            std::vector&lt;hsize_t&gt; block = {1, 1};\n            \n            std::vector&lt;double&gt; vdA(sizeA);\n            std::vector&lt;double&gt; vdB(sizeB);\n            \n            // Read entire vectors in single operations\n            dsA-&gt;readDatasetBlock({0, 0}, {rowsA, colsA}, stride, block, vdA.data());\n            dsB-&gt;readDatasetBlock({0, 0}, {dsB-&gt;nrows(), dsB-&gt;ncols()}, stride, block, vdB.data());\n            \n            // Perform element-wise addition using STL transform (highly optimized)\n            if (bparal && sizeA &gt; 10000) {\n                // Parallel processing for large vectors\n    #pragma omp parallel num_threads(get_threads(bparal, threads))\n    {\n    #pragma omp for schedule(static)\n        for (hsize_t i = 0; i &lt; sizeA; ++i) {\n            vdA[i] += vdB[i];\n        }\n    }\n            } else {\n                // Sequential processing (optimal for smaller vectors)\n                std::transform(vdA.begin(), vdA.end(), vdB.begin(), vdA.begin(), std::plus&lt;double&gt;());\n            }\n            \n            // Write result in single operation\n            dsC-&gt;writeDatasetBlock(vdA, {0, 0}, {rowsA, colsA}, stride, block);\n            \n        } catch(H5::FileIException& error) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_add_hdf5 (File IException)\";\n            // return dsC;\n        } catch(H5::DataSetIException& error) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_add_hdf5 (DataSet IException)\";\n            // return dsC;\n        } catch(std::exception& ex) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_add_hdf5: \" &lt;&lt; ex.what();\n            // return dsC;\n        } catch (...) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nC++ exception Rcpp_vector_add_hdf5 (unknown reason)\";\n            // return dsC;\n        }\n        \n        return dsC;\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_vector_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_vector_add_hdf5.html#usage-example",
    "title": "Rcpp_vector_add_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_vector_add_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_vector_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html",
    "title": "Rcpp_matrix_vector_blockSubstract",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_vector_blockSubstract(T A, T B, Rcpp::Nullable&lt; bool &gt; bparal, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#signature",
    "title": "Rcpp_matrix_vector_blockSubstract",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_vector_blockSubstract(T A, T B, Rcpp::Nullable&lt; bool &gt; bparal, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#description",
    "title": "Rcpp_matrix_vector_blockSubstract",
    "section": "2 Description",
    "text": "2 Description\nBlock-based matrix-vector subtraction.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#parameters",
    "title": "Rcpp_matrix_vector_blockSubstract",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (T): Input matrix\nB (T): Input vector (subtracted from A)\nbparal (Rcpp::Nullable&lt; bool &gt;): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel computation",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#returns",
    "title": "Rcpp_matrix_vector_blockSubstract",
    "section": "4 Returns",
    "text": "4 Returns\nResult of matrix-vector subtraction",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#details",
    "title": "Rcpp_matrix_vector_blockSubstract",
    "section": "5 Details",
    "text": "5 Details\nImplements block-based matrix-vector subtraction with optional parallel processing.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#call-graph",
    "title": "Rcpp_matrix_vector_blockSubstract",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#source-code",
    "title": "Rcpp_matrix_vector_blockSubstract",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memSubstract.hpp • Lines 352-457\ninline Rcpp::RObject Rcpp_matrix_vector_blockSubstract( T  A, T  B,  \n                                Rcpp::Nullable&lt;bool&gt; bparal, Rcpp::Nullable&lt;int&gt; threads)\n    {\n        \n        // NOTA: Per defecte, suma per columnes tal i com raja.... \n        \n        // static_assert(std::is_same&lt;T, Eigen::MatrixXd &gt;::value || \n        //               std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n        //               std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value,\n        //               \"Error - type not allowed\");\n        \n        bool btransposed = false;\n        \n        Rcpp::NumericMatrix X = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(A);\n        Rcpp::NumericVector Y = Rcpp::as&lt;Rcpp::NumericVector&gt;(B);\n        Rcpp::NumericMatrix C;\n        \n        // Matrix\n        hsize_t M = X.rows(),\n                N = X.cols();\n        \n        // Vector\n        hsize_t K = Y.length();\n        \n        try {\n            \n            // unsigned int ithreads;\n            hsize_t block_size;\n            \n            if( K==N || K==M) {\n                if ( K == N){\n                    // Sum vector to every col\n                    btransposed = true;\n                    \n                    X = Rcpp::transpose(X);\n                    \n                    //.. Revisar-ho comentat 2024/04/06 ..// hsize_t N = X.rows();\n                    //.. Revisar-ho comentat 2024/04/06 ..// hsize_t M = X.cols();\n                    N = X.rows();\n                    M = X.cols();\n                } \n                \n                std::vector&lt;hsize_t&gt; vsizetoRead;\n                std::vector&lt;hsize_t&gt; vstart;\n                \n                // ithreads = get_number_threads(threads, bparal);\n                \n                C = Rcpp::no_init( M, N);\n                \n                block_size = getMatrixBlockSize( N, M).at(0);\n                \n                // minimum block size: 2 columns\n                if(block_size &lt;= 0 ) {\n                    block_size = M*2;\n                }\n                \n                // Mínimum block size: 2 columns\n                getBlockPositionsSizes( M*N, block_size, vstart, vsizetoRead );\n                // int chunks = vstart.size()/ithreads;\n                \n                #pragma omp parallel num_threads( get_number_threads(threads, bparal) ) shared(A, B, C) //, chunks)\n                {\n                #pragma omp for schedule (dynamic) // collapse(2)\n                    for (hsize_t ii = 0; ii &lt; vstart.size(); ii ++)\n                    {\n                        // Duplicate vector\n                        std::size_t const no_of_duplicates = vsizetoRead[ii] / Y.length();\n                        \n                        std::vector&lt;double&gt; v = Rcpp::as&lt;std::vector&lt;double&gt; &gt;(Y); \n                        v.reserve(Y.size() * no_of_duplicates);\n                        auto end = std::end(v);\n                        \n                        for(std::size_t i = 1; i &lt; no_of_duplicates; ++i)\n                            v.insert(std::end(v), std::begin(v), end);\n                        \n                        // Sum vector to matrix by columns / rows\n                        if( vstart[ii] + vsizetoRead[ii] &gt;= M*N ) {\n                            std::transform (X.begin() + vstart[ii], X.end(),\n                                            v.begin(), C.begin() + vstart[ii], std::minus&lt;double&gt;());\n                        } else {\n                            std::transform (X.begin() + vstart[ii], X.begin() + vstart[ii] + vsizetoRead[ii],\n                                            v.begin() , C.begin() + vstart[ii], std::minus&lt;double&gt;());   \n                        }\n                    }\n                }\n    \n            } else {\n                \n                Rcpp::Rcout&lt;&lt; \"vector sum error: non-conformable arguments\\n\";\n                return(R_NilValue);\n            }\n            \n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception Rcpp_matrix_vector_blockSubstract: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n            return(R_NilValue);\n        }\n        \n        if(btransposed == true){\n            Rcpp::transpose(C);\n        } \n        \n        C.attr(\"dim\") = Rcpp::Dimension( M, N);\n        \n        return(C);\n        \n    }",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.html#usage-example",
    "title": "Rcpp_matrix_vector_blockSubstract",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrix_vector_blockSubstract(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_sum.html",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_sum.html",
    "title": "Rcpp_matrix_vect_sum",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_vect_sum(T A, U B)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_sum.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_sum.html#signature",
    "title": "Rcpp_matrix_vect_sum",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_vect_sum(T A, U B)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_sum.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_sum.html#parameters",
    "title": "Rcpp_matrix_vect_sum",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nA (T)\nB (U)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_sum.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_sum.html#returns",
    "title": "Rcpp_matrix_vect_sum",
    "section": "3 Returns",
    "text": "3 Returns\nType: typename T",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_sum.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_sum.html#source-code",
    "title": "Rcpp_matrix_vect_sum",
    "section": "4 Source Code",
    "text": "4 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memSum.hpp • Lines 121-150\ninline Rcpp::RObject Rcpp_matrix_vect_sum ( T  A, U  B)\n    {\n        \n        Rcpp::NumericMatrix m = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(A);\n        Rcpp::NumericVector v = Rcpp::as&lt;Rcpp::NumericVector&gt;(B);\n        \n        if( v.length() == m.rows()) {\n            \n            Rcpp::NumericMatrix C = Rcpp::no_init( m.rows(), m.cols());\n            \n            for( int i=0; i&lt;m.cols(); i++) {\n                C( Rcpp::_, i) = m( Rcpp::_, i) + v;  \n            }    \n            return(C);\n            \n        } else if( v.length() == m.cols()) {\n            \n            Rcpp::NumericMatrix C = Rcpp::no_init( m.rows(), m.cols());\n            \n            for( int i=0; i&lt;m.rows(); i++) {\n                C( i, Rcpp::_) = m( i, Rcpp::_) + v;  \n            }    \n            return(C);\n            \n        } else {\n            Rcpp::Rcout&lt;&lt;\"Error: non-conformable arguments\";\n        }\n        \n        return(R_NilValue);\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_sum.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_sum.html#usage-example",
    "title": "Rcpp_matrix_vect_sum",
    "section": "5 Usage Example",
    "text": "5 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrix_vect_sum(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html",
    "title": "Rcpp_matrix_vect_mult",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_vect_mult(T A, U B)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#signature",
    "title": "Rcpp_matrix_vect_mult",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_vect_mult(T A, U B)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#description",
    "title": "Rcpp_matrix_vect_mult",
    "section": "2 Description",
    "text": "2 Description\nMatrix-vector multiplication.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#parameters",
    "title": "Rcpp_matrix_vect_mult",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (T): Input matrix\nB (U): Input vector",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#returns",
    "title": "Rcpp_matrix_vect_mult",
    "section": "4 Returns",
    "text": "4 Returns\nResult of matrix-vector multiplication",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#details",
    "title": "Rcpp_matrix_vect_mult",
    "section": "5 Details",
    "text": "5 Details\nPerforms matrix-vector multiplication with dimension validation and efficient memory handling.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#source-code",
    "title": "Rcpp_matrix_vect_mult",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memMultiplication.hpp • Lines 313-342\ninline Rcpp::RObject Rcpp_matrix_vect_mult ( T  A, U  B)\n    {\n        \n        Rcpp::NumericMatrix m = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(A);\n        Rcpp::NumericVector v = Rcpp::as&lt;Rcpp::NumericVector&gt;(B);\n        \n        if( v.length() == m.rows()) {\n            \n            Rcpp::NumericMatrix C = Rcpp::no_init( m.rows(), m.cols());\n            \n            for( int i=0; i&lt;m.cols(); i++) {\n                C( Rcpp::_, i) = m( Rcpp::_, i) * v;  \n            }    \n            return(C);\n            \n        } else if( v.length() == m.cols()) {\n            \n            Rcpp::NumericMatrix C = Rcpp::no_init( m.rows(), m.cols());\n            \n            for( int i=0; i&lt;m.rows(); i++) {\n                C( i, Rcpp::_) = m( i, Rcpp::_) * v;  \n            }    \n            return(C);\n            \n        } else {\n            Rcpp::Rcout&lt;&lt;\"Error: non-conformable arguments\";\n        }\n        \n        return(R_NilValue);\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_mult.html#usage-example",
    "title": "Rcpp_matrix_vect_mult",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrix_vect_mult(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_mult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_substract.html",
    "href": "api-reference/cpp/functions/Rcpp_matrix_substract.html",
    "title": "Rcpp_matrix_substract",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_substract(T A, T B)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_substract.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrix_substract.html#signature",
    "title": "Rcpp_matrix_substract",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_substract(T A, T B)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_substract.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrix_substract.html#description",
    "title": "Rcpp_matrix_substract",
    "section": "2 Description",
    "text": "2 Description\nMatrix subtraction.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_substract.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrix_substract.html#parameters",
    "title": "Rcpp_matrix_substract",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (T): First input matrix\nB (T): Second input matrix (subtracted from A)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_substract.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrix_substract.html#returns",
    "title": "Rcpp_matrix_substract",
    "section": "4 Returns",
    "text": "4 Returns\nResult of matrix subtraction (A - B)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_substract.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrix_substract.html#details",
    "title": "Rcpp_matrix_substract",
    "section": "5 Details",
    "text": "5 Details\nSubtracts two matrices element-wise.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_substract.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrix_substract.html#source-code",
    "title": "Rcpp_matrix_substract",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memSubstract.hpp • Lines 146-165\ninline Rcpp::RObject Rcpp_matrix_substract ( T  A, T  B)\n    {\n        \n        \n        Rcpp::NumericMatrix m = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(A);\n        Rcpp::NumericMatrix m2 = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(B);\n        \n        if( m.rows() == m2.rows() && m.cols() == m2.cols()) {\n            Rcpp::NumericVector C = m - m2;\n            // C.attr(\"dim\") = Rcpp::Dimension( m.rows(), m.cols());\n            \n            return(C);\n            \n        } else {\n            Rcpp::Rcout&lt;&lt;\"Error: non-conformable arguments\";\n        }\n        \n        return(R_NilValue);\n        \n    }",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_substract.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrix_substract.html#usage-example",
    "title": "Rcpp_matrix_substract",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrix_substract(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html",
    "title": "Rcpp_matrix_blockSubstract",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_blockSubstract(T A, T B, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#signature",
    "title": "Rcpp_matrix_blockSubstract",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_blockSubstract(T A, T B, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#description",
    "title": "Rcpp_matrix_blockSubstract",
    "section": "2 Description",
    "text": "2 Description\nBlock-based matrix subtraction.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#parameters",
    "title": "Rcpp_matrix_blockSubstract",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (T): First input matrix\nB (T): Second input matrix (subtracted from A)\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel computation",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#returns",
    "title": "Rcpp_matrix_blockSubstract",
    "section": "4 Returns",
    "text": "4 Returns\nResult of matrix subtraction (A - B)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#details",
    "title": "Rcpp_matrix_blockSubstract",
    "section": "5 Details",
    "text": "5 Details\nImplements block-based matrix subtraction with optional parallel processing.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#call-graph",
    "title": "Rcpp_matrix_blockSubstract",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#source-code",
    "title": "Rcpp_matrix_blockSubstract",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memSubstract.hpp • Lines 256-334\ninline Rcpp::RObject Rcpp_matrix_blockSubstract ( T  A, T  B, Rcpp::Nullable&lt;int&gt; threads)\n    {\n        \n        // static_assert(std::is_same&lt;T, Eigen::MatrixXd &gt;::value || \n        //               std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n        //               std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value ,\n        //               \"Error - type not allowed\");\n        \n        Rcpp::NumericMatrix X = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(A);\n        Rcpp::NumericMatrix Y = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(B);\n        \n        hsize_t N = X.rows();\n        hsize_t M = X.cols();\n        \n        Rcpp::NumericMatrix C = Rcpp::no_init( N, M);\n        \n        try {\n            \n            // unsigned int ithreads;\n            hsize_t block_size; \n            \n            std::vector&lt;hsize_t&gt; vsizetoRead;\n            std::vector&lt;hsize_t&gt; vstart;\n            \n            std::vector&lt;hsize_t&gt; blockSize = getMatrixBlockSize( N, M);\n            \n            if(N &lt; M) {\n                block_size = blockSize.at(0);    \n            } else {\n                block_size = blockSize.at(1);\n            }\n            \n            if(block_size &gt; 0 ) {\n                \n                // if( N == Y.rows() && M == Y.cols())\n                if (N == static_cast&lt;hsize_t&gt;(Y.rows()) && M == static_cast&lt;hsize_t&gt;(Y.cols())) \n                {\n                    \n                    // ithreads = get_number_threads(threads, R_NilValue);\n                    \n                    getBlockPositionsSizes( N*M, block_size, vstart, vsizetoRead );\n                    // int chunks = vstart.size()/ithreads;\n                    \n                    #pragma omp parallel num_threads( get_number_threads(threads, R_NilValue) ) shared(A, B, C) //, chunks)\n                    {\n                    #pragma omp for schedule (dynamic)\n                        for (hsize_t ii = 0; ii &lt; vstart.size(); ii ++)\n                        {\n                            \n                            if( vstart[ii] + vsizetoRead[ii] &gt;= N*M ) {\n                                std::transform (X.begin() + vstart[ii], X.end(),\n                                                Y.begin() + vstart[ii], C.begin() + vstart[ii], std::minus&lt;double&gt;());\n                            } else {\n                                std::transform (X.begin() + vstart[ii], X.begin() + vstart[ii] + vsizetoRead[ii],\n                                                Y.begin() + vstart[ii], C.begin() + vstart[ii], std::minus&lt;double&gt;());   \n                            }\n                        }\n                        \n                    }\n    \n                } else {\n                    Rcpp::Rcout&lt;&lt;\"matrix sum error: non-conformable arguments\\n\";\n                    return(R_NilValue);\n                }\n                \n            } else{\n                Rcpp::Rcout&lt;&lt;\"matrix sum error: Error whent computing block sizes\\n\";\n                return(R_NilValue);\n            }\n            \n        } catch(std::exception &ex) {\n            Rcpp::Rcout&lt;&lt; ex.what();\n            return(R_NilValue);\n        }\n        \n        C.attr(\"dim\") = Rcpp::Dimension( N, M);\n        return(C);\n        \n    }",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSubstract.html#usage-example",
    "title": "Rcpp_matrix_blockSubstract",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrix_blockSubstract(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSubstract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html",
    "title": "Rcpp_matrixVectorSum_byCol",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorSum_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#signature",
    "title": "Rcpp_matrixVectorSum_byCol",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorSum_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#description",
    "title": "Rcpp_matrixVectorSum_byCol",
    "section": "2 Description",
    "text": "2 Description\nMatrix-vector addition by columns.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#parameters",
    "title": "Rcpp_matrixVectorSum_byCol",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (Eigen::MatrixXd): Input matrix\nv (Eigen::VectorXd): Input vector",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#returns",
    "title": "Rcpp_matrixVectorSum_byCol",
    "section": "4 Returns",
    "text": "4 Returns\nResult of column-wise addition",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#details",
    "title": "Rcpp_matrixVectorSum_byCol",
    "section": "5 Details",
    "text": "5 Details\nAdds a vector to each column of a matrix.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#caller-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#caller-graph",
    "title": "Rcpp_matrixVectorSum_byCol",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#source-code",
    "title": "Rcpp_matrixVectorSum_byCol",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectormatrix.hpp • Lines 136-139\ninline Eigen::MatrixXd Rcpp_matrixVectorSum_byCol(Eigen::MatrixXd X, Eigen::VectorXd v) {\n    X = X.array().rowwise() + v.transpose().array();    \n    return(X);\n}",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.html#usage-example",
    "title": "Rcpp_matrixVectorSum_byCol",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrixVectorSum_byCol(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html",
    "title": "Rcpp_matrixVectorSubstract_byCol",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorSubstract_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#signature",
    "title": "Rcpp_matrixVectorSubstract_byCol",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorSubstract_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#description",
    "title": "Rcpp_matrixVectorSubstract_byCol",
    "section": "2 Description",
    "text": "2 Description\nMatrix-vector subtraction by columns.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#parameters",
    "title": "Rcpp_matrixVectorSubstract_byCol",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (Eigen::MatrixXd): Input matrix\nv (Eigen::VectorXd): Input vector",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#returns",
    "title": "Rcpp_matrixVectorSubstract_byCol",
    "section": "4 Returns",
    "text": "4 Returns\nResult of column-wise subtraction",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#details",
    "title": "Rcpp_matrixVectorSubstract_byCol",
    "section": "5 Details",
    "text": "5 Details\nSubtracts a vector from each column of a matrix.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#caller-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#caller-graph",
    "title": "Rcpp_matrixVectorSubstract_byCol",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#source-code",
    "title": "Rcpp_matrixVectorSubstract_byCol",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectormatrix.hpp • Lines 149-152\ninline Eigen::MatrixXd Rcpp_matrixVectorSubstract_byCol(Eigen::MatrixXd X, Eigen::VectorXd v) {\n    X = X.array().rowwise() - v.transpose().array();    \n    return(X);\n}",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.html#usage-example",
    "title": "Rcpp_matrixVectorSubstract_byCol",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrixVectorSubstract_byCol(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html",
    "title": "Rcpp_matrixVectorPow_byRow",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorPow_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPow_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#signature",
    "title": "Rcpp_matrixVectorPow_byRow",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorPow_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPow_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#description",
    "title": "Rcpp_matrixVectorPow_byRow",
    "section": "2 Description",
    "text": "2 Description\nMatrix-vector power by rows.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPow_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#parameters",
    "title": "Rcpp_matrixVectorPow_byRow",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (Eigen::MatrixXd): Input matrix\nv (Eigen::VectorXd): Input vector",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPow_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#returns",
    "title": "Rcpp_matrixVectorPow_byRow",
    "section": "4 Returns",
    "text": "4 Returns\nResult of row-wise power",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPow_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#details",
    "title": "Rcpp_matrixVectorPow_byRow",
    "section": "5 Details",
    "text": "5 Details\nDivides each row of a matrix by a vector element-wise.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPow_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#caller-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#caller-graph",
    "title": "Rcpp_matrixVectorPow_byRow",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPow_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#source-code",
    "title": "Rcpp_matrixVectorPow_byRow",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectormatrix.hpp • Lines 105-112\ninline Eigen::MatrixXd Rcpp_matrixVectorPow_byRow(Eigen::MatrixXd X, Eigen::VectorXd v) {\n    for (int col = 0; col &lt; X.cols(); ++col) {\n        X.col(col) = X.col(col).array().unaryExpr(\n            [&v, col](double x) { return std::pow(x, v(col)); }\n        );\n    }\n    return(X);\n}",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPow_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.html#usage-example",
    "title": "Rcpp_matrixVectorPow_byRow",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrixVectorPow_byRow(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPow_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html",
    "title": "Rcpp_matrixVectorMultiplication_byCol",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorMultiplication_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#signature",
    "title": "Rcpp_matrixVectorMultiplication_byCol",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorMultiplication_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#description",
    "title": "Rcpp_matrixVectorMultiplication_byCol",
    "section": "2 Description",
    "text": "2 Description\nMatrix-vector multiplication by columns.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#parameters",
    "title": "Rcpp_matrixVectorMultiplication_byCol",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (Eigen::MatrixXd): Input matrix\nv (Eigen::VectorXd): Input vector",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#returns",
    "title": "Rcpp_matrixVectorMultiplication_byCol",
    "section": "4 Returns",
    "text": "4 Returns\nResult of column-wise multiplication",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#details",
    "title": "Rcpp_matrixVectorMultiplication_byCol",
    "section": "5 Details",
    "text": "5 Details\nMultiplies each column of a matrix by a vector element-wise.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#caller-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#caller-graph",
    "title": "Rcpp_matrixVectorMultiplication_byCol",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#source-code",
    "title": "Rcpp_matrixVectorMultiplication_byCol",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectormatrix.hpp • Lines 123-126\ninline Eigen::MatrixXd Rcpp_matrixVectorMultiplication_byCol(Eigen::MatrixXd X, Eigen::VectorXd v) {\n    X = X.array().rowwise() * v.transpose().array();    \n    return(X);\n}",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.html#usage-example",
    "title": "Rcpp_matrixVectorMultiplication_byCol",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrixVectorMultiplication_byCol(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html",
    "title": "Rcpp_matrixVectorDivision_byCol",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorDivision_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#signature",
    "title": "Rcpp_matrixVectorDivision_byCol",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorDivision_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#description",
    "title": "Rcpp_matrixVectorDivision_byCol",
    "section": "2 Description",
    "text": "2 Description\nMatrix-vector division by columns.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#parameters",
    "title": "Rcpp_matrixVectorDivision_byCol",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (Eigen::MatrixXd): Input matrix\nv (Eigen::VectorXd): Input vector",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#returns",
    "title": "Rcpp_matrixVectorDivision_byCol",
    "section": "4 Returns",
    "text": "4 Returns\nResult of column-wise division",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#details",
    "title": "Rcpp_matrixVectorDivision_byCol",
    "section": "5 Details",
    "text": "5 Details\nDivides each column of a matrix by a vector element-wise.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#caller-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#caller-graph",
    "title": "Rcpp_matrixVectorDivision_byCol",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#source-code",
    "title": "Rcpp_matrixVectorDivision_byCol",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectormatrix.hpp • Lines 162-165\ninline Eigen::MatrixXd Rcpp_matrixVectorDivision_byCol(Eigen::MatrixXd X, Eigen::VectorXd v) {\n    X = X.array().rowwise() / v.transpose().array();    \n    return(X);\n}",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.html#usage-example",
    "title": "Rcpp_matrixVectorDivision_byCol",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrixVectorDivision_byCol(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html",
    "title": "Rcpp_block_matrix_vector_substract_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_block_matrix_vector_substract_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#signature",
    "title": "Rcpp_block_matrix_vector_substract_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_block_matrix_vector_substract_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#description",
    "title": "Rcpp_block_matrix_vector_substract_hdf5",
    "section": "2 Description",
    "text": "2 Description\nBlock-based matrix-vector subtraction for HDF5 matrices.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#parameters",
    "title": "Rcpp_block_matrix_vector_substract_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): Input vector dataset\ndsB (BigDataStatMeth::hdf5Dataset *): Input matrix dataset\ndsC (BigDataStatMeth::hdf5Dataset *): Output matrix dataset\nhdf5_block (hsize_t): Block size for HDF5 I/O operations\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#returns",
    "title": "Rcpp_block_matrix_vector_substract_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nPointer to result dataset",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#details",
    "title": "Rcpp_block_matrix_vector_substract_hdf5",
    "section": "5 Details",
    "text": "5 Details\nPerforms block-based matrix-vector subtraction where one operand is a vector and the other is a matrix. Supports both row and column vectors.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#call-graph",
    "title": "Rcpp_block_matrix_vector_substract_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#source-code",
    "title": "Rcpp_block_matrix_vector_substract_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSubstract.hpp • Lines 206-350\ninline BigDataStatMeth::hdf5Dataset* Rcpp_block_matrix_vector_substract_hdf5( \n            BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, BigDataStatMeth::hdf5Dataset* dsC,\n            hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt;int&gt; threads  = R_NilValue)\n    {\n        \n        try {\n            \n            // Vector\n            hsize_t K = dsA-&gt;nrows();\n            hsize_t N = dsA-&gt;ncols();\n            \n            // Matrix\n            hsize_t M = dsB-&gt;nrows();\n            hsize_t L = dsB-&gt;ncols();\n            \n            std::vector&lt;hsize_t&gt; vstart, vsizetoRead;\n            // unsigned int ithreads;\n            \n            if(hdf5_block == 1) {\n                hdf5_block = ceil(MAXBLOCKSIZE/(K*N));\n            }\n            \n            // hsize_t isize = hdf5_block + 1;\n            std::vector&lt;hsize_t&gt; stride = {1, 1},\n                                 block = {1, 1};\n            \n            dsC-&gt;createDataset( L, M, \"real\");\n            \n            std::vector&lt;double&gt; vdA( K * N );\n            dsA-&gt;readDatasetBlock( {0, 0}, {K, N}, stride, block, vdA.data() );\n            \n            // ithreads = get_threads(bparal, threads);\n            \n            if(  K == M )\n            { // Sum vector to every col\n                \n                getBlockPositionsSizes( L, hdf5_block, vstart, vsizetoRead );\n                // int chunks = vstart.size()/ithreads;\n                \n                #pragma omp parallel num_threads( get_threads(bparal, threads) ) shared(dsA, dsB, dsC) //, chunks)\n                {\n                    #pragma omp for schedule (dynamic) // collapse(2)\n                    for (hsize_t ii = 0; ii &lt; vstart.size(); ii ++)\n                    {\n                        std::vector&lt;double&gt; vdB( K * vsizetoRead[ii] );\n                        #pragma omp critical (accessFile)\n                        {\n                            dsB-&gt;readDatasetBlock( {0, vstart[ii]}, {K, vsizetoRead[ii]}, stride, block, vdB.data() );\n                        }\n                        \n                        // Duplicate vector\n                        std::size_t const no_of_duplicates = (K * vsizetoRead[ii]) / vdA.size();\n                        \n                        std::vector&lt;double&gt; v = vdA; \n                        v.reserve(vdA.size() * no_of_duplicates);\n                        auto end = std::end(v);\n                        \n                        for(std::size_t i = 1; i &lt; no_of_duplicates; ++i)\n                            v.insert(std::end(v), std::begin(v), end);\n                        \n                        // Sum vector to matrix by columns / rows\n                        std::transform (vdB.begin(), vdB.end(),\n                                        v.begin(), vdB.begin(), std::minus&lt;double&gt;());\n                        \n                        \n                        std::vector&lt;hsize_t&gt; offset = { 0, vstart[ii]};\n                        std::vector&lt;hsize_t&gt; count = { K, vsizetoRead[ii]};\n                        \n                        #pragma omp critical (accessFile)\n                        {\n                            dsC-&gt;writeDatasetBlock( vdB, offset, count, stride, block);\n                        }\n                    }\n                }\n    \n            } else if(  K == L ) { // Sum vector to every row\n                \n                getBlockPositionsSizes( M, hdf5_block, vstart, vsizetoRead );\n                // int chunks = vstart.size()/ithreads;\n                \n                #pragma omp parallel num_threads( get_threads(bparal, threads) ) shared(dsA, dsB, dsC) //, chunks)\n                {\n                    #pragma omp for schedule (dynamic) // collapse(2)\n                    for (hsize_t ii = 0; ii &lt; vstart.size(); ii ++)\n                    {\n                        std::vector&lt;double&gt; vdB( L * vsizetoRead[ii] );\n                        #pragma omp critical (accessFile)\n                        {\n                            dsB-&gt;readDatasetBlock( {vstart[ii], 0}, {vsizetoRead[ii], K}, stride, block, vdB.data() );\n                        }\n                        Rcpp::NumericMatrix B (vsizetoRead[ii], K, vdB.begin());\n                        \n                        Rcpp::transpose(B);\n                        \n                        // Duplicate vector\n                        std::size_t const no_of_duplicates = (vsizetoRead[ii] * K) / vdA.size();\n                        \n                        std::vector&lt;double&gt; v = vdA; \n                        v.reserve(v.size() * no_of_duplicates);\n                        auto end = std::end(v);\n                        \n                        for(std::size_t i = 1; i &lt; no_of_duplicates; ++i)\n                            v.insert(std::end(v), std::begin(v), end);\n                        \n                        // Sum vector to matrix by columns / rows\n                        std::transform (B.begin(), B.end(),\n                                        v.begin(), B.begin() , std::minus&lt;double&gt;());\n                        \n                        std::vector&lt;hsize_t&gt; offset = { vstart[ii], 0};\n                        std::vector&lt;hsize_t&gt; count = { vsizetoRead[ii], K};\n                        \n                        #pragma omp critical (accessFile)\n                        {\n                            dsC-&gt;writeDatasetBlock( vdB, offset, count, stride, block);\n                        }\n                    }\n                }\n            } else {\n                Rcpp::Rcout&lt;&lt; \"vector substract error: non-conformable arguments\\n\";\n            }\n            \n        } catch( H5::FileIException& error ) { \n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_vector_substract_hdf5 (File IException)\";\n            return(dsC);\n        } catch( H5::GroupIException & error ) { \n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_vector_substract_hdf5 (Group IException)\";\n            return(dsC);\n        } catch( H5::DataSetIException& error ) { \n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_vector_substract_hdf5 (DataSet IException)\";\n            return(dsC);\n        } catch(std::exception& ex) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_vector_substract_hdf5 \" &lt;&lt; ex.what();\n            return(dsC);\n        } catch (...) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception Rcpp_block_matrix_vector_substract_hdf5 (unknown reason)\";\n            return(dsC);\n        }\n        \n        return(dsC);\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.html#usage-example",
    "title": "Rcpp_block_matrix_vector_substract_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_block_matrix_vector_substract_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html",
    "title": "Rcpp_block_matrix_sum_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_block_matrix_sum_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#signature",
    "title": "Rcpp_block_matrix_sum_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_block_matrix_sum_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#description",
    "title": "Rcpp_block_matrix_sum_hdf5",
    "section": "2 Description",
    "text": "2 Description\nBlock-based matrix addition for HDF5 matrices.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#parameters",
    "title": "Rcpp_block_matrix_sum_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input matrix dataset\ndsB (BigDataStatMeth::hdf5Dataset *): Second input matrix dataset\ndsC (BigDataStatMeth::hdf5Dataset *): Output matrix dataset\nhdf5_block (hsize_t): Block size for HDF5 I/O operations\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#returns",
    "title": "Rcpp_block_matrix_sum_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nPointer to result dataset",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#details",
    "title": "Rcpp_block_matrix_sum_hdf5",
    "section": "5 Details",
    "text": "5 Details\nPerforms block-based matrix addition C = A + B where A, B, and C are HDF5 datasets. Optimized for large matrices with parallel processing.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#call-graph",
    "title": "Rcpp_block_matrix_sum_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#source-code",
    "title": "Rcpp_block_matrix_sum_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSum.hpp • Lines 70-188\ninline BigDataStatMeth::hdf5Dataset*  Rcpp_block_matrix_sum_hdf5( \n            BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, BigDataStatMeth::hdf5Dataset* dsC,\n            hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt;int&gt; threads  = R_NilValue)\n    {\n        \n        try {\n            \n            hsize_t K = dsA-&gt;nrows();\n            hsize_t N = dsA-&gt;ncols();\n            \n            if( K == dsB-&gt;nrows() && N == dsB-&gt;ncols())\n            {\n                \n                // Parallellization and Block variables \n                // unsigned int ithreads;\n                std::vector&lt;hsize_t&gt; vstart, vsizetoRead;\n                std::vector&lt;hsize_t&gt; stride = {1, 1};\n                std::vector&lt;hsize_t&gt; block = {1, 1};\n                \n                dsC-&gt;createDataset( N, K, \"real\"); \n                \n                // ithreads = get_threads(bparal, threads);\n                \n                if( K&lt;=N ) {\n                    \n                    getBlockPositionsSizes( N, hdf5_block, vstart, vsizetoRead );\n                    // int chunks = vstart.size()/ithreads;\n                    \n                    #pragma omp parallel num_threads( get_threads(bparal, threads) ) shared(dsA, dsB, dsC) //, chunks)\n                    {\n                    #pragma omp for schedule (dynamic)\n                        for (hsize_t ii = 0; ii &lt; vstart.size(); ii ++)\n                        {\n                            \n                            std::vector&lt;double&gt; vdA( K * vsizetoRead[ii] ); \n                            #pragma omp critical(accessFile)\n                            {\n                                dsA-&gt;readDatasetBlock( {0, vstart[ii]}, { K, vsizetoRead[ii]}, stride, block, vdA.data() );\n                            }\n                            \n                            std::vector&lt;double&gt; vdB( K * vsizetoRead[ii] ); \n                            #pragma omp critical(accessFile)\n                            {\n                                dsB-&gt;readDatasetBlock( {0, vstart[ii]}, {K, vsizetoRead[ii]}, stride, block, vdB.data() );\n                            }\n                            std::transform (vdA.begin(), vdA.end(),\n                                            vdB.begin(), vdA.begin(), std::plus&lt;double&gt;());\n                            \n                            std::vector&lt;hsize_t&gt; offset = { 0, vstart[ii] };\n                            std::vector&lt;hsize_t&gt; count = { K, vsizetoRead[ii] };\n                            #pragma omp critical(accessFile) \n                            {\n                                dsC-&gt;writeDatasetBlock(vdA, offset, count, stride, block);\n                            }\n                        }\n                    }\n                    \n                } else {\n                    \n                    getBlockPositionsSizes( K, hdf5_block, vstart, vsizetoRead );\n                    // int chunks = vstart.size()/ithreads;\n                    \n                    #pragma omp parallel num_threads( get_threads(bparal, threads) ) shared(dsA, dsB, dsC) //, chunks)\n                    {\n                    #pragma omp for schedule (dynamic)\n                        for (hsize_t ii = 0; ii &lt; vstart.size(); ii++)\n                        {\n                            std::vector&lt;double&gt; vdA( vsizetoRead[ii] * N ); \n                            #pragma omp critical(accessFile)\n                            {\n                                dsA-&gt;readDatasetBlock( {vstart[ii], 0}, { vsizetoRead[ii], N}, stride, block, vdA.data() );\n                            }\n                            \n                            std::vector&lt;double&gt; vdB( vsizetoRead[ii] * N); \n                            #pragma omp critical(accessFile)\n                            {\n                                dsB-&gt;readDatasetBlock( {vstart[ii], 0}, {vsizetoRead[ii], N}, stride, block, vdB.data() );\n                            }\n                            \n                            std::transform (vdA.begin(), vdA.end(),\n                                            vdB.begin(), vdA.begin(), std::plus&lt;double&gt;());\n                            \n                            std::vector&lt;hsize_t&gt; offset = { vstart[ii], 0 };\n                            std::vector&lt;hsize_t&gt; count = { vsizetoRead[ii], N };\n                            #pragma omp critical \n                            {\n                                dsC-&gt;writeDatasetBlock(vdA, offset, count, stride, block);\n                            }\n                        }\n                    }\n                }\n            } else {\n                Rcpp::Rcout&lt;&lt;\"matrix sum error: non-conformable arguments\\n\";\n            }\n            \n        } catch( H5::FileIException& error ) { // catch failure caused by the H5File operations\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_sum_hdf5 (File IException)\";\n            return(dsC);\n        } catch( H5::GroupIException & error ) { // catch failure caused by the DataSet operations\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_sum_hdf5 (Group IException)\";\n            return(dsC);\n        } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_sum_hdf5 (DataSet IException)\";\n            return(dsC);\n        } catch(std::exception& ex) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_sum_hdf5: \" &lt;&lt; ex.what();\n            return(dsC);\n        } catch (...) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception Rcpp_block_matrix_sum_hdf5 (unknown reason)\";\n            return(dsC);\n        }\n        \n        return(dsC);\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.html#usage-example",
    "title": "Rcpp_block_matrix_sum_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_block_matrix_sum_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html",
    "title": "Rcpp_block_matrix_mul_parallel",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_block_matrix_mul_parallel(T X, U Y, bool transpX, bool transpY, Rcpp::Nullable&lt; int &gt; iblock_size, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#signature",
    "title": "Rcpp_block_matrix_mul_parallel",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_block_matrix_mul_parallel(T X, U Y, bool transpX, bool transpY, Rcpp::Nullable&lt; int &gt; iblock_size, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#description",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#description",
    "title": "Rcpp_block_matrix_mul_parallel",
    "section": "2 Description",
    "text": "2 Description\nParallel block-based matrix multiplication.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#parameters",
    "title": "Rcpp_block_matrix_mul_parallel",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (T): First input matrix\nY (U): Second input matrix\ntranspX (bool): Whether to transpose X before multiplication\ntranspY (bool): Whether to transpose Y before multiplication\niblock_size (Rcpp::Nullable&lt; int &gt;): Block size for computation\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#returns",
    "title": "Rcpp_block_matrix_mul_parallel",
    "section": "4 Returns",
    "text": "4 Returns\nResult of matrix multiplication",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#details",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#details",
    "title": "Rcpp_block_matrix_mul_parallel",
    "section": "5 Details",
    "text": "5 Details\nImplements parallel block-based matrix multiplication with configurable block sizes and thread count. This function:Supports matrix transposition before multiplicationUses OpenMP for parallel processingImplements cache-friendly block-based algorithmHandles edge cases for non-uniform block sizes",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#call-graph",
    "title": "Rcpp_block_matrix_mul_parallel",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#source-code",
    "title": "Rcpp_block_matrix_mul_parallel",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memMultiplication.hpp • Lines 212-298\ninline Eigen::MatrixXd Rcpp_block_matrix_mul_parallel( T X, U Y, \n                                                                  bool transpX,\n                                                                  bool transpY,\n                                                                  Rcpp::Nullable&lt;int&gt;  iblock_size, \n                                                                  Rcpp::Nullable&lt;int&gt; threads  = R_NilValue)\n    {\n        \n        Eigen::MatrixXd C;\n        try{\n            static_assert(std::is_same&lt;T, Eigen::MatrixXd &gt;::value || \n                          std::is_same&lt;T, Eigen::Map&lt; Eigen::MatrixXd &gt;&gt;::value || \n                          std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n                          std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value,\n                          \"Error - type not allowed\");\n            \n            static_assert(std::is_same&lt;U, Eigen::MatrixXd &gt;::value || \n                          std::is_same&lt;U, Eigen::Map&lt; Eigen::MatrixXd &gt;&gt;::value || \n                          std::is_same&lt;U, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n                          std::is_same&lt;U, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value,\n                          \"Error - type not allowed\");\n            \n            Eigen::MatrixXd A = X,\n                B = Y;\n            \n            if(transpX == true){ A = X.transpose(); }\n            if(transpY == true){ B = Y.transpose(); }\n            \n            // int chunks;//, tid;\n            hsize_t block_size;\n            \n            std::vector&lt;hsize_t&gt; vsizetoReadN, vstartN,\n            vsizetoReadM, vstartM,\n            vsizetoReadK, vstartK;\n            \n            // unsigned int ithreads;\n            hsize_t M = A.rows();\n            hsize_t K = A.cols();\n            hsize_t N = B.cols();\n            \n            if( iblock_size.isNotNull()) {\n                block_size =  Rcpp::as&lt;int&gt;(iblock_size);  \n            } else {\n                block_size =  MAXBLOCKSIZE/3;  \n            }\n            \n            C = Eigen::MatrixXd::Zero(M,N) ;\n            if(block_size &gt; std::min( N, std::min(M,K)) )\n                block_size = std::min( N, std::min(M,K)); \n            \n            getBlockPositionsSizes( N, block_size, vstartN, vsizetoReadN );\n            getBlockPositionsSizes( M, block_size, vstartM, vsizetoReadM );\n            getBlockPositionsSizes( K, block_size, vstartK, vsizetoReadK );\n            \n            // CAMBIO 1: Eliminar collapse(2) para compatibilidad\n            const int total_blocks = static_cast&lt;int&gt;(vstartM.size() * vstartN.size());\n            \n            #pragma omp parallel num_threads( get_number_threads(threads, R_NilValue) ) shared(A, B, C)\n            {\n                // CAMBIO 2: Usar bucle plano en lugar de collapse(2)\n            #pragma omp for schedule(dynamic)\n                for (int block_idx = 0; block_idx &lt; total_blocks; block_idx++)\n                {\n                    // Convertir índice plano a coordenadas (ii, jj)\n                    const int ii = block_idx / static_cast&lt;int&gt;(vstartN.size());\n                    const int jj = block_idx % static_cast&lt;int&gt;(vstartN.size());\n                    \n                    // CAMBIO 3: Buffer local para evitar race condition (CLAVE para eliminar NaN)\n                    Eigen::MatrixXd C_local = Eigen::MatrixXd::Zero(vsizetoReadM[ii], vsizetoReadN[jj]);\n                    \n                    // CAMBIO 4: Bucle k secuencial dentro del hilo (elimina race condition)\n                    for(int kk = 0; kk &lt; static_cast&lt;int&gt;(vstartK.size()); kk++)\n                    {\n                        // Acumulamos en buffer local (sin race condition)\n                        C_local += (A.block(vstartM[ii], vstartK[kk], vsizetoReadM[ii], vsizetoReadK[kk]) * \n                            B.block(vstartK[kk], vstartN[jj], vsizetoReadK[kk], vsizetoReadN[jj]));\n                    }\n                    \n                    // CAMBIO 5: Una sola escritura al final (sin race condition)\n                    C.block(vstartM[ii], vstartN[jj], vsizetoReadM[ii], vsizetoReadN[jj]) = C_local;\n                }\n            }\n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception Rcpp_block_matrix_mul_parallel: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n        }\n        \n        return(C);\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.html#usage-example",
    "title": "Rcpp_block_matrix_mul_parallel",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_block_matrix_mul_parallel(...);",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html",
    "title": "Rcpp_Remove_MAF_hdf5",
    "section": "",
    "text": "int BigDataStatMeth::Rcpp_Remove_MAF_hdf5(BigDataStatMeth::hdf5Dataset *dsIn, BigDataStatMeth::hdf5Dataset *dsOut, bool bycols, double pcent, int blocksize)",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_MAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#signature",
    "title": "Rcpp_Remove_MAF_hdf5",
    "section": "",
    "text": "int BigDataStatMeth::Rcpp_Remove_MAF_hdf5(BigDataStatMeth::hdf5Dataset *dsIn, BigDataStatMeth::hdf5Dataset *dsOut, bool bycols, double pcent, int blocksize)",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_MAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#description",
    "title": "Rcpp_Remove_MAF_hdf5",
    "section": "2 Description",
    "text": "2 Description\nRemoves rows or columns from a dataset based on MAF threshold.",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_MAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#parameters",
    "title": "Rcpp_Remove_MAF_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsIn (BigDataStatMeth::hdf5Dataset *): Input HDF5 dataset containing genomic data\ndsOut (BigDataStatMeth::hdf5Dataset *): Output HDF5 dataset for filtered data\nbycols (bool): If true, process by columns; if false, process by rows\npcent (double): MAF threshold percentage (0.0 to 1.0)\nblocksize (int): Number of rows/columns to process in each block",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_MAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#returns",
    "title": "Rcpp_Remove_MAF_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nint Number of removed rows/columns (negative) or error code",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_MAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#details",
    "title": "Rcpp_Remove_MAF_hdf5",
    "section": "5 Details",
    "text": "5 Details\ndsInInput HDF5 dataset containing genomic data dsOutOutput HDF5 dataset for filtered data bycolsIf true, process by columns; if false, process by rows pcentMAF threshold percentage (0.0 to 1.0) blocksizeNumber of rows/columns to process in each block int Number of removed rows/columns (negative) or error code Implementation approach:Processes data in blocks to manage memory usageDynamically creates and extends output datasetSupports both row-wise and column-wise filteringPreserves data structure while removing low-MAF variants",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_MAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#call-graph",
    "title": "Rcpp_Remove_MAF_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_MAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#source-code",
    "title": "Rcpp_Remove_MAF_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Omics/hdf5RemoveMAF.hpp • Lines 83-238\ninline int Rcpp_Remove_MAF_hdf5( BigDataStatMeth::hdf5Dataset* dsIn, \n                            BigDataStatMeth::hdf5Dataset* dsOut, \n                            bool bycols, double pcent, int blocksize)\n    {\n        \n        int itotrem = 0;\n        \n        try{\n        \n            bool bcreated = false;    \n            int ilimit;\n            \n            std::vector&lt;hsize_t&gt; offset = {0,0},\n                                 count = {0,0},\n                                 stride = {1,1},\n                                 block = {1,1},\n                                 newoffset = {0,0};\n            \n            // Real data set dimension\n            hsize_t* dims_out = dsIn-&gt;dim();\n            \n            // id bycols == true : read all rows by group of columns ; else : all columns by group of rows\n            if (bycols == true) {\n                ilimit = dims_out[0];\n                count[1] = dims_out[1];\n                offset[1] = 0;\n            } else {\n                ilimit = dims_out[1];\n                count[0] = dims_out[0];\n                offset[0] = 0;\n            }\n            \n            for( int i=0; i &lt;= (ilimit/blocksize); i++) \n            {\n                int iread;\n                int iblockrem = 0;\n                \n                if( (i+1)*blocksize &lt; ilimit) iread = blocksize;\n                else iread = ilimit - (i*blocksize);\n                \n                if(bycols == true) {\n                    count[0] = iread; \n                    offset[0] = i*blocksize;\n                } else {\n                    count[1] = iread; \n                    offset[1] = i*blocksize;\n                }\n                \n                // read block\n                std::vector&lt;double&gt; vdCurDataset( count[0] * count[1] ); \n                dsIn-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdCurDataset.data() );\n                Eigen::MatrixXd data = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vdCurDataset.data(), count[0], count[1] );\n                \n                if(bycols == true) // We have to do it by rows\n                {\n                    int readedrows = data.rows();\n                    for( int row = readedrows-1 ; row&gt;=0; row--)\n                    {\n                        if( calc_freq(Rcpp::wrap(data.row(row))) &lt;= pcent ) {\n                            removeRow(data, row);\n                            iblockrem = iblockrem + 1;\n                        } \n                    }\n                    \n                } else {\n                    \n                    int readedcols = data.cols();\n                    for( int col = readedcols-1 ; col&gt;=0; col--)\n                    { \n                        if( calc_freq(Rcpp::wrap(data.col(col))) &lt;= pcent ) {\n                            removeColumn(data, col);\n                            iblockrem = iblockrem + 1;\n                        } \n                    }\n                }\n                \n                int extendcols = data.cols();\n                int extendrows = data.rows();\n                \n                if( extendrows&gt;0 && extendcols&gt;0)\n                {\n                    if(bcreated == false) {\n                        // create_HDF5_unlimited_matrix_dataset_ptr(file, stroutdata, extendrows, extendcols, \"numeric\");\n                        // unlimDataset = new DataSet(file-&gt;openDataSet(stroutdata));\n                        // bcreated = true;\n                        \n                        dsOut-&gt;createUnlimitedDataset(extendrows, extendcols, \"real\");\n                        dsOut-&gt;openDataset();\n                        bcreated = true;\n                        \n                    } else {\n                        if(bycols == true){\n                            // extend_HDF5_matrix_subset_ptr(file, unlimDataset, extendrows, 0);\n                            dsOut-&gt;extendUnlimitedDataset(extendrows, 0);\n                        }else{\n                            // extend_HDF5_matrix_subset_ptr(file, unlimDataset, 0, extendcols);\n                            dsOut-&gt;extendUnlimitedDataset( 0, extendcols);\n                        }\n                    }\n                    \n                    if( bcreated == true) {\n                        \n                        std::vector&lt;hsize_t&gt; countblock = {(unsigned long long)extendrows, (unsigned long long)extendcols};\n                        dsOut-&gt;writeDatasetBlock( Rcpp::wrap(data), newoffset, countblock, stride, block, false);\n                        \n                        if(bycols == true)\n                            newoffset[0] =  newoffset[0] + extendrows;\n                        else\n                            newoffset[1] =  newoffset[1] + extendcols;\n                    }\n                    \n                    // IntegerVector countblock = IntegerVector::create(extendrows, extendcols);\n                    // write_HDF5_matrix_subset_v2(file, unlimDataset, newoffset, countblock, stride, block, wrap(data) );\n                    \n                    // if(bycols == true)\n                    //     newoffset[0] =  newoffset[0] + extendrows;\n                    // else\n                    //     newoffset[1] =  newoffset[1] + extendcols;\n                    \n                    itotrem = itotrem - iblockrem;\n                }\n            }\n            \n            // if (bcreated == true) {\n            //     unlimDataset-&gt;close();\n            // }\n            \n        } catch( H5::FileIException& error ){\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ c++ exception Rcpp_Remove_MAF_hdf5 (File IException)\\n\";\n            return -1;\n        } catch( H5::DataSetIException& error ) { \n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ c++ exception Rcpp_Remove_MAF_hdf5 (DataSet IException)\\n\";\n            return -1;\n        } catch( H5::DataSpaceIException& error ) { \n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ c++ exception Rcpp_Remove_MAF_hdf5 (DataSpace IException)\\n\";\n            return -1;\n        } catch( H5::DataTypeIException& error ) { \n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ c++ exception Rcpp_Remove_MAF_hdf5 (DataType IException)\\n\";\n            return -1;\n        } catch(std::exception &ex) {\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ c++ exception Rcpp_Remove_MAF_hdf5: \"&lt;&lt; ex.what()&lt;&lt;\"\\n\";\n            return -1;\n        }  catch (...) {\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception Rcpp_Remove_MAF_hdf5 (unknown reason)\";\n            return -1;\n        }\n        \n        return(itotrem);\n\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_MAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.html#usage-example",
    "title": "Rcpp_Remove_MAF_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_Remove_MAF_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_MAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html",
    "title": "Rcpp_InvCholesky_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Rcpp_InvCholesky_hdf5(BigDataStatMeth::hdf5Dataset *inDataset, BigDataStatMeth::hdf5DatasetInternal *outDataset, bool bfull, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Rcpp_InvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#signature",
    "title": "Rcpp_InvCholesky_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Rcpp_InvCholesky_hdf5(BigDataStatMeth::hdf5Dataset *inDataset, BigDataStatMeth::hdf5DatasetInternal *outDataset, bool bfull, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Rcpp_InvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#description",
    "title": "Rcpp_InvCholesky_hdf5",
    "section": "2 Description",
    "text": "2 Description\nComputes matrix inverse using Cholesky decomposition with HDF5 storage.",
    "crumbs": [
      "Functions",
      "Rcpp_InvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#parameters",
    "title": "Rcpp_InvCholesky_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ninDataset (BigDataStatMeth::hdf5Dataset *): Input matrix dataset (must be symmetric positive-definite)\noutDataset (BigDataStatMeth::hdf5DatasetInternal *): Output dataset for the inverse matrix\nbfull (bool): If true, computes full matrix inverse; if false, only lower triangular part\ndElementsBlock (long): Block size for processing (minimum 2 * matrix dimension)\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Rcpp_InvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#details",
    "title": "Rcpp_InvCholesky_hdf5",
    "section": "4 Details",
    "text": "4 Details\ninDatasetInput matrix dataset (must be symmetric positive-definite) outDatasetOutput dataset for the inverse matrix bfullIf true, computes full matrix inverse; if false, only lower triangular part dElementsBlockBlock size for processing (minimum 2 * matrix dimension) threadsNumber of threads for parallel processing (optional) This function performs matrix inversion in three steps:Cholesky decomposition (A = LL^T)Inverse of the Cholesky factor (L^-1)Computation of full inverse (A^-1 = L^-T L^-1)",
    "crumbs": [
      "Functions",
      "Rcpp_InvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#call-graph",
    "title": "Rcpp_InvCholesky_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_InvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#source-code",
    "title": "Rcpp_InvCholesky_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixInvCholesky.hpp • Lines 260-315\ninline void Rcpp_InvCholesky_hdf5 ( BigDataStatMeth::hdf5Dataset* inDataset, \n                           BigDataStatMeth::hdf5DatasetInternal* outDataset, \n                           bool bfull, long dElementsBlock, \n                           Rcpp::Nullable&lt;int&gt; threads = R_NilValue )\n{\n    \n    try{\n        \n        int nrows = inDataset-&gt;nrows();\n        int ncols = inDataset-&gt;ncols();\n        \n        // Rcpp::Rcout&lt;&lt;\"\\nIniciem cholesky\";\n        int res = Cholesky_decomposition_hdf5(inDataset, outDataset, nrows, ncols, dElementsBlock, threads);\n        \n        if(res == 0)\n        {\n            // Rcpp::Rcout&lt;&lt;\"\\nDins res==0 -&gt; anem Inverse_of_Cholesky_decomposition_hdf5...\";\n            Inverse_of_Cholesky_decomposition_hdf5( outDataset, nrows, ncols, dElementsBlock, threads);\n            // Rcpp::Rcout&lt;&lt;\"\\nDins res==0 -&gt; anem Inverse_Matrix_Cholesky_parallel...\";\n            Inverse_Matrix_Cholesky_hdf5( outDataset, nrows, ncols, dElementsBlock, threads);\n            \n            if( bfull==true ) {\n                setUpperTriangularMatrix( outDataset, dElementsBlock);\n            }\n        }\n        \n    } catch( H5::FileIException& error ) { \n        checkClose_file(inDataset, outDataset);\n        inDataset = outDataset = nullptr;\n        Rcpp::Rcerr&lt;&lt;\"c++ exception Rcpp_InvCholesky_hdf5 (File IException)\";\n        return void();\n    } catch( H5::GroupIException & error ) { \n        checkClose_file(inDataset, outDataset);\n        inDataset = outDataset = nullptr;\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Rcpp_InvCholesky_hdf5 (Group IException)\";\n        return void();\n    } catch( H5::DataSetIException& error ) { \n        checkClose_file(inDataset, outDataset);\n        inDataset = outDataset = nullptr;\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Rcpp_InvCholesky_hdf5 (DataSet IException)\";\n        return void();\n    } catch(std::exception& ex) {\n        checkClose_file(inDataset, outDataset);\n        inDataset = outDataset = nullptr;\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Rcpp_InvCholesky_hdf5\" &lt;&lt; ex.what();\n        return void();\n    } catch (...) {\n        checkClose_file(inDataset, outDataset);\n        inDataset = outDataset = nullptr;\n        Rcpp::Rcerr&lt;&lt;\"\\nC++ exception Rcpp_InvCholesky_hdf5 (unknown reason)\";\n        return void();\n    }\n    \n    return void();\n    \n}",
    "crumbs": [
      "Functions",
      "Rcpp_InvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.html#usage-example",
    "title": "Rcpp_InvCholesky_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_InvCholesky_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_InvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html",
    "title": "Rcpp_Import_File_to_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Rcpp_Import_File_to_hdf5(Rcpp::CharacterVector filename, BigDataStatMeth::hdf5Dataset *dsOut, Rcpp::Nullable&lt; std::string &gt; sep=R_NilValue, Rcpp::Nullable&lt; bool &gt; header=false, Rcpp::Nullable&lt; bool &gt; rownames=false, Rcpp::Nullable&lt; bool &gt; bparal=R_NilValue, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_Import_File_to_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#signature",
    "title": "Rcpp_Import_File_to_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Rcpp_Import_File_to_hdf5(Rcpp::CharacterVector filename, BigDataStatMeth::hdf5Dataset *dsOut, Rcpp::Nullable&lt; std::string &gt; sep=R_NilValue, Rcpp::Nullable&lt; bool &gt; header=false, Rcpp::Nullable&lt; bool &gt; rownames=false, Rcpp::Nullable&lt; bool &gt; bparal=R_NilValue, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_Import_File_to_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#description",
    "title": "Rcpp_Import_File_to_hdf5",
    "section": "2 Description",
    "text": "2 Description\nImports data from a file into an HDF5 dataset.",
    "crumbs": [
      "Functions",
      "Rcpp_Import_File_to_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#parameters",
    "title": "Rcpp_Import_File_to_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfilename (Rcpp::CharacterVector): Path to the input file\ndsOut (BigDataStatMeth::hdf5Dataset *): Pointer to the HDF5 dataset where data will be stored\nsep (Rcpp::Nullable&lt; std::string &gt;): Optional separator character (defaults to tab)\nheader (Rcpp::Nullable&lt; bool &gt;): Optional flag indicating presence of header row (defaults to false)\nrownames (Rcpp::Nullable&lt; bool &gt;): Optional flag indicating presence of row names (defaults to false)\nbparal (Rcpp::Nullable&lt; bool &gt;): Optional flag for parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Optional number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "Rcpp_Import_File_to_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#details",
    "title": "Rcpp_Import_File_to_hdf5",
    "section": "4 Details",
    "text": "4 Details\nfilenamePath to the input file dsOutPointer to the HDF5 dataset where data will be stored sepOptional separator character (defaults to tab) headerOptional flag indicating presence of header row (defaults to false) rownamesOptional flag indicating presence of row names (defaults to false) bparalOptional flag for parallel processing threadsOptional number of threads for parallel processingPerformance optimized through block-wise reading and processing Automatically adjusts block size based on number of columns Supports parallel processing for improved performance on large datasetshdf5Dataset for the dataset structure",
    "crumbs": [
      "Functions",
      "Rcpp_Import_File_to_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#call-graph",
    "title": "Rcpp_Import_File_to_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_Import_File_to_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#source-code",
    "title": "Rcpp_Import_File_to_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5ImportFiles.hpp • Lines 173-403\ninline void Rcpp_Import_File_to_hdf5( Rcpp::CharacterVector filename,\n                                   BigDataStatMeth::hdf5Dataset* dsOut,\n                                   Rcpp::Nullable&lt;std::string&gt; sep = R_NilValue,\n                                   Rcpp::Nullable&lt;bool&gt; header = false,\n                                   Rcpp::Nullable&lt;bool&gt; rownames = false,\n                                   Rcpp::Nullable&lt;bool&gt; bparal = R_NilValue,\n                                   Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n    {\n\n        try {\n\n            std::string path = Rcpp::as&lt;std::string&gt;(filename);\n            std::string stdsep;\n\n            // Colnames and rownames\n            Rcpp::CharacterVector svrcolnames;\n\n            // Blocks control\n            double counter = 0;\n            double blockCounter = 1000;\n\n\n            if(sep.isNull()){\n                stdsep = \"\\t\";\n            }else {\n                stdsep = Rcpp::as&lt;std::string&gt;(sep);\n            }\n\n            std::string delim = \"[^\" + stdsep  + \"]+\";\n            std::regex reg_expres(delim);\n\n\n            std::string line;\n\n            std::ifstream inFile(path.c_str()); //Opens the file. c_str is mandatory here so that ifstream accepts the string path\n            std::getline(inFile,line,'\\n'); //skip the first line (col names in our case). Remove those lines if note necessary\n\n            // Number of columns\n            std::ptrdiff_t const icols(std::distance(\n                    std::sregex_iterator(line.begin(), line.end(), reg_expres),\n                    std::sregex_iterator()));\n\n            hsize_t incols = icols;\n\n            if(Rcpp::as&lt;bool&gt;(rownames) == true) {\n                // Read next line and count number of columns again depending on how file is created we can have\n                // one empty space for rownames or not, then colnames will be different (-1 difference)\n                std::getline(inFile,line,'\\n'); //skip the first line (col names in our case). Remove those lines if not necessary\n\n                // Number of columns\n                std::ptrdiff_t const icols2(std::distance(\n                        std::sregex_iterator(line.begin(), line.end(), reg_expres),\n                        std::sregex_iterator()));\n\n                if(icols2 == icols){\n                    incols = icols-1; // Reduce in one the number of columns\n                } else if ( icols == icols2 -1){\n                    incols = icols;\n                } else {\n                    Rcpp::stop(\"Number of columns and headers are different, please review data, note that fields without values are not allowed\");\n                    // Rcpp::warning(\"Number of columns and headers are different, review data\");\n                    \n                }\n            }\n            \n            // Re-adjust block size\n            if(incols &lt; 100 ){\n                blockCounter = 10000;\n            }\n\n            //. 2025/01/15.// // Get number of rows (+1 to take in to account the last line without \\n)\n            //. 2025/01/15.// int irows = std::count(std::istreambuf_iterator&lt;char&gt;(inFile),\n            //. 2025/01/15.//                        std::istreambuf_iterator&lt;char&gt;(), '\\n') + 1 ;\n            \n            \n            // Get number of rows \n            int irows = std::count(std::istreambuf_iterator&lt;char&gt;(inFile),\n                                   std::istreambuf_iterator&lt;char&gt;(), '\\n') ;\n            \n            // +1 to take in to account the last line without \\n\n            if( get_NewLineEnding(path.c_str()) == false ) {\n                irows = irows + 1;\n            }\n\n            // Restore counter after read first line to get number of cols\n            if( Rcpp::as&lt;bool&gt;(header)==false ){\n                irows = irows + 1;\n            }\n\n            Rcpp::CharacterVector svrownames(irows);\n\n            // Reset iterator to beginning\n            inFile.clear();\n            inFile.seekg(0);\n\n            // Read again the first line if header = true\n            line.clear();\n\n            // If data contains header :  Store first row as a colnames and reads next line\n            if(Rcpp::as&lt;bool&gt;(header) == true) {\n                std::getline(inFile,line,'\\n');\n                svrcolnames = Rcpp::wrap( get_SplitData_in_vectorString(line, reg_expres));\n                // If rownames then remove first column from header (belonging to the rownames)\n                if(Rcpp::as&lt;bool&gt;(rownames) == true) {\n                    // if( incols ==  svrcolnames.size() || incols ==  (svrcolnames.size()-1)){\n                    if (incols == static_cast&lt;hsize_t&gt;(svrcolnames.size()) ||\n                        incols == static_cast&lt;hsize_t&gt;(svrcolnames.size() - 1)) {\n                        svrcolnames.erase(0);}\n                }\n                // Read next line\n                line.clear();\n                std::getline(inFile,line,'\\n');\n            }\n\n            dsOut-&gt;createDataset( (hsize_t)irows, (hsize_t)incols, \"real\");\n            \n            std::vector&lt;std::string&gt; strBlockValues;\n            std::vector&lt;hsize_t&gt; stride = {1,1},\n                                 block = {1,1},\n                                 count = { (hsize_t)incols, (hsize_t)irows},\n                                 offset = {0,0};\n\n            bool btowrite = false;\n            std::vector&lt;std::string&gt; strValues;\n            \n            while( !inFile.eof()  )\n            {\n\n                std::stringstream is(line); // take the line into a stringstream\n\n                btowrite = true;\n                \n                // Get splitted values\n                boost::split(strValues, line, boost::is_any_of(delim), boost::token_compress_on);\n                \n                if( Rcpp::as&lt;bool&gt;(rownames) == true ) {\n                    \n                    svrownames[counter] =  strValues.front();\n                    strValues.erase(strValues.begin());\n                }\n\n                // Concatenate Valutes to get a block with several rows\n                std::move(strValues.begin(), strValues.end(), std::back_inserter(strBlockValues));\n\n                // Empty vector\n                strValues.clear();\n\n                // Write block\n                if( counter&gt;0 && (int)counter % (int)blockCounter == 0)\n                {\n                    count[1] = strBlockValues.size() / incols;\n\n                    std::vector&lt;double&gt; doubleVector = get_data_as_Matrix(strBlockValues);\n                    \n                    double *p = doubleVector.data();\n                    \n                    Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; resMat (p, incols, strBlockValues.size() / incols );\n                    dsOut-&gt; writeDatasetBlock( Rcpp::wrap(resMat.transpose()), offset, count, stride, block, false);\n\n                    offset[1] = offset[1] + (strBlockValues.size() / incols);\n                    \n                    // Clear Vector\n                    strBlockValues.clear();\n                    btowrite = false;\n\n                }\n\n                // Clear Buffer and Read next line\n                line.clear();\n                std::getline(inFile,line,'\\n');\n\n                // Increment counter\n                counter++;\n\n            }\n            \n            count[1] = strBlockValues.size() / incols;\n\n            if((irows - (floor(irows/blockCounter)*blockCounter)&gt;0 && strBlockValues.size()&gt;0) || btowrite == true)\n            {\n                std::vector&lt;double&gt; doubleVector = get_data_as_Matrix(strBlockValues);\n                \n                double *p = doubleVector.data();\n                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; resMat (p, incols, count[1] );\n                \n                dsOut-&gt; writeDatasetBlock( Rcpp::wrap(resMat.transpose()), offset, count, stride, block, false);\n                    \n            }\n\n            \n            BigDataStatMeth::hdf5Dims* dsdims;\n            dsdims = new BigDataStatMeth::hdf5Dims(dsOut);\n            \n            if( Rcpp::as&lt;bool&gt;(rownames) == true || Rcpp::as&lt;bool&gt;(header) == true ) {\n                if( Rcpp::as&lt;bool&gt;(rownames) == false){\n                    Rcpp::StringVector svrownames(1);\n                    dsdims-&gt;writeDimnames( Rcpp::wrap(svrownames), Rcpp::wrap(svrcolnames));\n                } else if(Rcpp::as&lt;bool&gt;(header) == false){\n                    Rcpp::StringVector svrcolnames(1);\n                    dsdims-&gt;writeDimnames( svrownames, svrcolnames);\n                } else {\n                    // Write rownames and colnames\n                    dsdims-&gt;writeDimnames( svrownames, svrcolnames);\n                }\n            }\n            \n            delete dsdims;\n\n        } catch( H5::FileIException& error ) {\n            Rcpp::Rcerr&lt;&lt;\"c++ exception Convert_text_to_HDF5 (File IException)\" &lt;&lt; std::endl;\n            return void();\n        } catch( H5::GroupIException& error ) {\n            Rcpp::Rcerr&lt;&lt;\"c++ exception Convert_text_to_HDF5 (Group IException)\" &lt;&lt; std::endl;\n            return void();\n        } catch( H5::DataSetIException& error ) {\n            Rcpp::Rcerr&lt;&lt;\"c++ exception Convert_text_to_HDF5 (DataSet IException)\" &lt;&lt; std::endl;\n            return void();\n        } catch(const std::runtime_error& re) {\n            Rcpp::Rcerr &lt;&lt; \"Runtime error: \" &lt;&lt; re.what() &lt;&lt; std::endl;\n            return void();\n        } catch(const std::exception& ex) {\n            Rcpp::Rcerr &lt;&lt; \"Error occurred: \" &lt;&lt; ex.what() &lt;&lt; std::endl;\n            return void();\n        } catch(...) {\n            Rcpp::Rcerr &lt;&lt; \"Unknown failure occurred. Possible memory corruption\" &lt;&lt; std::endl;\n            return void();\n        }\n\n        return void();\n\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_Import_File_to_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.html#usage-example",
    "title": "Rcpp_Import_File_to_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_Import_File_to_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_Import_File_to_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppTypifyNormalizeHdf5.html",
    "href": "api-reference/cpp/functions/RcppTypifyNormalizeHdf5.html",
    "title": "RcppTypifyNormalizeHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppTypifyNormalizeHdf5(BigDataStatMeth::hdf5Dataset *dsA, bool bc, bool bs, bool bbyrows)",
    "crumbs": [
      "Functions",
      "RcppTypifyNormalizeHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppTypifyNormalizeHdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppTypifyNormalizeHdf5.html#signature",
    "title": "RcppTypifyNormalizeHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppTypifyNormalizeHdf5(BigDataStatMeth::hdf5Dataset *dsA, bool bc, bool bs, bool bbyrows)",
    "crumbs": [
      "Functions",
      "RcppTypifyNormalizeHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppTypifyNormalizeHdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppTypifyNormalizeHdf5.html#parameters",
    "title": "RcppTypifyNormalizeHdf5",
    "section": "2 Parameters",
    "text": "2 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *)\nbc (bool)\nbs (bool)\nbbyrows (bool)",
    "crumbs": [
      "Functions",
      "RcppTypifyNormalizeHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppTypifyNormalizeHdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppTypifyNormalizeHdf5.html#call-graph",
    "title": "RcppTypifyNormalizeHdf5",
    "section": "3 Call Graph",
    "text": "3 Call Graph",
    "crumbs": [
      "Functions",
      "RcppTypifyNormalizeHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppTypifyNormalizeHdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppTypifyNormalizeHdf5.html#source-code",
    "title": "RcppTypifyNormalizeHdf5",
    "section": "4 Source Code",
    "text": "4 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixNormalization.hpp • Lines 483-560\ninline void RcppTypifyNormalizeHdf5( BigDataStatMeth::hdf5Dataset* dsA,\n                                                bool bc, bool bs, bool bbyrows)\n    {\n        \n        BigDataStatMeth::hdf5Dataset* dsmean = nullptr;\n        BigDataStatMeth::hdf5Dataset* dssd = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsNormal = nullptr;\n        \n        try{\n            \n            Rcpp::Nullable&lt;int&gt; wsize = R_NilValue;\n            Eigen::MatrixXd datanormal;\n            hsize_t nrows, ncols;\n            std::string strgroupout;\n            bool corrected = true;\n            \n            nrows = dsA-&gt;nrows();\n            ncols = dsA-&gt;ncols();\n            \n            strgroupout = \"NORMALIZED_T/\" + dsA-&gt;getGroupName();\n            std::string strgroupout_ms = strgroupout + \"/mean_sd\";\n            std::string strdatasetmean = \"mean.\" + dsA-&gt;getDatasetName();\n            std::string strdatasetsd = \"sd.\" + dsA-&gt;getDatasetName();\n            \n            // Define blocksize atending number of elements in rows and cols\n            if( bbyrows == false) {\n                datanormal = Eigen::MatrixXd::Zero(2,nrows);\n                get_HDF5_mean_sd_by_column( dsA, datanormal, true, true, wsize);\n            } else {\n                datanormal = Eigen::MatrixXd::Zero(2,ncols);\n                get_HDF5_mean_sd_by_row( dsA, datanormal, true, true, wsize);\n            }\n            \n            dsmean = new BigDataStatMeth::hdf5Dataset(dsA-&gt;getFileName(), strgroupout_ms, strdatasetmean, true);\n            dsmean-&gt;createDataset( datanormal.cols(), 1, \"real\");\n            dsmean-&gt;writeDataset( Rcpp::wrap(datanormal.row(0)) );\n            delete dsmean; dsmean = nullptr;\n            \n            dssd = new BigDataStatMeth::hdf5Dataset(dsA-&gt;getFileName(), strgroupout_ms, strdatasetsd, true);\n            dssd-&gt;createDataset( datanormal.cols(), 1, \"real\");\n            dssd-&gt;writeDataset( Rcpp::wrap(datanormal.row(1)) );\n            delete dssd; dssd = nullptr;\n            \n            dsNormal = new BigDataStatMeth::hdf5Dataset(dsA-&gt;getFileName(), strgroupout, dsA-&gt;getDatasetName(), true);\n            dsNormal-&gt;createDataset( dsA, \"real\");\n            \n            if( dsA-&gt;getDatasetptr() != nullptr && dsNormal-&gt;getDatasetptr() != nullptr){\n                BigDataStatMeth::RcppNormalizeHdf5( dsA, dsNormal, datanormal, wsize, bc, bs, bbyrows, corrected);    \n            }\n            delete dsNormal; dsNormal = nullptr;\n            \n        } catch( H5::FileIException& error ) {\n            checkClose_file(dsA, dsmean, dssd, dsNormal);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppTypifyNormalizeHdf5 (File IException)\";\n            return void();\n        } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n            checkClose_file(dsA, dsmean, dssd, dsNormal);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppTypifyNormalizeHdf5 (DataSet IException)\";\n            return void();\n        } catch( H5::DataSpaceIException& error ) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsA, dsmean, dssd, dsNormal);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppTypifyNormalizeHdf5 (DataSpace IException)\";\n            return void();\n        } catch( H5::DataTypeIException& error ) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsA, dsmean, dssd, dsNormal);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppTypifyNormalizeHdf5 (DataType IException)\";\n            return void();\n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsmean, dssd, dsNormal);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception RcppTypifyNormalizeHdf5 : \"&lt;&lt; ex.what();\n        } catch (...) {\n            checkClose_file(dsA, dsmean, dssd, dsNormal);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception RcppTypifyNormalizeHdf5 (unknown reason)\";\n            return void();\n        }\n        \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "RcppTypifyNormalizeHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppTypifyNormalizeHdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppTypifyNormalizeHdf5.html#usage-example",
    "title": "RcppTypifyNormalizeHdf5",
    "section": "5 Usage Example",
    "text": "5 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppTypifyNormalizeHdf5(...);",
    "crumbs": [
      "Functions",
      "RcppTypifyNormalizeHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html",
    "title": "RcppSplit_matrix_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppSplit_matrix_hdf5(BigDataStatMeth::hdf5Dataset *dstosplit, bool bycols, std::string stroutgroup, std::string stroutdataset, int blocksize, int irows, int icols)",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#signature",
    "title": "RcppSplit_matrix_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppSplit_matrix_hdf5(BigDataStatMeth::hdf5Dataset *dstosplit, bool bycols, std::string stroutgroup, std::string stroutdataset, int blocksize, int irows, int icols)",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#description",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#description",
    "title": "RcppSplit_matrix_hdf5",
    "section": "2 Description",
    "text": "2 Description\nSplits an HDF5 dataset into multiple smaller datasets (R interface)",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#parameters",
    "title": "RcppSplit_matrix_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndstosplit (BigDataStatMeth::hdf5Dataset *): Input dataset to split\nbycols (bool): Whether to split by columns (true) or rows (false)\nstroutgroup (std::string): Output group path\nstroutdataset (std::string): Base name for output datasets\nblocksize (int): Size of each block\nirows (int): Number of rows in input dataset\nicols (int): Number of columns in input dataset",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#details",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#details",
    "title": "RcppSplit_matrix_hdf5",
    "section": "4 Details",
    "text": "4 Details\ndstosplitInput dataset to split bycolsWhether to split by columns (true) or rows (false) stroutgroupOutput group path stroutdatasetBase name for output datasets blocksizeSize of each block irowsNumber of rows in input dataset icolsNumber of columns in input dataset Implementation approach:Calculates number of blocks based on input dimensionsProcesses dataset in blocks:Reads block from input datasetCreates new dataset for blockWrites block to new dataset Handles edge cases for final blocks",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#call-graph",
    "title": "RcppSplit_matrix_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#source-code",
    "title": "RcppSplit_matrix_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5SplitDataset.hpp • Lines 65-140\ninline void RcppSplit_matrix_hdf5 ( BigDataStatMeth::hdf5Dataset* dstosplit, bool bycols, \n                                std::string stroutgroup, std::string stroutdataset, \n                                int blocksize, int irows, int icols )\n{\n        \n    BigDataStatMeth::hdf5Dataset* dsOut = nullptr;\n        \n    try {\n        \n        int blocks;\n        hsize_t inrows = irows, \n            incols = icols,\n            ii = 0,\n            kk = 0;\n        \n        std::vector&lt;hsize_t&gt; stride = {1, 1},\n            block = {1, 1};\n        \n        std::string newDatasetName = \"\";\n        \n        if( bycols == true ) {\n            blocks = (icols + blocksize - 1) / blocksize;\n            incols = blocksize;\n        } else {\n            blocks = (irows + blocksize - 1) / blocksize;\n            inrows = blocksize;\n        }\n        \n        for ( int i=0; i&lt;blocks; i++)\n        {\n            newDatasetName = stroutgroup + \"/\" + stroutdataset + \".\" + std::to_string(i);\n            \n            if( bycols == true) { \n                kk = i * blocksize;\n                if( kk + static_cast&lt;hsize_t&gt;(blocksize) &gt; static_cast&lt;hsize_t&gt;(icols)) \n                    { incols = static_cast&lt;hsize_t&gt;(icols) - kk; }\n            } else  {\n                ii = i * blocksize;\n                if( ii + static_cast&lt;hsize_t&gt;(blocksize) &gt; static_cast&lt;hsize_t&gt;(irows)) \n                    { inrows = static_cast&lt;hsize_t&gt;(irows) - ii; }\n            }\n            \n            std::vector&lt;double&gt; vdts( inrows * incols );\n            dstosplit-&gt;readDatasetBlock( {kk, ii}, {incols, inrows}, stride, block, vdts.data() );\n                \n            dsOut = new BigDataStatMeth::hdf5Dataset(dstosplit-&gt;getFileName(), newDatasetName, true);\n            dsOut-&gt;createDataset( inrows, incols, \"real\"); \n            \n            if( dsOut-&gt;getDatasetptr() != nullptr ){\n                dsOut-&gt;writeDataset(vdts.data());\n            }\n            \n            delete dsOut; dsOut = nullptr;\n            \n        }\n        \n    } catch( H5::FileIException& error ) {\n        checkClose_file(dstosplit, dsOut);\n        Rf_error( \"c++ exception RcppSplit_matrix_hdf5(File IException )\");\n    } catch( H5::DataSetIException& error ) { \n        checkClose_file(dstosplit, dsOut);\n        Rf_error( \"c++ exception RcppSplit_matrix_hdf5 (DataSet IException )\");\n    } catch( H5::DataSpaceIException& error ) { \n        checkClose_file(dstosplit, dsOut);\n        Rf_error( \"c++ exception RcppSplit_matrix_hdf5 (DataSpace IException )\");\n    } catch(std::exception &ex) {\n        checkClose_file(dstosplit, dsOut);\n        Rf_error( \"\\nC++ exception RcppSplit_matrix_hdf5 : %s\", ex.what());\n    } catch (...) {\n        checkClose_file(dstosplit, dsOut);\n        Rf_error( \"\\nC++ exception RcppSplit_matrix_hdf5 (unknown reason)\");\n    }\n    \n    return void();\n    \n}",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5.html#usage-example",
    "title": "RcppSplit_matrix_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppSplit_matrix_hdf5(...);",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolveHdf5.html",
    "href": "api-reference/cpp/functions/RcppSolveHdf5.html",
    "title": "RcppSolveHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppSolveHdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsX)",
    "crumbs": [
      "Functions",
      "RcppSolveHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolveHdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppSolveHdf5.html#signature",
    "title": "RcppSolveHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppSolveHdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsX)",
    "crumbs": [
      "Functions",
      "RcppSolveHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolveHdf5.html#description",
    "href": "api-reference/cpp/functions/RcppSolveHdf5.html#description",
    "title": "RcppSolveHdf5",
    "section": "2 Description",
    "text": "2 Description\nSolves the linear system AX = B using HDF5-stored matrices.",
    "crumbs": [
      "Functions",
      "RcppSolveHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolveHdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppSolveHdf5.html#parameters",
    "title": "RcppSolveHdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): Input HDF5 dataset containing matrix A\ndsB (BigDataStatMeth::hdf5Dataset *): Input HDF5 dataset containing matrix B\ndsX (BigDataStatMeth::hdf5Dataset *): Output HDF5 dataset for solution matrix X",
    "crumbs": [
      "Functions",
      "RcppSolveHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolveHdf5.html#details",
    "href": "api-reference/cpp/functions/RcppSolveHdf5.html#details",
    "title": "RcppSolveHdf5",
    "section": "4 Details",
    "text": "4 Details\ndsAInput HDF5 dataset containing matrix A dsBInput HDF5 dataset containing matrix B dsXOutput HDF5 dataset for solution matrix X This function provides an HDF5-based implementation for solving linear systems, suitable for large matrices that don’t fit in memory. It reads data in blocks and automatically handles symmetric cases.",
    "crumbs": [
      "Functions",
      "RcppSolveHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolveHdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppSolveHdf5.html#call-graph",
    "title": "RcppSolveHdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppSolveHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolveHdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppSolveHdf5.html#source-code",
    "title": "RcppSolveHdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixEquationSolver.hpp • Lines 154-229\ninline void RcppSolveHdf5(BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, BigDataStatMeth::hdf5Dataset* dsX )\n    {\n        \n        try {\n            \n            std::vector&lt;hsize_t&gt; offset = { 0, 0 },\n                                 countA = { dsA-&gt;nrows(), dsA-&gt;ncols() },\n                                 countB = { dsB-&gt;nrows(), dsB-&gt;ncols() },\n                                 stride = { 1, 1},\n                                 block = { 1, 1};\n            \n            std::vector&lt;double&gt; vdB( countB[0] * countB[1] );\n            dsB-&gt;readDatasetBlock( {offset[0], offset[1]}, {countB[0], countB[1]}, stride, block, vdB.data() );\n            Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; b (vdB.data(), countB[0], countB[1]);\n            \n            {\n                char Uchar = 'U';\n                int info = 0;\n                \n                // Declare matrix variables\n                int n = dsA-&gt;nrows();\n                int nrhs = dsB-&gt;nrows();\n                int lwork = std::max( 1, n );\n                int lda = std::max( 1, n );\n                int ldb = std::max( 1, n );\n                std::vector&lt;int&gt; ipiv(n);\n                std::vector&lt;double&gt; work(lwork);\n                \n                std::vector&lt;double&gt; vdA( countA[0] * countA[1] );\n                dsA-&gt;readDatasetBlock( {offset[0], offset[1]}, {countA[0], countA[1]}, stride, block, vdA.data() );\n                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; a (vdA.data(), countA[0], countA[1]);\n                \n                // Solve matrix equation\n                if( a == a.transpose()  )\n                {\n                    // dsysv_( char* UPLO, int* N , int* NRHS, double* A, int* LDA, int* IPIV, double* B, int* LDB, double* WORK, int* LWORK, int* INFO);\n                    dsysv_( &Uchar, &n, &nrhs, a.data(), &lda, ipiv.data(), b.data(), &ldb, work.data(), &lwork, &info);\n                } else {\n                    // dgesv( int N, int NRHS, double A, int LDA, int IPIV, double B, int LDB, int INFO);\n                    dgesv_( &n, &nrhs, a.data(), &lda, ipiv.data(), b.data(), &ldb, &info );\n                }\n                \n            }\n            \n            dsX-&gt;writeDataset(b.data());\n            \n        } catch( H5::FileIException& error ) { // catch failure caused by the H5File operations\n            checkClose_file(dsA, dsB, dsX);\n            dsA = dsB = dsX = nullptr;\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppSolveHdf5 (File IException)\";\n            return void();\n        } catch( H5::GroupIException & error ) { // catch failure caused by the DataSet operations\n            checkClose_file(dsA, dsB, dsX);\n            dsA = dsB = dsX = nullptr;\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppSolveHdf5 (Group IException)\";\n            return void();\n        } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n            checkClose_file(dsA, dsB, dsX);\n            dsA = dsB = dsX = nullptr;\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppSolveHdf5 (DataSet IException)\";\n            return void();\n        } catch(std::exception& ex) {\n            checkClose_file(dsA, dsB, dsX);\n            dsA = dsB = dsX = nullptr;\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppSolveHdf5\" &lt;&lt; ex.what();\n            return void();\n        } catch (...) {\n            checkClose_file(dsA, dsB, dsX);\n            dsA = dsB = dsX = nullptr;\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception RcppSolveHdf5 (unknown reason)\";\n            return void();\n        }\n        \n        return void();\n        \n    }",
    "crumbs": [
      "Functions",
      "RcppSolveHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolveHdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppSolveHdf5.html#usage-example",
    "title": "RcppSolveHdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppSolveHdf5(...);",
    "crumbs": [
      "Functions",
      "RcppSolveHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html",
    "href": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html",
    "title": "RcppRemove_hdf5_elements",
    "section": "",
    "text": "void BigDataStatMeth::RcppRemove_hdf5_elements(BigDataStatMeth::hdf5File *file, std::vector&lt; std::string &gt; elements)",
    "crumbs": [
      "Functions",
      "RcppRemove_hdf5_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#signature",
    "href": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#signature",
    "title": "RcppRemove_hdf5_elements",
    "section": "",
    "text": "void BigDataStatMeth::RcppRemove_hdf5_elements(BigDataStatMeth::hdf5File *file, std::vector&lt; std::string &gt; elements)",
    "crumbs": [
      "Functions",
      "RcppRemove_hdf5_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#description",
    "href": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#description",
    "title": "RcppRemove_hdf5_elements",
    "section": "2 Description",
    "text": "2 Description\nRemoves specified elements from an HDF5 file.",
    "crumbs": [
      "Functions",
      "RcppRemove_hdf5_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#parameters",
    "href": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#parameters",
    "title": "RcppRemove_hdf5_elements",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfile (BigDataStatMeth::hdf5File *): Pointer to the HDF5 file object\nelements (std::vector&lt; std::string &gt;): Vector of element paths to be removed",
    "crumbs": [
      "Functions",
      "RcppRemove_hdf5_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#details",
    "href": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#details",
    "title": "RcppRemove_hdf5_elements",
    "section": "4 Details",
    "text": "4 Details\nThis function safely removes multiple elements from an HDF5 file. It checks for the existence of each element before attempting removal and provides appropriate feedback messages.",
    "crumbs": [
      "Functions",
      "RcppRemove_hdf5_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#call-graph",
    "href": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#call-graph",
    "title": "RcppRemove_hdf5_elements",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppRemove_hdf5_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#source-code",
    "href": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#source-code",
    "title": "RcppRemove_hdf5_elements",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5RemoveElements.hpp • Lines 59-108\ninline void RcppRemove_hdf5_elements(BigDataStatMeth::hdf5File* file, std::vector&lt;std::string&gt; elements)\n    {\n        \n        try\n        {\n            H5::Exception::dontPrint();\n            \n            \n            if(elements.size() == 0) {\n                std::string strmessage = \"Nothing to be removed removed\";\n                Rcpp::message(Rcpp::wrap(strmessage));\n            } else { // Remove datasets\n                // for (int i=0; i&lt;elements.size(); i++) \n                for (size_t i = 0; i &lt; elements.size(); ++i)\n                {\n                    H5std_string element = \"\" + elements[i];\n                    if(exists_HDF5_element( file-&gt;getFileptr(), element))\n                    {\n                        int result = H5Ldelete( (file-&gt;getFileptr())-&gt;getId(), element.data(), H5P_DEFAULT);  \n                        if(result&lt;0) {\n                            Rcpp::Rcout&lt;&lt;\"\\n Error removing : \"&lt;&lt;elements[i]&lt;&lt;\"\\n\";\n                        } \n                    } else {\n                        Rcpp::Rcout&lt;&lt;\"\\n Element: \"&lt;&lt;elements[i]&lt;&lt;\" does not exists \\n\";\n                    }\n                }    \n            }\n            \n        } catch(H5::FileIException& error) { // catch failure caused by the H5File operations\n            Rcpp::Rcerr&lt;&lt;\"c++ exception RcppRemove_hdf5_elements (File IException)\";\n            return void();\n        } catch(H5::GroupIException& error) { // catch failure caused by the Group operations\n            Rcpp::Rcerr&lt;&lt;\"c++ exception RcppRemove_hdf5_elements (Group IException)\";\n            return void();\n        } catch(H5::DataSetIException& error) { // catch failure caused by the DataSet operations\n            Rcpp::Rcerr&lt;&lt;\"c++ exception RcppRemove_hdf5_elements (DataSet IException)\";\n            return void();\n        } catch(H5::DataSpaceIException& error) { // catch failure caused by the DataSpace operations\n            Rcpp::Rcerr&lt;&lt;\"c++ exception RcppRemove_hdf5_elements (DataSpace IException)\";\n            return void();\n        } catch(std::exception &ex) {\n            Rcpp::Rcerr &lt;&lt; \"c++ exception RcppRemove_hdf5_elements: \" &lt;&lt; ex.what();\n            return void();\n        } catch (...) {\n            Rcpp::Rcerr&lt;&lt;\"C++ exception RcppRemove_hdf5_elements (unknown reason)\";\n            return void();\n        }\n        \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "RcppRemove_hdf5_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#usage-example",
    "href": "api-reference/cpp/functions/RcppRemove_hdf5_elements.html#usage-example",
    "title": "RcppRemove_hdf5_elements",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppRemove_hdf5_elements(...);",
    "crumbs": [
      "Functions",
      "RcppRemove_hdf5_elements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQRHdf5.html",
    "href": "api-reference/cpp/functions/RcppQRHdf5.html",
    "title": "RcppQRHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppQRHdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsQ, BigDataStatMeth::hdf5Dataset *dsR, bool bthin, Rcpp::Nullable&lt; int &gt; block_size, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "RcppQRHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQRHdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppQRHdf5.html#signature",
    "title": "RcppQRHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppQRHdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsQ, BigDataStatMeth::hdf5Dataset *dsR, bool bthin, Rcpp::Nullable&lt; int &gt; block_size, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "RcppQRHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQRHdf5.html#description",
    "href": "api-reference/cpp/functions/RcppQRHdf5.html#description",
    "title": "RcppQRHdf5",
    "section": "2 Description",
    "text": "2 Description\nQR decomposition for HDF5 matrices.",
    "crumbs": [
      "Functions",
      "RcppQRHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQRHdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppQRHdf5.html#parameters",
    "title": "RcppQRHdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): Input matrix dataset\ndsQ (BigDataStatMeth::hdf5Dataset *): Output Q matrix dataset\ndsR (BigDataStatMeth::hdf5Dataset *): Output R matrix dataset\nbthin (bool): Whether to compute thin QR\nblock_size (Rcpp::Nullable&lt; int &gt;): Block size for processing (optional)\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "RcppQRHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQRHdf5.html#details",
    "href": "api-reference/cpp/functions/RcppQRHdf5.html#details",
    "title": "RcppQRHdf5",
    "section": "4 Details",
    "text": "4 Details\nComputes the QR decomposition of a matrix stored in HDF5 format. Supports both full and thin decomposition with parallel processing.",
    "crumbs": [
      "Functions",
      "RcppQRHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQRHdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppQRHdf5.html#call-graph",
    "title": "RcppQRHdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppQRHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQRHdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppQRHdf5.html#source-code",
    "title": "RcppQRHdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixQR.hpp • Lines 156-236\ninline void RcppQRHdf5( BigDataStatMeth::hdf5Dataset* dsA, \n                        BigDataStatMeth::hdf5Dataset* dsQ, \n                        BigDataStatMeth::hdf5Dataset* dsR, \n                        bool bthin,\n                        Rcpp::Nullable&lt;int&gt; block_size = R_NilValue,\n                        Rcpp::Nullable&lt;int&gt; threads = R_NilValue )\n{\n    \n    \n    try {\n\n        int irank; //,\n            // iblockfactor = 1;\n        std::vector&lt;hsize_t&gt; offset = {0,0},\n            count = {dsA-&gt;nrows(), dsA-&gt;ncols()},\n            stride = {1,1},\n            block = {1,1};\n        Eigen::MatrixXd Q;\n        \n        \n        std::vector&lt;double&gt; vdA( count[0] * count[1] ); \n        dsA-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdA.data() );\n        Eigen::MatrixXd A = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vdA.data(), count[0], count[1] );\n        A.transposeInPlace();\n        \n        \n        Eigen::FullPivLU&lt;Eigen::MatrixXd&gt;lu_decomp(A);\n        Eigen::HouseholderQR&lt;Eigen::MatrixXd&gt; qr(A);\n        \n        qr.compute(A);\n        irank = lu_decomp.rank();\n        \n        \n        //..//if (irank == count[0] + 1 || irank == count[1] + 1 )\n        if (static_cast&lt;unsigned long long&gt;(irank) == count[0] + 1ULL ||\n            static_cast&lt;unsigned long long&gt;(irank) == count[1] + 1ULL)\n        {\n            Eigen::MatrixXd R = qr.matrixQR().template triangularView&lt;Eigen::Upper&gt;();\n            dsR-&gt;createDataset( R.rows(), R.cols(), \"real\" );\n            dsR-&gt;writeDataset(Rcpp::wrap(R));\n        } else {\n            Eigen::MatrixXd R = qr.matrixQR().topLeftCorner(irank, irank).template triangularView&lt;Eigen::Upper&gt;();\n            dsR-&gt;createDataset( R.rows(), R.cols(), \"real\" );\n            dsR-&gt;writeDataset(Rcpp::wrap(R));\n        }\n        \n        \n        if (bthin == false)\n        {\n            Eigen::MatrixXd Q = qr.householderQ();\n            dsQ-&gt;createDataset( Q.rows(), Q.cols(), \"real\" );\n            dsQ-&gt;writeDataset( Rcpp::wrap(Q) );\n        } else {\n            \n            //. 2025/01/15 error with thin calculus.// Eigen::MatrixXd Qthin = qr.householderQ() * Eigen::MatrixXd::Identity(count[0], count[1]);\n            //. 2025/01/15 .// dsQ-&gt;createDataset( Qthin.rows(), Qthin.cols(), \"real\" );\n            //. 2025/01/15 .// dsQ-&gt;writeDataset( Rcpp::wrap(Qthin));\n            \n            Eigen::MatrixXd Qthin = qr.householderQ() * Eigen::MatrixXd::Identity(count[1], count[0]);\n            dsQ-&gt;createDataset( Qthin.rows(), Qthin.cols(), \"real\" );\n            dsQ-&gt;writeDataset( Rcpp::wrap(Qthin));\n            \n        }\n        \n    } catch( H5::FileIException& error ) { // catch failure caused by the H5File operations\n        Rcpp::Rcout&lt;&lt;\"c++ exception RcppQRHdf5 (File IException)\";\n        return void();\n    } catch( H5::GroupIException & error ) { // catch failure caused by the DataSet operations\n        Rcpp::Rcout &lt;&lt; \"c++ exception RcppQRHdf5 (Group IException)\";\n        return void();\n    } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n        Rcpp::Rcout &lt;&lt; \"c++ exception RcppQRHdf5 (DataSet IException)\";\n        return void();\n    } catch(std::exception& ex) {\n        Rcpp::Rcout &lt;&lt; \"c++ exception RcppQRHdf5\" &lt;&lt; ex.what();\n        return void();\n    }\n    \n    return void();\n    \n}",
    "crumbs": [
      "Functions",
      "RcppQRHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQRHdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppQRHdf5.html#usage-example",
    "title": "RcppQRHdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppQRHdf5(...);",
    "crumbs": [
      "Functions",
      "RcppQRHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinvHdf5.html",
    "href": "api-reference/cpp/functions/RcppPseudoinvHdf5.html",
    "title": "RcppPseudoinvHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppPseudoinvHdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsR, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppPseudoinvHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#signature",
    "title": "RcppPseudoinvHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppPseudoinvHdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsR, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppPseudoinvHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#description",
    "href": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#description",
    "title": "RcppPseudoinvHdf5",
    "section": "2 Description",
    "text": "2 Description\nCompute pseudoinverse of HDF5 matrix.",
    "crumbs": [
      "Functions",
      "RcppPseudoinvHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#parameters",
    "title": "RcppPseudoinvHdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): Input matrix dataset\ndsR (BigDataStatMeth::hdf5Dataset *): Output pseudoinverse dataset\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "RcppPseudoinvHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#details",
    "href": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#details",
    "title": "RcppPseudoinvHdf5",
    "section": "4 Details",
    "text": "4 Details\nComputes the Moore-Penrose pseudoinverse of a matrix stored in HDF5 format using SVD decomposition with parallel processing support.",
    "crumbs": [
      "Functions",
      "RcppPseudoinvHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#call-graph",
    "title": "RcppPseudoinvHdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppPseudoinvHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#source-code",
    "title": "RcppPseudoinvHdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixPseudoinverse.hpp • Lines 141-198\ninline void RcppPseudoinvHdf5( BigDataStatMeth::hdf5Dataset* dsA, \n                                      BigDataStatMeth::hdf5Dataset* dsR, \n                                      Rcpp::Nullable&lt;int&gt; threads = R_NilValue )\n{\n    \n    char Schar='S';\n    char Cchar='C';\n    int ione = 1; \n    double done = 1.0;\n    double dzero = 0.0;\n    \n    int n = dsA-&gt;nrows();\n    int m = dsA-&gt;ncols();\n    int lda = m;\n    int ldu = std::max(1,m);\n    int ldvt =  std::max(1, std::min(m, n));\n    int k = std::min(m,n);\n    int lwork = std::max( 1, 4*std::min(m,n)* std::min(m,n) + 7*std::min(m, n) );\n    int info = 0;\n    \n    std::vector&lt;hsize_t&gt; offset = {0,0},\n        count = {dsA-&gt;nrows(), dsA-&gt;ncols()},\n        stride = {1,1},\n        block = {1,1};\n    \n    Eigen::VectorXd s = Eigen::VectorXd::Zero(k);\n    Eigen::MatrixXd u = Eigen::MatrixXd::Zero(ldu,k);\n    Eigen::MatrixXd vt = Eigen::MatrixXd::Zero(ldvt,n);\n    \n    {\n        Eigen::VectorXd work = Eigen::VectorXd::Zero(lwork);\n        std::vector&lt;double&gt; vdA( count[0] * count[1] );\n        \n        dsA-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdA.data() );\n        Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; A (vdA.data(), count[0], count[1]);\n        \n        dgesvd_( &Schar, &Schar, &m, &n, A.transpose().data() , &lda, s.data(), u.data(), &ldu, vt.data(), &ldvt, work.data(), &lwork, &info);\n    }\n    \n    Eigen::MatrixXd pinv = Eigen::MatrixXd::Zero(n,m);\n    dsR-&gt;createDataset( n, m, \"real\" );\n    \n#pragma omp parallel for num_threads(get_number_threads(threads, R_NilValue))\n    for (int i = 0; i &lt; k; i++){\n        double tempS;\n        if(s[i] &gt; 1.0e-9)\n            tempS = 1.0/s[i];\n        else\n            tempS = s[i];\n        \n        dscal_( &m, &tempS, &(u(i*ldu)), &ione );\n    }\n    \n    dgemm_( &Cchar, &Cchar, &n, &m, &k, &done, vt.data(), &k, u.data(), &m, &dzero, pinv.data(), &n );\n    dsR-&gt;writeDataset(Rcpp::wrap(pinv));\n\n    return void();\n}",
    "crumbs": [
      "Functions",
      "RcppPseudoinvHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppPseudoinvHdf5.html#usage-example",
    "title": "RcppPseudoinvHdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppPseudoinvHdf5(...);",
    "crumbs": [
      "Functions",
      "RcppPseudoinvHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPCAHdf5.html",
    "href": "api-reference/cpp/functions/RcppPCAHdf5.html",
    "title": "RcppPCAHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppPCAHdf5(std::string filename, std::string strgroup, std::string strdataset, std::string strSVDgroup, int k, int q, int nev, bool bcenter, bool bscale, double dthreshold, bool bforce, bool asRowMajor, Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; method=R_NilValue, Rcpp::Nullable&lt; int &gt; ithreads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppPCAHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPCAHdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppPCAHdf5.html#signature",
    "title": "RcppPCAHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppPCAHdf5(std::string filename, std::string strgroup, std::string strdataset, std::string strSVDgroup, int k, int q, int nev, bool bcenter, bool bscale, double dthreshold, bool bforce, bool asRowMajor, Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; method=R_NilValue, Rcpp::Nullable&lt; int &gt; ithreads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppPCAHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPCAHdf5.html#description",
    "href": "api-reference/cpp/functions/RcppPCAHdf5.html#description",
    "title": "RcppPCAHdf5",
    "section": "2 Description",
    "text": "2 Description\nPerform Principal Component Analysis.",
    "crumbs": [
      "Functions",
      "RcppPCAHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPCAHdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppPCAHdf5.html#parameters",
    "title": "RcppPCAHdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfilename (std::string): HDF5 file name\nstrgroup (std::string): Group name for results\nstrdataset (std::string): Dataset name\nstrSVDgroup (std::string): SVD group name\nk (int): Number of components to compute\nq (int): Block size for processing\nnev (int): Number of eigenvalues\nbcenter (bool): Whether to center the data\nbscale (bool): Whether to scale the data\ndthreshold (double): Convergence threshold\nbforce (bool): Whether to force computation\nasRowMajor (bool): Whether data is in row-major order\nmethod (Rcpp::Nullable&lt; Rcpp::CharacterVector &gt;): Method selection (optional)\nithreads (Rcpp::Nullable&lt; int &gt;): Number of threads (optional)",
    "crumbs": [
      "Functions",
      "RcppPCAHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPCAHdf5.html#details",
    "href": "api-reference/cpp/functions/RcppPCAHdf5.html#details",
    "title": "RcppPCAHdf5",
    "section": "4 Details",
    "text": "4 Details\nPerforms PCA on an HDF5 dataset with options for:Full or truncated analysisData preprocessing (centering/scaling)Method selectionParallel processing",
    "crumbs": [
      "Functions",
      "RcppPCAHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPCAHdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppPCAHdf5.html#call-graph",
    "title": "RcppPCAHdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppPCAHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPCAHdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppPCAHdf5.html#source-code",
    "title": "RcppPCAHdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixPCA.hpp • Lines 347-457\ninline void RcppPCAHdf5( std::string filename, std::string strgroup, std::string strdataset,  \n                             std::string strSVDgroup, int k, int q, int nev, \n                             bool bcenter, bool bscale, double dthreshold, \n                             bool bforce, bool asRowMajor, \n                             Rcpp::Nullable&lt;Rcpp::CharacterVector&gt; method = R_NilValue,\n                             Rcpp::Nullable&lt;int&gt; ithreads = R_NilValue)\n    {\n        \n        BigDataStatMeth::hdf5Dataset* dsA = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsd = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsu = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsv = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsX = nullptr;\n        \n        try{\n            \n    \n            std::string strPCAgroup = \"PCA/\" + strdataset;\n                        \n            // Check for svd decomposition (u, v and d matrices) in hdf5 file or if we \n            // need to compute again the SVD ( foce = true )\n            BigDataStatMeth::hdf5File* file = new BigDataStatMeth::hdf5File(filename, false);\n            file-&gt;openFile(\"r\");\n            \n            bool bexistsSVD = exists_HDF5_element(file-&gt;getFileptr(), strSVDgroup);\n            bool bexistsPCA = exists_HDF5_element(file-&gt;getFileptr(), strPCAgroup);\n            \n            delete file; file = nullptr;\n            \n            if( bexistsSVD == 0 ||  bforce == true ) {\n                \n                dsA = new BigDataStatMeth::hdf5Dataset(filename, strgroup, strdataset, false);\n                dsA-&gt;openDataset();\n                if( dsA-&gt;getDatasetptr() != nullptr ) {\n                    RcppTypifyNormalizeHdf5( dsA, bcenter, bscale, false); // Normalize and tipify data ( ((x-mu)/(sd)) * 1/sqrt(n-1) )\n                } else {\n                    checkClose_file(dsA, dsd, dsu, dsv, dsX);\n                    return void();\n                }\n                \n                delete dsA; dsA = nullptr;\n                \n                BigDataStatMeth::RcppbdSVD_hdf5( filename, \"NORMALIZED_T/\" + strgroup, strdataset, k, q, nev, false, false, dthreshold, bforce, asRowMajor, method, ithreads );\n                \n                strSVDgroup = \"SVD/\" +  strdataset;\n                \n            }\n            \n            // Check if PCA decomposition exists\n            if( bexistsPCA != 0  && bforce == false) {\n                Rcpp::Rcout&lt;&lt;\"PCA decomposition exits, please set overwrite = true to overwrite the existing results\";\n                return void();\n            }\n            \n            // ------------ Variables ----------------\n            \n            dsd = new BigDataStatMeth::hdf5Dataset(filename, strSVDgroup, \"d\", false );\n            dsd-&gt;openDataset();\n            \n            dsv = new BigDataStatMeth::hdf5Dataset(filename, strSVDgroup, \"v\", false );\n            dsv-&gt;openDataset();\n            \n            if( dsd-&gt;getDatasetptr() != nullptr && dsv-&gt;getDatasetptr() != nullptr) {\n                RcppGetPCAVariablesHdf5( strPCAgroup, dsd, dsv, bforce );\n            }\n            \n            delete dsv; dsv = nullptr;\n            // delete dsA;\n            \n            // ------------ Individuals ----------------\n            \n            dsX = new BigDataStatMeth::hdf5Dataset(filename, strgroup, strdataset, false);\n            dsX-&gt;openDataset();\n            \n            dsu = new BigDataStatMeth::hdf5Dataset(filename, strSVDgroup, \"u\", false );\n            dsu-&gt;openDataset();\n            \n            if( dsX-&gt;getDatasetptr() != nullptr && dsd-&gt;getDatasetptr() != nullptr && dsu-&gt;getDatasetptr() != nullptr) {\n                RcppGetPCAIndividualsHdf5( strPCAgroup, dsX, dsd, dsu, bforce );\n            }\n            \n            delete dsd; dsd = nullptr;\n            delete dsu; dsu = nullptr;\n            delete dsX; dsX = nullptr;\n            // delete dsA;\n            \n        } catch( H5::FileIException& error ) { // catch failure caused by the H5File operations\n            checkClose_file(dsA, dsd, dsu, dsv, dsX);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppPCAHdf5 (File IException)\\n\";\n            return void();\n        } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n            checkClose_file(dsA, dsd, dsu, dsv, dsX);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppPCAHdf5 (DataSet IException)\\n\";\n            return void();\n        } catch( H5::DataSpaceIException& error ) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsA, dsd, dsu, dsv, dsX);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppPCAHdf5 (DataSpace IException)\\n\";\n            return void();\n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsd, dsu, dsv, dsX);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppPCAHdf5 \\n\"&lt;&lt; ex.what();\n            return void();\n        } catch (...) {\n            checkClose_file(dsA, dsd, dsu, dsv, dsX);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception RcppPCAHdf5 (unknown reason)\";\n            return void();\n        }\n        \n        return void();\n        \n    }",
    "crumbs": [
      "Functions",
      "RcppPCAHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPCAHdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppPCAHdf5.html#usage-example",
    "title": "RcppPCAHdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppPCAHdf5(...);",
    "crumbs": [
      "Functions",
      "RcppPCAHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data.html",
    "href": "api-reference/cpp/functions/RcppNormalize_Data.html",
    "title": "RcppNormalize_Data",
    "section": "",
    "text": "M BigDataStatMeth::RcppNormalize_Data(M X, bool bc, bool bs, bool bRowMajor)",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data.html#signature",
    "href": "api-reference/cpp/functions/RcppNormalize_Data.html#signature",
    "title": "RcppNormalize_Data",
    "section": "",
    "text": "M BigDataStatMeth::RcppNormalize_Data(M X, bool bc, bool bs, bool bRowMajor)",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data.html#parameters",
    "href": "api-reference/cpp/functions/RcppNormalize_Data.html#parameters",
    "title": "RcppNormalize_Data",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nX (M)\nbc (bool)\nbs (bool)\nbRowMajor (bool)",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data.html#returns",
    "href": "api-reference/cpp/functions/RcppNormalize_Data.html#returns",
    "title": "RcppNormalize_Data",
    "section": "3 Returns",
    "text": "3 Returns\nType: typename M",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data.html#call-graph",
    "href": "api-reference/cpp/functions/RcppNormalize_Data.html#call-graph",
    "title": "RcppNormalize_Data",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data.html#source-code",
    "href": "api-reference/cpp/functions/RcppNormalize_Data.html#source-code",
    "title": "RcppNormalize_Data",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixNormalization.hpp • Lines 237-248\ninline M RcppNormalize_Data ( M  X, bool bc, bool bs, bool bRowMajor )\n    {\n        static_assert(std::is_same&lt;M, Eigen::MatrixXd &gt;::value || \n                      std::is_same&lt;M, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n                      std::is_same&lt;M, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value,\n                      \"Error - type not allowed\");\n        \n        \n        X = RcppNormalizeColwise(X, bc, bs );\n        \n        return(X); // Return data as initial type\n    };",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data.html#usage-example",
    "href": "api-reference/cpp/functions/RcppNormalize_Data.html#usage-example",
    "title": "RcppNormalize_Data",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppNormalize_Data(...);",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeHdf5.html",
    "href": "api-reference/cpp/functions/RcppNormalizeHdf5.html",
    "title": "RcppNormalizeHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppNormalizeHdf5(BigDataStatMeth::hdf5Dataset *dsA, bool bc, bool bs, bool bbyrows)",
    "crumbs": [
      "Functions",
      "RcppNormalizeHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeHdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppNormalizeHdf5.html#signature",
    "title": "RcppNormalizeHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppNormalizeHdf5(BigDataStatMeth::hdf5Dataset *dsA, bool bc, bool bs, bool bbyrows)",
    "crumbs": [
      "Functions",
      "RcppNormalizeHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeHdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppNormalizeHdf5.html#parameters",
    "title": "RcppNormalizeHdf5",
    "section": "2 Parameters",
    "text": "2 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *)\nbc (bool)\nbs (bool)\nbbyrows (bool)",
    "crumbs": [
      "Functions",
      "RcppNormalizeHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeHdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppNormalizeHdf5.html#call-graph",
    "title": "RcppNormalizeHdf5",
    "section": "3 Call Graph",
    "text": "3 Call Graph",
    "crumbs": [
      "Functions",
      "RcppNormalizeHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeHdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppNormalizeHdf5.html#source-code",
    "title": "RcppNormalizeHdf5",
    "section": "4 Source Code",
    "text": "4 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixNormalization.hpp • Lines 400-477\ninline void RcppNormalizeHdf5( BigDataStatMeth::hdf5Dataset* dsA,\n                                                bool bc, bool bs, bool bbyrows)\n    {\n        \n        BigDataStatMeth::hdf5Dataset* dsmean = nullptr;\n        BigDataStatMeth::hdf5Dataset* dssd = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsNormal = nullptr;\n        \n        try{\n            \n            Rcpp::Nullable&lt;int&gt; wsize = R_NilValue;\n            Eigen::MatrixXd datanormal;\n            hsize_t nrows, ncols;\n            std::string strgroupout;\n            bool bcorrected = false;\n            \n            nrows = dsA-&gt;nrows();\n            ncols = dsA-&gt;ncols();\n            \n            strgroupout = \"NORMALIZED/\" + dsA-&gt;getGroupName();\n            std::string strdatasetmean = \"mean.\" + dsA-&gt;getDatasetName();\n            std::string strdatasetsd = \"sd.\" + dsA-&gt;getDatasetName();\n            \n            // Define blocksize atending number of elements in rows and cols\n            if( bbyrows == false) {\n                datanormal = Eigen::MatrixXd::Zero( 2, nrows);\n                get_HDF5_mean_sd_by_column( dsA, datanormal, true, true, wsize);\n            } else {\n                datanormal = Eigen::MatrixXd::Zero( 2, ncols);\n                get_HDF5_mean_sd_by_row( dsA, datanormal, true, true, wsize);\n            }\n            \n            dsmean = new BigDataStatMeth::hdf5Dataset(dsA-&gt;getFileName(), strgroupout, strdatasetmean, true);\n            dsmean-&gt;createDataset( datanormal.cols(), 1, \"real\");\n            dsmean-&gt;writeDataset( Rcpp::wrap(datanormal.row(0)) );\n            delete dsmean; dsmean = nullptr;\n            \n            dssd = new BigDataStatMeth::hdf5Dataset(dsA-&gt;getFileName(), strgroupout, strdatasetsd, true);\n            dssd-&gt;createDataset( datanormal.cols(), 1, \"real\");\n            dssd-&gt;writeDataset( Rcpp::wrap(datanormal.row(1)) );\n            delete dssd; dssd = nullptr;\n            \n            dsNormal = new BigDataStatMeth::hdf5Dataset(dsA-&gt;getFileName(), strgroupout, dsA-&gt;getDatasetName(), true);\n            dsNormal-&gt;createDataset( dsA, \"real\");\n            \n            if( dsA-&gt;getDatasetptr() != nullptr && dsNormal-&gt;getDatasetptr() != nullptr){\n                BigDataStatMeth::RcppNormalizeHdf5( dsA, dsNormal, datanormal, wsize, bc, bs, bbyrows, bcorrected);\n            }\n            \n            delete dsNormal; dsNormal = nullptr;\n            \n        } catch( H5::FileIException& error ) {\n            checkClose_file(dsA, dsmean, dssd, dsNormal);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppNormalizeHdf5_F (File IException)\";\n            return void();\n        } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n            checkClose_file(dsA, dsmean, dssd, dsNormal);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppNormalizeHdf5_F (DataSet IException)\";\n            return void();\n        } catch( H5::DataSpaceIException& error ) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsA, dsmean, dssd, dsNormal);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppNormalizeHdf5_F (DataSpace IException)\";\n            return void();\n        } catch( H5::DataTypeIException& error ) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsA, dsmean, dssd, dsNormal);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppNormalizeHdf5_F (DataType IException)\";\n            return void();\n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsmean, dssd, dsNormal);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception RcppNormalizeHdf5_F : \"&lt;&lt; ex.what();\n        } catch (...) {\n            checkClose_file(dsA, dsmean, dssd, dsNormal);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception RcppNormalizeHdf5_F (unknown reason)\";\n            return void();\n        }\n        \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "RcppNormalizeHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeHdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppNormalizeHdf5.html#usage-example",
    "title": "RcppNormalizeHdf5",
    "section": "5 Usage Example",
    "text": "5 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppNormalizeHdf5(...);",
    "crumbs": [
      "Functions",
      "RcppNormalizeHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html",
    "href": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html",
    "title": "RcppGetPCAVariablesHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppGetPCAVariablesHdf5(std::string strPCAgroup, BigDataStatMeth::hdf5Dataset *dsd, BigDataStatMeth::hdf5Dataset *dsv, bool overwrite)",
    "crumbs": [
      "Functions",
      "RcppGetPCAVariablesHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#signature",
    "title": "RcppGetPCAVariablesHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppGetPCAVariablesHdf5(std::string strPCAgroup, BigDataStatMeth::hdf5Dataset *dsd, BigDataStatMeth::hdf5Dataset *dsv, bool overwrite)",
    "crumbs": [
      "Functions",
      "RcppGetPCAVariablesHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#description",
    "href": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#description",
    "title": "RcppGetPCAVariablesHdf5",
    "section": "2 Description",
    "text": "2 Description\nCalculate PCA variables statistics.",
    "crumbs": [
      "Functions",
      "RcppGetPCAVariablesHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#parameters",
    "title": "RcppGetPCAVariablesHdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nstrPCAgroup (std::string): HDF5 group name for PCA results\ndsd (BigDataStatMeth::hdf5Dataset *): Singular values dataset\ndsv (BigDataStatMeth::hdf5Dataset *): Right singular vectors dataset\noverwrite (bool): Whether to overwrite existing results",
    "crumbs": [
      "Functions",
      "RcppGetPCAVariablesHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#details",
    "href": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#details",
    "title": "RcppGetPCAVariablesHdf5",
    "section": "4 Details",
    "text": "4 Details\nComputes and stores various PCA statistics for variables including:Eigenvalues (lambda)Variance contributionsCumulative varianceVariable coordinatesCos² quality metrics",
    "crumbs": [
      "Functions",
      "RcppGetPCAVariablesHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#call-graph",
    "title": "RcppGetPCAVariablesHdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppGetPCAVariablesHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#source-code",
    "title": "RcppGetPCAVariablesHdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixPCA.hpp • Lines 66-168\ninline void RcppGetPCAVariablesHdf5( std::string strPCAgroup, \n                                  BigDataStatMeth::hdf5Dataset* dsd, \n                                  BigDataStatMeth::hdf5Dataset* dsv, \n                                  bool overwrite )\n    {\n        \n        BigDataStatMeth::hdf5Dataset* dslambda = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsvar = nullptr;\n        BigDataStatMeth::hdf5Dataset* dscumvar = nullptr;\n        BigDataStatMeth::hdf5Dataset* dscoord = nullptr;\n        BigDataStatMeth::hdf5Dataset* dscos2 = nullptr;\n        \n        try\n        {\n            H5::Exception::dontPrint();\n            \n            // int ielements = 0;\n            std::vector&lt;hsize_t&gt; stride = {1, 1},\n                                 block = {1, 1},\n                                 offset_d = {0, 0},\n                                 count_d = {dsd-&gt;nrows(), dsd-&gt;ncols()},\n                                 offset_v = {0, 0},\n                                 count_v = {dsv-&gt;nrows(), dsv-&gt;ncols()};\n            \n            std::vector&lt;double&gt; vdd( count_d[0] * count_d[1] );\n            dsd-&gt;readDatasetBlock( {offset_d[0], offset_d[1]}, {count_d[0], count_d[1]}, stride, block, vdd.data() );\n            Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; d (vdd.data(), count_d[0], count_d[1]);\n            \n            {    \n                // lambda\n                Eigen::VectorXd vvar = d.array().pow(2);\n                dslambda = new BigDataStatMeth::hdf5Dataset(dsd-&gt;getFileName(), strPCAgroup, \"lambda\" ,overwrite );\n                dslambda-&gt;createDataset( d.rows(), d.cols(), \"real\");\n                dslambda-&gt;writeDataset( vvar.data() );\n                delete dslambda; dslambda = nullptr;\n                \n                // Variance\n                vvar = (vvar/vvar.array().sum()) * 100;\n                dsvar = new BigDataStatMeth::hdf5Dataset(dsd-&gt;getFileName(), strPCAgroup, \"variance\" ,overwrite );\n                dsvar-&gt;createDataset( d.rows(), d.cols(), \"real\");\n                dsvar-&gt;writeDataset( vvar.data() );\n                delete dsvar; dsvar = nullptr;\n                \n                // cumulative variance (max 1000 elements (firsts))\n                // if(vvar.size()&gt;1000){  ielements = 1000;    }\n                // else{ ielements = vvar.size();    }\n                \n                dscumvar = new BigDataStatMeth::hdf5Dataset(dsd-&gt;getFileName(), strPCAgroup, \"cumvar\" ,overwrite );\n                dscumvar-&gt;createDataset( d.rows(), d.cols(), \"real\");\n                dscumvar-&gt;writeDataset( (cumsum(vvar)).data());\n                delete dscumvar; dscumvar = nullptr;\n            }\n            \n            {\n                Eigen::MatrixXd var_coord;\n                {\n                    // Coord vars\n                    std::vector&lt;double&gt; vdv( count_v[0] * count_v[1] );\n                    dsv-&gt;readDatasetBlock( {offset_v[0], offset_v[1]}, {count_v[0], count_v[1]}, stride, block, vdv.data() );\n                    Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; v (vdv.data(), count_v[0], count_v[1]);\n                    \n                    var_coord = Rcpp_matrixVectorMultiplication_byRow(v, d);\n                }\n                \n                dscoord = new BigDataStatMeth::hdf5Dataset(dsd-&gt;getFileName(), strPCAgroup, \"var.coord\" ,overwrite );\n                dscoord-&gt;createDataset( var_coord.rows(), var_coord.cols(), \"real\");\n                dscoord-&gt;writeDataset( var_coord.transpose().data() );\n                delete dscoord; dscoord = nullptr;\n    \n                // Cos2\n                Eigen::MatrixXd var_cos2 = var_coord.unaryExpr([](double d) {return std::pow(d, 2);});\n                dscos2 = new BigDataStatMeth::hdf5Dataset(dsd-&gt;getFileName(), strPCAgroup, \"var.cos2\" ,overwrite );\n                dscos2-&gt;createDataset( var_cos2.rows(), var_cos2.cols(), \"real\");\n                dscos2-&gt;writeDataset( var_cos2.transpose().data() );\n                delete dscos2; dscos2 = nullptr;\n                \n            }\n            \n        } catch( H5::FileIException& error ) { \n            checkClose_file(dsd, dsv, dslambda, dsvar, dscumvar, dscoord, dscos2);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppGetPCAVariablesHdf5 (File IException)\\n\";\n            return void();\n        } catch( H5::DataSetIException& error ) { \n            checkClose_file(dsd, dsv, dslambda, dsvar, dscumvar, dscoord, dscos2);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppGetPCAVariablesHdf5 (DataSet IException)\\n\";\n            return void();\n        } catch( H5::DataSpaceIException& error ) { \n            checkClose_file(dsd, dsv, dslambda, dsvar, dscumvar, dscoord, dscos2);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppGetPCAVariablesHdf5 (DataSpace IException)\\n\";\n            return void();\n        } catch(std::exception &ex) {\n            checkClose_file(dsd, dsv, dslambda, dsvar, dscumvar, dscoord, dscos2);\n            Rcpp::Rcout&lt;&lt;\"c++ exception RcppGetPCAVariablesHdf5 \\n\"&lt;&lt; ex.what();\n            return void();\n        } catch (...) {\n            checkClose_file(dsd, dsv, dslambda, dsvar, dscumvar, dscoord, dscos2);\n            Rcpp::Rcout&lt;&lt;\"\\nC++ exception RcppGetPCAVariablesHdf5 (unknown reason)\";\n            return void();\n        }\n        \n        \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "RcppGetPCAVariablesHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppGetPCAVariablesHdf5.html#usage-example",
    "title": "RcppGetPCAVariablesHdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppGetPCAVariablesHdf5(...);",
    "crumbs": [
      "Functions",
      "RcppGetPCAVariablesHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html",
    "href": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html",
    "title": "RcppBind_datasets_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppBind_datasets_hdf5(std::string filename, std::string group, Rcpp::StringVector datasets, std::string outgroup, std::string outdataset, std::string func, bool binternal, Rcpp::Nullable&lt; bool &gt; overwrite=false)",
    "crumbs": [
      "Functions",
      "RcppBind_datasets_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#signature",
    "title": "RcppBind_datasets_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppBind_datasets_hdf5(std::string filename, std::string group, Rcpp::StringVector datasets, std::string outgroup, std::string outdataset, std::string func, bool binternal, Rcpp::Nullable&lt; bool &gt; overwrite=false)",
    "crumbs": [
      "Functions",
      "RcppBind_datasets_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#description",
    "href": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#description",
    "title": "RcppBind_datasets_hdf5",
    "section": "2 Description",
    "text": "2 Description\nHigh-level interface for binding HDF5 datasets.",
    "crumbs": [
      "Functions",
      "RcppBind_datasets_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#parameters",
    "title": "RcppBind_datasets_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfilename (std::string): HDF5 file path\ngroup (std::string): Source group containing input datasets\ndatasets (Rcpp::StringVector): Vector of dataset names to bind\noutgroup (std::string): Output group path\noutdataset (std::string): Output dataset name\nfunc (std::string): Binding function type:“bindCols”: Combine datasets by columns”bindRows”: Combine datasets by rows”bindRowsbyIndex”: Combine datasets by rows using an index\nbinternal (bool): Internal processing flag\noverwrite (Rcpp::Nullable&lt; bool &gt;): Optional flag to overwrite existing output dataset",
    "crumbs": [
      "Functions",
      "RcppBind_datasets_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#details",
    "href": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#details",
    "title": "RcppBind_datasets_hdf5",
    "section": "4 Details",
    "text": "4 Details\nfilenameHDF5 file path groupSource group containing input datasets datasetsVector of dataset names to bind outgroupOutput group path outdatasetOutput dataset name funcBinding function type:“bindCols”: Combine datasets by columns”bindRows”: Combine datasets by rows”bindRowsbyIndex”: Combine datasets by rows using an index binternalInternal processing flag overwriteOptional flag to overwrite existing output datasetstd::range_errorif func is not one of the allowed values H5::FileIExceptionon file access errors H5::GroupIExceptionon group operation errors H5::DataSetIExceptionon dataset operation errors std::exceptionon general errorsRcppBind_datasets_hdf5(std::string, std::string, Rcpp::StringVector, BigDataStatMeth::hdf5Dataset*, int, bool) for the lower-level implementation Example: RcppBind_datasets_hdf5(“data.h5”,“/input”,{“ds1”,“ds2”},“/output”,“combined”,“bindRows”,false,true);",
    "crumbs": [
      "Functions",
      "RcppBind_datasets_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#call-graph",
    "title": "RcppBind_datasets_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppBind_datasets_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#source-code",
    "title": "RcppBind_datasets_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5BindDatasets.hpp • Lines 213-268\ninline void RcppBind_datasets_hdf5( std::string filename, std::string group, Rcpp::StringVector datasets, \n                               std::string outgroup, std::string outdataset, std::string func, \n                               bool binternal, Rcpp::Nullable&lt;bool&gt; overwrite = false )\n    {\n        \n        BigDataStatMeth::hdf5Dataset* dsOut = nullptr;\n        \n        try\n        {\n            \n            Rcpp::NumericVector oper = {0, 1, 2};\n            oper.names() = Rcpp::CharacterVector({ \"bindCols\", \"bindRows\", \"bindRowsbyIndex\"});\n            \n            bool boverwrite;\n            \n            if( overwrite.isNull()) { boverwrite = false; } \n            else {   boverwrite = Rcpp::as&lt;bool&gt;(overwrite); }\n            \n            if (func.compare(\"bindCols\") != 0 && func.compare(\"bindRows\") != 0  && func.compare(\"bindRowsbyIndex\") != 0 ) {\n                throw std::range_error( \"Function to apply must be \\\"bindRows\\\", \\\"bindCols\\\" or \\\"bindRowsbyIndex\\\" other values are not allowed\" );\n                return void();\n            }\n            \n            int bindFunction = oper.findName( func );\n            \n            dsOut = new BigDataStatMeth::hdf5Dataset(filename, outgroup, outdataset, boverwrite);\n            \n            RcppBind_datasets_hdf5( filename, group, datasets, dsOut, bindFunction, binternal);\n            \n            delete dsOut; dsOut = nullptr;\n            \n        } catch( H5::FileIException& error ) { \n            checkClose_file(dsOut);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception RcppBind_datasets_hdf5_ (File IException)\";\n            return void();\n        } catch( H5::GroupIException & error ) { \n            checkClose_file(dsOut);\n            Rcpp::Rcerr &lt;&lt;\"c++ exception RcppBind_datasets_hdf5_ (Group IException)\";\n            return void();\n        } catch( H5::DataSetIException& error ) { \n            checkClose_file(dsOut);\n            Rcpp::Rcerr &lt;&lt;\"c++ exception RcppBind_datasets_hdf5_ (DataSet IException)\";\n            return void();\n        } catch(std::exception& ex) {\n            checkClose_file(dsOut);\n            Rcpp::Rcerr &lt;&lt;\"c++ exception RcppBind_datasets_hdf5_\" &lt;&lt; ex.what();\n            return void();\n        } catch (...) {\n            checkClose_file(dsOut);\n            Rcpp::Rcerr&lt;&lt;\"C++ exception RcppBind_datasets_hdf5_ (unknown reason)\";\n            return void();\n        }\n        \n        return void();\n        \n    }",
    "crumbs": [
      "Functions",
      "RcppBind_datasets_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppBind_datasets_hdf5.html#usage-example",
    "title": "RcppBind_datasets_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppBind_datasets_hdf5(...);",
    "crumbs": [
      "Functions",
      "RcppBind_datasets_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html",
    "href": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html",
    "title": "Next_level_SvdBlock_decomposition_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Next_level_SvdBlock_decomposition_hdf5(T *dsA, std::string strGroupName, int k, int q, double dthreshold, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Next_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html#signature",
    "title": "Next_level_SvdBlock_decomposition_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Next_level_SvdBlock_decomposition_hdf5(T *dsA, std::string strGroupName, int k, int q, double dthreshold, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Next_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html#parameters",
    "title": "Next_level_SvdBlock_decomposition_hdf5",
    "section": "2 Parameters",
    "text": "2 Parameters\n\ndsA (T *)\nstrGroupName (std::string)\nk (int)\nq (int)\ndthreshold (double)\nthreads (Rcpp::Nullable&lt; int &gt;)",
    "crumbs": [
      "Functions",
      "Next_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html#returns",
    "title": "Next_level_SvdBlock_decomposition_hdf5",
    "section": "3 Returns",
    "text": "3 Returns\nType: class T",
    "crumbs": [
      "Functions",
      "Next_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html#call-graph",
    "title": "Next_level_SvdBlock_decomposition_hdf5",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "Next_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html#source-code",
    "title": "Next_level_SvdBlock_decomposition_hdf5",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSvdBlock.hpp • Lines 474-637\ninline void Next_level_SvdBlock_decomposition_hdf5( T* dsA, std::string strGroupName, int k, int q, \n                                                           double dthreshold, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n    {\n    \n    \n    static_assert(std::is_same&lt;T*, BigDataStatMeth::hdf5Dataset* &gt;::value || \n                  std::is_same&lt;T*, BigDataStatMeth::hdf5DatasetInternal* &gt;::value,\n                  \"Error - type not allowed\");\n\n\n    BigDataStatMeth::hdf5Dataset* unlimDataset = nullptr;    \n    BigDataStatMeth::hdf5Dataset* dsCur = nullptr;\n    \n    try {\n        \n        int cummoffset = 0, M;\n            // ithreads,  M;\n        \n        std::vector&lt;hsize_t&gt; stride = {1, 1},\n            block = {1, 1},\n            offset = {0, 0},\n            count = {0, 0};\n        \n        Rcpp::CharacterVector strvmatnames = {\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\n                                              \"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\n                                              \"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\n                                              \"Y\",\"Z\"};\n\n        // Get dataset names\n        Rcpp::StringVector joindata =  dsA-&gt;getDatasetNames(strGroupName, (std::string)strvmatnames[q-1], \"\");\n        M = joindata.size();\n        \n        // ithreads = get_number_threads(threads, R_NilValue);\n        \n        #pragma omp parallel num_threads( get_number_threads(threads, R_NilValue) )\n\n        // Get data from M blocks in initial matrix\n        #pragma omp for ordered schedule (dynamic)\n        for( int i = 0; i&lt; M ; i++)\n        {\n            \n            std::string strDatasetName = strGroupName + \"/\" + strvmatnames[q] + std::to_string(i/(M/k));\n            \n            Eigen::MatrixXd restmp;\n            Eigen::MatrixXd X;\n\n            #pragma omp critical(accessFile)\n            {\n                //    a) Get dataset\n                dsCur = new BigDataStatMeth::hdf5Dataset(dsA-&gt;getFullPath(), strGroupName + \"/\" + joindata[i], false);\n                dsCur-&gt;openDataset();\n                hsize_t* dims_out = dsCur-&gt;dim();\n                \n                std::vector&lt;double&gt; vdCurDataset( dims_out[0] * dims_out[1] ); \n                dsCur-&gt;readDatasetBlock( {0, 0}, {dims_out[0], dims_out[1]}, stride, block, vdCurDataset.data() );\n                X = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vdCurDataset.data(), dims_out[0], dims_out[1] );\n                \n                delete dsCur; dsCur = nullptr;\n            }\n\n            {\n                //    b) SVD for each block\n                svdeig retsvd;\n                retsvd = RcppbdSVD_lapack(X, false, false, false);\n            \n                int size_d = (retsvd.d).size();\n                int nzeros = 0;\n            \n                if( (retsvd.d)[size_d - 1] &lt;= dthreshold ) {\n                    nzeros = 1;\n                    for( int j = (size_d - 2); ( j&gt;1 && (retsvd.d)[i] &lt;= dthreshold ); j-- ) {\n                        nzeros++;\n                    }\n                }\n            \n                //    c)  U*d\n                // Create diagonal matrix from svd decomposition d\n                int isize = (retsvd.d).size() - nzeros;\n                if( isize &lt; 2 ) {\n                    isize = 2;\n                }\n            \n                Eigen::MatrixXd d = Eigen::MatrixXd::Zero(isize, isize);\n                d.diagonal() = (retsvd.d).head(isize);\n            \n                Eigen::MatrixXd u = (retsvd.u).block(0, 0, (retsvd.u).rows(), isize);\n            \n                // if( u.size() &lt; MAXELEMSINBLOCK ) {\n                if (static_cast&lt;hsize_t&gt;(u.size()) &lt; MAXELEMSINBLOCK) {\n                    restmp = u*d;\n                } else{\n                    restmp = Rcpp_block_matrix_mul( u, d, R_NilValue);    \n                } \n            }\n            \n            //    d) Write results to dataset\n            count[0] = restmp.rows();\n            count[1] = restmp.cols();\n\n            #pragma omp ordered\n            {\n                #pragma omp critical(accessFile)\n                {\n                    \n                    if( i%(M/k) == 0 || ( (i%(M/k) &gt; 0 &&  !BigDataStatMeth::exists_HDF5_element(dsA-&gt;getFileptr(),  strDatasetName)) ) ) {\n                        // Create unlimited dataset in hdf5 file\n                        unlimDataset = new BigDataStatMeth::hdf5DatasetInternal(dsA-&gt;getFullPath(), strDatasetName, true );\n                        unlimDataset-&gt;createUnlimitedDataset(count[0], count[1], \"real\");\n                        delete unlimDataset; unlimDataset = nullptr;\n                        \n                        cummoffset = 0;\n                    }\n                    \n                    unlimDataset = new BigDataStatMeth::hdf5DatasetInternal(dsA-&gt;getFullPath(), strDatasetName, true );\n                    unlimDataset-&gt;openDataset();\n                    \n                    // Get write position\n                    offset[1] = cummoffset;\n                    cummoffset = cummoffset + restmp.cols();\n                    \n                    // Extend dataset before put data\n                    if((i%(M/k)) != 0 && cummoffset &gt; 0) {\n                        unlimDataset-&gt;extendUnlimitedDataset(0, count[1] );\n                    }\n                    \n                    unlimDataset-&gt;writeDatasetBlock( Rcpp::wrap(restmp), offset, count, stride, block, false);\n                    delete unlimDataset; unlimDataset = nullptr;\n                }\n            }\n        }\n\n    } catch( H5::FileIException& error ) { \n        checkClose_file(dsA, unlimDataset, dsCur);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Next_level_SvdBlock_decomposition_hdf5 (File IException)\\n\";\n        return void();\n    } catch( H5::DataSetIException& error ) { \n        checkClose_file(dsA, unlimDataset, dsCur);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Next_level_SvdBlock_decomposition_hdf5 (DataSet IException)\\n\";\n        return void();\n    } catch( H5::GroupIException& error ) { \n        checkClose_file(dsA, unlimDataset, dsCur);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Next_level_SvdBlock_decomposition_hdf5 (Group IException)\\n\";\n        return void();\n    } catch( H5::DataTypeIException& error ) { \n        checkClose_file(dsA, unlimDataset, dsCur);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Next_level_SvdBlock_decomposition_hdf5 (DataType IException)\\n\";\n        return void();\n    } catch( H5::DataSpaceIException& error ) { \n        checkClose_file(dsA, unlimDataset, dsCur);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Next_level_SvdBlock_decomposition_hdf5 (DataSpace IException)\\n\";\n        return void();\n    } catch(std::exception &ex) {\n        checkClose_file(dsA, unlimDataset, dsCur);\n        Rcpp::Rcerr&lt;&lt;\"c++ exception Next_level_SvdBlock_decomposition_hdf5 \\n\"&lt;&lt; ex.what();\n        return void();\n    } catch (...) {\n        checkClose_file(dsA, unlimDataset, dsCur);\n        Rcpp::Rcerr&lt;&lt;\"\\nC++ exception Next_level_SvdBlock_decomposition_hdf5 (unknown reason)\";\n        return void();\n    }\n    \n    return void();\n\n}",
    "crumbs": [
      "Functions",
      "Next_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.html#usage-example",
    "title": "Next_level_SvdBlock_decomposition_hdf5",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Next_level_SvdBlock_decomposition_hdf5(...);",
    "crumbs": [
      "Functions",
      "Next_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html",
    "title": "Inverse_of_Cholesky_decomposition_intermediate_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Inverse_of_Cholesky_decomposition_intermediate_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#signature",
    "title": "Inverse_of_Cholesky_decomposition_intermediate_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Inverse_of_Cholesky_decomposition_intermediate_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#description",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#description",
    "title": "Inverse_of_Cholesky_decomposition_intermediate_hdf5",
    "section": "2 Description",
    "text": "2 Description\nComputes inverse of Cholesky factor in-place.",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#parameters",
    "title": "Inverse_of_Cholesky_decomposition_intermediate_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nInOutDataset (BigDataStatMeth::hdf5Dataset *): Dataset containing Cholesky factor L (will be overwritten with L^-1)\nidim0 (int): Number of rows\nidim1 (int): Number of columns\ndElementsBlock (long): Block size for processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#details",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#details",
    "title": "Inverse_of_Cholesky_decomposition_intermediate_hdf5",
    "section": "4 Details",
    "text": "4 Details\nInOutDatasetDataset containing Cholesky factor L (will be overwritten with L^-1) idim0Number of rows idim1Number of columns dElementsBlockBlock size for processing threadsNumber of threads for parallel processing (optional) Implements block-wise inversion of lower triangular matrix:Processes matrix in blocks for memory efficiencyOverwrites input with its inverseUses parallel processing for independent computations",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#call-graph",
    "title": "Inverse_of_Cholesky_decomposition_intermediate_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#source-code",
    "title": "Inverse_of_Cholesky_decomposition_intermediate_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixInvCholesky.hpp • Lines 711-873\ninline void Inverse_of_Cholesky_decomposition_intermediate_hdf5( BigDataStatMeth::hdf5Dataset* InOutDataset, \n                                    int idim0, int idim1, long dElementsBlock, \n                                    Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n{\n    \n    try{\n        \n        int dimensionSize = idim0, \n            readedCols = 0,\n            colstoRead,\n            minimumBlockSize;\n        \n        std::vector&lt;hsize_t&gt; offset = {0,0},\n                             count = {1,1},\n                             stride = {1,1},\n                             block = {1,1};\n\n        // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_of_Cholesky_decomposition_intermediate_hdf5 -&gt; Inici\";\n        // Set minimum elements in block (mandatory : minimum = 2 * longest line)\n        if( dElementsBlock &lt; dimensionSize * 2 ) {\n            // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_of_Cholesky -&gt; if(dElementsBlock &lt; dimensionSize * 2)\";\n            minimumBlockSize = dimensionSize * 2;\n        } else {\n            // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_of_Cholesky -&gt; else\";\n            minimumBlockSize = dElementsBlock;\n        }\n        \n        // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_of_Cholesky -&gt; getDiagonalfromMatrix(InOutDataset)\";\n        Eigen::VectorXd Diagonal = Rcpp::as&lt;Eigen::VectorXd&gt;(getDiagonalfromMatrix(InOutDataset));\n        // Set the new diagonal in the result matrix\n        setDiagonalMatrix( InOutDataset, Rcpp::wrap(Diagonal.cwiseInverse()) );\n        \n        // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_of_Cholesky -&gt; Despres setDiagonal\";\n        \n        if( idim0 == idim1) {\n            \n            // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_of_Cholesky -&gt; idim0 == idim1\";\n            \n            while ( readedCols &lt; dimensionSize ) {\n                \n                // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_of_Cholesky -&gt; readedCols &lt; dimensionSize\";\n                \n                colstoRead = ( -2 * readedCols - 1 + std::sqrt( pow(2*readedCols, 2) - 4 * readedCols + 8 * minimumBlockSize + 1) ) / 2;\n                \n                if (colstoRead &lt;= 0) {\n                    colstoRead = minimumBlockSize; // Minimum progress to avoid infinite loop\n                }\n                \n                if( readedCols + colstoRead &gt; idim0) { // Max size bigger than data to read ?\n                    colstoRead = idim0 - readedCols;\n                }\n                \n                size_t potential_elements = static_cast&lt;size_t&gt;(dimensionSize - readedCols) * static_cast&lt;size_t&gt;(readedCols + colstoRead);\n                size_t optimalSize = getOptimalBlockElements();\n                if (potential_elements &gt; optimalSize) {\n                    int max_cols = static_cast&lt;int&gt;(MAXCHOLBLOCKSIZE / (dimensionSize - readedCols));\n                    colstoRead = max_cols - readedCols;\n                    if (colstoRead &lt; 1) colstoRead = 1;\n                }\n                \n                offset[0] = readedCols;\n                count[0] =  dimensionSize - offset[0];\n                count[1] = readedCols + colstoRead;\n                \n                Eigen::MatrixXd verticalData;\n                \n                std::vector&lt;double&gt; vverticalData( count[0] * count[1] ); \n                InOutDataset-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vverticalData.data() );\n                verticalData = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vverticalData.data(), count[0], count[1] );\n                \n                //. 20251121 .// for (int j = 1; j &lt; dimensionSize - offset[0]; j++)\n                //!!!!!!!!!!!! for (int j = 1; j &lt; dimensionSize - offset[0] && j &lt; static_cast&lt;int&gt;(count[0]); j++)\n                for (int j = 1; j &lt; dimensionSize - static_cast&lt;int&gt;(offset[0]); j++)\n                {\n                    // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_of_Cholesky -&gt; j &lt; dimensionSize - static_cast&lt;int&gt;(offset[0])\";\n                    \n                    Eigen::VectorXd vR = Eigen::VectorXd::Zero(j+static_cast&lt;int&gt;(offset[0]));\n                    Eigen::ArrayXd ar_j;\n                    \n                    int size_j;\n                    if( j &lt; colstoRead) {\n                        size_j = j;\n                    } else {\n                        size_j = colstoRead;\n                    }\n                    \n                    ar_j = verticalData.block( j, static_cast&lt;int&gt;(offset[0]), 1,  size_j).transpose().array();\n                    \n                    vR = Eigen::VectorXd::Zero(static_cast&lt;int&gt;(offset[0]) + ar_j.size());\n                    \n#pragma omp parallel for num_threads( get_number_threads(threads, R_NilValue) ) shared (ar_j, j, verticalData, offset, colstoRead, vR) schedule(dynamic) \n                    for (int i = 0; i &lt; static_cast&lt;int&gt;(offset[0]) + ar_j.size() ; i++) {\n                        \n                        Eigen::ArrayXd ar_i = verticalData.block( 0, i, size_j, 1).array();\n                        \n                        if( static_cast&lt;int&gt;(offset[0]) &gt; 0 ) {\n                            if( j  &lt;= colstoRead ) {\n                                if( i &lt; static_cast&lt;int&gt;(offset[0]) ){\n                                    vR(i) =  (( verticalData.coeff(j, i) + ((ar_j.transpose() * ar_i.transpose()).sum())) * (-1)) / Diagonal[j+static_cast&lt;int&gt;(offset[0])];\n                                } else {\n                                    vR(i) =   ((ar_j.transpose() * ar_i.transpose()) * (-1)).sum() / Diagonal[j+static_cast&lt;int&gt;(offset[0])];\n                                    \n                                }\n                            } else {\n                                if( i &lt; static_cast&lt;int&gt;(offset[0]) ){\n                                    vR(i) =  ( verticalData.coeff(j, i) + ((ar_j.transpose() * ar_i.transpose()).sum()));\n                                } else {\n                                    vR(i) =   (ar_j.transpose() * ar_i.transpose()).sum();\n                                }\n                            }\n                        } else {\n                            if( j &lt;= colstoRead ) {\n                                vR(i) =   ((ar_j.transpose() * ar_i.transpose()) * (-1)).sum() / Diagonal[j+static_cast&lt;int&gt;(offset[0])];\n                            } else {\n                                vR(i) =   (ar_j.transpose() * ar_i.transpose()).sum();\n                            }\n                        }\n                    }\n                    \n                    verticalData.block(j, 0, 1, vR.size()) = vR.transpose();\n                    \n                }\n                \n                InOutDataset-&gt;writeDatasetBlock( Rcpp::wrap(verticalData), offset, count, stride, block, false);\n                readedCols = readedCols + colstoRead; // Ho preparem perquè desprès necessitarem llegir a partir de la línea anterior\n                \n            }\n        } else {\n            throw std::range_error(\"non-conformable arguments\");\n        }\n        \n        // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_of_Cholesky -&gt; Bye Bye...\";\n        \n    } catch( H5::FileIException& error ) { \n        checkClose_file(InOutDataset);\n        InOutDataset = nullptr;\n        Rcpp::Rcerr&lt;&lt;\"c++ exception Inverse_of_Cholesky_decomposition_intermediate_hdf5 (File IException)\";\n        return void();\n    } catch( H5::GroupIException & error ) { \n        checkClose_file(InOutDataset);\n        InOutDataset = nullptr;\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Inverse_of_Cholesky_decomposition_intermediate_hdf5 (Group IException)\";\n        return void();\n    } catch( H5::DataSetIException& error ) { \n        checkClose_file(InOutDataset);\n        InOutDataset = nullptr;\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Inverse_of_Cholesky_decomposition_intermediate_hdf5 (DataSet IException)\";\n        return void();\n    } catch(std::exception& ex) {\n        checkClose_file(InOutDataset);\n        InOutDataset = nullptr;\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Inverse_of_Cholesky_decomposition_intermediate_hdf5\" &lt;&lt; ex.what();\n        return void();\n    } catch (...) {\n        checkClose_file(InOutDataset);\n        InOutDataset = nullptr;\n        Rcpp::Rcerr&lt;&lt;\"\\nC++ exception Inverse_of_Cholesky_decomposition_intermediate_hdf5 (unknown reason)\";\n        return void();\n    }\n    \n    return void();\n    \n}",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.html#usage-example",
    "title": "Inverse_of_Cholesky_decomposition_intermediate_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Inverse_of_Cholesky_decomposition_intermediate_hdf5(...);",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html",
    "title": "Inverse_Matrix_Cholesky_outofcore_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Inverse_Matrix_Cholesky_outofcore_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#signature",
    "title": "Inverse_Matrix_Cholesky_outofcore_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Inverse_Matrix_Cholesky_outofcore_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#description",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#description",
    "title": "Inverse_Matrix_Cholesky_outofcore_hdf5",
    "section": "2 Description",
    "text": "2 Description\nOut-of-core computation of A^-1 = L^-T L^-1 using tiles.",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#parameters",
    "title": "Inverse_Matrix_Cholesky_outofcore_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nInOutDataset (BigDataStatMeth::hdf5Dataset *): Dataset containing L^-1 (overwritten with A^-1)\nidim0 (int): Number of rows\nidim1 (int): Number of columns\ndElementsBlock (long): Block size (not used, tiles are fixed)\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#details",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#details",
    "title": "Inverse_Matrix_Cholesky_outofcore_hdf5",
    "section": "4 Details",
    "text": "4 Details\nInOutDatasetDataset containing L^-1 (overwritten with A^-1) idim0Number of rows idim1Number of columns dElementsBlockBlock size (not used, tiles are fixed) threadsNumber of threads for parallel processing (optional) Computes final inverse from inverted Cholesky factor:Accumulates triple-nested tile productsExploits symmetry (only lower triangle computed)Memory usage: ~1.6 GB constantComputes final inverse from inverted Cholesky factor Formula: A^-1(i,j) = sum_k L^-1(k,i) * L^-1(k,j) Reads column tiles vertically to compute dot products",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#call-graph",
    "title": "Inverse_Matrix_Cholesky_outofcore_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#source-code",
    "title": "Inverse_Matrix_Cholesky_outofcore_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixInvCholesky.hpp • Lines 1137-1237\ninline void Inverse_Matrix_Cholesky_outofcore_hdf5(BigDataStatMeth::hdf5Dataset* InOutDataset, \n                                                   int idim0, int idim1, long dElementsBlock, \n                                                   Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n{\n    \n    //  BigDataStatMeth::hdf5Dataset* dsA = nullptr;\n    \n    try {\n        int dimensionSize = idim0;\n        int tileSize = 10000; // Fixed tile size for memory control\n        int numTiles = (dimensionSize + tileSize - 1) / tileSize;\n        \n        std::vector&lt;hsize_t&gt; stride = {1,1}, block = {1,1};\n        \n        // Create temporary dataset to avoid overwriting L^-1 during computation\n        std::string tempDatasetPath = InOutDataset-&gt;getGroup() + \"/.tmp_inverse_\" + InOutDataset-&gt;getDatasetName();\n        BigDataStatMeth::hdf5DatasetInternal tempDataset(InOutDataset-&gt;getFileName(), tempDatasetPath, true);\n        tempDataset.createDataset(idim0, idim1, \"real\");\n        \n        // Parallel computation over result tiles (i,j)\n        // Result is symmetric, only compute lower triangle\n#pragma omp parallel for num_threads(get_number_threads(threads, R_NilValue)) schedule(dynamic)\n        for (int i = 0; i &lt; numTiles; i++) {\n            \n            hsize_t iStart = i * tileSize;\n            hsize_t iSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - iStart));\n            \n            // Only compute j &lt;= i (lower triangle)\n            for (int j = 0; j &lt;= i; j++) {\n                \n                hsize_t jStart = j * tileSize;\n                hsize_t jSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - jStart));\n                \n                // Initialize result tile\n                Eigen::MatrixXd Rij = Eigen::MatrixXd::Zero(iSize, jSize);\n                \n                // Sum over k: A^-1(i,j) = sum_k L^-1(k,i) * L^-1(k,j)\n                // L^-1 is lower triangular, so L^-1(k,i) = 0 for k &lt; i\n                // Start from max(i,j) to skip zero products\n                int k_start = (i &gt; j) ? i : j;\n                \n                for (int k = k_start; k &lt; numTiles; k++) {\n                    \n                    hsize_t kStart = k * tileSize;\n                    hsize_t kSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - kStart));\n                    \n                    // Read column tile i: rows k, columns i\n                    // This gives L^-1(k,i) which we need for the product\n                    Eigen::MatrixXd Lki(kSize, iSize);\n                    std::vector&lt;double&gt; vLki(kSize * iSize);\n                    InOutDataset-&gt;readDatasetBlock({kStart, iStart}, {kSize, iSize}, stride, block, vLki.data());\n                    Lki = Eigen::Map&lt;Eigen::MatrixXd&gt;(vLki.data(), kSize, iSize);\n                    \n                    // Read column tile j: rows k, columns j  \n                    // This gives L^-1(k,j)\n                    Eigen::MatrixXd Lkj(kSize, jSize);\n                    std::vector&lt;double&gt; vLkj(kSize * jSize);\n                    InOutDataset-&gt;readDatasetBlock({kStart, jStart}, {kSize, jSize}, stride, block, vLkj.data());\n                    Lkj = Eigen::Map&lt;Eigen::MatrixXd&gt;(vLkj.data(), kSize, jSize);\n                    \n                    // Accumulate: R(i,j) += L^-1(k,i)^T * L^-1(k,j)\n                    // Lki has rows k, cols i → transpose gives rows i, cols k\n                    // Result: (i × k) × (k × j) = (i × j)\n                    Rij += Lki.transpose() * Lkj;\n                }\n                \n                // Write result tile to temporary dataset\n                tempDataset.writeDatasetBlock(Rcpp::wrap(Rij), {iStart, jStart}, {iSize, jSize}, stride, block, false);\n            }\n        }\n        \n        // Copy result from temporary to original dataset\n        for (int i = 0; i &lt; numTiles; i++) {\n            hsize_t iStart = i * tileSize;\n            hsize_t iSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - iStart));\n            \n            for (int j = 0; j &lt;= i; j++) {\n                hsize_t jStart = j * tileSize;\n                hsize_t jSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - jStart));\n                \n                // Read tile from temporary dataset\n                Eigen::MatrixXd Rij(iSize, jSize);\n                std::vector&lt;double&gt; vRij(iSize * jSize);\n                tempDataset.readDatasetBlock({iStart, jStart}, {iSize, jSize}, stride, block, vRij.data());\n                Rij = Eigen::Map&lt;Eigen::MatrixXd&gt;(vRij.data(), iSize, jSize);\n                \n                // Write to original dataset\n                InOutDataset-&gt;writeDatasetBlock(Rcpp::wrap(Rij), {iStart, jStart}, {iSize, jSize}, stride, block, false);\n            }\n        }\n        \n        // Clean up temporary dataset\n        tempDataset.remove();\n        \n    } catch(std::exception& ex) {\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Inverse_Matrix_Cholesky_outofcore_hdf5: \" &lt;&lt; ex.what();\n        return void();\n    }\n    \n    return void();\n}",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.html#usage-example",
    "title": "Inverse_Matrix_Cholesky_outofcore_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Inverse_Matrix_Cholesky_outofcore_hdf5(...);",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html",
    "title": "Inverse_Matrix_Cholesky_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Inverse_Matrix_Cholesky_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#signature",
    "title": "Inverse_Matrix_Cholesky_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Inverse_Matrix_Cholesky_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#description",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#description",
    "title": "Inverse_Matrix_Cholesky_hdf5",
    "section": "2 Description",
    "text": "2 Description\nComputes final matrix inverse with automatic algorithm selection.",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#parameters",
    "title": "Inverse_Matrix_Cholesky_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nInOutDataset (BigDataStatMeth::hdf5Dataset *): Dataset containing L^-1 (will be overwritten with A^-1)\nidim0 (int): Number of rows\nidim1 (int): Number of columns\ndElementsBlock (long): Block size for processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#details",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#details",
    "title": "Inverse_Matrix_Cholesky_hdf5",
    "section": "4 Details",
    "text": "4 Details\nInOutDatasetDataset containing L^-1 (will be overwritten with A^-1) idim0Number of rows idim1Number of columns dElementsBlockBlock size for processing threadsNumber of threads for parallel processing (optional) Computes A^-1 = L^-T L^-1 from inverted Cholesky factor. Automatically selects algorithm based on matrix size.",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#call-graph",
    "title": "Inverse_Matrix_Cholesky_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#source-code",
    "title": "Inverse_Matrix_Cholesky_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixInvCholesky.hpp • Lines 948-959\ninline void Inverse_Matrix_Cholesky_hdf5(BigDataStatMeth::hdf5Dataset* InOutDataset, \n                                             int idim0, int idim1, long dElementsBlock, \n                                             Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n{\n    // Detect matrix size and select algorithm\n    if (idim0 &gt;= CHOLESKY_OUTOFCORE_THRESHOLD) {\n        Rcpp::Rcout &lt;&lt; \"\\nUsing out-of-core matrix inverse for large matrix (\" &lt;&lt; idim0 &lt;&lt; \"x\" &lt;&lt; idim1 &lt;&lt; \")\\n\";\n        Inverse_Matrix_Cholesky_outofcore_hdf5(InOutDataset, idim0, idim1, dElementsBlock, threads);\n    } else {\n        Inverse_Matrix_Cholesky_intermediate_hdf5(InOutDataset, idim0, idim1, dElementsBlock, threads);\n    }\n}",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.html#usage-example",
    "title": "Inverse_Matrix_Cholesky_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Inverse_Matrix_Cholesky_hdf5(...);",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html",
    "title": "Cholesky_decomposition_outofcore_hdf5",
    "section": "",
    "text": "int BigDataStatMeth::Cholesky_decomposition_outofcore_hdf5(BigDataStatMeth::hdf5Dataset *inDataset, BigDataStatMeth::hdf5Dataset *outDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#signature",
    "title": "Cholesky_decomposition_outofcore_hdf5",
    "section": "",
    "text": "int BigDataStatMeth::Cholesky_decomposition_outofcore_hdf5(BigDataStatMeth::hdf5Dataset *inDataset, BigDataStatMeth::hdf5Dataset *outDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#description",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#description",
    "title": "Cholesky_decomposition_outofcore_hdf5",
    "section": "2 Description",
    "text": "2 Description\nOut-of-core Cholesky decomposition for large matrices using tiled algorithm.",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#parameters",
    "title": "Cholesky_decomposition_outofcore_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ninDataset (BigDataStatMeth::hdf5Dataset *): Input matrix dataset\noutDataset (BigDataStatMeth::hdf5Dataset *): Output dataset for Cholesky factor L\nidim0 (int): Number of rows\nidim1 (int): Number of columns\ndElementsBlock (long): Block size (not used, tiles are fixed)\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#returns",
    "title": "Cholesky_decomposition_outofcore_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nint 0 on success, 1 if not positive definite, 2 on errors",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#details",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#details",
    "title": "Cholesky_decomposition_outofcore_hdf5",
    "section": "5 Details",
    "text": "5 Details\ninDatasetInput matrix dataset outDatasetOutput dataset for Cholesky factor L idim0Number of rows idim1Number of columns dElementsBlockBlock size (not used, tiles are fixed) threadsNumber of threads for parallel processing (optional) int 0 on success, 1 if not positive definite, 2 on errors Fixed-tile algorithm for constant memory usage:Processes matrix in 10k×10k tilesMemory usage: ~800 MB constant regardless of matrix sizeSuitable for matrices &gt;250k dimensionsThree-step process: diagonal factorization, column solve, submatrix updateFixed-tile algorithm for constant memory usage with complete matrix handling",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#call-graph",
    "title": "Cholesky_decomposition_outofcore_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#source-code",
    "title": "Cholesky_decomposition_outofcore_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixInvCholesky.hpp • Lines 545-692\ninline int Cholesky_decomposition_outofcore_hdf5(BigDataStatMeth::hdf5Dataset* inDataset,  \n                                                 BigDataStatMeth::hdf5Dataset* outDataset, \n                                                 int idim0, int idim1, long dElementsBlock, \n                                                 Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n{\n    \n    // BigDataStatMeth::hdf5DatasetInternal* tmpA = nullptr;\n    \n    try {\n        int dimensionSize = idim0;\n        int tileSize = 10000; // Fixed tile size for predictable memory usage\n        int numTiles = (dimensionSize + tileSize - 1) / tileSize;\n        \n        std::vector&lt;hsize_t&gt; stride = {1,1}, block = {1,1};\n        \n        // Create temporary working copy of input matrix\n        // Block-wise algorithm requires modification of A during computation\n        std::string tempAPath = inDataset-&gt;getGroup() + \"/.tmp_chol_A_\" + inDataset-&gt;getDatasetName();\n        BigDataStatMeth::hdf5DatasetInternal tempA(inDataset-&gt;getFileName(), tempAPath, true);\n        \n        tempA.createDataset(idim0, idim1, \"real\");\n\n        // Copy complete symmetric matrix to temporary working dataset\n        // Need full matrix for proper block-wise updates\n        for (int i = 0; i &lt; numTiles; i++) {\n            \n            hsize_t iStart = i * tileSize;\n            hsize_t iSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - iStart));\n            \n            for (int j = 0; j &lt; numTiles; j++) {  // Complete matrix, not just lower triangle\n                hsize_t jStart = j * tileSize;\n                hsize_t jSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - jStart));\n                \n                Eigen::MatrixXd Aij(iSize, jSize);\n                std::vector&lt;double&gt; vAij(iSize * jSize);\n                \n                if (j &lt;= i) {\n                    // Read from lower triangle of input matrix\n                    inDataset-&gt;readDatasetBlock({iStart, jStart}, {iSize, jSize}, stride, block, vAij.data());\n                    Aij = Eigen::Map&lt;Eigen::MatrixXd&gt;(vAij.data(), iSize, jSize);\n                } else {\n                    // Use symmetry: A(i,j) = A(j,i)^T for upper triangle tiles\n                    std::vector&lt;double&gt; vAji(jSize * iSize);\n                    inDataset-&gt;readDatasetBlock({jStart, iStart}, {jSize, iSize}, stride, block, vAji.data());\n                    Eigen::MatrixXd Aji = Eigen::Map&lt;Eigen::MatrixXd&gt;(vAji.data(), jSize, iSize);\n                    Aij = Aji.transpose();\n                }\n                \n                tempA.writeDatasetBlock(Rcpp::wrap(Aij), {iStart, jStart}, {iSize, jSize}, stride, block, false);\n            }\n        }\n        \n        // Block-wise Cholesky algorithm: A = L * L^T where L is lower triangular\n        // Process tiles in order: diagonal factorization -&gt; column solve -&gt; submatrix update\n        for (int k = 0; k &lt; numTiles; k++) {\n            \n            hsize_t kStart = k * tileSize;\n            hsize_t kSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - kStart));\n            \n            // Step 1: Diagonal factorization - L(k,k) = cholesky(A(k,k))\n            Eigen::MatrixXd Akk(kSize, kSize);\n            std::vector&lt;double&gt; vAkk(kSize * kSize);\n            tempA.readDatasetBlock({kStart, kStart}, {kSize, kSize}, stride, block, vAkk.data());\n            Akk = Eigen::Map&lt;Eigen::MatrixXd&gt;(vAkk.data(), kSize, kSize);\n            \n            // In-place Cholesky factorization of diagonal tile\n            Eigen::LLT&lt;Eigen::MatrixXd&gt; llt(Akk);\n            if (llt.info() != Eigen::Success) {\n                tempA.remove();\n                Rcpp::Rcout &lt;&lt; \"\\nMatrix not positive definite at tile \" &lt;&lt; k &lt;&lt; \"\\n\";\n                return 1;\n            }\n            Eigen::MatrixXd Lkk = llt.matrixL();\n            \n            // Write diagonal Cholesky factor to output\n            outDataset-&gt;writeDatasetBlock(Rcpp::wrap(Lkk), {kStart, kStart}, {kSize, kSize}, stride, block, false);\n            \n            // Step 2: Column solve - L(i,k) = A(i,k) * inv(L(k,k)^T)\n            // Columns are independent, can be parallelized\n            #pragma omp parallel for num_threads(get_number_threads(threads, R_NilValue)) schedule(dynamic)\n            for (int i = k + 1; i &lt; numTiles; i++) {\n                \n                hsize_t iStart = i * tileSize;\n                hsize_t iSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - iStart));\n                \n                Eigen::MatrixXd Aik(iSize, kSize);\n                std::vector&lt;double&gt; vAik(iSize * kSize);\n                tempA.readDatasetBlock({iStart, kStart}, {iSize, kSize}, stride, block, vAik.data());\n                Aik = Eigen::Map&lt;Eigen::MatrixXd&gt;(vAik.data(), iSize, kSize);\n                \n                // Solve triangular system: Lkk^T * Lik^T = Aik^T\n                Eigen::MatrixXd Lik = Lkk.triangularView&lt;Eigen::Lower&gt;().solve(Aik.transpose()).transpose();\n                \n                // Write column factor to output\n                outDataset-&gt;writeDatasetBlock(Rcpp::wrap(Lik), {iStart, kStart}, {iSize, kSize}, stride, block, false);\n            }\n            \n            // Step 3: Submatrix update - A(i,j) -= L(i,k) * L(j,k)^T\n            // Outer loop over tiles can be parallelized (tiles are independent)\n            #pragma omp parallel for num_threads(get_number_threads(threads, R_NilValue)) schedule(dynamic)\n            for (int i = k + 1; i &lt; numTiles; i++) {\n                \n                hsize_t iStart = i * tileSize;\n                hsize_t iSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - iStart));\n                \n                // Load computed L(i,k) factor\n                Eigen::MatrixXd Lik(iSize, kSize);\n                std::vector&lt;double&gt; vLik(iSize * kSize);\n                outDataset-&gt;readDatasetBlock({iStart, kStart}, {iSize, kSize}, stride, block, vLik.data());\n                Lik = Eigen::Map&lt;Eigen::MatrixXd&gt;(vLik.data(), iSize, kSize);\n                \n                // Update tiles in current row (inner loop cannot be parallelized due to dependencies)\n                for (int j = k + 1; j &lt;= i; j++) {\n                    \n                    hsize_t jStart = j * tileSize;\n                    hsize_t jSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - jStart));\n                    \n                    // Load computed L(j,k) factor\n                    Eigen::MatrixXd Ljk(jSize, kSize);\n                    std::vector&lt;double&gt; vLjk(jSize * kSize);\n                    outDataset-&gt;readDatasetBlock({jStart, kStart}, {jSize, kSize}, stride, block, vLjk.data());\n                    Ljk = Eigen::Map&lt;Eigen::MatrixXd&gt;(vLjk.data(), jSize, kSize);\n                    \n                    // Load current A(i,j) tile for update\n                    Eigen::MatrixXd Aij(iSize, jSize);\n                    std::vector&lt;double&gt; vAij(iSize * jSize);\n                    tempA.readDatasetBlock({iStart, jStart}, {iSize, jSize}, stride, block, vAij.data());\n                    Aij = Eigen::Map&lt;Eigen::MatrixXd&gt;(vAij.data(), iSize, jSize);\n                    \n                    // Apply rank-k update: A(i,j) -= L(i,k) * L(j,k)^T\n                    Aij -= Lik * Ljk.transpose();\n                    \n                    // Write updated tile back to working matrix\n                    tempA.writeDatasetBlock(Rcpp::wrap(Aij), {iStart, jStart}, {iSize, jSize}, stride, block, false);\n                }\n            }\n        }\n        \n        // Clean up temporary working dataset\n        tempA.remove();\n        \n    } catch(std::exception& ex) {\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Cholesky_decomposition_outofcore_hdf5: \" &lt;&lt; ex.what();\n        return 2;\n    }\n    \n    return 0;\n}",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.html#usage-example",
    "title": "Cholesky_decomposition_outofcore_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Cholesky_decomposition_outofcore_hdf5(...);",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html",
    "title": "Cholesky_decomposition_hdf5",
    "section": "",
    "text": "int BigDataStatMeth::Cholesky_decomposition_hdf5(BigDataStatMeth::hdf5Dataset *inDataset, BigDataStatMeth::hdf5Dataset *outDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#signature",
    "title": "Cholesky_decomposition_hdf5",
    "section": "",
    "text": "int BigDataStatMeth::Cholesky_decomposition_hdf5(BigDataStatMeth::hdf5Dataset *inDataset, BigDataStatMeth::hdf5Dataset *outDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#description",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#description",
    "title": "Cholesky_decomposition_hdf5",
    "section": "2 Description",
    "text": "2 Description\nPerforms Cholesky decomposition with automatic algorithm selection.",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#parameters",
    "title": "Cholesky_decomposition_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ninDataset (BigDataStatMeth::hdf5Dataset *): Input matrix dataset (must be symmetric positive-definite)\noutDataset (BigDataStatMeth::hdf5Dataset *): Output dataset for the Cholesky factor L\nidim0 (int): Number of rows\nidim1 (int): Number of columns\ndElementsBlock (long): Block size for processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#returns",
    "title": "Cholesky_decomposition_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nint 0 on success, 1 if not positive definite, 2 on errors",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#details",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#details",
    "title": "Cholesky_decomposition_hdf5",
    "section": "5 Details",
    "text": "5 Details\ninDatasetInput matrix dataset (must be symmetric positive-definite) outDatasetOutput dataset for the Cholesky factor L idim0Number of rows idim1Number of columns dElementsBlockBlock size for processing threadsNumber of threads for parallel processing (optional) int 0 on success, 1 if not positive definite, 2 on errors Automatically selects appropriate algorithm based on matrix size:Matrices &lt; CHOLESKY_OUTOFCORE_THRESHOLD: uses intermediate algorithmMatrices ≥ CHOLESKY_OUTOFCORE_THRESHOLD: uses out-of-core tiled algorithm",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#call-graph",
    "title": "Cholesky_decomposition_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#source-code",
    "title": "Cholesky_decomposition_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixInvCholesky.hpp • Lines 320-332\ninline int Cholesky_decomposition_hdf5(BigDataStatMeth::hdf5Dataset* inDataset,  \n                                       BigDataStatMeth::hdf5Dataset* outDataset, \n                                       int idim0, int idim1, long dElementsBlock, \n                                       Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n{\n    // Detect matrix size and select algorithm\n    if (idim0 &gt;= CHOLESKY_OUTOFCORE_THRESHOLD) {\n        Rcpp::Rcout &lt;&lt; \"\\nUsing out-of-core Cholesky for large matrix (\" &lt;&lt; idim0 &lt;&lt; \"x\" &lt;&lt; idim1 &lt;&lt; \")\\n\";\n        return Cholesky_decomposition_outofcore_hdf5(inDataset, outDataset, idim0, idim1, dElementsBlock, threads);\n    } else {\n        return Cholesky_decomposition_intermediate_hdf5(inDataset, outDataset, idim0, idim1, dElementsBlock, threads);\n    }\n}",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_hdf5.html#usage-example",
    "title": "Cholesky_decomposition_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Cholesky_decomposition_hdf5(...);",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/index.html",
    "href": "api-reference/cpp/classes/index.html",
    "title": "C++ Classes",
    "section": "",
    "text": "hdf5Dataset* hdf5DatasetInternal* hdf5DiagonalMatrix* hdf5Dims* hdf5File* hdf5Group",
    "crumbs": [
      "Classes"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/index.html#available-classes",
    "href": "api-reference/cpp/classes/index.html#available-classes",
    "title": "C++ Classes",
    "section": "",
    "text": "hdf5Dataset* hdf5DatasetInternal* hdf5DiagonalMatrix* hdf5Dims* hdf5File* hdf5Group",
    "crumbs": [
      "Classes"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5File.html",
    "href": "api-reference/cpp/classes/hdf5File.html",
    "title": "hdf5File",
    "section": "",
    "text": "Constructor with path and filename.",
    "crumbs": [
      "Classes",
      "hdf5File"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5File.html#overview",
    "href": "api-reference/cpp/classes/hdf5File.html#overview",
    "title": "hdf5File",
    "section": "",
    "text": "Constructor with path and filename.",
    "crumbs": [
      "Classes",
      "hdf5File"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5File.html#detailed-description",
    "href": "api-reference/cpp/classes/hdf5File.html#detailed-description",
    "title": "hdf5File",
    "section": "2 Detailed Description",
    "text": "2 Detailed Description\nrouteDirectory path filenFilename overwriteWhether to overwrite existing file",
    "crumbs": [
      "Classes",
      "hdf5File"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5File.html#class-hierarchy",
    "href": "api-reference/cpp/classes/hdf5File.html#class-hierarchy",
    "title": "hdf5File",
    "section": "3 Class Hierarchy",
    "text": "3 Class Hierarchy\n\n\n\n\nClass diagram",
    "crumbs": [
      "Classes",
      "hdf5File"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5File.html#public-interface",
    "href": "api-reference/cpp/classes/hdf5File.html#public-interface",
    "title": "hdf5File",
    "section": "4 Public Interface",
    "text": "4 Public Interface\n\n4.1 Methods\n\n4.1.1 hdf5File()\nBigDataStatMeth::hdf5File::hdf5File(H5::H5File *file)\nConstructor with file pointer.\nParameters:\n\nfile (H5::H5File *): HDF5 file pointer\n\nfileHDF5 file pointer\n\n\n\n4.1.2 createFile()\nint BigDataStatMeth::hdf5File::createFile()\nCreate a new HDF5 file.\nReturns: EXEC_OK on success, EXEC_ERROR on error, EXEC_WARNING if file exists\nCreates a new HDF5 file at the specified location. If the file exists and overwrite is true, it will be truncated.\n\n\n\n4.1.3 openFile()\nH5::H5File * BigDataStatMeth::hdf5File::openFile(std::string opentype)\nOpen an existing HDF5 file.\nParameters:\n\nopentype (std::string): Access mode (“r” for read-only, “rw” for read-write)\n\nReturns: Pointer to opened file or nullptr on error\nOpens an HDF5 file in read or read/write mode.\n\n\n\n4.1.4 getFileptr()\nH5::H5File * BigDataStatMeth::hdf5File::getFileptr()\nGet file pointer.\nReturns: Pointer to HDF5 file\nPointer to HDF5 file\n\n\n\n4.1.5 getFilename()\nstd::string BigDataStatMeth::hdf5File::getFilename()\nGet filename.\nReturns: Filename without path\nFilename without path\n\n\n\n4.1.6 getPath()\nstd::string BigDataStatMeth::hdf5File::getPath()\nGet file path.\nReturns: Directory path without filename\nDirectory path without filename\n\n\n\n4.1.7 getFullPath()\nstd::string BigDataStatMeth::hdf5File::getFullPath()\nGet full file path.\nReturns: Complete path including filename\nComplete path including filename\n\n\n\n4.1.8 checkFile()\nbool BigDataStatMeth::hdf5File::checkFile()\nCheck if file exists.\nReturns: True if file exists, false otherwise\nTrue if file exists, false otherwise\n\n\n\n4.1.9 getDatasetNames()\nRcpp::StringVector BigDataStatMeth::hdf5File::getDatasetNames(std::string strgroup, std::string strprefix, std::string strsufix)\nGet list of dataset names.\nParameters:\n\nstrgroup (std::string): Group path\nstrprefix (std::string): Prefix filter\nstrsufix (std::string): Suffix filter\n\nReturns: Vector of dataset names\nstrgroupGroup path strprefixPrefix filter strsufixSuffix filter Vector of dataset names\n\n\n\n4.1.10 close_file()\nvoid BigDataStatMeth::hdf5File::close_file()\nClose file and cleanup resources.\nCloses all open objects and the file itself. Used for emergency cleanup.\n\n\n\n4.1.11 ~hdf5File()\nBigDataStatMeth::hdf5File::~hdf5File()\nDestructor.\nCloses the file and releases resources",
    "crumbs": [
      "Classes",
      "hdf5File"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5File.html#protected-members",
    "href": "api-reference/cpp/classes/hdf5File.html#protected-members",
    "title": "hdf5File",
    "section": "5 Protected Members",
    "text": "5 Protected Members\n\n\n\n\n\n\nNoteDetails\n\n\n\n\n\n5.1 Attributes\n\n5.1.1 pfile\nType: H5::H5File *\n\n\n\n5.1.2 filename\nType: std::string\n\n\n\n5.1.3 path\nType: std::string",
    "crumbs": [
      "Classes",
      "hdf5File"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5File.html#private-implementation",
    "href": "api-reference/cpp/classes/hdf5File.html#private-implementation",
    "title": "hdf5File",
    "section": "6 Private Implementation",
    "text": "6 Private Implementation\n\n\n\n\n\n\nNote\n\n\n\nInternal implementation details for advanced users and contributors.\n\n\n\n\n\n\n\n\nNoteDetails\n\n\n\n\n\n\n6.1 Methods\n\n6.1.1 ResFileExist_filestream()\nbool BigDataStatMeth::hdf5File::ResFileExist_filestream()\nCheck if file exists using file stream.\nReturns: True if file exists and is accessible\nTrue if file exists and is accessible\n\n\n\n6.1.2 checkHDF5File()\nbool BigDataStatMeth::hdf5File::checkHDF5File()\nCheck if file is corrupt, open, accessible or has_valid_structure.\nReturns: True if file exists and is accessible\nTrue if file exists and is accessible\n\n\n\n6.1.3 lockedByOtherProcess()\nbool BigDataStatMeth::hdf5File::lockedByOtherProcess()\nReturn true if existing HDF5 file appears locked/busy.\nRequires HDF5 file locking enabled (env var set above).\n\n\n\n6.1.4 isHDF5FileOpen()\nbool BigDataStatMeth::hdf5File::isHDF5FileOpen()\nCheck if HDF5 file is already open.\nReturns: True if file is open\nTrue if file is open\n\n\n\n6.1.5 get_dataset_names_from_group()\nRcpp::StringVector BigDataStatMeth::hdf5File::get_dataset_names_from_group(std::string strgroup, std::string strprefix, std::string strsufix)\nGet dataset names from group.\nParameters:\n\nstrgroup (std::string): Group path\nstrprefix (std::string): Prefix filter\nstrsufix (std::string): Suffix filter\n\nReturns: Vector of dataset names\nstrgroupGroup path strprefixPrefix filter strsufixSuffix filter Vector of dataset names\n\n\n\n\n6.2 Attributes\n\n6.2.1 opentype\nType: std::string\n\n\n\n6.2.2 fullPath\nType: std::string\n\n\n\n6.2.3 boverwrite\nType: bool",
    "crumbs": [
      "Classes",
      "hdf5File"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5File.html#usage-example",
    "href": "api-reference/cpp/classes/hdf5File.html#usage-example",
    "title": "hdf5File",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"hdf5File.hpp\"\n\n// Example usage\nhdf5File obj;\n// Your code here",
    "crumbs": [
      "Classes",
      "hdf5File"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DiagonalMatrix.html",
    "href": "api-reference/cpp/classes/hdf5DiagonalMatrix.html",
    "title": "hdf5DiagonalMatrix",
    "section": "",
    "text": "Add diagonal elements from two matrices or vectors.",
    "crumbs": [
      "Classes",
      "hdf5DiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#overview",
    "href": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#overview",
    "title": "hdf5DiagonalMatrix",
    "section": "",
    "text": "Add diagonal elements from two matrices or vectors.",
    "crumbs": [
      "Classes",
      "hdf5DiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#detailed-description",
    "href": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#detailed-description",
    "title": "hdf5DiagonalMatrix",
    "section": "2 Detailed Description",
    "text": "2 Detailed Description\nPerforms optimized diagonal addition C_diag = A_diag + B_diag. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct vector operation). Uses vectorOperations.hpp for maximum efficiency.",
    "crumbs": [
      "Classes",
      "hdf5DiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#class-hierarchy",
    "href": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#class-hierarchy",
    "title": "hdf5DiagonalMatrix",
    "section": "3 Class Hierarchy",
    "text": "3 Class Hierarchy\n\n\n\n\nClass diagram",
    "crumbs": [
      "Classes",
      "hdf5DiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#inheritance",
    "href": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#inheritance",
    "title": "hdf5DiagonalMatrix",
    "section": "4 Inheritance",
    "text": "4 Inheritance\nInherits from:\n\nBigDataStatMeth::hdf5Dataset",
    "crumbs": [
      "Classes",
      "hdf5DiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#public-interface",
    "href": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#public-interface",
    "title": "hdf5DiagonalMatrix",
    "section": "5 Public Interface",
    "text": "5 Public Interface\n\n5.1 Methods\n\n5.1.1 hdf5DiagonalMatrix()\nBigDataStatMeth::hdf5DiagonalMatrix::hdf5DiagonalMatrix(BigDataStatMeth::hdf5File *oFile, std::string group, std::string datasetname, bool overwrite)\nConstructor with file object.\nParameters:\n\noFile (BigDataStatMeth::hdf5File *): HDF5 file object\ngroup (std::string): Group path\ndatasetname (std::string): Dataset name\noverwrite (bool): Whether to overwrite existing dataset\n\noFileHDF5 file object groupGroup path datasetnameDataset name overwriteWhether to overwrite existing dataset\n\n\n\n5.1.2 createScalarDiagonalMatrix()\nvoid BigDataStatMeth::hdf5DiagonalMatrix::createScalarDiagonalMatrix(hsize_t size, double scalar, const std::vector&lt; double &gt; &diagonal_vector, hsize_t block_size_hint=1024, int compression_level=6, Rcpp::Nullable&lt; int &gt; threads=R_NilValue, std::string output_type=\"matrix\")\nCreate diagonal matrix with scalar multiplication.\nParameters:\n\nsize (hsize_t): Size of the diagonal (N elements)\nscalar (double): Scalar to multiply with diagonal vector\ndiagonal_vector (const std::vector&lt; double &gt; &): Vector of diagonal values (length = size)\nblock_size_hint (hsize_t): Suggested block size for processing (default: 1024)\ncompression_level (int): Compression level (0-9, default: 6)\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads to use (default: auto-detect)\noutput_type (std::string): Output format: “matrix” for N×N sparse matrix, “vector” for 1×N vector (default: “matrix”)\n\nCreates a diagonal matrix where diagonal elements are scalar * vector[i]. Uses optimized block-wise processing that only processes diagonal blocks, resulting in ~250x reduction in I/O operations compared to full matrix processing.\n\n\n\n5.1.3 createScalarIdentityMatrix()\nvoid BigDataStatMeth::hdf5DiagonalMatrix::createScalarIdentityMatrix(hsize_t size, double scalar, hsize_t block_size_hint=1024, int compression_level=6, Rcpp::Nullable&lt; int &gt; threads=R_NilValue, std::string output_type=\"matrix\")\nCreate identity matrix scaled by scalar with flexible output format.\nParameters:\n\nsize (hsize_t): Size of the identity matrix/vector (N×N or 1×N)\nscalar (double): Scalar value for diagonal elements (default: 1.0 for standard identity)\nblock_size_hint (hsize_t): Suggested block size for processing\ncompression_level (int): Compression level (0-9, default: 6)\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads to use (default: auto-detect)\noutput_type (std::string): Output format: “matrix” for N×N identity matrix, “vector” for 1×N identity vector (default: “matrix”)\n\nCreates an identity matrix multiplied by a scalar value. More efficient than general diagonal matrix for identity cases. Supports both full matrix and vector output formats.\n\n\n\n5.1.4 setBlockSize()\nvoid BigDataStatMeth::hdf5DiagonalMatrix::setBlockSize(hsize_t new_block_size)\nSet block size for processing.\nParameters:\n\nnew_block_size (hsize_t): New block size to use\n\nnew_block_sizeNew block size to use\n\n\n\n5.1.5 getBlockSize()\nhsize_t BigDataStatMeth::hdf5DiagonalMatrix::getBlockSize() const\nGet current block size.\nReturns: Current block size\nCurrent block size",
    "crumbs": [
      "Classes",
      "hdf5DiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#private-implementation",
    "href": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#private-implementation",
    "title": "hdf5DiagonalMatrix",
    "section": "6 Private Implementation",
    "text": "6 Private Implementation\n\n\n\n\n\n\nNote\n\n\n\nInternal implementation details for advanced users and contributors.\n\n\n\n\n\n\n\n\nNoteDetails\n\n\n\n\n\n\n6.1 Methods\n\n6.1.1 writeDiagonal()\nvoid BigDataStatMeth::hdf5DiagonalMatrix::writeDiagonal(hsize_t size, double scalar, const std::vector&lt; double &gt; &diagonal_vector)\nWrite diagonal elements directly using HDF5 element selection.\nParameters:\n\nsize (hsize_t): Size of the square matrix\nscalar (double): Scalar multiplier for diagonal elements\ndiagonal_vector (const std::vector&lt; double &gt; &): Vector of diagonal values\n\nUses HDF5’s H5Sselect_elements to write all diagonal elements in a single operation. This is ~500x faster than block-based approaches for new datasets as it avoids unnecessary read-modify-write cycles.\n\n\n\n6.1.2 writeVectorDiagonal()\nvoid BigDataStatMeth::hdf5DiagonalMatrix::writeVectorDiagonal(hsize_t size, double scalar, const std::vector&lt; double &gt; &diagonal_vector)\nWrite diagonal values directly as vector dataset.\nParameters:\n\nsize (hsize_t): Number of diagonal elements to write\nscalar (double): Scalar multiplier applied to each diagonal element\ndiagonal_vector (const std::vector&lt; double &gt; &): Vector containing base diagonal values\n\nWrites diagonal values to a 1×N vector dataset using optimized direct write. This function bypasses matrix processing entirely and writes diagonal values directly to vector format, providing maximum efficiency for diagonal-only operations.\n\n\n\n\n6.2 Attributes\n\n6.2.1 block_size\nType: hsize_t\nAdd diagonal elements from two matrices or vectors.",
    "crumbs": [
      "Classes",
      "hdf5DiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#usage-example",
    "href": "api-reference/cpp/classes/hdf5DiagonalMatrix.html#usage-example",
    "title": "hdf5DiagonalMatrix",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"hdf5DiagonalMatrix.hpp\"\n\n// Example usage\nhdf5DiagonalMatrix obj;\n// Your code here",
    "crumbs": [
      "Classes",
      "hdf5DiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dataset.html",
    "href": "api-reference/cpp/classes/hdf5Dataset.html",
    "title": "hdf5Dataset",
    "section": "",
    "text": "Structure for string data.",
    "crumbs": [
      "Classes",
      "hdf5Dataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dataset.html#overview",
    "href": "api-reference/cpp/classes/hdf5Dataset.html#overview",
    "title": "hdf5Dataset",
    "section": "",
    "text": "Structure for string data.",
    "crumbs": [
      "Classes",
      "hdf5Dataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dataset.html#detailed-description",
    "href": "api-reference/cpp/classes/hdf5Dataset.html#detailed-description",
    "title": "hdf5Dataset",
    "section": "2 Detailed Description",
    "text": "2 Detailed Description\nUsed for storing fixed-length string data in HDF5",
    "crumbs": [
      "Classes",
      "hdf5Dataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dataset.html#class-hierarchy",
    "href": "api-reference/cpp/classes/hdf5Dataset.html#class-hierarchy",
    "title": "hdf5Dataset",
    "section": "3 Class Hierarchy",
    "text": "3 Class Hierarchy\n\n\n\n\nClass diagram",
    "crumbs": [
      "Classes",
      "hdf5Dataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dataset.html#inheritance",
    "href": "api-reference/cpp/classes/hdf5Dataset.html#inheritance",
    "title": "hdf5Dataset",
    "section": "4 Inheritance",
    "text": "4 Inheritance\nInherits from:\n\nBigDataStatMeth::hdf5Group",
    "crumbs": [
      "Classes",
      "hdf5Dataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dataset.html#public-interface",
    "href": "api-reference/cpp/classes/hdf5Dataset.html#public-interface",
    "title": "hdf5Dataset",
    "section": "5 Public Interface",
    "text": "5 Public Interface\n\n5.1 Methods\n\n5.1.1 hdf5Dataset()\nBigDataStatMeth::hdf5Dataset::hdf5Dataset(std::string filename, std::string group, std::string datasetname, bool overwrite)\nParameters:\n\nfilename (std::string)\ngroup (std::string)\ndatasetname (std::string)\noverwrite (bool)\n\n\n\n\n5.1.2 remove()\nvirtual void BigDataStatMeth::hdf5Dataset::remove()\nRemove the dataset.\nDeletes the dataset from the HDF5 file\n\n\n\n5.1.3 createDataset()\nvirtual void BigDataStatMeth::hdf5Dataset::createDataset(BigDataStatMeth::hdf5Dataset *dsLike, std::string strdatatype, int compression_level=6)\nCreate a dataset based on another dataset’s dimensions with optional compression.\nParameters:\n\ndsLike (BigDataStatMeth::hdf5Dataset *): Reference dataset to copy dimensions from\nstrdatatype (std::string): Data type for the new dataset (“int”, “numeric”, “real”, “string”)\ncompression_level (int): Compression level (0=none, 1-9=gzip level, 6=balanced default) 0 = No compression 1-3 = Light compression (fast) 4-6 = Balanced compression (recommended) 7-9 = Maximum compression (slower)\n\nCreates a new dataset with the same dimensions as the reference dataset but with a specified data type and compression settings. This is useful for creating derived datasets or transformations while maintaining dimensional consistency.\n\n\n\n5.1.4 createUnlimitedDataset()\nvirtual void BigDataStatMeth::hdf5Dataset::createUnlimitedDataset(size_t rows, size_t cols, std::string strdatatype, int compression_level=6)\nCreate an unlimited dataset with optional compression.\nParameters:\n\nrows (size_t): Initial number of rows (used as chunk size)\ncols (size_t): Initial number of columns (used as chunk size)\nstrdatatype (std::string): Data type for the dataset (“int”, “numeric”) Note: String type not supported for unlimited datasets\ncompression_level (int): Compression level (0=none, 1-9=gzip level, 6=balanced default) Recommended: 4-6 for unlimited datasets to balance compression ratio with extension performance\n\nCreates a new HDF5 dataset with unlimited dimensions, allowing it to grow in size dynamically. The dataset is created with initial dimensions but can be extended using extendUnlimitedDataset(). Compression is fully compatible with unlimited datasets.\n\n\n\n5.1.5 extendUnlimitedDataset()\nvirtual void BigDataStatMeth::hdf5Dataset::extendUnlimitedDataset(const size_t rows, const size_t cols)\nExtend an unlimited dataset.\nParameters:\n\nrows (const size_t): New number of rows\ncols (const size_t): New number of columns\n\nIncreases the dimensions of an unlimited dataset to accommodate more data. Only works with datasets created as unlimited.\n\n\n\n5.1.6 openDataset()\nH5::DataSet * BigDataStatMeth::hdf5Dataset::openDataset()\n\n\n\n5.1.7 writeDataset()\nvoid BigDataStatMeth::hdf5Dataset::writeDataset(double *mdata)\nWrite raw double data to dataset.\nParameters:\n\nmdata (double *): Pointer to double array\n\nDirect write of double array to dataset without type conversion.\n\n\n\n5.1.8 writeRowMajorDatasetBlock()\nvirtual void BigDataStatMeth::hdf5Dataset::writeRowMajorDatasetBlock(Eigen::Map&lt; Eigen::Matrix&lt; double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor &gt; &gt; DatasetValues, std::vector&lt; hsize_t &gt; vOffset, std::vector&lt; hsize_t &gt; vCount, std::vector&lt; hsize_t &gt; vStride, std::vector&lt; hsize_t &gt; vBlock)\nWrite block of data in row-major order.\nParameters:\n\nDatasetValues (Eigen::Map&lt; Eigen::Matrix&lt; double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor &gt; &gt;): Matrix data in row-major format\nvOffset (std::vector&lt; hsize_t &gt;): Starting position for write\nvCount (std::vector&lt; hsize_t &gt;): Number of elements to write\nvStride (std::vector&lt; hsize_t &gt;): Stride between elements\nvBlock (std::vector&lt; hsize_t &gt;): Size of blocks\n\nWrites a block of data to the dataset using row-major memory layout. Supports:Partial dataset updatesStrided accessBlock-based operations\n\n\n\n5.1.9 writeColMajorDatasetBlock()\nvirtual void BigDataStatMeth::hdf5Dataset::writeColMajorDatasetBlock(Eigen::MatrixXd DatasetValues, std::vector&lt; hsize_t &gt; vOffset, std::vector&lt; hsize_t &gt; vStride, std::vector&lt; hsize_t &gt; vBlock)\nWrite block of data in column-major order.\nParameters:\n\nDatasetValues (Eigen::MatrixXd): Matrix data in column-major format\nvOffset (std::vector&lt; hsize_t &gt;): Starting position for write\nvStride (std::vector&lt; hsize_t &gt;): Stride between elements\nvBlock (std::vector&lt; hsize_t &gt;): Size of blocks\n\nWrites a block of data to the dataset using column-major memory layout. Supports:Partial dataset updatesStrided accessBlock-based operations\n\n\n\n5.1.10 writeDatasetBlock()\nvirtual void BigDataStatMeth::hdf5Dataset::writeDatasetBlock(std::vector&lt; double &gt; DatasetValues, std::vector&lt; hsize_t &gt; vOffset, std::vector&lt; hsize_t &gt; vCount, std::vector&lt; hsize_t &gt; vStride, std::vector&lt; hsize_t &gt; vBlock)\nWrite block of vector data.\nParameters:\n\nDatasetValues (std::vector&lt; double &gt;): Vector data to write\nvOffset (std::vector&lt; hsize_t &gt;): Starting position\nvCount (std::vector&lt; hsize_t &gt;): Number of elements\nvStride (std::vector&lt; hsize_t &gt;): Stride between elements\nvBlock (std::vector&lt; hsize_t &gt;): Block size\n\nWrites a block of vector data to the dataset. Useful for column/row-wise operations.\n\n\n\n5.1.11 createSubsetDataset()\nvirtual void BigDataStatMeth::hdf5Dataset::createSubsetDataset(const std::vector&lt; int &gt; &indices, bool select_rows=true, const std::string &new_group=\"\", const std::string &new_name=\"\")\nCreate subset dataset with selected rows/columns (memory efficient)\nParameters:\n\nindices (const std::vector&lt; int &gt; &): Vector of row/column indices to include (0-based)\nselect_rows (bool): If true, selects rows; if false, selects columns\nnew_group (const std::string &): Target group for the new dataset (default: same group)\nnew_name (const std::string &): Name for the new dataset (default: original_name + “_subset”)\n\nCreates a new dataset containing only the specified rows or columns using HDF5’s hyperslab selection for direct disk-to-disk copy without loading data into memory. Ideal for big datasets.\n\n\n\n5.1.12 readDatasetBlock()\nvoid BigDataStatMeth::hdf5Dataset::readDatasetBlock(std::vector&lt; hsize_t &gt; ivoffset, std::vector&lt; hsize_t &gt; ivcount, std::vector&lt; hsize_t &gt; ivstride, std::vector&lt; hsize_t &gt; ivblock, double *rdatablock)\nParameters:\n\nivoffset (std::vector&lt; hsize_t &gt;)\nivcount (std::vector&lt; hsize_t &gt;)\nivstride (std::vector&lt; hsize_t &gt;)\nivblock (std::vector&lt; hsize_t &gt;)\nrdatablock (double *)\n\n\n\n\n5.1.13 addAttribute()\nint BigDataStatMeth::hdf5Dataset::addAttribute(std::string attrName, Rcpp::RObject attr_data)\nAdd an attribute to the dataset.\nParameters:\n\nattrName (std::string): Name of the attribute\nattr_data (Rcpp::RObject): Attribute data\n\nReturns: EXEC_OK on success, EXEC_ERROR on failure\nAdds or updates an attribute associated with the dataset. Supports various R data types.\n\n\n\n5.1.14 getAttribute()\nvoid BigDataStatMeth::hdf5Dataset::getAttribute(std::string strAtribute)\nGet an attribute from the dataset.\nParameters:\n\nstrAtribute (std::string): Name of the attribute to retrieve\n\nRetrieves the value of a named attribute from the dataset.\n\n\n\n5.1.15 moveDataset()\nvirtual void BigDataStatMeth::hdf5Dataset::moveDataset(const std::string &new_path, bool overwrite=false)\nParameters:\n\nnew_path (const std::string &)\noverwrite (bool)\n\n\n\n\n5.1.16 getDatasetptr()\nH5::DataSet * BigDataStatMeth::hdf5Dataset::getDatasetptr()\nGet dataset pointer.\nReturns: Pointer of the dataset\nPointer of the dataset\n\n\n\n5.1.17 getDatasetName()\nstd::string BigDataStatMeth::hdf5Dataset::getDatasetName()\nGet dataset name.\nReturns: Name of the dataset\nName of the dataset\n\n\n\n5.1.18 getGroup()\nstd::string BigDataStatMeth::hdf5Dataset::getGroup()\nGet group name.\nReturns: Name of the group containing the dataset\nName of the group containing the dataset\n\n\n\n5.1.19 getFileName()\nstd::string BigDataStatMeth::hdf5Dataset::getFileName()\nGet file name.\nReturns: Name of the file containing the dataset\nName of the file containing the dataset\n\n\n\n5.1.20 nrows()\nhsize_t BigDataStatMeth::hdf5Dataset::nrows()\nGet number of rows in internal format.\nReturns: Number of rows in the dataset’s internal storage\nNumber of rows in the dataset’s internal storage\n\n\n\n5.1.21 ncols()\nhsize_t BigDataStatMeth::hdf5Dataset::ncols()\nGet number of columns in internal format.\nReturns: Number of columns in the dataset’s internal storage\nNumber of columns in the dataset’s internal storage\n\n\n\n5.1.22 nrows_r()\nhsize_t BigDataStatMeth::hdf5Dataset::nrows_r()\nGet number of rows in R format.\nReturns: Number of rows in R’s representation (transposed)\nNumber of rows in R’s representation (transposed)\n\n\n\n5.1.23 ncols_r()\nhsize_t BigDataStatMeth::hdf5Dataset::ncols_r()\nGet number of columns in R format.\nReturns: Number of columns in R’s representation (transposed)\nNumber of columns in R’s representation (transposed)\n\n\n\n5.1.24 dim()\nhsize_t * BigDataStatMeth::hdf5Dataset::dim()\nGet dataset dimension.\nReturns: Pointer to dataset dimension (rows x columns)\nPointer to dataset dimension (rows x columns)\n\n\n\n5.1.25 dimFile()\nhsize_t * BigDataStatMeth::hdf5Dataset::dimFile()\nGet dataset file dimension.\nReturns: Pointer to dataset file dimension (rows x columns)\nPointer to dataset file dimension (rows x columns)\n\n\n\n5.1.26 nrows_file()\nhsize_t BigDataStatMeth::hdf5Dataset::nrows_file()\nGet number of rows in file.\nReturns: Number of rows in file storage\nNumber of rows in file storage\n\n\n\n5.1.27 ncols_file()\nhsize_t BigDataStatMeth::hdf5Dataset::ncols_file()\nGet number of columns in file.\nReturns: Number of columns in file storage\nNumber of columns in file storage\n\n\n\n5.1.28 isUnlimited()\nbool BigDataStatMeth::hdf5Dataset::isUnlimited()\nCheck if dataset is unlimited.\nReturns: True if dataset has unlimited dimensions\nTrue if dataset has unlimited dimensions\n\n\n\n5.1.29 isInternal()\nbool BigDataStatMeth::hdf5Dataset::isInternal()\nCheck if dataset is internal.\nReturns: True if dataset is used for internal storage\nTrue if dataset is used for internal storage\n\n\n\n5.1.30 isOpen()\nbool BigDataStatMeth::hdf5Dataset::isOpen()\nCheck if dataset is open.\nReturns: True if dataset is currently open\nTrue if dataset is currently open\n\n\n\n5.1.31 setRownamesDatasetPath()\nvoid BigDataStatMeth::hdf5Dataset::setRownamesDatasetPath(std::string fullpath)\nSet rownames path inside hdf5 data file.\nParameters:\n\nfullpath (std::string)\n\nReturns: void\nvoid\n\n\n\n5.1.32 setColnamesDatasetPath()\nvoid BigDataStatMeth::hdf5Dataset::setColnamesDatasetPath(std::string fullpath)\nSet colnames path inside hdf5 data file.\nParameters:\n\nfullpath (std::string)\n\nReturns: void\nvoid\n\n\n\n5.1.33 ~hdf5Dataset()\nvirtual BigDataStatMeth::hdf5Dataset::~hdf5Dataset() noexcept\nDestructor.\nCloses the dataset and releases resources",
    "crumbs": [
      "Classes",
      "hdf5Dataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dataset.html#protected-members",
    "href": "api-reference/cpp/classes/hdf5Dataset.html#protected-members",
    "title": "hdf5Dataset",
    "section": "6 Protected Members",
    "text": "6 Protected Members\n\n\n\n\n\n\nNoteDetails\n\n\n\n\n\n\n6.1 Methods\n\n6.1.1 close_dataset()\nvoid BigDataStatMeth::hdf5Dataset::close_dataset()\nClose the dataset.\nCloses the dataset and releases associated resources\n\n\n\n6.1.2 close_dataset_file()\nvoid BigDataStatMeth::hdf5Dataset::close_dataset_file()\nClose dataset and file.\nCloses both the dataset and its associated file\n\n\n\n6.1.3 convert_DataFrame_to_RangeList()\nnames * BigDataStatMeth::hdf5Dataset::convert_DataFrame_to_RangeList(Rcpp::RObject DatasetValues, bool bFullDataset)\nConvert DataFrame to range list.\nParameters:\n\nDatasetValues (Rcpp::RObject): R DataFrame to convert\nbFullDataset (bool): Whether to convert entire dataset\n\nReturns: Array of name structures\nConverts R DataFrame to HDF5-compatible range list format\n\n\n\n6.1.4 getDimensExistingDataset()\nvoid BigDataStatMeth::hdf5Dataset::getDimensExistingDataset()\nGet dimensions of existing dataset.\nRetrieves and stores the dimensions of an existing dataset\n\n\n\n6.1.5 isDatasetOpen()\nbool BigDataStatMeth::hdf5Dataset::isDatasetOpen()\nCheck if dataset is open.\nReturns: True if dataset is open, false otherwise\nVerifies if the dataset is currently open and accessible\n\n\n\n\n6.2 Attributes\n\n6.2.1 pdataset\nType: H5::DataSet *\nDataset pointer.\n\n\n\n6.2.2 dimDataset\nType: hsize_t\nDataset dimensions.\n\n\n\n6.2.3 dimDatasetinFile\nType: hsize_t\nFile storage dimensions.\n\n\n\n6.2.4 name\nType: std::string\nDataset name.\n\n\n\n6.2.5 type\nType: std::string\nDataset type.\n\n\n\n6.2.6 colnamesDataset\nType: std::string\nColumn names dataset.\n\n\n\n6.2.7 rownamesDataset\nType: std::string\nRow names dataset.\n\n\n\n6.2.8 boverwrite\nType: bool\nOverwrite flag.\n\n\n\n6.2.9 unlimited\nType: bool\nUnlimited dimensions flag.\n\n\n\n6.2.10 internalDataset\nType: bool\nInternal dataset flag.",
    "crumbs": [
      "Classes",
      "hdf5Dataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dataset.html#usage-example",
    "href": "api-reference/cpp/classes/hdf5Dataset.html#usage-example",
    "title": "hdf5Dataset",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"hdf5Dataset.hpp\"\n\n// Example usage\nhdf5Dataset obj;\n// Your code here",
    "crumbs": [
      "Classes",
      "hdf5Dataset"
    ]
  },
  {
    "objectID": "api-reference/cpp-api.html",
    "href": "api-reference/cpp-api.html",
    "title": "C++ API",
    "section": "",
    "text": "NoteAuto-Generated Documentation\n\n\n\nThis section will contain auto-generated C++ documentation from Doxygen."
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DatasetInternal.html",
    "href": "api-reference/cpp/classes/hdf5DatasetInternal.html",
    "title": "hdf5DatasetInternal",
    "section": "",
    "text": "Constructor with file, group, and dataset name.",
    "crumbs": [
      "Classes",
      "hdf5DatasetInternal"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DatasetInternal.html#overview",
    "href": "api-reference/cpp/classes/hdf5DatasetInternal.html#overview",
    "title": "hdf5DatasetInternal",
    "section": "",
    "text": "Constructor with file, group, and dataset name.",
    "crumbs": [
      "Classes",
      "hdf5DatasetInternal"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DatasetInternal.html#detailed-description",
    "href": "api-reference/cpp/classes/hdf5DatasetInternal.html#detailed-description",
    "title": "hdf5DatasetInternal",
    "section": "2 Detailed Description",
    "text": "2 Detailed Description\nfileHDF5 file pointer groupGroup path datasetnameDataset name overwriteWhether to overwrite existing dataset",
    "crumbs": [
      "Classes",
      "hdf5DatasetInternal"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DatasetInternal.html#class-hierarchy",
    "href": "api-reference/cpp/classes/hdf5DatasetInternal.html#class-hierarchy",
    "title": "hdf5DatasetInternal",
    "section": "3 Class Hierarchy",
    "text": "3 Class Hierarchy\n\n\n\n\nClass diagram",
    "crumbs": [
      "Classes",
      "hdf5DatasetInternal"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DatasetInternal.html#inheritance",
    "href": "api-reference/cpp/classes/hdf5DatasetInternal.html#inheritance",
    "title": "hdf5DatasetInternal",
    "section": "4 Inheritance",
    "text": "4 Inheritance\nInherits from:\n\nBigDataStatMeth::hdf5Dataset",
    "crumbs": [
      "Classes",
      "hdf5DatasetInternal"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DatasetInternal.html#public-interface",
    "href": "api-reference/cpp/classes/hdf5DatasetInternal.html#public-interface",
    "title": "hdf5DatasetInternal",
    "section": "5 Public Interface",
    "text": "5 Public Interface\n\n5.1 Methods\n\n5.1.1 hdf5DatasetInternal()\nBigDataStatMeth::hdf5DatasetInternal::hdf5DatasetInternal(std::string filename, std::string group, std::string datasetname, bool overwrite)\nConstructor with filename, group, and dataset name.\nParameters:\n\nfilename (std::string): Name of HDF5 file\ngroup (std::string): Group path\ndatasetname (std::string): Dataset name\noverwrite (bool): Whether to overwrite existing dataset\n\nfilenameName of HDF5 file groupGroup path datasetnameDataset name overwriteWhether to overwrite existing dataset\n\n\n\n5.1.2 createDataset()\nvirtual void BigDataStatMeth::hdf5Dataset::createDataset(BigDataStatMeth::hdf5Dataset *dsLike, std::string strdatatype, int compression_level=6)\nCreate a dataset based on another dataset’s dimensions with optional compression.\nParameters:\n\ndsLike (BigDataStatMeth::hdf5Dataset *): Reference dataset to copy dimensions from\nstrdatatype (std::string): Data type for the new dataset (“int”, “numeric”, “real”, “string”)\ncompression_level (int): Compression level (0=none, 1-9=gzip level, 6=balanced default) 0 = No compression 1-3 = Light compression (fast) 4-6 = Balanced compression (recommended) 7-9 = Maximum compression (slower)\n\nCreates a new dataset with the same dimensions as the reference dataset but with a specified data type and compression settings. This is useful for creating derived datasets or transformations while maintaining dimensional consistency.\n\n\n\n5.1.3 createUnlimitedDataset()\nvoid BigDataStatMeth::hdf5DatasetInternal::createUnlimitedDataset(size_t rows, size_t cols, std::string strdatatype, int compression_level=6)\nCreate an unlimited internal dataset with optional compression.\nParameters:\n\nrows (size_t): Initial number of rows\ncols (size_t): Initial number of columns\nstrdatatype (std::string): Data type for the dataset\ncompression_level (int): Compression level (0=none, 1-9=gzip level, 6=balanced default)\n\nCreates a new internal HDF5 dataset with unlimited dimensions. The dataset is marked as internal and unlimited and can grow in size.\n\n\n\n5.1.4 extendUnlimitedDataset()\nvoid BigDataStatMeth::hdf5DatasetInternal::extendUnlimitedDataset(const size_t rows, const size_t cols)\nExtend an unlimited internal dataset.\nParameters:\n\nrows (const size_t): New number of rows\ncols (const size_t): New number of columns\n\nIncreases the dimensions of an unlimited internal dataset. Only works with datasets created as unlimited.\n\n\n\n5.1.5 writeDataset()\nvoid BigDataStatMeth::hdf5DatasetInternal::writeDataset(Rcpp::RObject DatasetValues)\nWrite data to the internal dataset.\nParameters:\n\nDatasetValues (Rcpp::RObject): R object containing the data to write\n\nWrites data to the internal dataset, handling different data types and formats. Optimized for internal storage patterns.\n\n\n\n5.1.6 writeDatasetBlock()\nvoid BigDataStatMeth::hdf5DatasetInternal::writeDatasetBlock(std::vector&lt; double &gt; DatasetValues, std::vector&lt; hsize_t &gt; vOffset, std::vector&lt; hsize_t &gt; vCount, std::vector&lt; hsize_t &gt; vStride, std::vector&lt; hsize_t &gt; vBlock)\nWrite block of vector data to internal dataset.\nParameters:\n\nDatasetValues (std::vector&lt; double &gt;): Vector data to write\nvOffset (std::vector&lt; hsize_t &gt;): Starting position\nvCount (std::vector&lt; hsize_t &gt;): Number of elements\nvStride (std::vector&lt; hsize_t &gt;): Stride between elements\nvBlock (std::vector&lt; hsize_t &gt;): Block size\n\nWrites a block of vector data to the internal dataset. Optimized for vector operations.\n\n\n\n5.1.7 readDatasetBlock()\ndouble * BigDataStatMeth::hdf5DatasetInternal::readDatasetBlock(std::vector&lt; hsize_t &gt; ivoffset, std::vector&lt; hsize_t &gt; ivcount, std::vector&lt; hsize_t &gt; ivstride, std::vector&lt; hsize_t &gt; ivblock, double *rdatablock)\nRead a block of data from internal dataset.\nParameters:\n\nivoffset (std::vector&lt; hsize_t &gt;): Starting position for read\nivcount (std::vector&lt; hsize_t &gt;): Number of elements to read\nivstride (std::vector&lt; hsize_t &gt;): Stride between elements\nivblock (std::vector&lt; hsize_t &gt;): Size of blocks\nrdatablock (double *): Pointer to read data (double*)\n\nReturns: Pointer to read data (double*)\nReads a specified block of data from the internal dataset. Optimized for internal data access patterns.\n\n\n\n5.1.8 getDatasetptr()\nH5::DataSet * BigDataStatMeth::hdf5DatasetInternal::getDatasetptr()\n\n\n\n5.1.9 getDatasetName()\nstd::string BigDataStatMeth::hdf5DatasetInternal::getDatasetName()\nGet dataset name.\nReturns: Name of the internal dataset\nName of the internal dataset\n\n\n\n5.1.10 getGroup()\nstd::string BigDataStatMeth::hdf5DatasetInternal::getGroup()\nGet group name.\nReturns: Name of the group containing the internal dataset\nName of the group containing the internal dataset\n\n\n\n5.1.11 getFileName()\nstd::string BigDataStatMeth::hdf5DatasetInternal::getFileName()\nGet file name.\nReturns: Name of the file containing the internal dataset\nName of the file containing the internal dataset\n\n\n\n5.1.12 nrows()\nhsize_t BigDataStatMeth::hdf5DatasetInternal::nrows()\n\n\n\n5.1.13 ncols()\nhsize_t BigDataStatMeth::hdf5DatasetInternal::ncols()\n\n\n\n5.1.14 dim()\nhsize_t * BigDataStatMeth::hdf5DatasetInternal::dim()\n\n\n\n5.1.15 isUnlimited()\nbool BigDataStatMeth::hdf5DatasetInternal::isUnlimited()\n\n\n\n5.1.16 ~hdf5DatasetInternal()\nBigDataStatMeth::hdf5DatasetInternal::~hdf5DatasetInternal()\nDestructor.\nCloses the internal dataset and releases resources",
    "crumbs": [
      "Classes",
      "hdf5DatasetInternal"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DatasetInternal.html#private-implementation",
    "href": "api-reference/cpp/classes/hdf5DatasetInternal.html#private-implementation",
    "title": "hdf5DatasetInternal",
    "section": "6 Private Implementation",
    "text": "6 Private Implementation\n\n\n\n\n\n\nNote\n\n\n\nInternal implementation details for advanced users and contributors.\n\n\n\n\n\n\n\n\nNoteDetails\n\n\n\n\n\n6.1 Methods\n\n6.1.1 close_dataset()\nvoid BigDataStatMeth::hdf5DatasetInternal::close_dataset()\nClose the dataset.\nCloses the internal dataset and releases associated resources\n\n\n\n6.1.2 close_dataset_file()\nvoid BigDataStatMeth::hdf5DatasetInternal::close_dataset_file()\nClose dataset and file.\nCloses both the internal dataset and its associated file\n\n\n\n6.1.3 convert_DataFrame_to_RangeList()\nnames * BigDataStatMeth::hdf5DatasetInternal::convert_DataFrame_to_RangeList(Rcpp::RObject DatasetValues, bool bFullDataset)\nConvert DataFrame to range list.\nParameters:\n\nDatasetValues (Rcpp::RObject): R DataFrame to convert\nbFullDataset (bool): Whether to convert entire dataset\n\nReturns: Array of name structures\nConverts R DataFrame to HDF5-compatible range list format for internal storage.\n\n\n\n6.1.4 getDimensExistingDataset()\nvoid BigDataStatMeth::hdf5DatasetInternal::getDimensExistingDataset()\nGet dimensions of existing dataset.\nRetrieves and stores the dimensions of an existing internal dataset",
    "crumbs": [
      "Classes",
      "hdf5DatasetInternal"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5DatasetInternal.html#usage-example",
    "href": "api-reference/cpp/classes/hdf5DatasetInternal.html#usage-example",
    "title": "hdf5DatasetInternal",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"hdf5DatasetInternal.hpp\"\n\n// Example usage\nhdf5DatasetInternal obj;\n// Your code here",
    "crumbs": [
      "Classes",
      "hdf5DatasetInternal"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dims.html",
    "href": "api-reference/cpp/classes/hdf5Dims.html",
    "title": "hdf5Dims",
    "section": "",
    "text": "Structure for storing name strings.",
    "crumbs": [
      "Classes",
      "hdf5Dims"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dims.html#overview",
    "href": "api-reference/cpp/classes/hdf5Dims.html#overview",
    "title": "hdf5Dims",
    "section": "",
    "text": "Structure for storing name strings.",
    "crumbs": [
      "Classes",
      "hdf5Dims"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dims.html#detailed-description",
    "href": "api-reference/cpp/classes/hdf5Dims.html#detailed-description",
    "title": "hdf5Dims",
    "section": "2 Detailed Description",
    "text": "2 Detailed Description\npdatasetPointer to the HDF5 dataset to manage dimensions forCreates a hidden group to store dimension information If dimension information already exists, it will be removed",
    "crumbs": [
      "Classes",
      "hdf5Dims"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dims.html#class-hierarchy",
    "href": "api-reference/cpp/classes/hdf5Dims.html#class-hierarchy",
    "title": "hdf5Dims",
    "section": "3 Class Hierarchy",
    "text": "3 Class Hierarchy\n\n\n\n\nClass diagram",
    "crumbs": [
      "Classes",
      "hdf5Dims"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dims.html#inheritance",
    "href": "api-reference/cpp/classes/hdf5Dims.html#inheritance",
    "title": "hdf5Dims",
    "section": "4 Inheritance",
    "text": "4 Inheritance\nInherits from:\n\nBigDataStatMeth::hdf5Group",
    "crumbs": [
      "Classes",
      "hdf5Dims"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dims.html#public-interface",
    "href": "api-reference/cpp/classes/hdf5Dims.html#public-interface",
    "title": "hdf5Dims",
    "section": "5 Public Interface",
    "text": "5 Public Interface\n\n5.1 Methods\n\n5.1.1 hdf5Dims()\nBigDataStatMeth::hdf5Dims::hdf5Dims(BigDataStatMeth::hdf5Dataset *pdataset)\nConstructs a dimension manager for an HDF5 dataset.\nParameters:\n\npdataset (BigDataStatMeth::hdf5Dataset *): Pointer to the HDF5 dataset to manage dimensions for\n\npdatasetPointer to the HDF5 dataset to manage dimensions forCreates a hidden group to store dimension information If dimension information already exists, it will be removed\n\n\n\n5.1.2 writeDimnames()\nvoid BigDataStatMeth::hdf5Dims::writeDimnames(Rcpp::StringVector rownames, Rcpp::StringVector colnames)\nWrites dimension names to the HDF5 file.\nParameters:\n\nrownames (Rcpp::StringVector): Vector of row names\ncolnames (Rcpp::StringVector): Vector of column names\n\nrownamesVector of row names colnamesVector of column namesH5::FileIExceptionon file operation errors H5::DataSetIExceptionon dataset operation errors H5::GroupIExceptionon group operation errors H5::DataSpaceIExceptionon dataspace operation errors H5::DataTypeIExceptionon datatype operation errorsValidates dimensions against the main dataset Overwrites existing dimension names if they exist Performance considerations:Uses block-wise processing for large string vectorsImplements efficient string storage with fixed-length buffersHandles memory cleanup automatically\n\n\n\n5.1.3 ~hdf5Dims()\nvirtual BigDataStatMeth::hdf5Dims::~hdf5Dims()\nVirtual destructor.",
    "crumbs": [
      "Classes",
      "hdf5Dims"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dims.html#protected-members",
    "href": "api-reference/cpp/classes/hdf5Dims.html#protected-members",
    "title": "hdf5Dims",
    "section": "6 Protected Members",
    "text": "6 Protected Members\n\n\n\n\n\n\nNoteDetails\n\n\n\n\n\n\n6.1 Methods\n\n6.1.1 close_datasets()\nvoid BigDataStatMeth::hdf5Dims::close_datasets()\nCloses all open datasets.\n\n\n\n6.1.2 convert_DataFrame_to_RangeList()\nnames * BigDataStatMeth::hdf5Dims::convert_DataFrame_to_RangeList(Rcpp::RObject DatasetValues, std::string rowscols, bool bFullDataset)\nConverts R data frame to HDF5-compatible range list.\nParameters:\n\nDatasetValues (Rcpp::RObject): R object containing values to convert\nrowscols (std::string): Identifier for row or column conversion\nbFullDataset (bool): Flag indicating if full dataset conversion is needed\n\nReturns: names* Pointer to array of converted names\nDatasetValuesR object containing values to convert rowscolsIdentifier for row or column conversion bFullDatasetFlag indicating if full dataset conversion is needed names* Pointer to array of converted namesCaller is responsible for freeing returned memory\n\n\n\n\n6.2 Attributes\n\n6.2.1 pmaindataset\nType: BigDataStatMeth::hdf5Dataset *\nPointer to main dataset.\n\n\n\n6.2.2 pdsrownames\nType: H5::DataSet *\nDataset for row names.\n\n\n\n6.2.3 pdscolnames\nType: H5::DataSet *\nDataset for column names.\n\n\n\n6.2.4 strrows\nType: std::string\nIdentifier for row names.\n\n\n\n6.2.5 strcols\nType: std::string\nIdentifier for column names.\n\n\n\n6.2.6 dimcolnames\nType: hsize_t\nDimensions for column names.\n\n\n\n6.2.7 dimrownames\nType: hsize_t\nDimensions for row names.",
    "crumbs": [
      "Classes",
      "hdf5Dims"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dims.html#private-implementation",
    "href": "api-reference/cpp/classes/hdf5Dims.html#private-implementation",
    "title": "hdf5Dims",
    "section": "7 Private Implementation",
    "text": "7 Private Implementation\n\n\n\n\n\n\nNote\n\n\n\nInternal implementation details for advanced users and contributors.\n\n\n\n\n\n\n\n\nNoteDetails\n\n\n\n\n\n7.1 Methods\n\n7.1.1 writeStringVector()\nvoid BigDataStatMeth::hdf5Dims::writeStringVector(H5::DataSet *dataset, std::string datasetname, Rcpp::StringVector DatasetValues)\nWrites a string vector to an HDF5 dataset.\nParameters:\n\ndataset (H5::DataSet *): Pointer to HDF5 dataset\ndatasetname (std::string): Name of the dataset\nDatasetValues (Rcpp::StringVector): Vector of strings to write\n\ndatasetPointer to HDF5 dataset datasetnameName of the dataset DatasetValuesVector of strings to write Implementation details:Creates appropriate HDF5 datatype for stringsProcesses data in blocks for large vectorsHandles string truncation and null terminationManages memory efficiently",
    "crumbs": [
      "Classes",
      "hdf5Dims"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Dims.html#usage-example",
    "href": "api-reference/cpp/classes/hdf5Dims.html#usage-example",
    "title": "hdf5Dims",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"hdf5Dims.hpp\"\n\n// Example usage\nhdf5Dims obj;\n// Your code here",
    "crumbs": [
      "Classes",
      "hdf5Dims"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Group.html",
    "href": "api-reference/cpp/classes/hdf5Group.html",
    "title": "hdf5Group",
    "section": "",
    "text": "Constructor with filename and group name.",
    "crumbs": [
      "Classes",
      "hdf5Group"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Group.html#overview",
    "href": "api-reference/cpp/classes/hdf5Group.html#overview",
    "title": "hdf5Group",
    "section": "",
    "text": "Constructor with filename and group name.",
    "crumbs": [
      "Classes",
      "hdf5Group"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Group.html#detailed-description",
    "href": "api-reference/cpp/classes/hdf5Group.html#detailed-description",
    "title": "hdf5Group",
    "section": "2 Detailed Description",
    "text": "2 Detailed Description\nfilenameName of HDF5 file groupGroup name/path",
    "crumbs": [
      "Classes",
      "hdf5Group"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Group.html#class-hierarchy",
    "href": "api-reference/cpp/classes/hdf5Group.html#class-hierarchy",
    "title": "hdf5Group",
    "section": "3 Class Hierarchy",
    "text": "3 Class Hierarchy\n\n\n\n\nClass diagram",
    "crumbs": [
      "Classes",
      "hdf5Group"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Group.html#inheritance",
    "href": "api-reference/cpp/classes/hdf5Group.html#inheritance",
    "title": "hdf5Group",
    "section": "4 Inheritance",
    "text": "4 Inheritance\nInherits from:\n\nBigDataStatMeth::hdf5File",
    "crumbs": [
      "Classes",
      "hdf5Group"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Group.html#public-interface",
    "href": "api-reference/cpp/classes/hdf5Group.html#public-interface",
    "title": "hdf5Group",
    "section": "5 Public Interface",
    "text": "5 Public Interface\n\n5.1 Methods\n\n5.1.1 hdf5Group()\nBigDataStatMeth::hdf5Group::hdf5Group(BigDataStatMeth::hdf5File *objFile, std::string group)\nConstructor with file object and group name.\nParameters:\n\nobjFile (BigDataStatMeth::hdf5File *): HDF5 file object\ngroup (std::string): Group name/path\n\nCreates a new group if it doesn’t exist\n\n\n\n5.1.2 create_HDF5_groups()\nvoid BigDataStatMeth::hdf5Group::create_HDF5_groups(H5std_string mGroup)\nCreate multiple nested groups.\nParameters:\n\nmGroup (H5std_string): Group path with groups separated by “/”\n\nCreates a hierarchy of groups based on path separated by “/”\n\n\n\n5.1.3 getGroupName()\nstd::string BigDataStatMeth::hdf5Group::getGroupName()\n\n\n\n5.1.4 ~hdf5Group()\nBigDataStatMeth::hdf5Group::~hdf5Group()",
    "crumbs": [
      "Classes",
      "hdf5Group"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Group.html#protected-members",
    "href": "api-reference/cpp/classes/hdf5Group.html#protected-members",
    "title": "hdf5Group",
    "section": "6 Protected Members",
    "text": "6 Protected Members\n\n\n\n\n\n\nNoteDetails\n\n\n\n\n\n6.1 Attributes\n\n6.1.1 groupname\nType: std::string",
    "crumbs": [
      "Classes",
      "hdf5Group"
    ]
  },
  {
    "objectID": "api-reference/cpp/classes/hdf5Group.html#usage-example",
    "href": "api-reference/cpp/classes/hdf5Group.html#usage-example",
    "title": "hdf5Group",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"hdf5Group.hpp\"\n\n// Example usage\nhdf5Group obj;\n// Your code here",
    "crumbs": [
      "Classes",
      "hdf5Group"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html",
    "href": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html",
    "title": "Bblock_matrix_mul_parallel",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Bblock_matrix_mul_parallel(Eigen::MatrixXd A, Eigen::MatrixXd B, int block_size, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Bblock_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#signature",
    "href": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#signature",
    "title": "Bblock_matrix_mul_parallel",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Bblock_matrix_mul_parallel(Eigen::MatrixXd A, Eigen::MatrixXd B, int block_size, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Bblock_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#description",
    "href": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#description",
    "title": "Bblock_matrix_mul_parallel",
    "section": "2 Description",
    "text": "2 Description\nParallel block-based matrix multiplication.",
    "crumbs": [
      "Functions",
      "Bblock_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#parameters",
    "href": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#parameters",
    "title": "Bblock_matrix_mul_parallel",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (Eigen::MatrixXd): First input matrix\nB (Eigen::MatrixXd): Second input matrix\nblock_size (int): Size of blocks for computation\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "Bblock_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#returns",
    "href": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#returns",
    "title": "Bblock_matrix_mul_parallel",
    "section": "4 Returns",
    "text": "4 Returns\nResult of matrix multiplication",
    "crumbs": [
      "Functions",
      "Bblock_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#details",
    "href": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#details",
    "title": "Bblock_matrix_mul_parallel",
    "section": "5 Details",
    "text": "5 Details\nImplements parallel block-based matrix multiplication for in-memory matrices.",
    "crumbs": [
      "Functions",
      "Bblock_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#call-graph",
    "href": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#call-graph",
    "title": "Bblock_matrix_mul_parallel",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Bblock_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#source-code",
    "href": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#source-code",
    "title": "Bblock_matrix_mul_parallel",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/multiplication 2.hpp • Lines 110-171\ninline Eigen::MatrixXd Bblock_matrix_mul_parallel( Eigen::MatrixXd A, Eigen::MatrixXd B, \n                                                             int block_size, Rcpp::Nullable&lt;int&gt; threads  = R_NilValue)\n    {\n        \n        // unsigned int ithreads;\n        Eigen::MatrixXd C;\n        \n        try {\n\n            int M = A.rows();\n            int K = A.cols();\n            int N = B.cols();\n            \n            C = Eigen::MatrixXd::Zero(M,N) ;\n            \n            if( A.rows() == B.cols())\n            {\n                \n                if(block_size &gt; std::min( N, std::min(M,K)) )\n                    block_size = std::min( N, std::min(M,K)); \n                \n                std::vector&lt;hsize_t&gt; vsizetoRead, vstart,\n                                     vsizetoReadM, vstartM,\n                                     vsizetoReadK, vstartK;\n                \n                getBlockPositionsSizes_hdf5( N, block_size, vstart, vsizetoRead );\n                getBlockPositionsSizes_hdf5( M, block_size, vstartM, vsizetoReadM );\n                getBlockPositionsSizes_hdf5( K, block_size, vstartK, vsizetoReadK );\n                \n                // int ithreads = get_number_threads(threads, R_NilValue);\n                // int chunks = vstart.size()/ithreads;\n                \n                #pragma omp parallel num_threads( get_number_threads(threads, R_NilValue) ) shared(A, B, C) //..// , chunk) private(tid ) \n                {\n                    \n                    #pragma omp for schedule (static) // collapse(3)\n                    for (hsize_t ii = 0; ii &lt; vstart.size(); ii ++)\n                    {\n                        for (hsize_t jj = 0; jj &lt; vstartM.size(); jj++)\n                        {\n                            for (hsize_t kk = 0; kk &lt; vstartK.size(); kk++)\n                            {\n                                C.block(vstart[ii], vstartM[jj], vsizetoRead[ii], vsizetoReadM[jj]) = \n                                    C.block(vstart[ii], vstartM[jj], vsizetoRead[ii], vsizetoReadM[jj]) + \n                                    ( A.block(vstart[ii], vstartK[kk], vsizetoRead[ii], vsizetoReadK[kk]) * \n                                      B.block(vstartK[kk], vstartM[jj], vsizetoReadK[kk], vsizetoReadM[jj]) );\n                            }\n                        }\n                    }\n                }\n                \n            } else {\n                throw std::range_error(\"multiplication error: non-conformable arguments\");\n            }\n            \n        } catch(std::exception& ex) {\n            Rcpp::Rcerr&lt;&lt; \"c++ exception Bblock_matrix_mul_parallel: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n        }\n        \n        return(C);\n        \n    }",
    "crumbs": [
      "Functions",
      "Bblock_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#usage-example",
    "href": "api-reference/cpp/functions/Bblock_matrix_mul_parallel.html#usage-example",
    "title": "Bblock_matrix_mul_parallel",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Bblock_matrix_mul_parallel(...);",
    "crumbs": [
      "Functions",
      "Bblock_matrix_mul_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html",
    "title": "Cholesky_decomposition_intermediate_hdf5",
    "section": "",
    "text": "int BigDataStatMeth::Cholesky_decomposition_intermediate_hdf5(BigDataStatMeth::hdf5Dataset *inDataset, BigDataStatMeth::hdf5Dataset *outDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#signature",
    "title": "Cholesky_decomposition_intermediate_hdf5",
    "section": "",
    "text": "int BigDataStatMeth::Cholesky_decomposition_intermediate_hdf5(BigDataStatMeth::hdf5Dataset *inDataset, BigDataStatMeth::hdf5Dataset *outDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#description",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#description",
    "title": "Cholesky_decomposition_intermediate_hdf5",
    "section": "2 Description",
    "text": "2 Description\nPerforms Cholesky decomposition on a matrix stored in HDF5 format.",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#parameters",
    "title": "Cholesky_decomposition_intermediate_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ninDataset (BigDataStatMeth::hdf5Dataset *): Input matrix dataset\noutDataset (BigDataStatMeth::hdf5Dataset *): Output dataset for Cholesky factor L\nidim0 (int): Number of rows\nidim1 (int): Number of columns\ndElementsBlock (long): Block size for processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#returns",
    "title": "Cholesky_decomposition_intermediate_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nint 0 on success, 1 if matrix is not positive definite, 2 on HDF5 errors",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#details",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#details",
    "title": "Cholesky_decomposition_intermediate_hdf5",
    "section": "5 Details",
    "text": "5 Details\ninDatasetInput matrix dataset outDatasetOutput dataset for Cholesky factor L idim0Number of rows idim1Number of columns dElementsBlockBlock size for processing threadsNumber of threads for parallel processing (optional) int 0 on success, 1 if matrix is not positive definite, 2 on HDF5 errors Implements block-wise Cholesky decomposition algorithm:Processes matrix in blocks to manage memory usageComputes lower triangular factor L where A = LL^TUses parallel processing for row computations",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#call-graph",
    "title": "Cholesky_decomposition_intermediate_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#source-code",
    "title": "Cholesky_decomposition_intermediate_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixInvCholesky.hpp • Lines 335-538\ninline int Cholesky_decomposition_intermediate_hdf5( BigDataStatMeth::hdf5Dataset* inDataset,  \n                           BigDataStatMeth::hdf5Dataset* outDataset, \n                           int idim0,  int idim1,  long dElementsBlock, \n                           Rcpp::Nullable&lt;int&gt; threads  = R_NilValue)\n{\n    \n    try {\n        \n        int dimensionSize = idim0,\n            readedRows = 0,\n            rowstoRead,\n            minimumBlockSize;\n            // chunk = 1,\n            \n        bool bcancel = false;\n        double sum = 0;\n        \n        std::vector&lt;hsize_t&gt; offset = {0,0},\n                             count = {1,1},\n                             stride = {1,1},\n                             block = {1,1};\n        \n        // Set minimum elements in block (mandatory : minimum = 2 * longest line)\n        if( dElementsBlock &lt; dimensionSize * 2 ) {\n            minimumBlockSize = dimensionSize * 2;\n        } else {\n            minimumBlockSize = dElementsBlock;\n        }\n        \n        // Rcpp::Rcout&lt;&lt;\"\\nDebug 2: minimumBlockSize=\" &lt;&lt; minimumBlockSize;\n        // Poso el codi per llegir els blocks aquí i desprès a dins hauria d'anar-hi la j\n        if( idim0 == idim1)\n        {\n            \n            // Rcpp::Rcout&lt;&lt;\"\\nDins Cholesky_decomposition -&gt; idim0 == idim1\";\n            while ( readedRows &lt; dimensionSize ) {\n                \n                rowstoRead = ( -2 * readedRows - 1 + std::sqrt( pow(2*readedRows, 2) - 4 * readedRows + 8 * minimumBlockSize + 1) ) / 2;\n                \n                if (rowstoRead &lt;= 0) {\n                    rowstoRead = minimumBlockSize; // Minimum progress to avoid infinite loop\n                }\n\n                if( readedRows + rowstoRead &gt; idim0) { // Max size bigger than data to read ?\n                    rowstoRead = idim0 - readedRows;\n                }\n                \n                if (readedRows == 0) {\n                    // Primer bloque: count[0] = dimensionSize, count[1] = rowstoRead\n                    size_t potential_elements = static_cast&lt;size_t&gt;(dimensionSize) * static_cast&lt;size_t&gt;(rowstoRead);\n                    size_t optimalSize = getOptimalBlockElements();\n                    if (potential_elements &gt; optimalSize) {\n                        rowstoRead = static_cast&lt;int&gt;(MAXCHOLBLOCKSIZE / dimensionSize);\n                        if (rowstoRead &lt; 1) rowstoRead = 1;\n                    }\n                } else {\n                    // Bloques siguientes: count[0] = dimensionSize - readedRows + 1, count[1] = readedRows + rowstoRead\n                    size_t potential_elements = static_cast&lt;size_t&gt;(dimensionSize - readedRows + 1) * static_cast&lt;size_t&gt;(readedRows + rowstoRead);\n                    size_t optimalSize = getOptimalBlockElements();\n                    if (potential_elements &gt; optimalSize) {\n                        int max_cols = static_cast&lt;int&gt;(MAXCHOLBLOCKSIZE / (dimensionSize - readedRows + 1));\n                        rowstoRead = max_cols - readedRows;\n                        if (rowstoRead &lt; 1) rowstoRead = 1;\n                    }\n                }\n                \n                offset[0] = readedRows;\n                \n                if( readedRows == 0) {\n                    count[0] = dimensionSize - readedRows;\n                    count[1] = rowstoRead;\n                } else {\n                    offset[0] = offset[0] - 1; // We need results from previous line to compute next line results\n                    count[1] = readedRows + rowstoRead;\n                    count[0] = dimensionSize - readedRows + 1;\n                }\n                \n                readedRows = readedRows + rowstoRead; // Ho preparem perquè desprès necessitarem llegir a partir de la línea anterior\n                \n                Eigen::MatrixXd A, L;\n                \n                // Rcpp::Rcout&lt;&lt;\"\\nDebug 7: A L\";\n                \n                // CAMBIO QUIRÚRGICO: Verificar tamaño de bloque para big-matrix\n                size_t block_elements = static_cast&lt;size_t&gt;(count[0]) * static_cast&lt;size_t&gt;(count[1]);\n                // size_t max_elements = MAXCHOLBLOCKSIZE; // ~400 MB por vector, 800 MB total\n                \n                if (block_elements &gt; MAXCHOLBLOCKSIZE) {\n                    // Recalcular rowstoRead para bloque más pequeño\n                    rowstoRead = static_cast&lt;int&gt;(MAXCHOLBLOCKSIZE / count[0]);\n                    if (rowstoRead &lt; 1) rowstoRead = 1;\n                    \n                    // Recalcular count[1]\n                    if( readedRows == 0) {\n                        count[1] = rowstoRead;\n                    } else {\n                        count[1] = readedRows + rowstoRead;\n                    }\n                    \n                    // Actualizar readedRows\n                    readedRows = (readedRows &gt; rowstoRead) ? (readedRows - rowstoRead + rowstoRead) : readedRows + rowstoRead;\n                }\n                \n                std::vector&lt;double&gt; vdA( count[0] * count[1] ); \n                inDataset-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdA.data() );\n                A = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vdA.data(), count[0], count[1] );\n                \n                std::vector&lt;double&gt; vdL( count[0] * count[1] ); \n                outDataset-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdL.data() );\n                L = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vdL.data(), count[0], count[1] );\n                \n                if( offset[0] != 0 )\n                    rowstoRead = rowstoRead + 1;\n                \n                if( rowstoRead &gt; static_cast&lt;int&gt;(count[1]) ) {\n                    rowstoRead = count[1];\n                }\n                \n                //. 2025/11/21 .// for (int j = 0; j &lt; rowstoRead; j++)  {\n                for (int j = 0; j &lt; rowstoRead; j++)  {\n                    if( j + offset[0] == 0) {\n                        L(j, j) = std::sqrt(A(j,j));\n                    } else {\n                        L(j, j + offset[0]) = std::sqrt(A(j,j + offset[0]) - (L.row(j).head(j + offset[0]).array().pow(2).sum() ));    \n                    }\n                    \n                    \n                    #pragma omp parallel for num_threads( get_number_threads(threads, R_NilValue) ) private(sum) shared (A,L,j) schedule(dynamic) if (j &lt; readedRows - 1)\n                    //. 2025/11/21 .// for ( int i = j + 1; i &lt; dimensionSize - offset[0]  ; i++ )\n                    for ( int i = j + 1; i &lt; dimensionSize - static_cast&lt;int&gt;(offset[0])  ; i++ )\n                    {\n                        if(bcancel == false) {\n                            if( j + static_cast&lt;int&gt;(offset[0]) &gt; 0) {\n                                sum = (L.block(i, 0, 1, j + static_cast&lt;int&gt;(offset[0])).array() * L.block(j, 0, 1, j + static_cast&lt;int&gt;(offset[0])).array()).array().sum();\n                                if( sum != sum ) {\n                                    Rcpp::Rcout&lt;&lt;\"\\n Can't get inverse matrix using Cholesky decomposition matrix is not positive definite\\n\";\n                                    bcancel = true;\n                                }\n                            } else {\n                                sum = 0;\n                            }\n                            if(!bcancel){\n                                L(i,j + static_cast&lt;int&gt;(offset[0])) =  (1/L(j,j + static_cast&lt;int&gt;(offset[0]))*(A(i,j + static_cast&lt;int&gt;(offset[0])) - sum));    \n                            }    \n                        }\n                    }\n                }\n                if(bcancel == true) {\n                    return(1);\n                }\n                \n                if( offset[0] != 0) {\n                    offset[0] = offset[0] + 1;\n                    count[0] = count[0] - 1;\n                    \n                    outDataset-&gt;writeDatasetBlock( Rcpp::wrap(L.block(1, 0, L.rows()-1, L.cols())), offset, count, stride, block, false);\n                    \n                } else {\n                    outDataset-&gt;writeDatasetBlock( Rcpp::wrap(L), offset, count, stride, block, false);\n                }\n                \n                // Rcpp::Rcout&lt;&lt;\"\\nDebug 13\";\n                \n                offset[0] = offset[0] + count[0] - 1;\n                \n                // Rcpp::Rcout&lt;&lt;\"\\nDins Cholesky_decomposition -&gt; Despres escriptura \";\n            }\n            \n        } else {\n            throw std::range_error(\"non-conformable arguments\");\n        }\n        \n        // Rcpp::Rcout&lt;&lt;\"\\nDins Cholesky_decomposition -&gt; Bye Bye...\";\n        \n        \n    } catch( H5::FileIException& error ) { \n        checkClose_file(inDataset, outDataset);\n        inDataset = outDataset = nullptr;\n        Rcpp::Rcerr&lt;&lt;\"c++ exception Cholesky_decomposition_intermediate_hdf5 (File IException)\";\n        return(2);\n    } catch( H5::GroupIException & error ) { \n        checkClose_file(inDataset, outDataset);\n        inDataset = outDataset = nullptr;\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Cholesky_decomposition_intermediate_hdf5 (Group IException)\";\n        return(2);\n    } catch( H5::DataSetIException& error ) { \n        checkClose_file(inDataset, outDataset);\n        inDataset = outDataset = nullptr;\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Cholesky_decomposition_intermediate_hdf5 (DataSet IException)\";\n        return(2);\n    } catch(std::exception& ex) {\n        checkClose_file(inDataset, outDataset);\n        inDataset = outDataset = nullptr;\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Cholesky_decomposition_intermediate_hdf5\" &lt;&lt; ex.what();\n        return(2);\n    } catch (...) {\n        checkClose_file(inDataset, outDataset);\n        inDataset = outDataset = nullptr;\n        Rcpp::Rcerr&lt;&lt;\"\\nC++ exception Cholesky_decomposition_intermediate_hdf5 (unknown reason)\";\n        return(2);\n    }\n    return(0);\n    \n}",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.html#usage-example",
    "title": "Cholesky_decomposition_intermediate_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Cholesky_decomposition_intermediate_hdf5(...);",
    "crumbs": [
      "Functions",
      "Cholesky_decomposition_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html",
    "href": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html",
    "title": "First_level_SvdBlock_decomposition_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::First_level_SvdBlock_decomposition_hdf5(T *dsA, std::string strGroupName, int k, int q, int nev, bool bcenter, bool bscale, double dthreshold, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "First_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html#signature",
    "href": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html#signature",
    "title": "First_level_SvdBlock_decomposition_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::First_level_SvdBlock_decomposition_hdf5(T *dsA, std::string strGroupName, int k, int q, int nev, bool bcenter, bool bscale, double dthreshold, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "First_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html#parameters",
    "title": "First_level_SvdBlock_decomposition_hdf5",
    "section": "2 Parameters",
    "text": "2 Parameters\n\ndsA (T *)\nstrGroupName (std::string)\nk (int)\nq (int)\nnev (int)\nbcenter (bool)\nbscale (bool)\ndthreshold (double)\nthreads (Rcpp::Nullable&lt; int &gt;)",
    "crumbs": [
      "Functions",
      "First_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html#returns",
    "href": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html#returns",
    "title": "First_level_SvdBlock_decomposition_hdf5",
    "section": "3 Returns",
    "text": "3 Returns\nType: class T",
    "crumbs": [
      "Functions",
      "First_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html#call-graph",
    "title": "First_level_SvdBlock_decomposition_hdf5",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "First_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html#source-code",
    "title": "First_level_SvdBlock_decomposition_hdf5",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSvdBlock.hpp • Lines 238-467\ninline void First_level_SvdBlock_decomposition_hdf5( T* dsA, std::string strGroupName, int k, int q, int nev, bool bcenter, bool bscale, \n                                             double dthreshold, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n{\n    \n    static_assert(std::is_same&lt;T*, BigDataStatMeth::hdf5Dataset* &gt;::value || \n                  std::is_same&lt;T*, BigDataStatMeth::hdf5DatasetInternal* &gt;::value,\n                  \"Error - type not allowed\");\n    \n    BigDataStatMeth::hdf5DatasetInternal* normalizedData = nullptr;\n    BigDataStatMeth::hdf5Dataset* unlimDataset = nullptr;\n    \n    try{\n        \n        std::vector&lt;svdPositions&gt; paralPos;\n        \n        int  M, p, n, irows, icols, normalsize;\n        // int ithreads;\n        bool transp = false;\n        Rcpp::Nullable&lt;int&gt; wsize = R_NilValue;\n        \n        irows = dsA-&gt;ncols();\n        icols = dsA-&gt;nrows();\n        \n        // ithreads = get_number_threads(threads, R_NilValue);\n        \n        // Work with transposed matrix\n        if( irows &gt;= icols ) {\n            transp = true;\n            n = icols;\n            p = irows;\n            normalsize = n;\n        } else {\n            n = irows;\n            p = icols;\n            normalsize = p;\n        }\n        \n        Eigen::MatrixXd datanormal = Eigen::MatrixXd::Zero(2, normalsize);\n        get_HDF5_mean_sd_by_column(dsA, datanormal, true, true, wsize);\n        \n        M = pow(k, q);\n        if(M&gt;p)\n            throw std::runtime_error(\"k^q must not be greater than the number of columns in the matrix\");\n        \n        //.. 2025-02-10 ..// double block_size = std::floor((double)icols/(double)M);\n        double block_size = std::round((double)p/(double)M); \n        \n        if (bcenter==true || bscale==true) { // Create dataset to store normalized data\n            normalizedData = new BigDataStatMeth::hdf5DatasetInternal (dsA-&gt;getFullPath(), strGroupName, \"normalmatrix\", true);\n            // normalizedData-&gt;createDataset( irows, icols, \"real\");\n            normalizedData-&gt;createDataset( irows, icols, \"real\");\n            //.Caldria revisar... .// normalizedData-&gt;createDataset( n, p, \"real\");\n        }\n        \n        // Get all the offsets and counts inside the file to write and read\n        paralPos = prepareForParallelization( dsA, M, k, transp, block_size, strGroupName + \"/A\");\n        #pragma omp parallel num_threads( get_number_threads(threads, R_NilValue) ) shared (normalizedData)\n        {\n            \n            // Get data from M blocks in initial matrix\n            #pragma omp for schedule (dynamic) ordered\n            for( int i = 0; i&lt; M ; i++)  \n            {\n                \n                Eigen::MatrixXd restmp;\n                \n                // 1.- Get SVD from all blocks\n                //    a) Get all blocks from initial matrix \n                \n                Eigen::MatrixXd X;\n                    \n                std::vector&lt;double&gt; vdX( paralPos[i].count[0] * paralPos[i].count[1] ); \n                #pragma omp critical(accessFile)\n                {\n                    dsA-&gt;readDatasetBlock( {paralPos[i].totOffset[0], paralPos[i].totOffset[1]}, {paralPos[i].count[0], paralPos[i].count[1]}, paralPos[i].stride, paralPos[i].block, vdX.data() );\n                }\n                \n                if(transp==false){    \n                    X = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; (vdX.data(), paralPos[i].count[1], paralPos[i].count[0] );\n                } else {\n                    X = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vdX.data(), paralPos[i].count[0], paralPos[i].count[1] );\n                }\n                \n                // Normalize data\n                if (bcenter==true || bscale==true) \n                {\n                    std::vector&lt;hsize_t&gt; offset_tmp = {paralPos[i].totOffset[1], paralPos[i].totOffset[0]};\n                    std::vector&lt;hsize_t&gt; count_tmp = {paralPos[i].count[1], paralPos[i].count[0]};\n                    \n                    if(transp == true) {\n                        offset_tmp = {paralPos[i].totOffset[0], paralPos[i].totOffset[1]};\n                        count_tmp = {paralPos[i].count[0], paralPos[i].count[1]};\n                    } \n                        \n                    if( transp == false) {\n                        // X = RcppNormalize_Data(X, bcenter, bscale, transp, datanormal.block(0, offset_tmp[1], 2, count_tmp[1]));\n                        X = RcppNormalizeColwise(X, bcenter, bscale);\n                        \n                        #pragma omp critical(accessFile)\n                        {   \n                            normalizedData-&gt;writeDatasetBlock( Rcpp::wrap(X), offset_tmp, count_tmp, paralPos[i].stride, paralPos[i].block, false);\n                        }\n                        \n                    } else {\n                        \n                        X = RcppNormalize_Data(X, bcenter, bscale, transp, datanormal.block(0, offset_tmp[0], 2, count_tmp[0]));\n                        \n                        count_tmp = {(unsigned long long)X.cols(), (unsigned long long)X.rows()};\n                        offset_tmp = {paralPos[i].totOffset[1], paralPos[i].totOffset[0]};\n                        \n                        #pragma omp critical(accessFile)\n                        {   \n                            normalizedData-&gt;writeDatasetBlock( Rcpp::wrap(X.transpose()), offset_tmp, count_tmp, paralPos[i].stride, paralPos[i].block, false);\n                        }\n                    }\n                }\n                \n\n                // Rcpp::Rcout&lt;&lt;\"\\nPeta a l'step 1? - 1\";\n                {\n                    //    b) SVD for each block\n                    svdeig retsvd;\n                    retsvd = RcppbdSVD_lapack(X, false, false, false);\n\n                    int size_d = (retsvd.d).size();\n                    int nzeros = 0;\n\n                    if( (retsvd.d)[size_d - 1] &lt;= dthreshold ){\n                        nzeros = 1;\n                        for( int j = (size_d - 2); ( j&gt;1 && (retsvd.d)[i] &lt;= dthreshold ); j-- ) {\n                            nzeros++;\n                        }\n                    }\n\n                    //    c)  U*d\n                    // Create diagonal matrix from svd decomposition d\n                    int isize = (retsvd.d).size() - nzeros;\n\n                    if( isize &lt; 2 ) {\n                        isize = 2;\n                    }\n\n                    Eigen::MatrixXd d = Eigen::MatrixXd::Zero(isize, isize);\n                    d.diagonal() = (retsvd.d).head(isize);\n\n                    Eigen::MatrixXd u = (retsvd.u).block(0, 0, (retsvd.u).rows(), isize);\n\n                    // if( u.size() &lt; MAXELEMSINBLOCK ) {\n                    if (static_cast&lt;hsize_t&gt;(u.size()) &lt; MAXELEMSINBLOCK) {\n                        restmp = u*d;\n                    } else{\n                        restmp = Rcpp_block_matrix_mul( u, d, R_NilValue);\n                    }\n                }\n\n                paralPos[i].write_count = {(hsize_t)restmp.rows(), (hsize_t)restmp.cols()};\n\n                //    d) Write results to hdf5 file\n                #pragma omp ordered\n                {\n                    #pragma omp critical(accessFile)\n                    {\n\n                        if( i%(M/k) == 0 || ( (i%(M/k) &gt; 0 &&  !exists_HDF5_element(dsA-&gt;getFileptr(),  paralPos[i].strDatasetName )) ) )\n                        {\n                            unlimDataset = new BigDataStatMeth::hdf5DatasetInternal(dsA-&gt;getFullPath(), paralPos[i].strDatasetName, true );\n                            unlimDataset-&gt;createUnlimitedDataset(paralPos[i].write_count[0], paralPos[i].write_count[1], \"real\");\n                            delete unlimDataset; unlimDataset = nullptr;\n\n                            paralPos[i].write_offset = 0;\n                        }\n\n                        unlimDataset = new BigDataStatMeth::hdf5DatasetInternal(dsA-&gt;getFullPath(), paralPos[i].strDatasetName, true );\n                        unlimDataset-&gt;openDataset();\n\n                        // Extend dataset before to put the data\n                        if((i%(M/k)) != 0 && paralPos[i].write_offset &gt; 0) {\n                            unlimDataset-&gt;extendUnlimitedDataset(0, paralPos[i].write_count[1] );\n                        }\n\n                        // std::vector&lt;double&gt; vtmpc(restmp.data(), restmp.data() + restmp.rows() * restmp.cols());\n                        unlimDataset-&gt;writeDatasetBlock( Rcpp::wrap(restmp), {0, paralPos[i].write_offset}, paralPos[i].write_count, paralPos[i].stride, paralPos[i].block, false);\n                        delete unlimDataset; unlimDataset = nullptr;\n\n                        if( i&lt;M-1 )\n                            paralPos[i+1].write_offset = paralPos[i].write_offset + paralPos[i].write_count[1];\n\n                    }\n                }\n            }\n        }\n        \n        if (bcenter==true || bscale==true) { \n            delete normalizedData; normalizedData = nullptr;\n        }\n        \n    } catch( H5::FileIException& error ) { \n        checkClose_file(dsA, unlimDataset, normalizedData);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception First_level_SvdBlock_decomposition_hdf5 (File IException)\\n\";\n        return void();\n    } catch( H5::DataSetIException& error ) { \n        checkClose_file(dsA, unlimDataset, normalizedData);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception First_level_SvdBlock_decomposition_hdf5 (DataSet IException)\\n\";\n        return void();\n    } catch( H5::GroupIException& error ) { \n        checkClose_file(dsA, unlimDataset, normalizedData);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception First_level_SvdBlock_decomposition_hdf5 (Group IException)\\n\";\n        return void();\n    } catch( H5::DataTypeIException& error ) { \n        checkClose_file(dsA, unlimDataset, normalizedData);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception First_level_SvdBlock_decomposition_hdf5 (DataType IException)\\n\";\n        return void();\n    } catch( H5::DataSpaceIException& error ) { \n        checkClose_file(dsA, unlimDataset, normalizedData);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception First_level_SvdBlock_decomposition_hdf5 (DataSpace IException)\\n\";\n        return void();\n    } catch(std::exception &ex) {\n        checkClose_file(dsA, unlimDataset, normalizedData);\n        Rcpp::Rcerr&lt;&lt;\"c++ exception First_level_SvdBlock_decomposition_hdf5 \\n\"&lt;&lt; ex.what();\n        return void();\n    } catch (...) {\n        checkClose_file(dsA, unlimDataset, normalizedData);\n        Rcpp::Rcerr&lt;&lt;\"\\nC++ exception First_level_SvdBlock_decomposition_hdf5 (unknown reason)\";\n        return void();\n    }\n    \n    \n    return void();\n    \n}",
    "crumbs": [
      "Functions",
      "First_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.html#usage-example",
    "title": "First_level_SvdBlock_decomposition_hdf5",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = First_level_SvdBlock_decomposition_hdf5(...);",
    "crumbs": [
      "Functions",
      "First_level_SvdBlock_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html",
    "title": "Inverse_Matrix_Cholesky_intermediate_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Inverse_Matrix_Cholesky_intermediate_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#signature",
    "title": "Inverse_Matrix_Cholesky_intermediate_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Inverse_Matrix_Cholesky_intermediate_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#description",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#description",
    "title": "Inverse_Matrix_Cholesky_intermediate_hdf5",
    "section": "2 Description",
    "text": "2 Description\nComputes final matrix inverse using inverted Cholesky factors.",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#parameters",
    "title": "Inverse_Matrix_Cholesky_intermediate_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nInOutDataset (BigDataStatMeth::hdf5Dataset *): Dataset containing L^-1 (will be overwritten with A^-1)\nidim0 (int): Number of rows\nidim1 (int): Number of columns\ndElementsBlock (long): Block size for processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#details",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#details",
    "title": "Inverse_Matrix_Cholesky_intermediate_hdf5",
    "section": "4 Details",
    "text": "4 Details\nInOutDatasetDataset containing L^-1 (will be overwritten with A^-1) idim0Number of rows idim1Number of columns dElementsBlockBlock size for processing threadsNumber of threads for parallel processing (optional) Computes A^-1 = L^-T L^-1 where L^-1 is the inverse of Cholesky factor:Uses block matrix multiplication for memory efficiencyImplements parallel processing for block operationsResult is symmetric but only lower triangular part is computed",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#call-graph",
    "title": "Inverse_Matrix_Cholesky_intermediate_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#source-code",
    "title": "Inverse_Matrix_Cholesky_intermediate_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixInvCholesky.hpp • Lines 962-1128\ninline void Inverse_Matrix_Cholesky_intermediate_hdf5( BigDataStatMeth::hdf5Dataset* InOutDataset, \n                                     int idim0, int idim1, long dElementsBlock, \n                                     Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n{\n    \n    try {\n        \n        int dimensionSize = idim0,\n            readedCols = 0,\n            colstoRead;\n            //.. 20251121 ..// minimumBlockSize;\n        \n        Eigen::VectorXd newDiag(idim0);\n        \n        std::vector&lt;hsize_t&gt; offset = {0,0},\n                             count = {1,1},\n                             stride = {1,1},\n                             block = {1,1};\n        \n        // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_Matrix_Cholesky_intermediate_hdf5 -&gt; Hi...\";\n        // Set minimum elements in block (mandatory : minimum = 2 * longest line)\n        //.. 20251121 ..// if( dElementsBlock &lt; dimensionSize * 2 ) {\n        //.. 20251121 ..//     minimumBlockSize = dimensionSize * 2;\n        //.. 20251121 ..// } else {\n        //.. 20251121 ..//     minimumBlockSize = dElementsBlock;\n        //.. 20251121 ..// }\n        \n        if( idim0 == idim1 )\n        {\n            // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_Matrix_Cholesky_parallel -&gt; idim0 == idim1\";\n            while ( readedCols &lt; dimensionSize ) {\n                \n                // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_Matrix_Cholesky_parallel -&gt;  readedCols &lt; dimensionSize\";\n                // Set minimum elements in block - prevent overflow and optimize for any matrix size\n                //.. 20251121 ..// minimumBlockSize = std::min(static_cast&lt;long&gt;(dElementsBlock), static_cast&lt;long&gt;(std::max(2000L, dimensionSize * 2L)));\n                \n                // Adaptive block size calculation for any matrix dimensions\n                // Target: ~50MB blocks for optimal cache usage and memory efficiency\n                // const int targetElements = 6250000; // ~50MB / 8 bytes per double\n                const int targetElements = (dimensionSize &gt; 10000) ? 25000000 : 6250000;\n                const int minCols = std::max(1, std::min(16, dimensionSize / 100)); // 1-16 or 1% of matrix\n                const int maxCols = std::min(dimensionSize, static_cast&lt;int&gt;(std::sqrt(targetElements / dimensionSize))); // Cache-friendly limit\n                \n                colstoRead = std::max(minCols, std::min(maxCols, (dimensionSize - readedCols + 2) / 3));\n                \n                // Ensure we don't exceed remaining columns\n                colstoRead = std::min(colstoRead, dimensionSize - readedCols);\n                \n                // Final safety check\n                if (colstoRead &lt;= 0) colstoRead = 1;\n                \n                \n                \n                // colstoRead = ( -2 * readedCols - 1 + std::sqrt( pow(2*readedCols, 2) - 4 * readedCols + 8 * minimumBlockSize + 1) ) / 2;\n                if(colstoRead ==1) {\n                    colstoRead = 2;\n                }\n                \n                if( readedCols + colstoRead &gt; idim0) { // Max size bigger than data to read ?\n                    colstoRead = idim0 - readedCols;\n                }\n                \n                size_t potential_elements = static_cast&lt;size_t&gt;(dimensionSize - readedCols) * static_cast&lt;size_t&gt;(readedCols + colstoRead);\n                size_t optimalSize = getOptimalBlockElements();\n                if (potential_elements &gt; optimalSize) {\n                    int max_cols = static_cast&lt;int&gt;(MAXCHOLBLOCKSIZE / (dimensionSize - readedCols));\n                    colstoRead = max_cols - readedCols;\n                    if (colstoRead &lt; 2) colstoRead = 2; // Esta función necesita mínimo 2\n                }\n                \n                offset[0] = readedCols;\n                count[0] =  dimensionSize - readedCols;\n                count[1] = readedCols + colstoRead;\n                \n                Eigen::MatrixXd verticalData;\n                \n                std::vector&lt;double&gt; vverticalData( count[0] * count[1] ); \n                InOutDataset-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vverticalData.data() );\n                verticalData = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vverticalData.data(), count[0], count[1] );\n                \n                // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_Matrix_Cholesky_parallel -&gt;  Anem a per el for parallel\";\n                \n                //!!!!! for ( int i = 0; i &lt; colstoRead + offset[0]; i++)   // Columns\n// #pragma omp parallel for num_threads(ithreads) shared (verticalData, colstoRead, offset) schedule(dynamic)\n                // int max_i = std::min(static_cast&lt;int&gt;(colstoRead + offset[0]), static_cast&lt;int&gt;(count[1]));\n                \n                if (offset[0] &gt; static_cast&lt;hsize_t&gt;(std::numeric_limits&lt;Eigen::Index&gt;::max())) {\n                    Rcpp::stop(\"offset[0] is too large to convert to Eigen::Index\");\n                }\n                \n                #pragma omp parallel for num_threads( get_number_threads(threads, R_NilValue) ) shared (verticalData, colstoRead, offset) schedule(static) ordered\n                for ( int i = 0; i &lt; colstoRead + static_cast&lt;int&gt;(offset[0]); i++)   // Columns\n                // for ( int i = 0; i &lt; max_i; i++)   // Columns\n                {\n                    int init;\n                    \n                    if(static_cast&lt;int&gt;(offset[0]) == 0) {\n                        init = i + 1;\n                        newDiag(i) = verticalData.block(i, i, idim0-i, 1 ).array().pow(2).sum();\n                    } else {\n                        if(  i &lt; static_cast&lt;int&gt;(offset[0])) {\n                            init = 0;\n                        } else {\n                            newDiag(i) = verticalData.block( i-static_cast&lt;int&gt;(offset[0]), i, idim0-i, 1 ).array().pow(2).sum();\n                            if( i &lt; static_cast&lt;int&gt;(offset[0]) + colstoRead - 1) {\n                                init = i - static_cast&lt;int&gt;(offset[0]) + 1;\n                            } else {\n                                init = colstoRead; // force end\n                            }\n                        }\n                    }\n                    \n                    #pragma omp ordered\n                    {\n                        for ( int j = init; j &lt; colstoRead ; j++) { // Rows\n                            \n                            if( static_cast&lt;int&gt;(offset[0]) + j &lt; verticalData.cols()) {\n                                verticalData(j,i) = (verticalData.block( j, i , verticalData.rows() - j, 1).array() * verticalData.block( j, j + static_cast&lt;int&gt;(offset[0]),  verticalData.rows() - j, 1).array()).sum();\n                            }\n                        }\n                    }\n                   \n                }\n                \n                // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_Matrix_Cholesky_parallel -&gt;  Fi volta, anem a escriure\";\n                InOutDataset-&gt;writeDatasetBlock( Rcpp::wrap(verticalData), offset, count, stride, block, false);\n                \n                readedCols = readedCols + colstoRead; \n            }\n            \n            setDiagonalMatrix( InOutDataset, Rcpp::wrap(newDiag) );\n            \n        } else {\n            throw std::range_error(\"non-conformable arguments\");\n        }\n        \n        // Rcpp::Rcout&lt;&lt;\"\\nDins Inverse_Matrix_Cholesky_parallel -&gt; Bye Bye...\";\n        \n    } catch( H5::FileIException& error ) { \n        checkClose_file(InOutDataset);\n        InOutDataset = nullptr;\n        Rcpp::Rcerr&lt;&lt;\"c++ exception Inverse_Matrix_Cholesky_intermediate_hdf5 (File IException)\";\n        return void();\n    } catch( H5::GroupIException & error ) { \n        checkClose_file(InOutDataset);\n        InOutDataset = nullptr;\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Inverse_Matrix_Cholesky_intermediate_hdf5 (Group IException)\";\n        return void();\n    } catch( H5::DataSetIException& error ) { \n        checkClose_file(InOutDataset);\n        InOutDataset = nullptr;\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Inverse_Matrix_Cholesky_intermediate_hdf5 (DataSet IException)\";\n        return void();\n    } catch(std::exception& ex) {\n        checkClose_file(InOutDataset);\n        InOutDataset = nullptr;\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Inverse_Matrix_Cholesky_intermediate_hdf5: \" &lt;&lt; ex.what();\n        return void();\n    } catch (...) {\n        checkClose_file(InOutDataset);\n        InOutDataset = nullptr;\n        Rcpp::Rcerr&lt;&lt;\"\\nC++ exception Inverse_Matrix_Cholesky_intermediate_hdf5 (unknown reason)\";\n        return void();\n    }\n    \n    return void();\n}",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.html#usage-example",
    "title": "Inverse_Matrix_Cholesky_intermediate_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Inverse_Matrix_Cholesky_intermediate_hdf5(...);",
    "crumbs": [
      "Functions",
      "Inverse_Matrix_Cholesky_intermediate_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html",
    "title": "Inverse_of_Cholesky_decomposition_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Inverse_of_Cholesky_decomposition_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#signature",
    "title": "Inverse_of_Cholesky_decomposition_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Inverse_of_Cholesky_decomposition_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#description",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#description",
    "title": "Inverse_of_Cholesky_decomposition_hdf5",
    "section": "2 Description",
    "text": "2 Description\nComputes inverse of Cholesky factor with automatic algorithm selection.",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#parameters",
    "title": "Inverse_of_Cholesky_decomposition_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nInOutDataset (BigDataStatMeth::hdf5Dataset *): Dataset containing Cholesky factor L (will be overwritten with L^-1)\nidim0 (int): Number of rows\nidim1 (int): Number of columns\ndElementsBlock (long): Block size for processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#details",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#details",
    "title": "Inverse_of_Cholesky_decomposition_hdf5",
    "section": "4 Details",
    "text": "4 Details\nInOutDatasetDataset containing Cholesky factor L (will be overwritten with L^-1) idim0Number of rows idim1Number of columns dElementsBlockBlock size for processing threadsNumber of threads for parallel processing (optional) Automatically selects appropriate algorithm based on matrix size:Matrices &lt; CHOLESKY_OUTOFCORE_THRESHOLD: uses intermediate algorithmMatrices ≥ CHOLESKY_OUTOFCORE_THRESHOLD: uses out-of-core tiled algorithm",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#call-graph",
    "title": "Inverse_of_Cholesky_decomposition_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#source-code",
    "title": "Inverse_of_Cholesky_decomposition_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixInvCholesky.hpp • Lines 696-707\ninline void Inverse_of_Cholesky_decomposition_hdf5(BigDataStatMeth::hdf5Dataset* InOutDataset, \n                                                   int idim0, int idim1, long dElementsBlock, \n                                                   Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n{\n    // Detect matrix size and select algorithm\n    if (idim0 &gt;= CHOLESKY_OUTOFCORE_THRESHOLD) {\n        Rcpp::Rcout &lt;&lt; \"\\nUsing out-of-core inverse Cholesky for large matrix (\" &lt;&lt; idim0 &lt;&lt; \"x\" &lt;&lt; idim1 &lt;&lt; \")\\n\";\n        Inverse_of_Cholesky_decomposition_outofcore_hdf5(InOutDataset, idim0, idim1, dElementsBlock, threads);\n    } else {\n        Inverse_of_Cholesky_decomposition_intermediate_hdf5(InOutDataset, idim0, idim1, dElementsBlock, threads);\n    }\n}",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.html#usage-example",
    "title": "Inverse_of_Cholesky_decomposition_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Inverse_of_Cholesky_decomposition_hdf5(...);",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html",
    "title": "Inverse_of_Cholesky_decomposition_outofcore_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Inverse_of_Cholesky_decomposition_outofcore_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#signature",
    "title": "Inverse_of_Cholesky_decomposition_outofcore_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Inverse_of_Cholesky_decomposition_outofcore_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#description",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#description",
    "title": "Inverse_of_Cholesky_decomposition_outofcore_hdf5",
    "section": "2 Description",
    "text": "2 Description\nOut-of-core inverse of Cholesky factor using tiled back-substitution.",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#parameters",
    "title": "Inverse_of_Cholesky_decomposition_outofcore_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nInOutDataset (BigDataStatMeth::hdf5Dataset *): Dataset containing Cholesky factor L (overwritten with L^-1)\nidim0 (int): Number of rows\nidim1 (int): Number of columns\ndElementsBlock (long): Block size (not used, tiles are fixed)\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#details",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#details",
    "title": "Inverse_of_Cholesky_decomposition_outofcore_hdf5",
    "section": "4 Details",
    "text": "4 Details\nInOutDatasetDataset containing Cholesky factor L (overwritten with L^-1) idim0Number of rows idim1Number of columns dElementsBlockBlock size (not used, tiles are fixed) threadsNumber of threads for parallel processing (optional) Inverts lower triangular matrix using fixed tiles:Processes backward from last tile to firstEach tile inversion updates dependent tilesMemory usage: ~800 MB constantInverts lower triangular matrix L in-place using tiles",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#call-graph",
    "title": "Inverse_of_Cholesky_decomposition_outofcore_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#source-code",
    "title": "Inverse_of_Cholesky_decomposition_outofcore_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixInvCholesky.hpp • Lines 881-944\ninline void Inverse_of_Cholesky_decomposition_outofcore_hdf5(BigDataStatMeth::hdf5Dataset* InOutDataset, \n                                                             int idim0, int idim1, long dElementsBlock, \n                                                             Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n{\n    \n    try {\n        int dimensionSize = idim0;\n        int tileSize = 10000;\n        int numTiles = (dimensionSize + tileSize - 1) / tileSize;\n        \n        std::vector&lt;hsize_t&gt; stride = {1,1}, block = {1,1};\n        \n        // Process tiles backward for inverse: last diagonal to first\n        for (int k = numTiles - 1; k &gt;= 0; k--) {\n            \n            hsize_t kStart = k * tileSize;\n            hsize_t kSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - kStart));\n            \n            // 1. Invert diagonal tile L(k,k) in-place\n            Eigen::MatrixXd Lkk(kSize, kSize);\n            std::vector&lt;double&gt; vLkk(kSize * kSize);\n            InOutDataset-&gt;readDatasetBlock({kStart, kStart}, {kSize, kSize}, stride, block, vLkk.data());\n            Lkk = Eigen::Map&lt;Eigen::MatrixXd&gt;(vLkk.data(), kSize, kSize);\n            \n            // Invert lower triangular tile\n            Lkk = Lkk.triangularView&lt;Eigen::Lower&gt;().solve(Eigen::MatrixXd::Identity(kSize, kSize));\n            \n            // Write inverted diagonal tile\n            InOutDataset-&gt;writeDatasetBlock(Rcpp::wrap(Lkk), {kStart, kStart}, {kSize, kSize}, stride, block, false);\n            \n            // 2. Update column tiles below diagonal: L(i,k) = -L(i,i) * L_old(i,k) * L(k,k)\n            #pragma omp parallel for num_threads(get_number_threads(threads, R_NilValue)) schedule(dynamic)\n            for (int i = k + 1; i &lt; numTiles; i++) {\n                \n                hsize_t iStart = i * tileSize;\n                hsize_t iSize = std::min(static_cast&lt;hsize_t&gt;(tileSize), static_cast&lt;hsize_t&gt;(dimensionSize - iStart));\n                \n                // Load L(i,k) - old value\n                Eigen::MatrixXd Lik_old(iSize, kSize);\n                std::vector&lt;double&gt; vLik(iSize * kSize);\n                InOutDataset-&gt;readDatasetBlock({iStart, kStart}, {iSize, kSize}, stride, block, vLik.data());\n                Lik_old = Eigen::Map&lt;Eigen::MatrixXd&gt;(vLik.data(), iSize, kSize);\n                \n                // Load L(i,i) - already inverted\n                Eigen::MatrixXd Lii(iSize, iSize);\n                std::vector&lt;double&gt; vLii(iSize * iSize);\n                InOutDataset-&gt;readDatasetBlock({iStart, iStart}, {iSize, iSize}, stride, block, vLii.data());\n                Lii = Eigen::Map&lt;Eigen::MatrixXd&gt;(vLii.data(), iSize, iSize);\n                \n                // Update: L(i,k) = -L(i,i) * L_old(i,k) * L(k,k)\n                Eigen::MatrixXd Lik_new = -Lii * Lik_old * Lkk;\n                \n                // Write updated column tile\n                InOutDataset-&gt;writeDatasetBlock(Rcpp::wrap(Lik_new), {iStart, kStart}, {iSize, kSize}, stride, block, false);\n            }\n        }\n        \n    } catch(std::exception& ex) {\n        Rcpp::Rcerr &lt;&lt; \"c++ exception Inverse_of_Cholesky_decomposition_outofcore_hdf5: \" &lt;&lt; ex.what();\n        return void();\n    }\n    \n    return void();\n}",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.html#usage-example",
    "title": "Inverse_of_Cholesky_decomposition_outofcore_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Inverse_of_Cholesky_decomposition_outofcore_hdf5(...);",
    "crumbs": [
      "Functions",
      "Inverse_of_Cholesky_decomposition_outofcore_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html",
    "href": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html",
    "title": "RcppApplyFunctionHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppApplyFunctionHdf5(std::string filename, std::string group, Rcpp::StringVector datasets, std::string outgroup, std::string func, Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; b_group=R_NilValue, Rcpp::Nullable&lt; Rcpp::StringVector &gt; b_datasets=R_NilValue, Rcpp::Nullable&lt; bool &gt; overwrite=false, Rcpp::Nullable&lt; bool &gt; transp_dataset=false, Rcpp::Nullable&lt; bool &gt; transp_bdataset=false, Rcpp::Nullable&lt; bool &gt; fullMatrix=false, Rcpp::Nullable&lt; bool &gt; byrows=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppApplyFunctionHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#signature",
    "title": "RcppApplyFunctionHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppApplyFunctionHdf5(std::string filename, std::string group, Rcpp::StringVector datasets, std::string outgroup, std::string func, Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; b_group=R_NilValue, Rcpp::Nullable&lt; Rcpp::StringVector &gt; b_datasets=R_NilValue, Rcpp::Nullable&lt; bool &gt; overwrite=false, Rcpp::Nullable&lt; bool &gt; transp_dataset=false, Rcpp::Nullable&lt; bool &gt; transp_bdataset=false, Rcpp::Nullable&lt; bool &gt; fullMatrix=false, Rcpp::Nullable&lt; bool &gt; byrows=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppApplyFunctionHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#description",
    "href": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#description",
    "title": "RcppApplyFunctionHdf5",
    "section": "2 Description",
    "text": "2 Description\nApplies mathematical functions to HDF5 datasets.",
    "crumbs": [
      "Functions",
      "RcppApplyFunctionHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#parameters",
    "title": "RcppApplyFunctionHdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfilename (std::string): Path to the HDF5 file\ngroup (std::string): Group containing input datasets\ndatasets (Rcpp::StringVector): Vector of input dataset names\noutgroup (std::string): Output group path\nfunc (std::string): Operation to apply:“QR”: QR decomposition”CrossProd”: Cross product”tCrossProd”: Transposed cross product”invChol”: Inverse using Cholesky”blockmult”: Block matrix multiplication”solve”: Solve matrix equation”normalize”: Normalize data”sdmean”: Compute SD and mean”descChol”: Cholesky decomposition\nb_group (Rcpp::Nullable&lt; Rcpp::CharacterVector &gt;): Optional group for second dataset in two-dataset operations\nb_datasets (Rcpp::Nullable&lt; Rcpp::StringVector &gt;): Optional second dataset names for two-dataset operations\noverwrite (Rcpp::Nullable&lt; bool &gt;): Optional flag to overwrite existing datasets\ntransp_dataset (Rcpp::Nullable&lt; bool &gt;): Optional flag to transpose first dataset\ntransp_bdataset (Rcpp::Nullable&lt; bool &gt;): Optional flag to transpose second dataset\nfullMatrix (Rcpp::Nullable&lt; bool &gt;): Optional flag for full matrix output\nbyrows (Rcpp::Nullable&lt; bool &gt;): Optional flag for row-wise operations\nthreads (Rcpp::Nullable&lt; int &gt;): Optional number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "RcppApplyFunctionHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#details",
    "href": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#details",
    "title": "RcppApplyFunctionHdf5",
    "section": "4 Details",
    "text": "4 Details\nThis function serves as a high-level interface for applying various mathematical operations to HDF5 datasets. It supports both single-dataset operations and operations between multiple datasets.",
    "crumbs": [
      "Functions",
      "RcppApplyFunctionHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#call-graph",
    "title": "RcppApplyFunctionHdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppApplyFunctionHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#source-code",
    "title": "RcppApplyFunctionHdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5ApplytoDatasets.hpp • Lines 98-386\ninline void RcppApplyFunctionHdf5( std::string filename, \n                                std::string group, \n                                Rcpp::StringVector datasets, \n                                std::string outgroup, \n                                std::string func, \n                                Rcpp::Nullable&lt;Rcpp::CharacterVector&gt; b_group = R_NilValue, \n                                Rcpp::Nullable&lt;Rcpp::StringVector&gt; b_datasets = R_NilValue,\n                                Rcpp::Nullable&lt;bool&gt; overwrite = false,\n                                Rcpp::Nullable&lt;bool&gt; transp_dataset = false,\n                                Rcpp::Nullable&lt;bool&gt; transp_bdataset = false,\n                                Rcpp::Nullable&lt;bool&gt; fullMatrix = false,\n                                Rcpp::Nullable&lt;bool&gt; byrows = false,\n                                Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n    {\n        \n        Rcpp::StringVector str_bdatasets;\n        std::string str_bgroup, outputdataset;\n        bool btransdataA, btransdataB, bfullMatrix, bbyrows;\n        long dElementsBlock = MAXELEMSINBLOCK; \n        Rcpp::NumericVector oper = {0, 1, 2, 3, 4, 11, 22, 5, 6, 7, 8};\n        oper.names() = Rcpp::CharacterVector({\"QR\", \"CrossProd\", \"tCrossProd\",\n                   \"invChol\", \"blockmult\", \"CrossProd_double\", \"tCrossProd_double\",\n                   \"solve\", \"normalize\", \"sdmean\", \"descChol\"});\n        \n        BigDataStatMeth::hdf5Dataset* dsA = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsB = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsQ = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsR = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsOut = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsmean = nullptr;\n        BigDataStatMeth::hdf5Dataset* dssd = nullptr;\n        \n        try\n        {\n            \n            bool bforce;\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1},\n                block = {1, 1};\n            \n            if(overwrite.isNull()) { bforce = false; }\n            else {   bforce = Rcpp::as&lt;bool&gt;(overwrite); }\n            \n            if(transp_dataset.isNull()) { btransdataA = false; }\n            else {   btransdataA = Rcpp::as&lt;bool&gt;(transp_dataset); }\n\n            if(transp_bdataset.isNull()) { btransdataB = false; }\n            else {   btransdataB = Rcpp::as&lt;bool&gt;(transp_bdataset); }\n\n            if(fullMatrix.isNull()) { bfullMatrix = false; }\n            else {   bfullMatrix = Rcpp::as&lt;bool&gt;(fullMatrix); }\n\n            if(byrows.isNull()) { bbyrows = false; }\n            else {   bbyrows = Rcpp::as&lt;bool&gt;(byrows); }\n            \n            // if( b_group == \"\") { str_bgroup = group; } \n            // else {   str_bgroup = b_group; }\n            \n            if( b_datasets.isNotNull() &&  ( oper(oper.findName(func)) == 1 ||\n                oper(oper.findName(func)) == 2 || oper(oper.findName(func)) == 4 ||\n                oper(oper.findName(func)) == 5 || oper(oper.findName(func)) == 6) ) {\n\n                str_bdatasets = Rcpp::as&lt;Rcpp::StringVector&gt;(b_datasets);\n\n                if( oper.findName( func ) == 1) {\n                    func = \"CrossProd_double\";\n                } else if( oper.findName( func ) == 2) {\n                    func = \"tCrossProd_double\";\n                }\n            }\n            \n            \n            if( b_group.isNull()) {\n                str_bgroup = group;\n            } else {\n                str_bgroup = Rcpp::as&lt;std::string&gt;(b_group);\n            }\n            \n            // Seek all datasets to perform calculus\n            for( int i=0; i &lt; datasets.size(); i++ ) \n            {\n                \n                dsA = new BigDataStatMeth::hdf5Dataset(filename, group, Rcpp::as&lt;std::string&gt;(datasets(i)), false);\n                dsA-&gt;openDataset();\n                \n                if( dsA-&gt;getDatasetptr() != nullptr ) { \n                    \n                    if( oper(oper.findName( func )) == 0) {\n                        // ==&gt; QR Decomposition \n                        \n                        BigDataStatMeth::hdf5Dataset* dsQ = new BigDataStatMeth::hdf5Dataset(filename, outgroup, Rcpp::as&lt;std::string&gt;(datasets(i)) + \".Q\", bforce);\n                        BigDataStatMeth::hdf5Dataset* dsR = new BigDataStatMeth::hdf5Dataset(filename, outgroup, Rcpp::as&lt;std::string&gt;(datasets(i)) + \".R\", bforce);\n                        \n                        RcppQRHdf5(dsA, dsQ, dsR, true, R_NilValue, threads);\n                        \n                        delete dsA; dsA = nullptr;\n                        delete dsQ; dsQ = nullptr;\n                        delete dsR; dsR = nullptr;\n                        \n                    } else if( oper(oper.findName( func )) == 1 || oper(oper.findName( func )) == 2) {\n                        // ==&gt; CrossProd and transposed CrossProd\n                        \n                        hsize_t* dims_out = dsA-&gt;dim();\n                        \n                        std::vector&lt;double&gt; vdA( dims_out[0] * dims_out[1] ); \n                        dsA-&gt;readDatasetBlock( {0, 0}, {dims_out[0], dims_out[1]}, stride, block, vdA.data() );\n                        Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; original (vdA.data(), dims_out[1], dims_out[0] );\n                        \n                        delete dsA; dsA = nullptr;\n                        \n                        Eigen::MatrixXd results;\n                        dsOut = new BigDataStatMeth::hdf5Dataset(filename, outgroup, Rcpp::as&lt;std::string&gt;(datasets(i)) , bforce);\n                        \n                        if(  oper(oper.findName( func )) == 1 ) {\n                            results = BigDataStatMeth::bdcrossproduct(original);\n                        } else {\n                            results = BigDataStatMeth::bdtcrossproduct(original);\n                        }\n                        \n                        dsOut-&gt;createDataset(results.rows(), results.cols(), \"numeric\"); \n                        dsOut-&gt;openDataset(); \n                        if( dsOut-&gt;getDatasetptr() != nullptr )  {\n                            dsOut-&gt;writeDataset(Rcpp::wrap(results));    \n                        }\n                        delete dsOut; dsOut = nullptr;\n                        \n                    } else if( oper(oper.findName( func )) == 3 || oper(oper.findName( func )) == 8) {\n                        // ==&gt; Inverse Cholesky and Cholesky decomposition\n                        \n                        int nrows = dsA-&gt;nrows();\n                        int ncols = dsA-&gt;ncols();\n                        \n                        if(nrows == ncols) {\n                            \n                            BigDataStatMeth::hdf5DatasetInternal* dsOut = new BigDataStatMeth::hdf5DatasetInternal(filename, outgroup, Rcpp::as&lt;std::string&gt;(datasets(i)) , bforce);\n                            dsOut-&gt;createDataset(nrows, ncols, \"real\");\n                            \n                            if(oper(oper.findName( func )) == 3 ) {\n                                BigDataStatMeth::Rcpp_InvCholesky_hdf5( dsA, dsOut, bfullMatrix, dElementsBlock, threads);    \n                            } else {\n                                int res [[maybe_unused]] = BigDataStatMeth::Cholesky_decomposition_hdf5( dsA, dsOut,  nrows, ncols, dElementsBlock, threads);\n                            }\n                            \n                            delete dsA; dsA = nullptr;\n                            delete dsOut; dsOut = nullptr;\n                            \n                        } else {\n                            delete dsA; dsA = nullptr;\n                            Rcpp::Rcout&lt;&lt;\"\\n Can't get inverse matrix for \"&lt;&lt;Rcpp::as&lt;std::string&gt;(datasets(i))&lt;&lt;\" dataset using Cholesky decomposition - not an square matrix\\n\";\n                            return void();\n                        }\n                        \n                        \n                    } else if( oper(oper.findName( func )) == 4 ||  oper(oper.findName( func )) == 11 ||  oper(oper.findName( func )) == 22) {\n                        // ==&gt; blockmult, CrossProd Double, tCrossProd Double\"\n                        \n                        dsB = new BigDataStatMeth::hdf5Dataset(filename, str_bgroup, Rcpp::as&lt;std::string&gt;(str_bdatasets(i)), false);\n                        dsB-&gt;openDataset();\n                        \n                        if( dsB-&gt;getDatasetptr() != nullptr )  {\n                            // Real data set dimension\n                            hsize_t* dims_outB = dsB-&gt;dim();\n                            hsize_t* dims_out = dsA-&gt;dim();\n                            \n                            std::vector&lt;double&gt; vdA( dims_out[0] * dims_out[1] ); \n                            dsA-&gt;readDatasetBlock( {0, 0}, {dims_out[0], dims_out[1]}, stride, block, vdA.data() );\n                            \n                            std::vector&lt;double&gt; vdB( dims_outB[0] * dims_outB[1] ); \n                            dsB-&gt;readDatasetBlock( {0, 0}, {dims_outB[0], dims_outB[1]}, stride, block, vdB.data() );\n                            \n                            Eigen::MatrixXd original;\n                            Eigen::MatrixXd originalB;\n                            \n                            if( oper(oper.findName( func )) == 4 ) {\n                                original = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; (vdA.data(), dims_out[1], dims_out[0] );\n                                originalB = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; (vdB.data(), dims_outB[1], dims_outB[0] );\n                                outputdataset = Rcpp::as&lt;std::string&gt;(datasets(i)) + \"_\" + str_bdatasets(i);\n                            } else if  (oper(oper.findName( func )) == 11) {\n                                original = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vdA.data(), dims_out[0], dims_out[1] );\n                                originalB = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; (vdB.data(), dims_outB[1], dims_outB[0] );\n                                outputdataset = \"Cross_\" + datasets(i) + \"_\" + str_bdatasets(i);\n                            } else if ( oper(oper.findName( func )) == 22) {\n                                original = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; (vdA.data(), dims_out[1], dims_out[0] );\n                                originalB = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vdB.data(), dims_outB[0], dims_outB[1] );\n                                outputdataset =  \"tCross_\" + datasets(i) + \"_\" + str_bdatasets(i);\n                            }\n                            \n                            Eigen::MatrixXd  results = BigDataStatMeth::Rcpp_block_matrix_mul_parallel(original, originalB, btransdataA, btransdataB, R_NilValue, R_NilValue);\n                            \n                            if( results != Eigen::MatrixXd::Zero(original.rows(),originalB.cols()) ) {\n                                \n                                dsOut = new BigDataStatMeth::hdf5Dataset(filename, outgroup, outputdataset , bforce);\n                                dsOut-&gt;createDataset(results.rows(), results.cols(), \"real\");\n                                dsOut-&gt;writeDataset(Rcpp::wrap(results));\n                                \n                                delete dsOut; dsOut = nullptr;\n                                \n                            } else {\n                                Rcpp::Rcout&lt;&lt;\"Multiplication: \"&lt;&lt; group&lt;&lt;\"/\"&lt;&lt; Rcpp::as&lt;std::string&gt;(datasets(i))&lt;&lt; \" x \"&lt;&lt; str_bgroup&lt;&lt;\"/\"&lt;&lt; Rcpp::as&lt;std::string&gt;(str_bdatasets(i)) &lt;&lt;\" can not be computed \\n\";\n                            }\n                            \n                            delete dsA; dsA = nullptr;\n                            delete dsB; dsB = nullptr;\n                            \n                        }\n                        \n                        \n                    } else if( oper(oper.findName( func )) == 5) {\n                        // ==&gt; Solve matrix equation Ax = B\n                        \n                        dsB = new BigDataStatMeth::hdf5Dataset(filename, str_bgroup, Rcpp::as&lt;std::string&gt;(str_bdatasets(i)), false);\n                        dsB-&gt;openDataset();\n                        \n                        dsOut = new BigDataStatMeth::hdf5DatasetInternal(filename, outgroup, Rcpp::as&lt;std::string&gt;(datasets(i)) + \"_eq_\" + Rcpp::as&lt;std::string&gt;(str_bdatasets(i)) , bforce);\n                        dsOut-&gt;createDataset( dsB-&gt;nrows(), dsB-&gt;ncols(), \"real\" );\n                        \n                        if( dsB-&gt;getDatasetptr() != nullptr )  {\n                            RcppSolveHdf5(dsA, dsB, dsOut );\n                        }\n                        \n                        delete dsOut; dsOut = nullptr;\n                        delete dsB; dsB = nullptr;\n                        delete dsA; dsA = nullptr;\n                        \n                    } else if( oper(oper.findName( func )) == 7) {\n                        // ==&gt; Compute sd and mean by rows or columns\n                        \n                        Eigen::MatrixXd datanormal;\n                        hsize_t* dims_out = dsA-&gt;dim();\n                        \n                        if( bbyrows == false) {\n                            datanormal = Eigen::MatrixXd::Zero( 2, (int)dims_out[0]);\n                            get_HDF5_mean_sd_by_column( dsA, datanormal, true, true, R_NilValue );\n                            \n                        } else {\n                            datanormal = Eigen::MatrixXd::Zero( 2, (int)dims_out[1]);\n                            get_HDF5_mean_sd_by_row( dsA, datanormal, true, true, R_NilValue );\n                            \n                        }\n                        \n                        BigDataStatMeth::hdf5Dataset* dsmean = new BigDataStatMeth::hdf5Dataset(filename, outgroup, \"mean.\" + datasets(i) , bforce);\n                        dsmean-&gt;createDataset(1, datanormal.cols(), \"real\");\n                        dsmean-&gt;writeDataset(Rcpp::wrap(datanormal.row(0)));\n                        \n                        BigDataStatMeth::hdf5Dataset* dssd = new BigDataStatMeth::hdf5Dataset(filename, outgroup, \"sd.\" + datasets(i) , bforce);\n                        dssd-&gt;createDataset(1, datanormal.cols(), \"real\");\n                        dssd-&gt;writeDataset(Rcpp::wrap(datanormal.row(1)));\n                        \n                        delete dssd; dssd = nullptr;\n                        delete dsmean; dsmean = nullptr;\n                        delete dsA; dsA = nullptr;\n                        \n                    } else {\n                        delete dsA; dsA = nullptr;\n                        Rcpp::Rcout&lt;&lt; \"Function does not exists, please use one of the following : \\\"QR\\\", \\\"CrossProd\\\",\"&lt;&lt;\n                            \" \\\"tCrossProd\\\", \\\"invChol\\\", \\\"blockmult\\\", \\\"CrossProd_double\\\", \\\"tCrossProd_double\\\",\"&lt;&lt;\n                                \" \\\"solve\\\", \\\"normalize\\\", \\\"sdmean\\\", \\\"descChol\\\" \";\n                        return void();\n                    }    \n                    \n                } else {\n                    checkClose_file(dsA);        \n                    Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppApplyFunctionHdf5 error with \"&lt;&lt;Rcpp::as&lt;std::string&gt;(datasets(i))&lt;&lt;\" dataset\\n\";\n                    return void();\n                }\n                \n            }\n            \n        }  catch( H5::FileIException& error ) { // catch failure caused by the H5File operations\n            checkClose_file(dsA, dsB, dsQ, dsR, dsOut, dsmean, dssd);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception RcppApplyFunctionHdf5 (File IException)\\n\";\n            return void();\n        } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n            checkClose_file(dsA, dsB, dsQ, dsR, dsOut, dsmean, dssd);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception RcppApplyFunctionHdf5 (DataSet IException)\\n\";\n            return void();\n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsB, dsQ, dsR, dsOut, dsmean, dssd);\n            Rcpp::Rcerr &lt;&lt; \"c++ exception blockmult_hdf5: \" &lt;&lt; ex.what();\n            return void();\n        } catch (...) {\n            checkClose_file(dsA, dsB, dsQ, dsR, dsOut, dsmean, dssd);\n            Rcpp::Rcerr&lt;&lt;\"C++ exception RcppApplyFunctionHdf5 (unknown reason)\";\n            return void();\n        }\n        \n        // Rcpp::Rcout&lt;&lt; func &lt;&lt;\" function has been computed in all blocks\\n\";  \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "RcppApplyFunctionHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppApplyFunctionHdf5.html#usage-example",
    "title": "RcppApplyFunctionHdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppApplyFunctionHdf5(...);",
    "crumbs": [
      "Functions",
      "RcppApplyFunctionHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html",
    "href": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html",
    "title": "RcppGetPCAIndividualsHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppGetPCAIndividualsHdf5(std::string strPCAgroup, BigDataStatMeth::hdf5Dataset *dsX, BigDataStatMeth::hdf5Dataset *dsd, BigDataStatMeth::hdf5Dataset *dsu, bool overwrite)",
    "crumbs": [
      "Functions",
      "RcppGetPCAIndividualsHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#signature",
    "title": "RcppGetPCAIndividualsHdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppGetPCAIndividualsHdf5(std::string strPCAgroup, BigDataStatMeth::hdf5Dataset *dsX, BigDataStatMeth::hdf5Dataset *dsd, BigDataStatMeth::hdf5Dataset *dsu, bool overwrite)",
    "crumbs": [
      "Functions",
      "RcppGetPCAIndividualsHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#description",
    "href": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#description",
    "title": "RcppGetPCAIndividualsHdf5",
    "section": "2 Description",
    "text": "2 Description\nCalculate PCA individuals statistics.",
    "crumbs": [
      "Functions",
      "RcppGetPCAIndividualsHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#parameters",
    "title": "RcppGetPCAIndividualsHdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nstrPCAgroup (std::string): HDF5 group name for PCA results\ndsX (BigDataStatMeth::hdf5Dataset *): Input data matrix dataset\ndsd (BigDataStatMeth::hdf5Dataset *): Singular values dataset\ndsu (BigDataStatMeth::hdf5Dataset *): Left singular vectors dataset\noverwrite (bool): Whether to overwrite existing results",
    "crumbs": [
      "Functions",
      "RcppGetPCAIndividualsHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#details",
    "href": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#details",
    "title": "RcppGetPCAIndividualsHdf5",
    "section": "4 Details",
    "text": "4 Details\nComputes and stores various PCA statistics for individuals including:DistancesComponent coordinatesIndividual coordinatesCos² quality metricsContributions",
    "crumbs": [
      "Functions",
      "RcppGetPCAIndividualsHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#call-graph",
    "title": "RcppGetPCAIndividualsHdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppGetPCAIndividualsHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#source-code",
    "title": "RcppGetPCAIndividualsHdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixPCA.hpp • Lines 188-321\ninline void RcppGetPCAIndividualsHdf5( std::string strPCAgroup, \n                                    BigDataStatMeth::hdf5Dataset* dsX,\n                                    BigDataStatMeth::hdf5Dataset* dsd, \n                                    BigDataStatMeth::hdf5Dataset* dsu, \n                                    bool overwrite )\n    {\n        \n        \n        BigDataStatMeth::hdf5Dataset* dsdist2 = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsComp = nullptr;\n        BigDataStatMeth::hdf5Dataset* dscoord = nullptr;\n        BigDataStatMeth::hdf5Dataset* dscos2 = nullptr;\n        BigDataStatMeth::hdf5Dataset* dscontrib = nullptr;\n        \n        \n        try\n        {\n            H5::Exception::dontPrint();\n            \n            // int ielements = 0;\n            std::vector&lt;hsize_t&gt; stride = {1, 1},\n                block = {1, 1},\n                offset = {0, 0},\n                count_x = {dsX-&gt;nrows(), dsX-&gt;ncols()},\n                count_d = {dsd-&gt;nrows(), dsd-&gt;ncols()},\n                count_u = {dsu-&gt;nrows(), dsu-&gt;ncols()};\n            \n            Eigen::VectorXd dist2;\n            Eigen::MatrixXd adjust;\n            double weights = 1/(double)dsX-&gt;ncols();\n            {\n                \n                std::vector&lt;double&gt; vdX( count_x[0] * count_x[1] );\n                dsX-&gt;readDatasetBlock( {0,0}, count_x, stride, block, vdX.data() );\n                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; X (vdX.data(), count_x[0], count_x[1]);\n    \n                X = RcppNormalizeRowwise(X, true, false);\n                adjust = Eigen::MatrixXd::Constant(1, X.cols(), weights);\n                \n                Eigen::RowVectorXd ecart =  adjust / adjust.sum() ;\n                ecart = ecart * (X.unaryExpr([](double d) {return std::pow(d, 2);})).transpose();\n                ecart = ecart.unaryExpr([](double d) {return std::sqrt(d);});\n                \n                X = X.array().colwise() / ecart.transpose().array();\n    \n                dist2 =  (X.unaryExpr([](double d) {return std::pow(d, 2);})).colwise().sum();\n                Eigen::VectorXd dist = dist2.array().sqrt();\n                \n                dsdist2 = new BigDataStatMeth::hdf5Dataset(dsd-&gt;getFileName(), strPCAgroup, \"ind.dist\" ,overwrite );\n                dsdist2-&gt;createDataset( dist2.rows(), dist2.cols(), \"real\");\n                dsdist2-&gt;writeDataset( dist.data() );\n                delete dsdist2; dsdist2 = nullptr;\n            }\n    \n            // Load d\n            std::vector&lt;double&gt; vdd( count_d[0] * count_d[1] );\n            dsd-&gt;readDatasetBlock( offset, count_d, stride, block, vdd.data() );\n            Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; d (vdd.data(), count_d[0], count_d[1]); \n            \n            Eigen::MatrixXd var_coord;\n            {\n                // Load u\n                std::vector&lt;double&gt; vdu( count_u[0] * count_u[1] );\n                dsu-&gt;readDatasetBlock( {0, 0}, count_u, stride, block, vdu.data() );\n                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; u (vdu.data(), count_u[0], count_u[1]);\n                \n                u = u * sqrt(1/weights); \n            \n                // Components\n                dsComp = new BigDataStatMeth::hdf5Dataset(dsd-&gt;getFileName(), strPCAgroup, \"components\" ,overwrite );\n                dsComp-&gt;createDataset( u.cols(), u.rows(), \"real\");\n                dsComp-&gt;writeDataset( u.data() );\n                delete dsComp; dsComp = nullptr;\n                \n                var_coord = Rcpp_matrixVectorMultiplication_byRow(u, d);\n            }\n            \n            // Coord inds\n            dscoord = new BigDataStatMeth::hdf5Dataset(dsd-&gt;getFileName(), strPCAgroup, \"ind.coord\" ,overwrite );\n            dscoord-&gt;createDataset( var_coord.cols(), var_coord.rows(), \"real\");\n            dscoord-&gt;writeDataset( Rcpp::wrap(var_coord.transpose()));\n            delete dscoord; dscoord = nullptr;\n            \n            // Cos2 inds\n            Eigen::MatrixXd coord2 = var_coord.unaryExpr([](double d) {return std::pow(d, 2);});\n            \n            {\n                Eigen::MatrixXd ind_cos2 = coord2.array().rowwise() / dist2.transpose().array();\n                \n                dscos2 = new BigDataStatMeth::hdf5Dataset(dsd-&gt;getFileName(), strPCAgroup, \"ind.cos2\" ,overwrite );\n                dscos2-&gt;createDataset( ind_cos2.cols(), ind_cos2.rows(), \"real\");\n                dscos2-&gt;writeDataset( Rcpp::wrap(ind_cos2.transpose()) );\n                delete dscos2; dscos2 = nullptr;\n            }\n            \n            Eigen::MatrixXd ind_contrib = coord2.array().rowwise() * adjust.row(0).array();\n            \n            d = d.unaryExpr([](double xd) {return std::pow(xd, 2);});\n            \n            ind_contrib = ind_contrib.array().colwise() / d.col(0).array();\n            ind_contrib = ind_contrib.unaryExpr([](double d) {return d*100;});\n            \n            dscontrib = new BigDataStatMeth::hdf5Dataset(dsd-&gt;getFileName(), strPCAgroup, \"ind.contrib\" ,overwrite );\n            dscontrib-&gt;createDataset( ind_contrib.cols(), ind_contrib.rows(), \"real\");\n            dscontrib-&gt;writeDataset( Rcpp::wrap(ind_contrib.transpose()) );\n            delete dscontrib; dscontrib = nullptr;\n                \n            // Components\n            // createHardLink(dsu-&gt;getFileptr(), dsu-&gt;getGroupName() + \"/\" + dsu-&gt;getDatasetName(), strPCAgroup + \"/components\");\n            \n        } catch( H5::FileIException& error ) { // catch failure caused by the H5File operations\n            checkClose_file( dsX, dsd, dsu, dsdist2, dsComp, dscoord, dscos2, dscontrib);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppGetPCAIndividualsHdf5 (File IException)\\n\";\n            return void();\n        } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n            checkClose_file( dsX, dsd, dsu, dsdist2, dsComp, dscoord, dscos2, dscontrib);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppGetPCAIndividualsHdf5 (DataSet IException)\\n\";\n            return void();\n        } catch( H5::DataSpaceIException& error ) { // catch failure caused by the DataSpace operations\n            checkClose_file( dsX, dsd, dsu, dsdist2, dsComp, dscoord, dscos2, dscontrib);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppGetPCAIndividualsHdf5 (DataSpace IException)\\n\";\n            return void();\n        } catch(std::exception &ex) {\n            checkClose_file( dsX, dsd, dsu, dsdist2, dsComp, dscoord, dscos2, dscontrib);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppGetPCAIndividualsHdf5 \\n\"&lt;&lt; ex.what();\n            return void();\n        } catch (...) {\n            checkClose_file( dsX, dsd, dsu, dsdist2, dsComp, dscoord, dscos2, dscontrib);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception RcppGetPCAIndividualsHdf5 (unknown reason)\";\n            return void();\n        }\n        \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "RcppGetPCAIndividualsHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.html#usage-example",
    "title": "RcppGetPCAIndividualsHdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppGetPCAIndividualsHdf5(...);",
    "crumbs": [
      "Functions",
      "RcppGetPCAIndividualsHdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeColwise.html",
    "href": "api-reference/cpp/functions/RcppNormalizeColwise.html",
    "title": "RcppNormalizeColwise",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::RcppNormalizeColwise(M X, bool bc, bool bs)",
    "crumbs": [
      "Functions",
      "RcppNormalizeColwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeColwise.html#signature",
    "href": "api-reference/cpp/functions/RcppNormalizeColwise.html#signature",
    "title": "RcppNormalizeColwise",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::RcppNormalizeColwise(M X, bool bc, bool bs)",
    "crumbs": [
      "Functions",
      "RcppNormalizeColwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeColwise.html#parameters",
    "href": "api-reference/cpp/functions/RcppNormalizeColwise.html#parameters",
    "title": "RcppNormalizeColwise",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nX (M)\nbc (bool)\nbs (bool)",
    "crumbs": [
      "Functions",
      "RcppNormalizeColwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeColwise.html#returns",
    "href": "api-reference/cpp/functions/RcppNormalizeColwise.html#returns",
    "title": "RcppNormalizeColwise",
    "section": "3 Returns",
    "text": "3 Returns\nType: typename M",
    "crumbs": [
      "Functions",
      "RcppNormalizeColwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeColwise.html#caller-graph",
    "href": "api-reference/cpp/functions/RcppNormalizeColwise.html#caller-graph",
    "title": "RcppNormalizeColwise",
    "section": "4 Caller Graph",
    "text": "4 Caller Graph",
    "crumbs": [
      "Functions",
      "RcppNormalizeColwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeColwise.html#source-code",
    "href": "api-reference/cpp/functions/RcppNormalizeColwise.html#source-code",
    "title": "RcppNormalizeColwise",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixNormalization.hpp • Lines 140-169\ninline Eigen::MatrixXd RcppNormalizeColwise ( M  X, bool bc, bool bs )\n    {\n        \n        static_assert(std::is_same&lt;M, Eigen::MatrixXd &gt;::value || \n                      std::is_same&lt;M, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n                      std::is_same&lt;M, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value,\n                      \"Error - type not allowed\");\n        \n        Eigen::MatrixXd rX = X;\n        \n        if( bc==true && bs==true )  {\n            \n            Eigen::RowVectorXd mean = X.colwise().mean();\n            Eigen::RowVectorXd std = ((X.rowwise() - mean).array().square().colwise().sum() / (X.rows() - 1)).sqrt();\n            rX = (X.rowwise() - mean).array().rowwise() / std.array();\n            \n        }   else if (bc == true  && bs==false)   {\n            \n            Eigen::RowVectorXd mean = X.colwise().mean();\n            rX = (X.rowwise() - mean);\n            \n        }  else if ( bc == false && bs == true)   {\n            \n            Eigen::RowVectorXd mean = X.colwise().mean();\n            Eigen::RowVectorXd std = (X.array().square().colwise().sum() / (X.rows() - 1)).sqrt();\n            rX = X.array().rowwise() / std.array();\n        }\n        \n        return(rX);\n    }",
    "crumbs": [
      "Functions",
      "RcppNormalizeColwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeColwise.html#usage-example",
    "href": "api-reference/cpp/functions/RcppNormalizeColwise.html#usage-example",
    "title": "RcppNormalizeColwise",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppNormalizeColwise(...);",
    "crumbs": [
      "Functions",
      "RcppNormalizeColwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeRowwise.html",
    "href": "api-reference/cpp/functions/RcppNormalizeRowwise.html",
    "title": "RcppNormalizeRowwise",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::RcppNormalizeRowwise(M X, bool bc, bool bs)",
    "crumbs": [
      "Functions",
      "RcppNormalizeRowwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeRowwise.html#signature",
    "href": "api-reference/cpp/functions/RcppNormalizeRowwise.html#signature",
    "title": "RcppNormalizeRowwise",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::RcppNormalizeRowwise(M X, bool bc, bool bs)",
    "crumbs": [
      "Functions",
      "RcppNormalizeRowwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeRowwise.html#parameters",
    "href": "api-reference/cpp/functions/RcppNormalizeRowwise.html#parameters",
    "title": "RcppNormalizeRowwise",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nX (M)\nbc (bool)\nbs (bool)",
    "crumbs": [
      "Functions",
      "RcppNormalizeRowwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeRowwise.html#returns",
    "href": "api-reference/cpp/functions/RcppNormalizeRowwise.html#returns",
    "title": "RcppNormalizeRowwise",
    "section": "3 Returns",
    "text": "3 Returns\nType: typename M",
    "crumbs": [
      "Functions",
      "RcppNormalizeRowwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeRowwise.html#caller-graph",
    "href": "api-reference/cpp/functions/RcppNormalizeRowwise.html#caller-graph",
    "title": "RcppNormalizeRowwise",
    "section": "4 Caller Graph",
    "text": "4 Caller Graph",
    "crumbs": [
      "Functions",
      "RcppNormalizeRowwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeRowwise.html#source-code",
    "href": "api-reference/cpp/functions/RcppNormalizeRowwise.html#source-code",
    "title": "RcppNormalizeRowwise",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixNormalization.hpp • Lines 173-199\ninline Eigen::MatrixXd RcppNormalizeRowwise ( M  X, bool bc, bool bs )\n    {\n        \n        static_assert(std::is_same&lt;M, Eigen::MatrixXd &gt;::value || \n                      std::is_same&lt;M, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n                      std::is_same&lt;M, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value,\n                      \"Error - type not allowed\");\n        \n        Eigen::MatrixXd rX = X;\n        \n        Eigen::VectorXd mean = X.rowwise().mean();\n        if( bc==true && bs==true )  {\n\n            Eigen::VectorXd std = ((X.colwise() - mean).array().square().rowwise().sum() / (X.cols() - 1)).sqrt();\n            rX = (X.colwise() - mean).array().colwise() / std.array();\n            \n        }   else if (bc == true  && bs==false)   {\n            rX = (X.colwise() - mean);\n            \n        }  else if ( bc == false && bs == true)   {\n            \n            Eigen::VectorXd std = ((X.colwise() - mean).array().square().rowwise().sum() / (X.cols() - 1)).sqrt();\n            rX = X.array().colwise() / std.array();\n        }\n        \n        return(rX);\n    }",
    "crumbs": [
      "Functions",
      "RcppNormalizeRowwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalizeRowwise.html#usage-example",
    "href": "api-reference/cpp/functions/RcppNormalizeRowwise.html#usage-example",
    "title": "RcppNormalizeRowwise",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppNormalizeRowwise(...);",
    "crumbs": [
      "Functions",
      "RcppNormalizeRowwise"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html",
    "href": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html",
    "title": "RcppNormalize_Data_R_hdf5",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::RcppNormalize_Data_R_hdf5(Eigen::MatrixXd X, bool bc, bool bs, bool btransp, Eigen::MatrixXd normdata)",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data_R_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html#signature",
    "title": "RcppNormalize_Data_R_hdf5",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::RcppNormalize_Data_R_hdf5(Eigen::MatrixXd X, bool bc, bool bs, bool btransp, Eigen::MatrixXd normdata)",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data_R_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html#parameters",
    "title": "RcppNormalize_Data_R_hdf5",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nX (Eigen::MatrixXd)\nbc (bool)\nbs (bool)\nbtransp (bool)\nnormdata (Eigen::MatrixXd)",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data_R_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html#returns",
    "href": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html#returns",
    "title": "RcppNormalize_Data_R_hdf5",
    "section": "3 Returns",
    "text": "3 Returns\nType: Eigen::MatrixXd",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data_R_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html#caller-graph",
    "href": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html#caller-graph",
    "title": "RcppNormalize_Data_R_hdf5",
    "section": "4 Caller Graph",
    "text": "4 Caller Graph",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data_R_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html#source-code",
    "title": "RcppNormalize_Data_R_hdf5",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixNormalization.hpp • Lines 251-275\ninline Eigen::MatrixXd RcppNormalize_Data_R_hdf5( Eigen::MatrixXd  X, bool bc, bool bs,\n                                               bool btransp, Eigen::MatrixXd normdata)\n    {\n        Eigen::MatrixXd rX;\n\n        if( btransp == true) {\n            if( bc==true && bs==true )  {\n                rX = (X.colwise() - normdata.row(0).transpose() ).array().colwise() / normdata.row(1).transpose().array();\n            }   else if (bc == true  && bs==false)   {\n                rX = (X.colwise() - normdata.row(0).transpose());\n            }  else if ( bc == false && bs == true)   {\n                rX = X.array().colwise() / normdata.row(1).transpose().array();\n            }\n        } else {\n            if( bc==true && bs==true )  {\n                rX = (X.rowwise() - normdata.row(0)).array().rowwise() / normdata.row(1).array();\n            } else if (bc == true  && bs==false) {\n                rX = (X.rowwise() - normdata.row(0));\n            }  else if ( bc == false && bs == true)   {\n                rX = X.array().rowwise() / normdata.row(1).array();\n            }\n        }\n\n        return(rX);\n    }",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data_R_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.html#usage-example",
    "title": "RcppNormalize_Data_R_hdf5",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppNormalize_Data_R_hdf5(...);",
    "crumbs": [
      "Functions",
      "RcppNormalize_Data_R_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinv.html",
    "href": "api-reference/cpp/functions/RcppPseudoinv.html",
    "title": "RcppPseudoinv",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::RcppPseudoinv(Eigen::MatrixXd *A, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppPseudoinv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinv.html#signature",
    "href": "api-reference/cpp/functions/RcppPseudoinv.html#signature",
    "title": "RcppPseudoinv",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::RcppPseudoinv(Eigen::MatrixXd *A, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppPseudoinv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinv.html#description",
    "href": "api-reference/cpp/functions/RcppPseudoinv.html#description",
    "title": "RcppPseudoinv",
    "section": "2 Description",
    "text": "2 Description\nCompute pseudoinverse of in-memory matrix.",
    "crumbs": [
      "Functions",
      "RcppPseudoinv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinv.html#parameters",
    "href": "api-reference/cpp/functions/RcppPseudoinv.html#parameters",
    "title": "RcppPseudoinv",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (Eigen::MatrixXd *): Input matrix to compute pseudoinverse of\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "RcppPseudoinv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinv.html#returns",
    "href": "api-reference/cpp/functions/RcppPseudoinv.html#returns",
    "title": "RcppPseudoinv",
    "section": "4 Returns",
    "text": "4 Returns\nPseudoinverse matrix",
    "crumbs": [
      "Functions",
      "RcppPseudoinv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinv.html#details",
    "href": "api-reference/cpp/functions/RcppPseudoinv.html#details",
    "title": "RcppPseudoinv",
    "section": "5 Details",
    "text": "5 Details\nComputes the Moore-Penrose pseudoinverse using SVD decomposition with parallel processing support.",
    "crumbs": [
      "Functions",
      "RcppPseudoinv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinv.html#call-graph",
    "href": "api-reference/cpp/functions/RcppPseudoinv.html#call-graph",
    "title": "RcppPseudoinv",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "RcppPseudoinv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinv.html#source-code",
    "href": "api-reference/cpp/functions/RcppPseudoinv.html#source-code",
    "title": "RcppPseudoinv",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixPseudoinverse.hpp • Lines 81-129\ninline Eigen::MatrixXd RcppPseudoinv(Eigen::MatrixXd* A, \n                                            Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n{\n    \n    char Schar='S';\n    char Cchar='C';\n    int ione = 1; \n    double done = 1.0;\n    double dzero = 0.0;\n    \n    int m = A-&gt;rows();\n    int n = A-&gt;cols();\n    int lda = m;\n    int ldu = std::max(1,m);\n    int ldvt =  std::max(1, std::min(m, n));\n    int k = std::min(m,n);\n    int lwork = std::max( 1, 4*std::min(m,n)* std::min(m,n) + 7*std::min(m, n) );\n    int info = 0;\n    \n    Eigen::VectorXd s = Eigen::VectorXd::Zero(k);\n    Eigen::VectorXd work = Eigen::VectorXd::Zero(lwork);\n    Eigen::MatrixXd u = Eigen::MatrixXd::Zero(ldu,k);\n    Eigen::MatrixXd vt = Eigen::MatrixXd::Zero(ldvt,n);\n    \n    \n    // dgesvd_( char JOBU, char JOBVT, int M, int N, double* A, int LDA, double* S, double* U, int LDU, double* VT, int LDVT, double WORK, int LWORK, int INFO  );\n    dgesvd_( &Schar, &Schar, &m, &n, A-&gt;data(), &lda, s.data(), u.data(), &ldu, vt.data(), &ldvt, work.data(), &lwork, &info);\n    Eigen::MatrixXd pinv = Eigen::MatrixXd::Zero(n,m);\n    \n    //.OpenMP.// omp_set_dynamic(1);\n    \n#pragma omp parallel for num_threads(get_number_threads(threads, R_NilValue))\n    for (int i = 0; i &lt; k; i++){\n        double tempS;\n        if(s[i] &gt; 1.0e-9)\n            tempS = 1.0/s[i];\n        else\n            tempS = s[i];\n        \n        // zscal_ (int* N, double* DA, double* DX, int* INCX )\n        dscal_( &m, &tempS, &(u(i*ldu)), &ione );\n    }\n    // dgemm_( char TRANSA, char TRANSB, int M, int N, int K, double ALPHA, double* A, int LDA, double* B, int LDB, double BETA, double* C, int LDC )   \n    dgemm_( &Cchar, &Cchar, &n, &m, &k, &done, vt.data(), &k, u.data(), &m, &dzero, pinv.data(), &n );\n    \n    \n    return(pinv);\n    \n}",
    "crumbs": [
      "Functions",
      "RcppPseudoinv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppPseudoinv.html#usage-example",
    "href": "api-reference/cpp/functions/RcppPseudoinv.html#usage-example",
    "title": "RcppPseudoinv",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppPseudoinv(...);",
    "crumbs": [
      "Functions",
      "RcppPseudoinv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQR.html",
    "href": "api-reference/cpp/functions/RcppQR.html",
    "title": "RcppQR",
    "section": "",
    "text": "strQR BigDataStatMeth::RcppQR(M X, bool bthin)",
    "crumbs": [
      "Functions",
      "RcppQR"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQR.html#signature",
    "href": "api-reference/cpp/functions/RcppQR.html#signature",
    "title": "RcppQR",
    "section": "",
    "text": "strQR BigDataStatMeth::RcppQR(M X, bool bthin)",
    "crumbs": [
      "Functions",
      "RcppQR"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQR.html#description",
    "href": "api-reference/cpp/functions/RcppQR.html#description",
    "title": "RcppQR",
    "section": "2 Description",
    "text": "2 Description\nTemplate function for QR decomposition.",
    "crumbs": [
      "Functions",
      "RcppQR"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQR.html#parameters",
    "href": "api-reference/cpp/functions/RcppQR.html#parameters",
    "title": "RcppQR",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (M): Input matrix to decompose\nbthin (bool): Whether to compute thin QR",
    "crumbs": [
      "Functions",
      "RcppQR"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQR.html#returns",
    "href": "api-reference/cpp/functions/RcppQR.html#returns",
    "title": "RcppQR",
    "section": "4 Returns",
    "text": "4 Returns\nStructure containing Q and R matrices",
    "crumbs": [
      "Functions",
      "RcppQR"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQR.html#details",
    "href": "api-reference/cpp/functions/RcppQR.html#details",
    "title": "RcppQR",
    "section": "5 Details",
    "text": "5 Details\nComputes the QR decomposition of a matrix using Householder transformations. Supports both full and thin decomposition.",
    "crumbs": [
      "Functions",
      "RcppQR"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQR.html#source-code",
    "href": "api-reference/cpp/functions/RcppQR.html#source-code",
    "title": "RcppQR",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixQR.hpp • Lines 74-113\ninline strQR RcppQR ( M X, bool bthin )\n{\n    \n    static_assert(std::is_same&lt;M, Eigen::MatrixXd &gt;::value || \n                  std::is_same&lt;M, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n                  std::is_same&lt;M, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value,\n                  \"Error - type not allowed\");\n    \n    Eigen::MatrixXd A = X;\n    int m = A.rows(), n = A.cols();\n    int irank;\n    strQR vQR;\n    \n    Eigen::MatrixXd R;\n    Eigen::MatrixXd Q;\n    \n    Eigen::FullPivLU&lt;Eigen::MatrixXd&gt;lu_decomp(A);\n    Eigen::HouseholderQR&lt;Eigen::MatrixXd&gt; qr(A);\n    \n    qr.compute(A);\n    irank = lu_decomp.rank();\n    \n    if (irank == m + 1 || irank == n + 1 )\n    {\n        vQR.R = qr.matrixQR().template triangularView&lt;Eigen::Upper&gt;();\n    } else {\n        vQR.R = qr.matrixQR().topLeftCorner(irank, irank).template triangularView&lt;Eigen::Upper&gt;(); \n    }\n    \n    if (bthin == false)\n    {\n        vQR.Q =  qr.householderQ();       // Full decomposition\n    } else {\n        \n        vQR.Q = Eigen::MatrixXd::Identity(m,n);\n        vQR.Q = qr.householderQ() * vQR.Q;    // Thin decomposition\n    }\n    \n    return(vQR);\n}",
    "crumbs": [
      "Functions",
      "RcppQR"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppQR.html#usage-example",
    "href": "api-reference/cpp/functions/RcppQR.html#usage-example",
    "title": "RcppQR",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppQR(...);",
    "crumbs": [
      "Functions",
      "RcppQR"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html",
    "href": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html",
    "title": "RcppReduce_dataset_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppReduce_dataset_hdf5(std::string filename, std::string stringroup, std::string stroutgroup, std::string stroutdataset, std::string strreducefunction, bool boverwrite, bool bremove, bool binternal)",
    "crumbs": [
      "Functions",
      "RcppReduce_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#signature",
    "title": "RcppReduce_dataset_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppReduce_dataset_hdf5(std::string filename, std::string stringroup, std::string stroutgroup, std::string stroutdataset, std::string strreducefunction, bool boverwrite, bool bremove, bool binternal)",
    "crumbs": [
      "Functions",
      "RcppReduce_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#description",
    "href": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#description",
    "title": "RcppReduce_dataset_hdf5",
    "section": "2 Description",
    "text": "2 Description\nReduces multiple HDF5 datasets into a single dataset using specified operation.",
    "crumbs": [
      "Functions",
      "RcppReduce_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#parameters",
    "title": "RcppReduce_dataset_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfilename (std::string): HDF5 file path\nstringroup (std::string): Input group containing datasets to reduce\nstroutgroup (std::string): Output group for reduced dataset\nstroutdataset (std::string): Name of output dataset\nstrreducefunction (std::string): Reduction operation (“+” or “-”)\nboverwrite (bool): Whether to overwrite existing output dataset\nbremove (bool): Whether to remove input datasets after reduction\nbinternal (bool): Whether this is an internal call (affects data layout)",
    "crumbs": [
      "Functions",
      "RcppReduce_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#details",
    "href": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#details",
    "title": "RcppReduce_dataset_hdf5",
    "section": "4 Details",
    "text": "4 Details\nfilenameHDF5 file path stringroupInput group containing datasets to reduce stroutgroupOutput group for reduced dataset stroutdatasetName of output dataset strreducefunctionReduction operation (“+” or “-”) boverwriteWhether to overwrite existing output dataset bremoveWhether to remove input datasets after reduction binternalWhether this is an internal call (affects data layout) Implementation approach:Opens input HDF5 file and gets dataset listProcesses datasets sequentially:Reads each dataset into memoryAdjusts dimensions if necessaryApplies reduction operation Creates output dataset with reduced result",
    "crumbs": [
      "Functions",
      "RcppReduce_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#call-graph",
    "title": "RcppReduce_dataset_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppReduce_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#source-code",
    "title": "RcppReduce_dataset_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5ReduceDataset.hpp • Lines 101-254\ninline void RcppReduce_dataset_hdf5 ( std::string filename, \n                                   std::string stringroup, \n                                   std::string stroutgroup, \n                                   std::string stroutdataset, \n                                   std::string strreducefunction, \n                                   bool boverwrite,\n                                   bool bremove,\n                                   bool binternal)\n    {\n        \n        BigDataStatMeth::hdf5File* objFile = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsIn = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsOut = nullptr;\n        try {\n            \n            hsize_t* dims_out;\n            std::vector&lt;hsize_t&gt; stride = {1, 1},\n                block = {1, 1},\n                offset = {0, 0};\n            \n            Eigen::MatrixXd fullReduced;\n            Eigen::MatrixXd newRead;\n            int ndatasets;\n            \n            objFile = new BigDataStatMeth::hdf5File(filename, false);\n            objFile-&gt;openFile(\"r\");\n            \n            // Get dataset names without prefix, all datasets inside the group\n            Rcpp::StringVector joindata =  objFile-&gt;getDatasetNames(stringroup, \"\", \"\");\n            \n            delete objFile; // Close file \n            \n            ndatasets = joindata.size();\n            \n            for ( int i=0; i&lt; ndatasets; i++)\n            {\n                dsIn = new BigDataStatMeth::hdf5Dataset(filename, stringroup + \"/\" + joindata[i], false);\n                dsIn-&gt;openDataset();\n\n                if( dsIn-&gt;getDatasetptr() == nullptr) {\n                    checkClose_file(dsIn);\n                    Rcpp::Rcerr&lt;&lt; \"c++ exception RcppReduce_dataset_hdf5 (Dataset IException )\" &lt;&lt; std::endl;\n                    return void();\n                }\n                \n                dims_out =   dsIn-&gt;dim();\n                \n                std::vector&lt;double&gt; vdIn( dims_out[0] * dims_out[1] ); \n                dsIn-&gt;readDatasetBlock( {offset[0], offset[1]}, {dims_out[0], dims_out[1]}, stride, block, vdIn.data() );\n                \n                if( i == 0 ) {\n                    \n                    if(binternal == true)\n                        fullReduced = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vdIn.data(), dims_out[0], dims_out[1] );\n                    else\n                        fullReduced = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; (vdIn.data(), dims_out[0], dims_out[1] );\n                    \n                } else {\n                    \n                    if(binternal == true)\n                        newRead = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt;  (vdIn.data(), dims_out[0], dims_out[1] );\n                    else\n                        newRead = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt;  (vdIn.data(), dims_out[0], dims_out[1] );\n                    \n                    if( newRead.rows() != fullReduced.rows()){\n                        \n                        int difference = std::abs(fullReduced.rows() - newRead.rows());\n                        if( newRead.rows() &gt; fullReduced.rows()) {\n                            newRead.resize( newRead.rows() + difference, Eigen::NoChange);    \n                        } else {\n                            fullReduced.resize( fullReduced.rows() + difference, Eigen::NoChange);    \n                        }\n                    }\n                    \n                    if( newRead.cols() != fullReduced.cols()){\n                        \n                        int difference = std::abs(fullReduced.cols() - newRead.cols());\n                        if( newRead.cols() &gt; fullReduced.cols()){\n                            newRead.resize( Eigen::NoChange, newRead.cols() + difference );\n                        } else {\n                            fullReduced.resize( Eigen::NoChange, fullReduced.cols() + difference );\n                        }\n                    }\n                    \n                    // Reduce matrix\n                    if( strreducefunction.compare(\"+\")==0) {\n                        fullReduced = fullReduced + newRead;\n                    } else if (strreducefunction.compare(\"-\")==0) {\n                        fullReduced = fullReduced - newRead;\n                    } \n                    \n                }\n                \n                if( bremove == true){\n                    dsIn-&gt;remove();\n                }\n                \n                delete dsIn; dsIn = nullptr;\n            }\n            \n            dsOut = new BigDataStatMeth::hdf5Dataset(filename, stroutgroup, stroutdataset, boverwrite);\n            \n            if(binternal == true) {\n                dsOut-&gt;createDataset( fullReduced.rows() , fullReduced.cols(), \"real\");\n                // if( dsOut-&gt;getDatasetptr() != nullptr) {\n                //     dsOut-&gt;writeDataset(Rcpp::wrap(fullReduced));\n                // } else {\n                //     checkClose_file(dsOut);\n                //     Rcpp::Rcerr&lt;&lt; \"c++ exception RcppReduce_dataset_hdf5 (Dataset IException )\" &lt;&lt; std::endl;\n                //     return void();\n                // }\n            } else {\n                dsOut-&gt;createDataset( fullReduced.cols() , fullReduced.rows(), \"real\");\n                fullReduced.transposeInPlace();\n                \n                // if( dsOut-&gt;getDatasetptr() != nullptr) {\n                //     fullReduced.transposeInPlace();\n                //     dsOut-&gt;writeDataset(Rcpp::wrap(fullReduced));\n                // } else {\n                //     checkClose_file(dsOut);\n                //     Rcpp::Rcerr&lt;&lt; \"c++ exception RcppReduce_dataset_hdf5 (Dataset IException )\" &lt;&lt; std::endl;\n                //     return void();\n                // }\n            }\n            \n            \n            if( dsOut-&gt;getDatasetptr() != nullptr) {\n                dsOut-&gt;writeDataset(Rcpp::wrap(fullReduced));\n            } else {\n                checkClose_file(dsOut);\n                Rcpp::Rcerr&lt;&lt; \"c++ exception RcppReduce_dataset_hdf5 (Dataset IException )\" &lt;&lt; std::endl;\n                return void();\n            }\n            \n            delete dsOut; dsOut = nullptr;\n            \n        }catch( H5::FileIException& error ) {\n            checkClose_file(dsIn, dsOut);\n            // ::Rf_error( \"c++ exception RcppReduce_dataset_hdf5 (File IException )\" );\n            Rcpp::Rcerr&lt;&lt; \"c++ exception RcppReduce_dataset_hdf5 (File IException )\" &lt;&lt; std::endl;\n            return void();\n        } catch( H5::DataSetIException& error ) { // catch failure caused by the dstosplit operations\n            checkClose_file(dsIn, dsOut);\n            // ::Rf_error( \"c++ exception RcppReduce_dataset_hdf5 (dstosplit IException )\" );\n            Rcpp::Rcerr&lt;&lt; \"c++ exception RcppReduce_dataset_hdf5 (dstosplit IException )\" &lt;&lt; std::endl;\n            return void();\n        } catch( H5::DataSpaceIException& error ) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsIn, dsOut);\n            // ::Rf_error( \"c++ exception RcppReduce_dataset_hdf5 (DataSpace IException )\" );\n            Rcpp::Rcerr&lt;&lt; \"c++ exception RcppReduce_dataset_hdf5 (DataSpace IException )\" &lt;&lt; std::endl;\n            return void();\n        } \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "RcppReduce_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppReduce_dataset_hdf5.html#usage-example",
    "title": "RcppReduce_dataset_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppReduce_dataset_hdf5(...);",
    "crumbs": [
      "Functions",
      "RcppReduce_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolve.html",
    "href": "api-reference/cpp/functions/RcppSolve.html",
    "title": "RcppSolve",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::RcppSolve(Eigen::Map&lt; Eigen::MatrixXd &gt; a, Eigen::Map&lt; Eigen::MatrixXd &gt; b)",
    "crumbs": [
      "Functions",
      "RcppSolve"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolve.html#signature",
    "href": "api-reference/cpp/functions/RcppSolve.html#signature",
    "title": "RcppSolve",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::RcppSolve(Eigen::Map&lt; Eigen::MatrixXd &gt; a, Eigen::Map&lt; Eigen::MatrixXd &gt; b)",
    "crumbs": [
      "Functions",
      "RcppSolve"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolve.html#description",
    "href": "api-reference/cpp/functions/RcppSolve.html#description",
    "title": "RcppSolve",
    "section": "2 Description",
    "text": "2 Description\nSolves the linear system AX = B using Eigen matrices.",
    "crumbs": [
      "Functions",
      "RcppSolve"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolve.html#parameters",
    "href": "api-reference/cpp/functions/RcppSolve.html#parameters",
    "title": "RcppSolve",
    "section": "3 Parameters",
    "text": "3 Parameters\n\na (Eigen::Map&lt; Eigen::MatrixXd &gt;): Input matrix A (coefficient matrix)\nb (Eigen::Map&lt; Eigen::MatrixXd &gt;): Input/output matrix B (right-hand side matrix, will contain solution X)",
    "crumbs": [
      "Functions",
      "RcppSolve"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolve.html#returns",
    "href": "api-reference/cpp/functions/RcppSolve.html#returns",
    "title": "RcppSolve",
    "section": "4 Returns",
    "text": "4 Returns\nEigen::MatrixXd Solution matrix X",
    "crumbs": [
      "Functions",
      "RcppSolve"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolve.html#details",
    "href": "api-reference/cpp/functions/RcppSolve.html#details",
    "title": "RcppSolve",
    "section": "5 Details",
    "text": "5 Details\naInput matrix A (coefficient matrix) bInput/output matrix B (right-hand side matrix, will contain solution X) Eigen::MatrixXd Solution matrix X This function automatically detects if A is symmetric and uses the appropriate LAPACK solver (DSYSV for symmetric, DGESV for general matrices). The solution is computed in-place in matrix b.",
    "crumbs": [
      "Functions",
      "RcppSolve"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolve.html#call-graph",
    "href": "api-reference/cpp/functions/RcppSolve.html#call-graph",
    "title": "RcppSolve",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "RcppSolve"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolve.html#source-code",
    "href": "api-reference/cpp/functions/RcppSolve.html#source-code",
    "title": "RcppSolve",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixEquationSolver.hpp • Lines 89-124\ninline Eigen::MatrixXd RcppSolve(Eigen::Map&lt;Eigen::MatrixXd&gt; a, Eigen::Map&lt;Eigen::MatrixXd&gt; b)\n    {\n        \n        try {\n            \n            char Uchar = 'U';\n            int info = 0;\n            \n            // Declare matrix variables\n            int n = a.rows();\n            int nrhs = b.cols();\n            int lwork = std::max( 1, n );\n            int lda = std::max( 1, n );\n            int ldb = std::max( 1, n );\n            std::vector&lt;int&gt; ipiv(n);\n            std::vector&lt;double&gt; work(lwork);\n            \n            // Solve matrix equation\n            if( a == a.transpose()  )\n            {\n                // dsysv_( char* UPLO, int* N , int* NRHS, double* A, int* LDA, int* IPIV, double* B, int* LDB, double* WORK, int* LWORK, int* INFO);\n                dsysv_( &Uchar, &n, &nrhs, a.data(), &lda, ipiv.data(), b.data(), &ldb, work.data(), &lwork, &info);\n            } else {\n                \n                // dgesv( int N, int NRHS, double A, int LDA, int IPIV, double B, int LDB, int INFO);\n                dgesv_( &n, &nrhs, a.data(), &lda, ipiv.data(), b.data(), &ldb, &info );\n            }\n            \n        } catch(std::exception &ex) {\n            Rcpp::Rcout&lt;&lt; ex.what();\n            return(Eigen::MatrixXd::Zero(2,2));\n        }\n        \n        return(b);\n        \n    }",
    "crumbs": [
      "Functions",
      "RcppSolve"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSolve.html#usage-example",
    "href": "api-reference/cpp/functions/RcppSolve.html#usage-example",
    "title": "RcppSolve",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppSolve(...);",
    "crumbs": [
      "Functions",
      "RcppSolve"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html",
    "href": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html",
    "title": "RcppSort_dataset_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppSort_dataset_hdf5(BigDataStatMeth::hdf5Dataset *dsIn, BigDataStatMeth::hdf5Dataset *dsOut, Rcpp::List blockedSortlist, std::string func)",
    "crumbs": [
      "Functions",
      "RcppSort_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#signature",
    "href": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#signature",
    "title": "RcppSort_dataset_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::RcppSort_dataset_hdf5(BigDataStatMeth::hdf5Dataset *dsIn, BigDataStatMeth::hdf5Dataset *dsOut, Rcpp::List blockedSortlist, std::string func)",
    "crumbs": [
      "Functions",
      "RcppSort_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#description",
    "href": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#description",
    "title": "RcppSort_dataset_hdf5",
    "section": "2 Description",
    "text": "2 Description\nSorts an HDF5 dataset by rows or columns.",
    "crumbs": [
      "Functions",
      "RcppSort_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#parameters",
    "title": "RcppSort_dataset_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsIn (BigDataStatMeth::hdf5Dataset *): Pointer to input HDF5 dataset\ndsOut (BigDataStatMeth::hdf5Dataset *): Pointer to output HDF5 dataset where sorted data will be stored\nblockedSortlist (Rcpp::List): List containing sorting specifications:Column 1: Original orderColumn 2: New orderColumn 3: Diagonal indicatorsColumn 4: Additional order information\nfunc (std::string): Sorting function type:“sortRows”: Sort by rows”sortCols”: Sort by columns",
    "crumbs": [
      "Functions",
      "RcppSort_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#details",
    "href": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#details",
    "title": "RcppSort_dataset_hdf5",
    "section": "4 Details",
    "text": "4 Details\nThis function performs sorting operations on an HDF5 dataset based on a provided sorting specification. It supports both row-wise and column-wise sorting, with special handling for diagonal elements.",
    "crumbs": [
      "Functions",
      "RcppSort_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#call-graph",
    "title": "RcppSort_dataset_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppSort_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#source-code",
    "title": "RcppSort_dataset_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5SortDataset.hpp • Lines 102-185\ninline void RcppSort_dataset_hdf5(BigDataStatMeth::hdf5Dataset* dsIn,\n                                           BigDataStatMeth::hdf5Dataset* dsOut,\n                                           Rcpp::List blockedSortlist,\n                                           std::string func)\n    {\n        \n        try {\n            \n            Rcpp::NumericVector oper = {0, 1};\n            oper.names() = Rcpp::CharacterVector({ \"sortRows\", \"sortCols\"});\n            \n            Rcpp::StringVector rownames, colnames;\n            std::vector&lt;hsize_t&gt; stride = {1, 1},\n                                 block = {1, 1},\n                                 offset = {0, 0},\n                                 count = {0, 0};\n            \n            hsize_t* dims_out = dsIn-&gt;dim();\n            \n            for( int i = 0; i &lt; blockedSortlist.length(); i++) {\n                \n                Rcpp::DataFrame df(blockedSortlist[i]);\n                std::vector&lt;double&gt; order = df[0];\n                std::vector&lt;double&gt; neworder = df[2];\n                std::vector&lt;double&gt; diagonal = df[1];\n                \n                auto indices_0 = find_all(diagonal.begin(), diagonal.end(), 0);\n                \n                if( indices_0.size() &gt; 0) {\n                    // for(int t=0; t&lt;indices_0.size(); t++){\n                    //     Rcpp::Rcout&lt;&lt;\"Indices val : \" &lt;&lt;&indices_0[t]&lt;&lt;\"\\n\";    \n                    // }\n                    \n                } else {\n                    if( oper.findName( func ) == 0 ) {\n                        offset[0] = order[0] - 1;\n                        count[0] = order.size();\n                        count[1] = dims_out[1]; \n                        \n                    } else if( oper.findName( func ) == 1 ) {\n                        offset[1] = order[0] - 1;\n                        count[1] = dims_out[1]; \n                        count[0] = order[order.size() - order[0]];\n                    } \n                    \n                    std::vector&lt;double&gt; vdIn( count[0] * count[1] ); \n                    dsIn-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdIn.data() );\n                    \n                    if( oper.findName( func ) == 0 ) {\n                        offset[0] = neworder[0]-1;\n                    } else if( oper.findName( func ) == 1 ) {\n                        offset[1] = neworder[0]-1;\n                    }\n                    \n                    dsOut-&gt;writeDatasetBlock(vdIn, offset, count, stride, block);\n                    \n                }\n            }\n            \n        } catch( H5::FileIException& error ) {\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception RcppSort_dataset_hdf5 (File IException )\" &lt;&lt; std::endl;\n            return void();\n        } catch( H5::DataSetIException& error ) { // catch failure caused by the dstosplit operations\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception RcppSort_dataset_hdf5 (dstosplit IException )\" &lt;&lt; std::endl;\n            return void();\n        } catch( H5::DataSpaceIException& error ) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception RcppSort_dataset_hdf5 (DataSpace IException )\" &lt;&lt; std::endl;\n            return void();\n        } catch(std::exception &ex) {\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr &lt;&lt; \"c++ exception RcppSort_dataset_hdf5: \" &lt;&lt; ex.what();\n            return void();\n        } catch (...) {\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"C++ exception RcppSort_dataset_hdf5 (unknown reason)\";\n            return void();\n        } \n        \n        return void();\n        \n    }",
    "crumbs": [
      "Functions",
      "RcppSort_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/RcppSort_dataset_hdf5.html#usage-example",
    "title": "RcppSort_dataset_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppSort_dataset_hdf5(...);",
    "crumbs": [
      "Functions",
      "RcppSort_dataset_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html",
    "title": "RcppSplit_matrix_hdf5_internal",
    "section": "",
    "text": "void BigDataStatMeth::RcppSplit_matrix_hdf5_internal(BigDataStatMeth::hdf5Dataset *dstosplit, std::string stroutgroup, std::string stroutdataset, bool bycols, int nblocks, int iblocksize, int irows, int icols)",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5_internal"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#signature",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#signature",
    "title": "RcppSplit_matrix_hdf5_internal",
    "section": "",
    "text": "void BigDataStatMeth::RcppSplit_matrix_hdf5_internal(BigDataStatMeth::hdf5Dataset *dstosplit, std::string stroutgroup, std::string stroutdataset, bool bycols, int nblocks, int iblocksize, int irows, int icols)",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5_internal"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#description",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#description",
    "title": "RcppSplit_matrix_hdf5_internal",
    "section": "2 Description",
    "text": "2 Description\nSplits an HDF5 dataset into multiple smaller datasets (internal C++ interface)",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5_internal"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#parameters",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#parameters",
    "title": "RcppSplit_matrix_hdf5_internal",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndstosplit (BigDataStatMeth::hdf5Dataset *): Input dataset to split\nstroutgroup (std::string): Output group path\nstroutdataset (std::string): Base name for output datasets\nbycols (bool): Whether to split by columns (true) or rows (false)\nnblocks (int): Number of blocks to create (if &gt; 0)\niblocksize (int): Size of each block (if &gt; 0)\nirows (int): Number of rows in input dataset\nicols (int): Number of columns in input dataset",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5_internal"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#details",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#details",
    "title": "RcppSplit_matrix_hdf5_internal",
    "section": "4 Details",
    "text": "4 Details\ndstosplitInput dataset to split stroutgroupOutput group path stroutdatasetBase name for output datasets bycolsWhether to split by columns (true) or rows (false) nblocksNumber of blocks to create (if &gt; 0) iblocksizeSize of each block (if &gt; 0) irowsNumber of rows in input dataset icolsNumber of columns in input dataset Implementation features:Flexible block specification:By number of blocks (nblocks &gt; 0)By block size (iblocksize &gt; 0) Automatic block size calculationEfficient memory usageError checking for parameters",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5_internal"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#call-graph",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#call-graph",
    "title": "RcppSplit_matrix_hdf5_internal",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5_internal"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#source-code",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#source-code",
    "title": "RcppSplit_matrix_hdf5_internal",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5SplitDataset.hpp • Lines 196-299\ninline void RcppSplit_matrix_hdf5_internal ( BigDataStatMeth::hdf5Dataset* dstosplit,\n                                   std::string stroutgroup, std::string stroutdataset,\n                                   bool bycols, int nblocks, int iblocksize, \n                                   int irows, int icols )\n{\n        \n    BigDataStatMeth::hdf5Dataset* dsOut = nullptr;\n        \n    try {\n        \n        std::string newDatasetName = \"\";\n        hsize_t inrows = irows, \n                incols = icols,\n                ii = 0,\n                kk = 0;\n        int blocks = nblocks, \n            blocksize = iblocksize;;\n        std::vector&lt;hsize_t&gt; stride = {1, 1},\n                             block = {1, 1};\n        \n        if( nblocks &lt;= 0 && iblocksize &lt;= 0 ){\n            checkClose_file(dstosplit);\n            Rf_error( \"\\n Block size or number of blocks are needed to proceed with matrix split. Please, review parameters\");\n            return void();\n            \n        } else if (nblocks &gt; 0 && iblocksize &gt; 0 ) {\n            checkClose_file(dstosplit);\n            Rf_error( \"\\nBlock size and number of blocks are defined, please define only one option, split by number of blocks or by block size\");\n            return void();\n            \n        } else if( nblocks &gt; 0 ) {\n            double module;\n            \n            if(bycols == true) {\n                blocksize = icols / blocks;\n                module = icols % blocksize;\n            } else {\n                blocksize = irows / blocks;\n                module = irows % blocksize;\n            }\n            if (module &gt; 0) { blocksize = blocksize + 1; }\n        } \n        \n        if(blocksize &gt; 0) {\n            \n            if( bycols == true ) {\n                blocks = (icols + blocksize - 1) / blocksize;\n                incols = blocksize;\n            } else {\n                blocks = (irows + blocksize - 1) / blocksize;\n                inrows = blocksize;\n            }    \n        }\n        \n        for ( int i=0; i&lt;blocks; i++)\n        {\n            newDatasetName = stroutgroup + \"/\" + stroutdataset + \".\" + std::to_string(i);\n            \n            if( bycols == true) {\n                kk = i * blocksize;\n                if( kk + static_cast&lt;hsize_t&gt;(blocksize) &gt; static_cast&lt;hsize_t&gt;(icols)) { \n                    incols = static_cast&lt;hsize_t&gt;(icols) - kk; \n                }\n            } else  {\n                ii = i * blocksize;\n                if( ii + static_cast&lt;hsize_t&gt;(blocksize) &gt; static_cast&lt;hsize_t&gt;(irows)) { \n                    inrows = static_cast&lt;hsize_t&gt;(irows) - ii; \n                }\n            }\n    \n            std::vector&lt;double&gt; vdts( inrows * incols );\n            dstosplit-&gt;readDatasetBlock( {ii, kk}, {inrows, incols}, stride, block, vdts.data() );\n            \n            dsOut = new BigDataStatMeth::hdf5Dataset(dstosplit-&gt;getFileName(), newDatasetName, true);\n            dsOut-&gt;createDataset(  incols, inrows, \"real\"); \n            \n            if( dsOut-&gt;getDatasetptr() != nullptr ){\n                dsOut-&gt;writeDataset(vdts.data());\n            }\n            \n            delete dsOut; dsOut = nullptr;\n            \n        }\n        \n    } catch( H5::FileIException& error ) {\n        checkClose_file(dstosplit, dsOut);\n        Rf_error( \"c++ exception RcppSplit_matrix_hdf5_internal(File IException )\");\n    } catch( H5::DataSetIException& error ) { \n        checkClose_file(dstosplit, dsOut);\n        Rf_error( \"c++ exception RcppSplit_matrix_hdf5_internal (DataSet IException )\");\n    } catch( H5::DataSpaceIException& error ) { \n        checkClose_file(dstosplit, dsOut);\n        Rf_error( \"c++ exception RcppSplit_matrix_hdf5_internal (DataSpace IException )\");\n    } catch(std::exception &ex) {\n        checkClose_file(dstosplit, dsOut);\n        Rf_error( \"\\nC++ exception RcppSplit_matrix_hdf5_internal : %s\",ex.what());\n    } catch (...) {\n        checkClose_file(dstosplit, dsOut);\n        Rf_error( \"\\nC++ exception RcppSplit_matrix_hdf5_internal (unknown reason)\");\n    }\n    \n    return void();\n    \n}",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5_internal"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#usage-example",
    "href": "api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.html#usage-example",
    "title": "RcppSplit_matrix_hdf5_internal",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppSplit_matrix_hdf5_internal(...);",
    "crumbs": [
      "Functions",
      "RcppSplit_matrix_hdf5_internal"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_FileExist.html",
    "href": "api-reference/cpp/functions/Rcpp_FileExist.html",
    "title": "Rcpp_FileExist",
    "section": "",
    "text": "bool BigDataStatMeth::Rcpp_FileExist(std::string fullPath)",
    "crumbs": [
      "Functions",
      "Rcpp_FileExist"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_FileExist.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_FileExist.html#signature",
    "title": "Rcpp_FileExist",
    "section": "",
    "text": "bool BigDataStatMeth::Rcpp_FileExist(std::string fullPath)",
    "crumbs": [
      "Functions",
      "Rcpp_FileExist"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_FileExist.html#description",
    "href": "api-reference/cpp/functions/Rcpp_FileExist.html#description",
    "title": "Rcpp_FileExist",
    "section": "2 Description",
    "text": "2 Description\nChecks if a file exists at the specified path.",
    "crumbs": [
      "Functions",
      "Rcpp_FileExist"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_FileExist.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_FileExist.html#parameters",
    "title": "Rcpp_FileExist",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfullPath (std::string): Complete file path to check",
    "crumbs": [
      "Functions",
      "Rcpp_FileExist"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_FileExist.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_FileExist.html#returns",
    "title": "Rcpp_FileExist",
    "section": "4 Returns",
    "text": "4 Returns\nbool True if file exists, false otherwise",
    "crumbs": [
      "Functions",
      "Rcpp_FileExist"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_FileExist.html#details",
    "href": "api-reference/cpp/functions/Rcpp_FileExist.html#details",
    "title": "Rcpp_FileExist",
    "section": "5 Details",
    "text": "5 Details\nfullPathComplete file path to check bool True if file exists, false otherwise File check:Uses system calls to verify existenceHandles both relative and absolute pathsThread-safe implementation",
    "crumbs": [
      "Functions",
      "Rcpp_FileExist"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_FileExist.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_FileExist.html#source-code",
    "title": "Rcpp_FileExist",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/Utilities.hpp • Lines 474-487\ninline bool Rcpp_FileExist(std::string fullPath) \n    {\n        bool exists = false;\n        \n        std::fstream fileStream;\n        fileStream.open(fullPath);\n        \n        if (fileStream.good()) {\n            exists = true;\n        } else {\n            exists = false;\n        }\n        return(exists);\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_FileExist"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_FileExist.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_FileExist.html#usage-example",
    "title": "Rcpp_FileExist",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_FileExist(...);",
    "crumbs": [
      "Functions",
      "Rcpp_FileExist"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html",
    "title": "Rcpp_Impute_snps_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Rcpp_Impute_snps_hdf5(BigDataStatMeth::hdf5Dataset *dsIn, BigDataStatMeth::hdf5DatasetInternal *dsOut, bool bycols, std::string stroutdataset, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_Impute_snps_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#signature",
    "title": "Rcpp_Impute_snps_hdf5",
    "section": "",
    "text": "void BigDataStatMeth::Rcpp_Impute_snps_hdf5(BigDataStatMeth::hdf5Dataset *dsIn, BigDataStatMeth::hdf5DatasetInternal *dsOut, bool bycols, std::string stroutdataset, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_Impute_snps_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#description",
    "title": "Rcpp_Impute_snps_hdf5",
    "section": "2 Description",
    "text": "2 Description\nImputes missing values in an HDF5 dataset.",
    "crumbs": [
      "Functions",
      "Rcpp_Impute_snps_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#parameters",
    "title": "Rcpp_Impute_snps_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsIn (BigDataStatMeth::hdf5Dataset *): Input HDF5 dataset\ndsOut (BigDataStatMeth::hdf5DatasetInternal *): Output HDF5 dataset for imputed data\nbycols (bool): If true, process by columns; if false, process by rows\nstroutdataset (std::string): Name for the output dataset\nthreads (Rcpp::Nullable&lt; int &gt;): Optional number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "Rcpp_Impute_snps_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#details",
    "title": "Rcpp_Impute_snps_hdf5",
    "section": "4 Details",
    "text": "4 Details\nThis function performs imputation of missing values (represented by 3) in an HDF5 dataset. It can operate on either rows or columns and supports parallel processing for improved performance.",
    "crumbs": [
      "Functions",
      "Rcpp_Impute_snps_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#call-graph",
    "title": "Rcpp_Impute_snps_hdf5",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_Impute_snps_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#source-code",
    "title": "Rcpp_Impute_snps_hdf5",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5ImputeData.hpp • Lines 176-314\ninline void Rcpp_Impute_snps_hdf5(BigDataStatMeth::hdf5Dataset* dsIn, BigDataStatMeth::hdf5DatasetInternal* dsOut,\n                         bool bycols, std::string stroutdataset, Rcpp::Nullable&lt;int&gt; threads  = R_NilValue)\n    {\n        \n        try{\n            \n            std::vector&lt;hsize_t&gt; stride = {1,1},\n                                 block = {1,1};\n            \n            int ilimit,\n                blocksize = 1000;\n            \n            hsize_t* dims_out = dsIn-&gt;dim();\n            \n            // id bycols == true : read all rows by group of columns ; else : all columns by group of rows\n            if (bycols == true) {\n                ilimit = dims_out[0];\n            } else {\n                ilimit = dims_out[1];\n            };\n            \n            \n            if( stroutdataset.compare(\"\")!=0) {\n                dsOut-&gt;createDataset(dims_out[0], dims_out[1], \"real\");\n            } \n            \n            dsOut-&gt;openDataset();\n            \n            int chunks = (ilimit + (blocksize - 1)) / blocksize; //(ilimit/blocksize);\n\n            #pragma omp parallel num_threads(get_number_threads(threads, R_NilValue)) shared(dsIn, dsOut, chunks)\n            {\n                #pragma omp for schedule(auto)\n                for( int i=0; i &lt; chunks; i++) \n                {\n                    \n                    std::vector&lt;hsize_t&gt; offset = {0,0},\n                                         count = {0,0};\n                    \n                    int iread;\n                    \n                    if( (i+1)*blocksize &lt; ilimit) iread = blocksize;\n                    else iread = ilimit - (i*blocksize);\n                    \n                    // id bycols == true : read all rows by group of columns ; else : all columns by group of rows\n                    if(bycols == true) {\n                        count[0] = iread; \n                        count[1] = dims_out[1];\n                        offset[0] = i*blocksize;\n                    } else {\n                        count[0] = dims_out[0];\n                        count[1] = iread; \n                        offset[1] = i*blocksize;\n                    }\n                    \n                    // read block\n                    std::vector&lt;double&gt; vdIn( count[0] * count[1] ); \n                    #pragma omp critical(accessFile)\n                    {\n                        dsIn-&gt;readDatasetBlock( { offset[0], offset[1]}, { count[0], count[1]}, stride, block, vdIn.data() );\n                    }\n                    Eigen::MatrixXd data = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vdIn.data(), count[0], count[1] );\n                    \n                    if(bycols == true) // We have to do it by rows\n                    {\n                        for( int row = 0; row&lt;data.rows(); row++)  // COMPLETE EXECUTION\n                        {\n                            std::map&lt;double, double&gt; myMap;\n                            myMap = VectortoOrderedMap_SNP_counts(data.row(row));\n                            \n                            Eigen::VectorXd ev = data.row(row);\n                            std::vector&lt;double&gt; v(ev.data(), ev.data() + ev.size());\n                            \n                            auto it = std::find_if(std::begin(v), std::end(v), [](int i){return i == 3;});\n                            while (it != std::end(v)) {\n                                \n                                if(*it==3) *it = get_value_to_impute_discrete(myMap);\n                                it = std::find_if(std::next(it), std::end(v), [](int i){return i == 3;});\n                            }\n                            \n                            Eigen::VectorXd X = Eigen::Map&lt;Eigen::VectorXd&gt;(v.data(), v.size());\n                            data.row(row) = X;\n                            \n                        }\n                        \n                    } else {\n                        for( int col = 0; col&lt;data.cols(); col++) \n                        {\n                            std::map&lt;double, double&gt; myMap;\n                            myMap = VectortoOrderedMap_SNP_counts(data.col(col));\n                            \n                            Eigen::VectorXd ev = data.col(col);\n                            std::vector&lt;double&gt; v(ev.data(), ev.data() + ev.size());\n                            \n                            auto it = std::find_if(std::begin(v), std::end(v), [](int i){return i == 3;});\n                            while (it != std::end(v)) {\n                                if(*it==3) *it = get_value_to_impute_discrete(myMap);\n                                it = std::find_if(std::next(it), std::end(v), [](int i){return i == 3;});\n                            }\n                            \n                            Eigen::VectorXd X = Eigen::Map&lt;Eigen::VectorXd&gt;(v.data(), v.size());\n                            data.col(col) = X;\n                            \n                        }\n                    }\n                    \n                    #pragma omp critical(accessFile)\n                    {\n                        dsOut-&gt;writeDatasetBlock( Rcpp::wrap(data), offset, count, stride, block, false);\n                    }\n                }\n                \n            }\n            \n        } catch( H5::FileIException& error) { // catch failure caused by the H5File operations\n            checkClose_file(dsIn, dsOut);\n            Rf_error(\"c++ exception Rcpp_Impute_snps_hdf5 (File IException)\");\n        } catch( H5::DataSetIException& error) { // catch failure caused by the DataSet operations\n            checkClose_file(dsIn, dsOut);\n            Rf_error(\"c++ exception Rcpp_Impute_snps_hdf5 (DataSet IException)\");\n        } catch( H5::GroupIException& error) { // catch failure caused by the Group operations\n            checkClose_file(dsIn, dsOut);\n            Rf_error(\"c++ exception Rcpp_Impute_snps_hdf5 (Group IException)\");\n        } catch( H5::DataSpaceIException& error) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsIn, dsOut);\n            Rf_error(\"c++ exception Rcpp_Impute_snps_hdf5 (DataSpace IException)\");\n        } catch( H5::DataTypeIException& error) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsIn, dsOut);\n            Rf_error(\"c++ exception Rcpp_Impute_snps_hdf5 (Data TypeIException)\");\n        } catch(std::exception &ex) {\n            checkClose_file(dsIn, dsOut);\n            Rf_error( \"c++ exception Rcpp_Impute_snps_hdf5 : %s\", ex.what());\n        } catch (...) {\n            checkClose_file(dsIn, dsOut);\n            Rf_error(\"C++ exception Rcpp_Impute_snps_hdf5 (unknown reason)\");\n        }\n        \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_Impute_snps_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.html#usage-example",
    "title": "Rcpp_Impute_snps_hdf5",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_Impute_snps_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_Impute_snps_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html",
    "title": "Rcpp_Remove_Low_Data_hdf5",
    "section": "",
    "text": "int BigDataStatMeth::Rcpp_Remove_Low_Data_hdf5(BigDataStatMeth::hdf5Dataset *dsIn, BigDataStatMeth::hdf5Dataset *dsOut, bool bycols, double pcent)",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_Low_Data_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#signature",
    "title": "Rcpp_Remove_Low_Data_hdf5",
    "section": "",
    "text": "int BigDataStatMeth::Rcpp_Remove_Low_Data_hdf5(BigDataStatMeth::hdf5Dataset *dsIn, BigDataStatMeth::hdf5Dataset *dsOut, bool bycols, double pcent)",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_Low_Data_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#description",
    "title": "Rcpp_Remove_Low_Data_hdf5",
    "section": "2 Description",
    "text": "2 Description\nRemoves rows or columns with high percentage of missing data from HDF5 dataset.",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_Low_Data_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#parameters",
    "title": "Rcpp_Remove_Low_Data_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsIn (BigDataStatMeth::hdf5Dataset *): Pointer to input HDF5 dataset\ndsOut (BigDataStatMeth::hdf5Dataset *): Pointer to output HDF5 dataset where filtered data will be stored\nbycols (bool): If true, process by columns; if false, process by rows\npcent (double): Threshold percentage (0.0-1.0) of missing data to trigger removal",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_Low_Data_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#returns",
    "title": "Rcpp_Remove_Low_Data_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nint Number of rows/columns removed (negative) or error code",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_Low_Data_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#details",
    "title": "Rcpp_Remove_Low_Data_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function processes an HDF5 dataset in blocks, removing rows or columns where the percentage of missing data (represented by value 3) exceeds the specified threshold. The filtered data is written to a new dataset.",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_Low_Data_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#call-graph",
    "title": "Rcpp_Remove_Low_Data_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_Low_Data_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#source-code",
    "title": "Rcpp_Remove_Low_Data_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5RemoveLowData.hpp • Lines 72-220\ninline int Rcpp_Remove_Low_Data_hdf5( BigDataStatMeth::hdf5Dataset* dsIn, BigDataStatMeth::hdf5Dataset* dsOut, bool bycols, double pcent)\n    {\n        \n        int itotrem = 0;\n        \n        try{\n        \n            int ilimit,\n                blocksize = 1000;\n            \n            bool bcreated = false;\n            \n            std::vector&lt;hsize_t&gt; offset = {0,0},\n                                 count = {0,0},\n                                 stride = {1,1},\n                                 block = {1,1},\n                                 newoffset = {0,0};\n            \n            hsize_t* dims_out = dsIn-&gt;dim();\n            \n            // id bycols == true : read all rows by group of columns ; else : all columns by group of rows\n            if (bycols == true) {\n                ilimit = dims_out[0];\n                count[1] = dims_out[1];\n                offset[1] = 0;\n            } else {\n                ilimit = dims_out[1];\n                count[0] = dims_out[0];\n                offset[0] = 0;\n            };\n            \n            for( int i=0; i&lt;=(ilimit/blocksize); i++) \n            {\n                int iread;\n                int iblockrem = 0;\n                \n                if( (i+1)*blocksize &lt; ilimit) iread = blocksize;\n                else iread = ilimit - (i*blocksize);\n                \n                if(bycols == true) {\n                    count[0] = iread; \n                    offset[0] = i*blocksize;\n                } else {\n                    count[1] = iread; \n                    offset[1] = i*blocksize;\n                }\n                \n                // read block\n                std::vector&lt;double&gt; vdCurDataset( count[0] * count[1] ); \n                dsIn-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdCurDataset.data() );\n                Eigen::MatrixXd data = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vdCurDataset.data(), count[0], count[1] );\n                \n                \n                if(bycols == true) // We have to do it by rows\n                {\n                    int readedrows = data.rows();\n                    \n                    for( int row = readedrows-1 ; row&gt;=0; row--)  // COMPLETE EXECUTION\n                    {\n                        if((data.row(row).array() == 3).count()/(double)count[1]&gt;= pcent )\n                        {\n                            removeRow(data, row);\n                            iblockrem = iblockrem + 1;\n                        } \n                    }\n                    \n                } else {\n                    \n                    int readedcols = data.cols();\n                    \n                    for( int col = readedcols-1 ; col&gt;=0; col--)  // COMPLETE EXECUTION\n                    { \n                        if((data.col(col).array() == 3).count()/(double)count[0]&gt;=pcent )\n                        {\n                            removeColumn(data, col);\n                            iblockrem = iblockrem + 1;\n                        } \n                    }\n                }\n                \n                int extendcols = data.cols();\n                int extendrows = data.rows();\n                \n                if( bcreated == false) {\n                    \n                    if( extendcols &gt; 0 && extendrows &gt; 0){\n                        dsOut-&gt;createUnlimitedDataset(extendrows, extendcols, \"real\");\n                        dsOut-&gt;openDataset();\n                        bcreated = true;\n                    } \n                    \n                } else {\n                    if(bycols == true){\n                        dsOut-&gt;extendUnlimitedDataset(extendrows, 0);\n                    }else{\n                        dsOut-&gt;extendUnlimitedDataset( 0, extendcols);\n                    }\n                }\n\n                if( bcreated == true) {\n                    std::vector&lt;hsize_t&gt; countblock = {(unsigned long long)extendrows, (unsigned long long)extendcols};\n                    dsOut-&gt;writeDatasetBlock( Rcpp::wrap(data), newoffset, countblock, stride, block, false);\n                    \n                    if(bycols == true)\n                        newoffset[0] =  newoffset[0] + extendrows;\n                    else\n                        newoffset[1] =  newoffset[1] + extendcols;\n                }\n                \n                itotrem = itotrem - iblockrem;\n                \n            }\n            \n            if( bcreated == false ) {\n                Rcpp::warning(\"All data removed - please adjust pcent parameter or review data\");\n            }\n            \n        } catch( H5::FileIException& error) { // catch failure caused by the H5File operations\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception Rcpp_Remove_Low_Data_hdf5 (File IException)\" &lt;&lt; std::endl;\n            return -1;\n        } catch( H5::DataSetIException& error) { // catch failure caused by the DataSet operations\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception Rcpp_Remove_Low_Data_hdf5 (DataSet IException)\" &lt;&lt; std::endl;\n            return -1;\n        } catch( H5::GroupIException& error) { // catch failure caused by the Group operations\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception Rcpp_Remove_Low_Data_hdf5 (Group IException)\" &lt;&lt; std::endl;\n            return -1;\n        } catch( H5::DataSpaceIException& error) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception Rcpp_Remove_Low_Data_hdf5 (DataSpace IException)\" &lt;&lt; std::endl;\n            return -1;\n        } catch( H5::DataTypeIException& error) { // catch failure caused by the DataSpace operations\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"c++ exception Rcpp_Remove_Low_Data_hdf5 (Data TypeIException)\" &lt;&lt; std::endl;\n            return -1;\n        } catch(std::exception &ex) {\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr &lt;&lt; \"c++ exception Rcpp_Remove_Low_Data_hdf5: \" &lt;&lt; ex.what();\n            return -1;\n        } catch (...) {\n            checkClose_file(dsIn, dsOut);\n            Rcpp::Rcerr&lt;&lt;\"C++ exception Rcpp_Remove_Low_Data_hdf5 (unknown reason)\";\n            return -1;\n        }\n        \n        return(itotrem);\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_Low_Data_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.html#usage-example",
    "title": "Rcpp_Remove_Low_Data_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_Remove_Low_Data_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_Remove_Low_Data_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html",
    "title": "Rcpp_block_matrix_mul",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_block_matrix_mul(T X, U Y, Rcpp::Nullable&lt; int &gt; iblock_size)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#signature",
    "title": "Rcpp_block_matrix_mul",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_block_matrix_mul(T X, U Y, Rcpp::Nullable&lt; int &gt; iblock_size)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#description",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#description",
    "title": "Rcpp_block_matrix_mul",
    "section": "2 Description",
    "text": "2 Description\nBlock-based matrix multiplication.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#parameters",
    "title": "Rcpp_block_matrix_mul",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (T): First input matrix\nY (U): Second input matrix\niblock_size (Rcpp::Nullable&lt; int &gt;): Block size for computation",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#returns",
    "title": "Rcpp_block_matrix_mul",
    "section": "4 Returns",
    "text": "4 Returns\nResult of matrix multiplication",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#details",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#details",
    "title": "Rcpp_block_matrix_mul",
    "section": "5 Details",
    "text": "5 Details\nImplements efficient block-based matrix multiplication using cache-friendly algorithms.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#call-graph",
    "title": "Rcpp_block_matrix_mul",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#source-code",
    "title": "Rcpp_block_matrix_mul",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memMultiplication.hpp • Lines 109-188\ninline Eigen::MatrixXd Rcpp_block_matrix_mul( T X, U Y, Rcpp::Nullable&lt;int&gt;  iblock_size)\n   {\n       \n       Eigen::MatrixXd C;\n       \n       try{\n\n           static_assert(std::is_same&lt;T, Eigen::MatrixXd &gt;::value || \n                         std::is_same&lt;T, Eigen::Map&lt; Eigen::MatrixXd &gt;&gt;::value || \n                         std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n                         std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value,\n                         \"Error - type not allowed\");\n           \n           static_assert(std::is_same&lt;U, Eigen::MatrixXd &gt;::value || \n                         std::is_same&lt;U, Eigen::Map&lt; Eigen::MatrixXd &gt;&gt;::value || \n                         std::is_same&lt;U, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n                         std::is_same&lt;U, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value,\n                         \"Error - type not allowed\");\n           \n           Eigen::MatrixXd A = X,\n                           B = Y;\n           \n           // if(transpX == true){ A = X.transpose(); }\n           // if(transpY == true){ B = Y.transpose(); }\n           \n           // int chunks;//, tid;\n           hsize_t block_size;\n           \n           std::vector&lt;hsize_t&gt; vsizetoReadN, vstartN,\n                                vsizetoReadM, vstartM,\n                                vsizetoReadK, vstartK;\n           \n           // unsigned int ithreads;\n           hsize_t M = A.rows();\n           hsize_t K = A.cols();\n           hsize_t N = B.cols();\n           \n           //. 20251121 .// if( K == B.rows()) {\n           if( static_cast&lt;Eigen::Index&gt;(K) == B.rows()) {\n               if( iblock_size.isNotNull()) {\n                   block_size =  Rcpp::as&lt;int&gt;(iblock_size);  \n               } else {\n                   block_size =  MAXBLOCKSIZE/3;  \n               }\n               \n               C = Eigen::MatrixXd::Zero(M,N) ;\n               if(block_size &gt; std::min( N, std::min(M,K)) )\n                   block_size = std::min( N, std::min(M,K)); \n               \n               getBlockPositionsSizes( N, block_size, vstartN, vsizetoReadN );\n               getBlockPositionsSizes( M, block_size, vstartM, vsizetoReadM );\n               getBlockPositionsSizes( K, block_size, vstartK, vsizetoReadK );\n               \n               for (size_t ii = 0; ii &lt; vstartM.size(); ii++)\n               {\n                   for (size_t jj = 0; jj &lt; vstartN.size(); jj++)\n                   {\n                       for(size_t kk = 0; kk &lt; vstartK.size(); kk++)\n                       {\n                           C.block(vstartM[ii], vstartN[jj], vsizetoReadM[ii], vsizetoReadN[jj]) =  C.block(vstartM[ii], vstartN[jj], vsizetoReadM[ii], vsizetoReadN[jj]) + \n                               (A.block(vstartM[ii], vstartK[kk], vsizetoReadM[ii], vsizetoReadK[kk]) * B.block(vstartK[kk], vstartN[jj], vsizetoReadK[kk], vsizetoReadN[jj]));\n                       }\n                   }\n               }\n               \n           } else {\n               throw std::range_error(\"non-conformable arguments\");\n               }\n           \n           \n       } catch(std::exception &ex) {\n           Rcpp::Rcout&lt;&lt;\"c++ error : Bblock_matrix_mul : \" &lt;&lt;ex.what();\n           return(C);\n           \n       } catch(...) { \n           Rf_error(\"c++ exception in Bblock_matrix_mul (unknown reason)\"); \n       }\n       \n       return(C);\n   }",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_mul.html#usage-example",
    "title": "Rcpp_block_matrix_mul",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_block_matrix_mul(...);",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_mul"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html",
    "title": "Rcpp_block_matrix_substract_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_block_matrix_substract_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#signature",
    "title": "Rcpp_block_matrix_substract_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_block_matrix_substract_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#description",
    "title": "Rcpp_block_matrix_substract_hdf5",
    "section": "2 Description",
    "text": "2 Description\nBlock-based matrix subtraction for HDF5 matrices.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#parameters",
    "title": "Rcpp_block_matrix_substract_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input matrix dataset\ndsB (BigDataStatMeth::hdf5Dataset *): Second input matrix dataset\ndsC (BigDataStatMeth::hdf5Dataset *): Output matrix dataset\nhdf5_block (hsize_t): Block size for HDF5 I/O operations\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#returns",
    "title": "Rcpp_block_matrix_substract_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nPointer to result dataset",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#details",
    "title": "Rcpp_block_matrix_substract_hdf5",
    "section": "5 Details",
    "text": "5 Details\nPerforms block-based matrix subtraction C = A - B where A, B, and C are HDF5 datasets. Optimized for large matrices with parallel processing.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#call-graph",
    "title": "Rcpp_block_matrix_substract_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#source-code",
    "title": "Rcpp_block_matrix_substract_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSubstract.hpp • Lines 69-188\ninline BigDataStatMeth::hdf5Dataset*  Rcpp_block_matrix_substract_hdf5( \n            BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, BigDataStatMeth::hdf5Dataset* dsC,\n            hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt;int&gt; threads  = R_NilValue)\n    {\n        \n        try {\n            \n            hsize_t K = dsA-&gt;nrows();\n            hsize_t N = dsA-&gt;ncols();\n            \n            if( K == dsB-&gt;nrows() && N == dsB-&gt;ncols())\n            {\n                \n                // Parallellization and Block variables \n                // unsigned int ithreads;\n                std::vector&lt;hsize_t&gt; stride = {1, 1},\n                                     block = {1, 1},\n                                     vstart, vsizetoRead;\n                // hsize_t isize = hdf5_block + 1;\n                \n                dsC-&gt;createDataset( N, K, \"real\"); \n                \n                // ithreads = get_threads(bparal, threads);\n                \n                if( K&lt;=N ) {\n                    \n                    getBlockPositionsSizes( N, hdf5_block, vstart, vsizetoRead );\n                    // int chunks = vstart.size()/ithreads;\n                    \n                    #pragma omp parallel num_threads( get_threads(bparal, threads) ) shared(dsA, dsB, dsC) //, chunks)\n                    {\n                        #pragma omp for schedule (dynamic)\n                        for (hsize_t ii = 0; ii &lt; vstart.size(); ii ++)\n                        {\n                            \n                            std::vector&lt;double&gt; vdA( K * vsizetoRead[ii] ); \n                            #pragma omp critical (accessFile)\n                            {\n                            dsA-&gt;readDatasetBlock( {0, vstart[ii]}, { K, vsizetoRead[ii]}, stride, block, vdA.data() );\n                            }\n                            \n                            std::vector&lt;double&gt; vdB( K * vsizetoRead[ii] ); \n                            #pragma omp critical (accessFile)\n                            {\n                            dsB-&gt;readDatasetBlock( {0, vstart[ii]}, {K, vsizetoRead[ii]}, stride, block, vdB.data() );\n                            }\n                            std::transform (vdA.begin(), vdA.end(),\n                                            vdB.begin(), vdA.begin(), std::minus&lt;double&gt;());\n                            \n                            std::vector&lt;hsize_t&gt; offset = { 0, vstart[ii] };\n                            std::vector&lt;hsize_t&gt; count = { K, vsizetoRead[ii] };\n                            #pragma omp critical  (accessFile)\n                            {\n                                dsC-&gt;writeDatasetBlock(vdA, offset, count, stride, block);\n                            }\n                        }\n                    }\n    \n                } else {\n                    \n                    getBlockPositionsSizes( K, hdf5_block, vstart, vsizetoRead );\n                    // int chunks = vstart.size()/ithreads;\n                    \n                    #pragma omp parallel num_threads( get_threads(bparal, threads) ) shared(dsA, dsB, dsC)\n                    {\n                        #pragma omp for schedule (dynamic)\n                        for (hsize_t ii = 0; ii &lt; vstart.size(); ii++)\n                        {\n                            std::vector&lt;double&gt; vdA( vsizetoRead[ii] * N ); \n                            #pragma omp critical (accessFile)\n                            {\n                                dsA-&gt;readDatasetBlock( {vstart[ii], 0}, { vsizetoRead[ii], N}, stride, block, vdA.data() );\n                            }\n                            \n                            std::vector&lt;double&gt; vdB( vsizetoRead[ii] * N); \n                            #pragma omp critical (accessFile)\n                            {\n                                dsB-&gt;readDatasetBlock( {vstart[ii], 0}, {vsizetoRead[ii], N}, stride, block, vdB.data() );\n                            }\n                            \n                            std::transform (vdA.begin(), vdA.end(),\n                                            vdB.begin(), vdA.begin(), std::minus&lt;double&gt;());\n                            \n                            std::vector&lt;hsize_t&gt; offset = { vstart[ii], 0 };\n                            std::vector&lt;hsize_t&gt; count = { vsizetoRead[ii], N };\n                            #pragma omp critical (accessFile)\n                            {\n                                dsC-&gt;writeDatasetBlock(vdA, offset, count, stride, block);\n                            }\n                        }\n                    }\n                }\n            } else {\n                Rcpp::Rcout&lt;&lt;\"matrix substract error: non-conformable arguments\\n\";\n            }\n            \n        } catch( H5::FileIException& error ) { \n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_substract_hdf5 (File IException)\";\n            return(dsC);\n        } catch( H5::GroupIException & error ) { \n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_substract_hdf5 (Group IException)\";\n            return(dsC);\n        } catch( H5::DataSetIException& error ) { \n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_substract_hdf5 (DataSet IException)\";\n            return(dsC);\n        } catch(std::exception& ex) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_substract_hdf5\" &lt;&lt; ex.what();\n            return(dsC);\n        } catch (...) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception Rcpp_block_matrix_substract_hdf5 (unknown reason)\";\n            return(dsC);\n        }\n        \n        return(dsC);\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.html#usage-example",
    "title": "Rcpp_block_matrix_substract_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_block_matrix_substract_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_substract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html",
    "title": "Rcpp_block_matrix_vector_substract",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_block_matrix_vector_substract(T A, T B, hsize_t block_size, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html#signature",
    "title": "Rcpp_block_matrix_vector_substract",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_block_matrix_vector_substract(T A, T B, hsize_t block_size, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html#description",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html#description",
    "title": "Rcpp_block_matrix_vector_substract",
    "section": "2 Description",
    "text": "2 Description\nLow-level block-based matrix-vector subtraction.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html#parameters",
    "title": "Rcpp_block_matrix_vector_substract",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (T): Input matrix\nB (T): Input vector (subtracted from A)\nblock_size (hsize_t): Size of blocks for computation\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel computation",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html#returns",
    "title": "Rcpp_block_matrix_vector_substract",
    "section": "4 Returns",
    "text": "4 Returns\nResult of matrix-vector subtraction",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html#details",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html#details",
    "title": "Rcpp_block_matrix_vector_substract",
    "section": "5 Details",
    "text": "5 Details\nInternal implementation of block-based matrix-vector subtraction with configurable block size and parallel processing.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.html#usage-example",
    "title": "Rcpp_block_matrix_vector_substract",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_block_matrix_vector_substract(...);",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html",
    "title": "Rcpp_block_matrix_vector_sum_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_block_matrix_vector_sum_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#signature",
    "title": "Rcpp_block_matrix_vector_sum_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_block_matrix_vector_sum_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#description",
    "title": "Rcpp_block_matrix_vector_sum_hdf5",
    "section": "2 Description",
    "text": "2 Description\nBlock-based matrix-vector addition for HDF5 matrices.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#parameters",
    "title": "Rcpp_block_matrix_vector_sum_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): Input vector dataset\ndsB (BigDataStatMeth::hdf5Dataset *): Input matrix dataset\ndsC (BigDataStatMeth::hdf5Dataset *): Output matrix dataset\nhdf5_block (hsize_t): Block size for HDF5 I/O operations\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#returns",
    "title": "Rcpp_block_matrix_vector_sum_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nPointer to result dataset",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#details",
    "title": "Rcpp_block_matrix_vector_sum_hdf5",
    "section": "5 Details",
    "text": "5 Details\nPerforms block-based matrix-vector addition where one operand is a vector and the other is a matrix. Supports both row and column vectors.",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#call-graph",
    "title": "Rcpp_block_matrix_vector_sum_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#source-code",
    "title": "Rcpp_block_matrix_vector_sum_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSum.hpp • Lines 206-350\ninline BigDataStatMeth::hdf5Dataset* Rcpp_block_matrix_vector_sum_hdf5( \n            BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, BigDataStatMeth::hdf5Dataset* dsC,\n            hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt;int&gt; threads  = R_NilValue)\n    {\n    \n        try {\n            \n            // Vector\n            hsize_t K = dsA-&gt;nrows();\n            hsize_t N = dsA-&gt;ncols();\n            \n            // Matrix\n            hsize_t M = dsB-&gt;nrows();\n            hsize_t L = dsB-&gt;ncols();\n            \n            std::vector&lt;hsize_t&gt; vstart, vsizetoRead;\n            // unsigned int ithreads;\n    \n            if(hdf5_block == 1) {\n                hdf5_block = ceil(MAXBLOCKSIZE/(K*N));\n            }\n    \n            // hsize_t isize = hdf5_block + 1;\n            std::vector&lt;hsize_t&gt; stride = {1, 1},\n                                 block = {1, 1};\n            \n            dsC-&gt;createDataset( L, M, \"real\");\n            \n            std::vector&lt;double&gt; vdA( K * N );\n            dsA-&gt;readDatasetBlock( {0, 0}, {K, N}, stride, block, vdA.data() );\n            \n            // ithreads = get_threads(bparal, threads);\n            \n            if(  K == M )\n            { // Sum vector to every col\n                \n                getBlockPositionsSizes( L, hdf5_block, vstart, vsizetoRead );\n                // int chunks = vstart.size()/ithreads;\n                \n                #pragma omp parallel num_threads( get_threads(bparal, threads) ) shared(dsA, dsB, dsC) //, chunks)\n                {\n                    #pragma omp for schedule (dynamic) // collapse(2)\n                    for (hsize_t ii = 0; ii &lt; vstart.size(); ii ++)\n                    {\n                        std::vector&lt;double&gt; vdB( K * vsizetoRead[ii] );\n                        #pragma omp critical(accessFile)\n                        {\n                            dsB-&gt;readDatasetBlock( {0, vstart[ii]}, {K, vsizetoRead[ii]}, stride, block, vdB.data() );\n                        }\n                        \n                        // Duplicate vector\n                        std::size_t const no_of_duplicates = (K * vsizetoRead[ii]) / vdA.size();\n                        \n                        std::vector&lt;double&gt; v = vdA; \n                        v.reserve(vdA.size() * no_of_duplicates);\n                        auto end = std::end(v);\n                        \n                        for(std::size_t i = 1; i &lt; no_of_duplicates; ++i)\n                            v.insert(std::end(v), std::begin(v), end);\n                        \n                        // Sum vector to matrix by columns / rows\n                        std::transform (vdB.begin(), vdB.end(),\n                                        v.begin(), vdB.begin(), std::plus&lt;double&gt;());\n                        \n                        std::vector&lt;hsize_t&gt; offset = { 0, vstart[ii]};\n                        std::vector&lt;hsize_t&gt; count = { K, vsizetoRead[ii]};\n                        \n                        #pragma omp critical(accessFile)\n                        {\n                            dsC-&gt;writeDatasetBlock( vdB, offset, count, stride, block);\n                        }\n                    }\n                }\n                \n    \n            } else if(  K == L ) { // Sum vector to every row\n                \n                getBlockPositionsSizes( M, hdf5_block, vstart, vsizetoRead );\n                // int chunks = vstart.size()/ithreads;\n                \n                #pragma omp parallel num_threads( get_threads(bparal, threads) ) shared(dsA, dsB, dsC) //, chunks)\n                {\n                    #pragma omp for schedule (dynamic) // collapse(2)\n                    for (hsize_t ii = 0; ii &lt; vstart.size(); ii ++)\n                    {\n                        std::vector&lt;double&gt; vdB( L * vsizetoRead[ii] );\n                        #pragma omp critical (accessFile)\n                        {\n                            dsB-&gt;readDatasetBlock( {vstart[ii], 0}, {vsizetoRead[ii], K}, stride, block, vdB.data() );\n                        }\n                        Rcpp::NumericMatrix B (vsizetoRead[ii], K, vdB.begin());\n                        \n                        Rcpp::transpose(B);\n                        \n                        // Duplicate vector\n                        std::size_t const no_of_duplicates = (vsizetoRead[ii] * K) / vdA.size();\n                        \n                        std::vector&lt;double&gt; v = vdA; \n                        v.reserve(v.size() * no_of_duplicates);\n                        auto end = std::end(v);\n                        \n                        for(std::size_t i = 1; i &lt; no_of_duplicates; ++i)\n                            v.insert(std::end(v), std::begin(v), end);\n                        \n                        // Sum vector to matrix by columns / rows\n                        std::transform (B.begin(), B.end(),\n                                        v.begin(), B.begin() , std::plus&lt;double&gt;());\n                    \n                        std::vector&lt;hsize_t&gt; offset = { vstart[ii], 0};\n                        std::vector&lt;hsize_t&gt; count = { vsizetoRead[ii], K};\n                        \n                        #pragma omp critical (accessFile)\n                        {\n                            dsC-&gt;writeDatasetBlock( vdB, offset, count, stride, block);\n                        }\n                    }\n                }\n            } else {\n                Rcpp::Rcout&lt;&lt; \"vector sum error: non-conformable arguments\\n\";\n            }\n    \n        } catch( H5::FileIException& error ) { \n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_vector_sum_hdf5 (File IException)\";\n            return(dsC);\n        } catch( H5::GroupIException & error ) { \n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_vector_sum_hdf5 (Group IException)\";\n            return(dsC);\n        } catch( H5::DataSetIException& error ) { \n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_vector_sum_hdf5 (DataSet IException)\";\n            return(dsC);\n        } catch(std::exception& ex) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception Rcpp_block_matrix_vector_sum_hdf5: \" &lt;&lt; ex.what();\n            return(dsC);\n        } catch (...) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception Rcpp_block_matrix_vector_sum_hdf5 (unknown reason)\";\n            return(dsC);\n        }\n        \n        return(dsC);\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.html#usage-example",
    "title": "Rcpp_block_matrix_vector_sum_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_block_matrix_vector_sum_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_block_matrix_vector_sum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html",
    "title": "Rcpp_matrixVectorDivision_byRow",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorDivision_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#signature",
    "title": "Rcpp_matrixVectorDivision_byRow",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorDivision_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#description",
    "title": "Rcpp_matrixVectorDivision_byRow",
    "section": "2 Description",
    "text": "2 Description\nMatrix-vector division by rows.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#parameters",
    "title": "Rcpp_matrixVectorDivision_byRow",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (Eigen::MatrixXd): Input matrix\nv (Eigen::VectorXd): Input vector",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#returns",
    "title": "Rcpp_matrixVectorDivision_byRow",
    "section": "4 Returns",
    "text": "4 Returns\nResult of row-wise division",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#details",
    "title": "Rcpp_matrixVectorDivision_byRow",
    "section": "5 Details",
    "text": "5 Details\nDivides each row of a matrix by a vector element-wise.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#caller-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#caller-graph",
    "title": "Rcpp_matrixVectorDivision_byRow",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#source-code",
    "title": "Rcpp_matrixVectorDivision_byRow",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectormatrix.hpp • Lines 91-94\ninline Eigen::MatrixXd Rcpp_matrixVectorDivision_byRow(Eigen::MatrixXd X, Eigen::VectorXd v) {\n    X = X.array().colwise() / v.array();\n    return(X);\n}",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.html#usage-example",
    "title": "Rcpp_matrixVectorDivision_byRow",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrixVectorDivision_byRow(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorDivision_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html",
    "title": "Rcpp_matrixVectorMultiplication_byRow",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorMultiplication_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#signature",
    "title": "Rcpp_matrixVectorMultiplication_byRow",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorMultiplication_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#description",
    "title": "Rcpp_matrixVectorMultiplication_byRow",
    "section": "2 Description",
    "text": "2 Description\nMatrix-vector multiplication by rows.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#parameters",
    "title": "Rcpp_matrixVectorMultiplication_byRow",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (Eigen::MatrixXd): Input matrix\nv (Eigen::VectorXd): Input vector",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#returns",
    "title": "Rcpp_matrixVectorMultiplication_byRow",
    "section": "4 Returns",
    "text": "4 Returns\nResult of row-wise multiplication",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#details",
    "title": "Rcpp_matrixVectorMultiplication_byRow",
    "section": "5 Details",
    "text": "5 Details\nMultiplies each row of a matrix by a vector element-wise.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#caller-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#caller-graph",
    "title": "Rcpp_matrixVectorMultiplication_byRow",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#source-code",
    "title": "Rcpp_matrixVectorMultiplication_byRow",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectormatrix.hpp • Lines 52-55\ninline Eigen::MatrixXd Rcpp_matrixVectorMultiplication_byRow(Eigen::MatrixXd X, Eigen::VectorXd v) {\n    X = X.array().colwise() * v.array();\n    return(X);\n}",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.html#usage-example",
    "title": "Rcpp_matrixVectorMultiplication_byRow",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrixVectorMultiplication_byRow(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorMultiplication_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html",
    "title": "Rcpp_matrixVectorPower_byCol",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorPower_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPower_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#signature",
    "title": "Rcpp_matrixVectorPower_byCol",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorPower_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPower_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#description",
    "title": "Rcpp_matrixVectorPower_byCol",
    "section": "2 Description",
    "text": "2 Description\nMatrix-vector power by columns.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPower_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#parameters",
    "title": "Rcpp_matrixVectorPower_byCol",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (Eigen::MatrixXd): Input matrix\nv (Eigen::VectorXd): Input vector",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPower_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#returns",
    "title": "Rcpp_matrixVectorPower_byCol",
    "section": "4 Returns",
    "text": "4 Returns\nResult of column-wise power",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPower_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#details",
    "title": "Rcpp_matrixVectorPower_byCol",
    "section": "5 Details",
    "text": "5 Details\nPower each column of a matrix by a vector element-wise.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPower_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#caller-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#caller-graph",
    "title": "Rcpp_matrixVectorPower_byCol",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPower_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#source-code",
    "title": "Rcpp_matrixVectorPower_byCol",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectormatrix.hpp • Lines 176-183\ninline Eigen::MatrixXd Rcpp_matrixVectorPower_byCol(Eigen::MatrixXd X, Eigen::VectorXd v) {\n    for (int row = 0; row &lt; X.rows(); ++row) {\n        X.row(row) = X.row(row).array().unaryExpr(\n            [&v, row](double x) { return std::pow(x, v(row)); }\n        );\n    }\n    return(X);\n}",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPower_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.html#usage-example",
    "title": "Rcpp_matrixVectorPower_byCol",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrixVectorPower_byCol(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorPower_byCol"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html",
    "title": "Rcpp_matrixVectorSubstract_byRow",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorSubstract_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#signature",
    "title": "Rcpp_matrixVectorSubstract_byRow",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorSubstract_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#description",
    "title": "Rcpp_matrixVectorSubstract_byRow",
    "section": "2 Description",
    "text": "2 Description\nMatrix-vector subtraction by rows.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#parameters",
    "title": "Rcpp_matrixVectorSubstract_byRow",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (Eigen::MatrixXd): Input matrix\nv (Eigen::VectorXd): Input vector",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#returns",
    "title": "Rcpp_matrixVectorSubstract_byRow",
    "section": "4 Returns",
    "text": "4 Returns\nResult of row-wise subtraction",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#details",
    "title": "Rcpp_matrixVectorSubstract_byRow",
    "section": "5 Details",
    "text": "5 Details\nSubtracts a vector from each row of a matrix.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#caller-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#caller-graph",
    "title": "Rcpp_matrixVectorSubstract_byRow",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#source-code",
    "title": "Rcpp_matrixVectorSubstract_byRow",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectormatrix.hpp • Lines 78-81\ninline Eigen::MatrixXd Rcpp_matrixVectorSubstract_byRow(Eigen::MatrixXd X, Eigen::VectorXd v) {\n    X = X.array().colwise() - v.array();\n    return(X);\n}",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.html#usage-example",
    "title": "Rcpp_matrixVectorSubstract_byRow",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrixVectorSubstract_byRow(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSubstract_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html",
    "title": "Rcpp_matrixVectorSum_byRow",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorSum_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#signature",
    "title": "Rcpp_matrixVectorSum_byRow",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorSum_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#description",
    "title": "Rcpp_matrixVectorSum_byRow",
    "section": "2 Description",
    "text": "2 Description\nMatrix-vector addition by rows.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#parameters",
    "title": "Rcpp_matrixVectorSum_byRow",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (Eigen::MatrixXd): Input matrix\nv (Eigen::VectorXd): Input vector",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#returns",
    "title": "Rcpp_matrixVectorSum_byRow",
    "section": "4 Returns",
    "text": "4 Returns\nResult of row-wise addition",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#details",
    "title": "Rcpp_matrixVectorSum_byRow",
    "section": "5 Details",
    "text": "5 Details\nAdds a vector to each row of a matrix.",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#caller-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#caller-graph",
    "title": "Rcpp_matrixVectorSum_byRow",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#source-code",
    "title": "Rcpp_matrixVectorSum_byRow",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectormatrix.hpp • Lines 65-68\ninline Eigen::MatrixXd Rcpp_matrixVectorSum_byRow(Eigen::MatrixXd X, Eigen::VectorXd v) {\n    X = X.array().colwise() + v.array();\n    return(X);\n}",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.html#usage-example",
    "title": "Rcpp_matrixVectorSum_byRow",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrixVectorSum_byRow(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrixVectorSum_byRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html",
    "title": "Rcpp_matrix_blockSum",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_blockSum(T A, T B, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#signature",
    "title": "Rcpp_matrix_blockSum",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_blockSum(T A, T B, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#description",
    "title": "Rcpp_matrix_blockSum",
    "section": "2 Description",
    "text": "2 Description\nBlock-based matrix addition implementation.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#parameters",
    "title": "Rcpp_matrix_blockSum",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (T): First input matrix\nB (T): Second input matrix\nthreads (Rcpp::Nullable&lt; int &gt;): Optional number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#returns",
    "title": "Rcpp_matrix_blockSum",
    "section": "4 Returns",
    "text": "4 Returns\nRcpp::RObject containing the result matrix",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#details",
    "title": "Rcpp_matrix_blockSum",
    "section": "5 Details",
    "text": "5 Details\nInternal implementation of block-based matrix addition that processes matrices in blocks for better cache utilization and memory efficiency.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#call-graph",
    "title": "Rcpp_matrix_blockSum",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#source-code",
    "title": "Rcpp_matrix_blockSum",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memSum.hpp • Lines 194-271\ninline Rcpp::RObject Rcpp_matrix_blockSum ( T  A, T  B, Rcpp::Nullable&lt;int&gt; threads)\n    {\n        \n        // static_assert(std::is_same&lt;T, Eigen::MatrixXd &gt;::value || \n        //               std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n        //               std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value ,\n        //               \"Error - type not allowed\");\n        \n        Rcpp::NumericMatrix X = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(A);\n        Rcpp::NumericMatrix Y = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(B);\n        // unsigned int ithreads;\n        \n        hsize_t N = X.rows();\n        hsize_t M = X.cols();\n        \n        Rcpp::NumericMatrix C = Rcpp::no_init( N, M);\n        \n        hsize_t block_size; \n        \n        try {\n            \n            std::vector&lt;hsize_t&gt; vsizetoRead;\n            std::vector&lt;hsize_t&gt; vstart;\n            \n            std::vector&lt;hsize_t&gt; blockSize = getMatrixBlockSize( N, M);\n            if(N &lt; M) {\n                block_size = blockSize.at(0);    \n            } else {\n                block_size = blockSize.at(1);\n            }\n            \n            if(block_size &gt; 0 ) {\n                \n                // if( N == Y.rows() && M == Y.cols())\n                if (N == static_cast&lt;hsize_t&gt;(Y.rows()) && M == static_cast&lt;hsize_t&gt;(Y.cols())) \n                {\n                    // hsize_t size = block_size + 1;\n                    \n                    // ithreads = get_number_threads(threads, R_NilValue);\n                    \n                    getBlockPositionsSizes( N*M, block_size, vstart, vsizetoRead );\n                    // int chunks = vstart.size()/ithreads;\n                    \n                    #pragma omp parallel num_threads( get_number_threads(threads, R_NilValue) ) shared(A, B, C)\n                    {\n                    #pragma omp for schedule (dynamic)\n                        for (hsize_t ii = 0; ii &lt; vstart.size(); ii ++)\n                        {\n                            \n                            if( vstart[ii] + vsizetoRead[ii] &gt;= N*M ) {\n                                std::transform (X.begin() + vstart[ii], X.end(),\n                                                Y.begin() + vstart[ii], C.begin() + vstart[ii], std::plus&lt;double&gt;());\n                            } else {\n                                std::transform (X.begin() + vstart[ii], X.begin() + vstart[ii] + vsizetoRead[ii],\n                                                Y.begin() + vstart[ii], C.begin() + vstart[ii], std::plus&lt;double&gt;());   \n                            }\n                        }\n                    }\n\n                } else {\n                    Rcpp::Rcout&lt;&lt;\"matrix sum error: non-conformable arguments\\n\";\n                    return(R_NilValue);\n                }\n                \n            } else{\n                Rcpp::Rcout&lt;&lt;\"matrix sum error: Error whent computing block sizes\\n\";\n                return(R_NilValue);\n            }\n            \n        } catch(std::exception &ex) {\n            Rcpp::Rcout&lt;&lt; ex.what();\n            return(R_NilValue);\n        }\n        \n        C.attr(\"dim\") = Rcpp::Dimension( N, M);\n        return(C);\n        \n    }",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrix_blockSum.html#usage-example",
    "title": "Rcpp_matrix_blockSum",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrix_blockSum(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_sum.html",
    "href": "api-reference/cpp/functions/Rcpp_matrix_sum.html",
    "title": "Rcpp_matrix_sum",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_sum(T A, T B)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_sum.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrix_sum.html#signature",
    "title": "Rcpp_matrix_sum",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_sum(T A, T B)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_sum.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrix_sum.html#description",
    "title": "Rcpp_matrix_sum",
    "section": "2 Description",
    "text": "2 Description\nLow-level block-based matrix-vector addition implementation.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_sum.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrix_sum.html#parameters",
    "title": "Rcpp_matrix_sum",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (T): Input matrix\nB (T): Input vector\nblock_size (``): Size of processing blocks\nbparal (``): Enable/disable parallel processing\nthreads (``): Number of threads for parallel computation",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_sum.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrix_sum.html#returns",
    "title": "Rcpp_matrix_sum",
    "section": "4 Returns",
    "text": "4 Returns\nEigen::MatrixXd containing the result",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_sum.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrix_sum.html#details",
    "title": "Rcpp_matrix_sum",
    "section": "5 Details",
    "text": "5 Details\nCore implementation that:Processes matrix in cache-friendly blocksSupports parallel execution through OpenMPOptimizes memory access patternsHandles edge cases for non-uniform block sizes",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_sum.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrix_sum.html#source-code",
    "title": "Rcpp_matrix_sum",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memSum.hpp • Lines 91-115\ninline Rcpp::RObject Rcpp_matrix_sum ( T  A, T  B)\n    {\n        \n        // static_assert(std::is_same&lt;T, Eigen::MatrixXd &gt;::value ||\n        //               std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value ||\n        //               std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value ||\n        //               std::is_same&lt;T, Rcpp::NumericMatrix &gt;::value,\n        //               \"Error - type not allowed\");\n        \n        Rcpp::NumericMatrix m = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(A);\n        Rcpp::NumericMatrix m2 = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(B);\n        \n        if( m.rows() == m2.rows() && m.cols() == m2.cols()) {\n            Rcpp::NumericVector C = m + m2;\n            C.attr(\"dim\") = Rcpp::Dimension( m.rows(), m.cols());\n            \n            return(C);\n            \n        } else {\n            Rcpp::Rcout&lt;&lt;\"Error: non-conformable arguments\";\n        }\n        \n        return(R_NilValue);\n        \n    }",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_sum.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrix_sum.html#usage-example",
    "title": "Rcpp_matrix_sum",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrix_sum(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html",
    "title": "Rcpp_matrix_vect_substract",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_vect_substract(T A, U B)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#signature",
    "title": "Rcpp_matrix_vect_substract",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_vect_substract(T A, U B)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#description",
    "title": "Rcpp_matrix_vect_substract",
    "section": "2 Description",
    "text": "2 Description\nMatrix-vector subtraction.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#parameters",
    "title": "Rcpp_matrix_vect_substract",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (T): Input matrix\nB (U): Input vector (subtracted from A)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#returns",
    "title": "Rcpp_matrix_vect_substract",
    "section": "4 Returns",
    "text": "4 Returns\nResult of matrix-vector subtraction",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#details",
    "title": "Rcpp_matrix_vect_substract",
    "section": "5 Details",
    "text": "5 Details\nSubtracts a vector from each row or column of a matrix.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#source-code",
    "title": "Rcpp_matrix_vect_substract",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memSubstract.hpp • Lines 181-210\ninline Rcpp::RObject Rcpp_matrix_vect_substract ( T  A, U  B)\n    {\n        \n        Rcpp::NumericMatrix m = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(A);\n        Rcpp::NumericVector v = Rcpp::as&lt;Rcpp::NumericVector&gt;(B);\n        \n        if( v.length() == m.rows()) {\n            \n            Rcpp::NumericMatrix C = Rcpp::no_init( m.rows(), m.cols());\n            \n            for( int i=0; i&lt;m.cols(); i++) {\n                C( Rcpp::_, i) = m( Rcpp::_, i) - v;  \n            }    \n            return(C);\n            \n        } else if( v.length() == m.cols()) {\n            \n            Rcpp::NumericMatrix C = Rcpp::no_init( m.rows(), m.cols());\n            \n            for( int i=0; i&lt;m.rows(); i++) {\n                C( i, Rcpp::_) = m( i, Rcpp::_) - v;  \n            }    \n            return(C);\n            \n        } else {\n            Rcpp::Rcout&lt;&lt;\"Error: non-conformable arguments\";\n        }\n        \n        return(R_NilValue);\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vect_substract.html#usage-example",
    "title": "Rcpp_matrix_vect_substract",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrix_vect_substract(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vect_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html",
    "title": "Rcpp_matrix_vector_blockMult",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_vector_blockMult(T A, T B, Rcpp::Nullable&lt; bool &gt; bparal, Rcpp::Nullable&lt; int &gt; iblock_size, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockMult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#signature",
    "title": "Rcpp_matrix_vector_blockMult",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_vector_blockMult(T A, T B, Rcpp::Nullable&lt; bool &gt; bparal, Rcpp::Nullable&lt; int &gt; iblock_size, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockMult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#description",
    "title": "Rcpp_matrix_vector_blockMult",
    "section": "2 Description",
    "text": "2 Description\nBlock-based matrix-vector multiplication.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockMult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#parameters",
    "title": "Rcpp_matrix_vector_blockMult",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (T): Input matrix\nB (T): Input vector\nbparal (Rcpp::Nullable&lt; bool &gt;): Whether to use parallel processing\niblock_size (Rcpp::Nullable&lt; int &gt;): Block size for computation\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockMult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#returns",
    "title": "Rcpp_matrix_vector_blockMult",
    "section": "4 Returns",
    "text": "4 Returns\nResult of matrix-vector multiplication",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockMult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#details",
    "title": "Rcpp_matrix_vector_blockMult",
    "section": "5 Details",
    "text": "5 Details\nImplements block-based matrix-vector multiplication with:Optional parallel processingConfigurable block sizesThread count controlCache-friendly algorithms",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockMult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#call-graph",
    "title": "Rcpp_matrix_vector_blockMult",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockMult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#source-code",
    "title": "Rcpp_matrix_vector_blockMult",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memMultiplication.hpp • Lines 394-494\ninline Rcpp::RObject Rcpp_matrix_vector_blockMult( T  A, T  B, Rcpp::Nullable&lt;bool&gt; bparal, \n                            Rcpp::Nullable&lt;int&gt; iblock_size, Rcpp::Nullable&lt;int&gt; threads)\n    {\n        \n        // NOTA: Per defecte, multiplica per columnes tal i com raja.... \n\n        bool btransposed = false;\n        // unsigned int ithreads;\n        hsize_t block_size;\n        // int chunks;\n        \n        Rcpp::NumericMatrix X = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(A);\n        Rcpp::NumericVector Y = Rcpp::as&lt;Rcpp::NumericVector&gt;(B);\n        Rcpp::NumericMatrix C;\n        \n        // Matrix\n        hsize_t M = X.rows(), N = X.cols();\n        // Vector\n        hsize_t K = Y.length();\n        \n        try {\n            \n            if( K==N || K==M) {\n                if ( K == N){\n                    // multiplies vector to every col\n                    btransposed = true;\n                    \n                    X = Rcpp::transpose(X);\n                    \n                    N = X.rows();\n                    M = X.cols();\n                } \n                \n                std::vector&lt;hsize_t&gt; vsizetoRead;\n                std::vector&lt;hsize_t&gt; vstart;\n                \n                // ithreads = get_number_threads(threads, bparal);\n                \n                C = Rcpp::no_init( M, N);\n                \n                if( iblock_size.isNotNull()) {\n                    block_size =  Rcpp::as&lt;int&gt;(iblock_size);  \n                } else {\n                    block_size = getMatrixBlockSize( N, M).at(0);\n                }\n                \n                // minimum block size: 2 columns\n                if(block_size &lt;= 0 ) {\n                    block_size = M*2;\n                }\n                \n                // Mínimum block size: 2 columns\n                getBlockPositionsSizes( M*N, block_size, vstart, vsizetoRead );\n                \n                // chunks = vstart.size()/ithreads;\n                \n                #pragma omp parallel num_threads( get_number_threads(threads, bparal) ) shared(A, B, C) //, chunks)\n                {\n                #pragma omp for schedule (static)\n                    for (hsize_t ii = 0; ii &lt; vstart.size(); ii ++)\n                    {\n                        // Duplicate vector\n                        std::size_t const no_of_duplicates = vsizetoRead[ii] / Y.length();\n                        \n                        std::vector&lt;double&gt; v = Rcpp::as&lt;std::vector&lt;double&gt; &gt;(Y); \n                        v.reserve(Y.size() * no_of_duplicates);\n                        auto end = std::end(v);\n                        \n                        for(std::size_t i = 1; i &lt; no_of_duplicates; ++i)\n                            v.insert(std::end(v), std::begin(v), end);\n                        \n                        // Mult vector to matrix by columns / rows\n                        if( vstart[ii] + vsizetoRead[ii] &gt;= M*N ) {\n                            std::transform (X.begin() + vstart[ii], X.end(),\n                                            v.begin(), C.begin() + vstart[ii], std::multiplies&lt;double&gt;());\n                        } else {\n                            std::transform (X.begin() + vstart[ii], X.begin() + vstart[ii] + vsizetoRead[ii],\n                                            v.begin() , C.begin() + vstart[ii], std::multiplies&lt;double&gt;());   \n                        }\n                    }\n                }\n\n            } else {\n                \n                Rcpp::Rcout&lt;&lt; \"vector sum error: non-conformable arguments\\n\";\n                return(R_NilValue);\n            }\n            \n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception Rcpp_matrix_vector_blockMult: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n            return(R_NilValue);\n        }\n        \n        if(btransposed == true){\n            Rcpp::transpose(C);\n        } \n        \n        C.attr(\"dim\") = Rcpp::Dimension( M, N);\n        \n        return(C);\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockMult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.html#usage-example",
    "title": "Rcpp_matrix_vector_blockMult",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrix_vector_blockMult(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockMult"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html",
    "title": "Rcpp_matrix_vector_blockSum",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_vector_blockSum(T A, T B, Rcpp::Nullable&lt; bool &gt; bparal, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#signature",
    "title": "Rcpp_matrix_vector_blockSum",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_matrix_vector_blockSum(T A, T B, Rcpp::Nullable&lt; bool &gt; bparal, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#description",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#description",
    "title": "Rcpp_matrix_vector_blockSum",
    "section": "2 Description",
    "text": "2 Description\nBlock-based matrix-vector addition with parallel processing.",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#parameters",
    "title": "Rcpp_matrix_vector_blockSum",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (T): Input matrix\nB (T): Input vector\nbparal (Rcpp::Nullable&lt; bool &gt;): Boolean flag to enable/disable parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel computation (if enabled)",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#returns",
    "title": "Rcpp_matrix_vector_blockSum",
    "section": "4 Returns",
    "text": "4 Returns\nRcpp::RObject containing the result",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#details",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#details",
    "title": "Rcpp_matrix_vector_blockSum",
    "section": "5 Details",
    "text": "5 Details\nHigh-level interface for block-based matrix-vector addition that:Validates input dimensionsConfigures parallel processing based on input parametersDelegates to low-level implementation",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#call-graph",
    "title": "Rcpp_matrix_vector_blockSum",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#source-code",
    "title": "Rcpp_matrix_vector_blockSum",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memSum.hpp • Lines 291-395\ninline Rcpp::RObject Rcpp_matrix_vector_blockSum( T  A, T  B,  \n                                 Rcpp::Nullable&lt;bool&gt; bparal, Rcpp::Nullable&lt;int&gt; threads)\n    {\n        \n        // NOTA: Per defecte, suma per columnes tal i com raja.... \n        \n        // static_assert(std::is_same&lt;T, Eigen::MatrixXd &gt;::value || \n        //               std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n        //               std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value,\n        //               \"Error - type not allowed\");\n        \n        bool btransposed = false;\n        // unsigned int ithreads;\n        hsize_t block_size;\n        \n        Rcpp::NumericMatrix X = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(A);\n        Rcpp::NumericVector Y = Rcpp::as&lt;Rcpp::NumericVector&gt;(B);\n        Rcpp::NumericMatrix C;\n        \n        // Matrix\n        hsize_t M = X.rows(),\n                N = X.cols();\n        \n        // Vector\n        hsize_t K = Y.length();\n        \n        try {\n            \n            if( K==N || K==M) {\n                if ( K == N){\n                    // Sum vector to every col\n                    btransposed = true;\n                    \n                    X = Rcpp::transpose(X);\n                    \n                    //.Comentat 2024-10-29.// hsize_t N = X.rows();\n                    //.Comentat 2024-10-29.// hsize_t M = X.cols();\n                    N = X.rows();\n                    M = X.cols();\n                } \n                \n                std::vector&lt;hsize_t&gt; vsizetoRead;\n                std::vector&lt;hsize_t&gt; vstart;\n                \n                // ithreads = get_number_threads(threads, bparal);\n                \n                C = Rcpp::no_init( M, N);\n                \n                block_size = getMatrixBlockSize( N, M).at(0);\n                \n                // minimum block size: 2 columns\n                if(block_size &lt;= 0 ) {\n                    block_size = M*2;\n                }\n                \n                // Mínimum block size: 2 columns\n                getBlockPositionsSizes( M*N, block_size, vstart, vsizetoRead );\n                // int chunks = vstart.size()/ithreads;\n                \n                #pragma omp parallel num_threads( get_number_threads(threads, bparal) ) shared(A, B, C) //, chunks)\n                {\n                    #pragma omp for schedule (dynamic) // collapse(2)\n                    for (hsize_t ii = 0; ii &lt; vstart.size(); ii ++)\n                    {\n                        // Duplicate vector\n                        std::size_t const no_of_duplicates = vsizetoRead[ii] / Y.length();\n                        \n                        std::vector&lt;double&gt; v = Rcpp::as&lt;std::vector&lt;double&gt; &gt;(Y); \n                        v.reserve(Y.size() * no_of_duplicates);\n                        auto end = std::end(v);\n                        \n                        for(std::size_t i = 1; i &lt; no_of_duplicates; ++i)\n                            v.insert(std::end(v), std::begin(v), end);\n                        \n                        // Sum vector to matrix by columns / rows\n                        if( vstart[ii] + vsizetoRead[ii] &gt;= M*N ) {\n                            std::transform (X.begin() + vstart[ii], X.end(),\n                                            v.begin(), C.begin() + vstart[ii], std::plus&lt;double&gt;());\n                        } else {\n                            std::transform (X.begin() + vstart[ii], X.begin() + vstart[ii] + vsizetoRead[ii],\n                                            v.begin() , C.begin() + vstart[ii], std::plus&lt;double&gt;());   \n                        }\n                    }\n                }\n            \n            } else {\n                \n                Rcpp::Rcout&lt;&lt; \"vector sum error: non-conformable arguments\\n\";\n                return(R_NilValue);\n            }\n            \n    \n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception Rcpp_matrix_vector_blockSum: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n            return(R_NilValue);\n        }\n        \n        if(btransposed == true){\n            Rcpp::transpose(C);\n        } \n        \n        C.attr(\"dim\") = Rcpp::Dimension( M, N);\n        return(C);\n    \n    }",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.html#usage-example",
    "title": "Rcpp_matrix_vector_blockSum",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_matrix_vector_blockSum(...);",
    "crumbs": [
      "Functions",
      "Rcpp_matrix_vector_blockSum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html",
    "title": "Rcpp_vector_divide_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_divide_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#signature",
    "title": "Rcpp_vector_divide_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_divide_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#description",
    "title": "Rcpp_vector_divide_hdf5",
    "section": "2 Description",
    "text": "2 Description\nPure vector division for HDF5 vectors.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#parameters",
    "title": "Rcpp_vector_divide_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input vector dataset (dividend)\ndsB (BigDataStatMeth::hdf5Dataset *): Second input vector dataset (divisor)\ndsC (BigDataStatMeth::hdf5Dataset *): Output vector dataset\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#returns",
    "title": "Rcpp_vector_divide_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nPointer to result dataset",
    "crumbs": [
      "Functions",
      "Rcpp_vector_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#details",
    "title": "Rcpp_vector_divide_hdf5",
    "section": "5 Details",
    "text": "5 Details\nPerforms optimized element-wise division C = A / B where A, B, and C are HDF5 vector datasets. Includes division by zero protection.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#call-graph",
    "title": "Rcpp_vector_divide_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_vector_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#source-code",
    "title": "Rcpp_vector_divide_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectorOperations.hpp • Lines 339-399\ninline BigDataStatMeth::hdf5Dataset* Rcpp_vector_divide_hdf5(\n            BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, BigDataStatMeth::hdf5Dataset* dsC,\n            bool bparal, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n    {\n        try {\n            hsize_t sizeA = validateVector(dsA);\n            hsize_t sizeB = validateVector(dsB);\n            \n            if (sizeA == 0 || sizeB == 0) {\n                Rcpp::Rcout &lt;&lt; \"vector divide error: inputs are not vectors\\n\";\n                return dsC;\n            }\n            \n            if (sizeA != sizeB) {\n                Rcpp::Rcout &lt;&lt; \"vector divide error: non-conformable vector dimensions\\n\";\n                return dsC;\n            }\n            \n            hsize_t rowsA = dsA-&gt;nrows();\n            hsize_t colsA = dsA-&gt;ncols();\n            dsC-&gt;createDataset(colsA, rowsA, \"real\");\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1};\n            std::vector&lt;hsize_t&gt; block = {1, 1};\n            \n            std::vector&lt;double&gt; vdA(sizeA);\n            std::vector&lt;double&gt; vdB(sizeB);\n            \n            dsA-&gt;readDatasetBlock({0, 0}, {rowsA, colsA}, stride, block, vdA.data());\n            dsB-&gt;readDatasetBlock({0, 0}, {dsB-&gt;nrows(), dsB-&gt;ncols()}, stride, block, vdB.data());\n            \n            if (bparal && sizeA &gt; 10000) {\n    #pragma omp parallel num_threads(get_threads(bparal, threads))\n    {\n    #pragma omp for schedule(static)\n        for (hsize_t i = 0; i &lt; sizeA; ++i) {\n            vdA[i] /= vdB[i];  // IEEE 754 handles division by zero\n        }\n    }\n            } else {\n                std::transform(vdA.begin(), vdA.end(), vdB.begin(), vdA.begin(), std::divides&lt;double&gt;());\n            }\n            \n            dsC-&gt;writeDatasetBlock(vdA, {0, 0}, {rowsA, colsA}, stride, block);\n            \n        } catch(H5::FileIException& error) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_divide_hdf5 (File IException)\";\n            // return dsC;\n        } catch(H5::DataSetIException& error) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_divide_hdf5 (DataSet IException)\";\n            // return dsC;\n        } catch(std::exception& ex) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_divide_hdf5: \" &lt;&lt; ex.what();\n            // return dsC;\n        }\n        \n        return dsC;\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_vector_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_vector_divide_hdf5.html#usage-example",
    "title": "Rcpp_vector_divide_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_vector_divide_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_vector_divide_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html",
    "href": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html",
    "title": "Rcpp_vector_multiply_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_multiply_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#signature",
    "title": "Rcpp_vector_multiply_hdf5",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_multiply_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#description",
    "href": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#description",
    "title": "Rcpp_vector_multiply_hdf5",
    "section": "2 Description",
    "text": "2 Description\nPure vector multiplication for HDF5 vectors.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#parameters",
    "title": "Rcpp_vector_multiply_hdf5",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input vector dataset\ndsB (BigDataStatMeth::hdf5Dataset *): Second input vector dataset\ndsC (BigDataStatMeth::hdf5Dataset *): Output vector dataset\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#returns",
    "title": "Rcpp_vector_multiply_hdf5",
    "section": "4 Returns",
    "text": "4 Returns\nPointer to result dataset",
    "crumbs": [
      "Functions",
      "Rcpp_vector_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#details",
    "href": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#details",
    "title": "Rcpp_vector_multiply_hdf5",
    "section": "5 Details",
    "text": "5 Details\nPerforms optimized element-wise multiplication C = A * B where A, B, and C are HDF5 vector datasets. Uses same optimization strategy as vector addition.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#call-graph",
    "href": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#call-graph",
    "title": "Rcpp_vector_multiply_hdf5",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Rcpp_vector_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#source-code",
    "title": "Rcpp_vector_multiply_hdf5",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/vectorOperations.hpp • Lines 263-323\ninline BigDataStatMeth::hdf5Dataset* Rcpp_vector_multiply_hdf5(\n            BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, BigDataStatMeth::hdf5Dataset* dsC,\n            bool bparal, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n    {\n        try {\n            hsize_t sizeA = validateVector(dsA);\n            hsize_t sizeB = validateVector(dsB);\n            \n            if (sizeA == 0 || sizeB == 0) {\n                Rcpp::Rcout &lt;&lt; \"vector multiply error: inputs are not vectors\\n\";\n                return dsC;\n            }\n            \n            if (sizeA != sizeB) {\n                Rcpp::Rcout &lt;&lt; \"vector multiply error: non-conformable vector dimensions\\n\";\n                return dsC;\n            }\n            \n            hsize_t rowsA = dsA-&gt;nrows();\n            hsize_t colsA = dsA-&gt;ncols();\n            dsC-&gt;createDataset(colsA, rowsA, \"real\");\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1};\n            std::vector&lt;hsize_t&gt; block = {1, 1};\n            \n            std::vector&lt;double&gt; vdA(sizeA);\n            std::vector&lt;double&gt; vdB(sizeB);\n            \n            dsA-&gt;readDatasetBlock({0, 0}, {rowsA, colsA}, stride, block, vdA.data());\n            dsB-&gt;readDatasetBlock({0, 0}, {dsB-&gt;nrows(), dsB-&gt;ncols()}, stride, block, vdB.data());\n            \n            if (bparal && sizeA &gt; 10000) {\n    #pragma omp parallel num_threads(get_threads(bparal, threads))\n    {\n    #pragma omp for schedule(static)\n        for (hsize_t i = 0; i &lt; sizeA; ++i) {\n            vdA[i] *= vdB[i];\n        }\n    }\n            } else {\n                std::transform(vdA.begin(), vdA.end(), vdB.begin(), vdA.begin(), std::multiplies&lt;double&gt;());\n            }\n            \n            dsC-&gt;writeDatasetBlock(vdA, {0, 0}, {rowsA, colsA}, stride, block);\n            \n        } catch(H5::FileIException& error) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_multiply_hdf5 (File IException)\";\n            // return dsC;\n        } catch(H5::DataSetIException& error) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_multiply_hdf5 (DataSet IException)\";\n            // return dsC;\n        } catch(std::exception& ex) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception Rcpp_vector_multiply_hdf5: \" &lt;&lt; ex.what();\n            // return dsC;\n        }\n        \n        return dsC;\n    }",
    "crumbs": [
      "Functions",
      "Rcpp_vector_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.html#usage-example",
    "title": "Rcpp_vector_multiply_hdf5",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_vector_multiply_hdf5(...);",
    "crumbs": [
      "Functions",
      "Rcpp_vector_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_substract.html",
    "href": "api-reference/cpp/functions/Rcpp_vector_substract.html",
    "title": "Rcpp_vector_substract",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_vector_substract(T A, T B)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_substract.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_vector_substract.html#signature",
    "title": "Rcpp_vector_substract",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_vector_substract(T A, T B)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_substract.html#description",
    "href": "api-reference/cpp/functions/Rcpp_vector_substract.html#description",
    "title": "Rcpp_vector_substract",
    "section": "2 Description",
    "text": "2 Description\nVector subtraction.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_substract.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_vector_substract.html#parameters",
    "title": "Rcpp_vector_substract",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nA (T): First input vector\nB (T): Second input vector (subtracted from A)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_substract.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_vector_substract.html#returns",
    "title": "Rcpp_vector_substract",
    "section": "4 Returns",
    "text": "4 Returns\nResult of vector subtraction (A - B)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_substract.html#details",
    "href": "api-reference/cpp/functions/Rcpp_vector_substract.html#details",
    "title": "Rcpp_vector_substract",
    "section": "5 Details",
    "text": "5 Details\nSubtracts two vectors element-wise.",
    "crumbs": [
      "Functions",
      "Rcpp_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_substract.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_vector_substract.html#source-code",
    "title": "Rcpp_vector_substract",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memSubstract.hpp • Lines 223-242\ninline Rcpp::RObject Rcpp_vector_substract ( T  A, T  B)\n    {\n        \n        Rcpp::NumericVector v = Rcpp::as&lt;Rcpp::NumericVector&gt;(A);\n        Rcpp::NumericVector v2 = Rcpp::as&lt;Rcpp::NumericVector&gt;(B);\n        \n        if(v.size() == v2.size()) {\n            \n            Rcpp::NumericVector C = Rcpp::no_init( v.size());\n            \n            std::transform (v.begin(), v.end(), v2.begin(), C.begin(), std::minus&lt;double&gt;());\n            \n            C.attr(\"dim\") = Rcpp::Dimension( C.size(), 1); \n            \n            return(C);\n        }\n        \n        return(R_NilValue);\n        \n    }",
    "crumbs": [
      "Functions",
      "Rcpp_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_substract.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_vector_substract.html#usage-example",
    "title": "Rcpp_vector_substract",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_vector_substract(...);",
    "crumbs": [
      "Functions",
      "Rcpp_vector_substract"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_sum.html",
    "href": "api-reference/cpp/functions/Rcpp_vector_sum.html",
    "title": "Rcpp_vector_sum",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_vector_sum(T A, T B)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_sum.html#signature",
    "href": "api-reference/cpp/functions/Rcpp_vector_sum.html#signature",
    "title": "Rcpp_vector_sum",
    "section": "",
    "text": "Rcpp::RObject BigDataStatMeth::Rcpp_vector_sum(T A, T B)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_sum.html#parameters",
    "href": "api-reference/cpp/functions/Rcpp_vector_sum.html#parameters",
    "title": "Rcpp_vector_sum",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nA (T)\nB (T)",
    "crumbs": [
      "Functions",
      "Rcpp_vector_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_sum.html#returns",
    "href": "api-reference/cpp/functions/Rcpp_vector_sum.html#returns",
    "title": "Rcpp_vector_sum",
    "section": "3 Returns",
    "text": "3 Returns\nType: typename T",
    "crumbs": [
      "Functions",
      "Rcpp_vector_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_sum.html#source-code",
    "href": "api-reference/cpp/functions/Rcpp_vector_sum.html#source-code",
    "title": "Rcpp_vector_sum",
    "section": "4 Source Code",
    "text": "4 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memSum.hpp • Lines 154-172\ninline Rcpp::RObject Rcpp_vector_sum ( T  A, T  B)\n    {\n        \n        Rcpp::NumericVector v = Rcpp::as&lt;Rcpp::NumericVector&gt;(A);\n        Rcpp::NumericVector v2 = Rcpp::as&lt;Rcpp::NumericVector&gt;(B);\n        \n        if(v.size() == v2.size()) {\n            Rcpp::NumericVector C = Rcpp::no_init( v.size());\n            \n            std::transform (v.begin(), v.end(), v2.begin(), C.begin(), std::plus&lt;double&gt;());\n            \n            C.attr(\"dim\") = Rcpp::Dimension( C.size(), 1); \n            \n            return(C);\n        }\n        \n        return(R_NilValue);\n        \n    }",
    "crumbs": [
      "Functions",
      "Rcpp_vector_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Rcpp_vector_sum.html#usage-example",
    "href": "api-reference/cpp/functions/Rcpp_vector_sum.html#usage-example",
    "title": "Rcpp_vector_sum",
    "section": "5 Usage Example",
    "text": "5 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Rcpp_vector_sum(...);",
    "crumbs": [
      "Functions",
      "Rcpp_vector_sum"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html",
    "title": "RcppbdCorr_hdf5_Block_single",
    "section": "",
    "text": "void BigDataStatMeth::RcppbdCorr_hdf5_Block_single(T *dsA, BigDataStatMeth::hdf5Dataset *dsCorr, BigDataStatMeth::hdf5Dataset *dsPval, const std::string &method=\"pearson\", bool use_complete_obs=true, bool compute_pvalues=false, bool trans_x=false, int block_size=1000, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html#signature",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html#signature",
    "title": "RcppbdCorr_hdf5_Block_single",
    "section": "",
    "text": "void BigDataStatMeth::RcppbdCorr_hdf5_Block_single(T *dsA, BigDataStatMeth::hdf5Dataset *dsCorr, BigDataStatMeth::hdf5Dataset *dsPval, const std::string &method=\"pearson\", bool use_complete_obs=true, bool compute_pvalues=false, bool trans_x=false, int block_size=1000, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html#parameters",
    "title": "RcppbdCorr_hdf5_Block_single",
    "section": "2 Parameters",
    "text": "2 Parameters\n\ndsA (T *)\ndsCorr (BigDataStatMeth::hdf5Dataset *)\ndsPval (BigDataStatMeth::hdf5Dataset *)\nmethod (const std::string &)\nuse_complete_obs (bool)\ncompute_pvalues (bool)\ntrans_x (bool)\nblock_size (int)\nthreads (Rcpp::Nullable&lt; int &gt;)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html#returns",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html#returns",
    "title": "RcppbdCorr_hdf5_Block_single",
    "section": "3 Returns",
    "text": "3 Returns\nType: class T",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html#call-graph",
    "title": "RcppbdCorr_hdf5_Block_single",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html#source-code",
    "title": "RcppbdCorr_hdf5_Block_single",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixCorrelation.hpp • Lines 1233-1366\ninline void RcppbdCorr_hdf5_Block_single(T* dsA, \n                                             BigDataStatMeth::hdf5Dataset* dsCorr,\n                                             BigDataStatMeth::hdf5Dataset* dsPval,\n                                             const std::string& method = \"pearson\",\n                                             bool use_complete_obs = true,\n                                             bool compute_pvalues = false,\n                                             bool trans_x = false,\n                                             int block_size = 1000,\n                                             Rcpp::Nullable&lt;int&gt; threads = R_NilValue) {\n        \n        static_assert(std::is_same&lt;T*, BigDataStatMeth::hdf5Dataset*&gt;::value ||\n                      std::is_same&lt;T*, BigDataStatMeth::hdf5DatasetInternal*&gt;::value,\n                      \"Error - type not allowed\");\n        \n        try {\n            std::vector&lt;hsize_t&gt; stride = {1, 1}, block = {1, 1};\n            \n            // CRITICAL FIX: R→HDF5 data is implicitly transposed\n            // HDF5 dimensions are NOT the \"real\" data dimensions from R\n            hsize_t n_rows_hdf5 = dsA-&gt;nrows();   // Variables in HDF5 (e.g., genes)\n            hsize_t n_cols_hdf5 = dsA-&gt;ncols();   // Observations in HDF5 (e.g., samples)\n            \n            // Real dimensions (as they were in R before saving to HDF5):\n            hsize_t n_obs_real = n_cols_hdf5;     // Real observations (samples)\n            hsize_t n_vars_real = n_rows_hdf5;    // Real variables (genes)\n            \n            // Apply user transpose logic to REAL dimensions\n            hsize_t n_rows = trans_x ? n_vars_real : n_obs_real;   // Effective observations after user transpose\n            hsize_t n_cols = trans_x ? n_obs_real : n_vars_real;   // Effective variables after user transpose\n            \n            // Strategy: Read entire matrix if possible (most cases)\n            const hsize_t MEMORY_LIMIT = 500000; // 500K elements ~ 4GB for double\n            \n            if (n_rows_hdf5 * n_cols_hdf5 &lt; MEMORY_LIMIT) {\n                \n                // Load full matrix and use efficient in-memory correlation with transpose\n                std::vector&lt;double&gt; matrix_data(n_rows_hdf5 * n_cols_hdf5);\n                dsA-&gt;readDatasetBlock({0, 0}, {n_rows_hdf5, n_cols_hdf5}, stride, block, matrix_data.data());\n                \n                // CRITICAL FIX: Create Eigen matrix accounting for R→HDF5 transposition\n                // Data in HDF5 is transposed compared to how it was in R\n                Eigen::MatrixXd X = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt;(\n                    matrix_data.data(), n_rows_hdf5, n_cols_hdf5);\n                \n                corr_result result = RcppbdCorr_matrix_single(X, method, use_complete_obs, compute_pvalues, trans_x, threads);\n                \n                if (!result.bcomputed) {\n                    checkClose_file(dsA, dsCorr, dsPval);\n                    throw std::runtime_error(\"In-memory correlation computation failed\");\n                }\n                \n                // Write results\n                dsCorr-&gt;createDataset(result.correlation_matrix.rows(), result.correlation_matrix.cols(), \"real\");\n                dsCorr-&gt;writeDataset(Rcpp::wrap(result.correlation_matrix));\n                \n                // Write p-values matrix if computed (vectorized result)\n                if (compute_pvalues && result.has_pvalues && dsPval && result.pvalues.rows() &gt; 0 && result.pvalues.cols() &gt; 0) {\n                    dsPval-&gt;createDataset(result.pvalues.rows(), result.pvalues.cols(), \"real\");\n                    dsPval-&gt;writeDataset(Rcpp::wrap(result.pvalues));\n                }\n                \n            } else {\n                \n                Eigen::MatrixXd corr_matrix = Eigen::MatrixXd::Identity(n_cols, n_cols);\n                \n                \n                \n#ifdef _OPENMP\n                int num_threads = std::min(2, omp_get_max_threads());\n            #pragma omp parallel for num_threads(num_threads) schedule(dynamic, 1)\n#endif\n                for (hsize_t i = 0; i &lt; n_cols; ++i) {\n                    std::vector&lt;double&gt; vec_i_data(n_rows);\n                    \n                    // CRITICAL FIX: Transpose-aware I/O patterns for HDF5 data (R→HDF5 transposition)\n                    if (trans_x) {\n                        // User wants correlation between samples\n                        // In HDF5: samples are columns, so read column i\n                        dsA-&gt;readDatasetBlock({0, i}, {n_rows_hdf5, 1}, stride, block, vec_i_data.data());\n                    } else {\n                        // User wants correlation between variables (default)\n                        // In HDF5: variables are rows, so read row i\n                        dsA-&gt;readDatasetBlock({i, 0}, {1, n_cols_hdf5}, stride, block, vec_i_data.data());\n                    }\n                    \n                    Eigen::VectorXd vec_i = Eigen::Map&lt;Eigen::VectorXd&gt;(vec_i_data.data(), n_rows);\n                    \n                    // Compute correlations for upper triangle (symmetric matrix)\n                    for (hsize_t j = i + 1; j &lt; n_cols; ++j) {\n                        std::vector&lt;double&gt; vec_j_data(n_rows);\n                        \n                        // Consistent reading for second vector (final correction)\n                        if (trans_x) {\n                            // User wants correlation between samples\n                            // Read column j (this currently gives correct cor(ds) result)\n                            dsA-&gt;readDatasetBlock({0, j}, {n_rows_hdf5, 1}, stride, block, vec_j_data.data());\n                        } else {\n                            // User wants correlation between variables (default - same as cor(ds))\n                            // Read row j (swap to make this give cor(ds) result)\n                            dsA-&gt;readDatasetBlock({j, 0}, {1, n_cols_hdf5}, stride, block, vec_j_data.data());\n                        }\n                        \n                        Eigen::VectorXd vec_j = Eigen::Map&lt;Eigen::VectorXd&gt;(vec_j_data.data(), n_rows);\n                        \n                        double corr_val;\n                        if (method == \"spearman\") {\n                            corr_val = spearman_correlation(vec_i, vec_j, use_complete_obs);\n                        } else {\n                            corr_val = pearson_correlation(vec_i, vec_j, use_complete_obs);\n                        }\n                        \n                        corr_matrix(i, j) = corr_val;\n                        corr_matrix(j, i) = corr_val;\n                        \n                    }\n                }\n                \n                dsCorr-&gt;createDataset(corr_matrix.rows(), corr_matrix.cols(), \"real\");\n                dsCorr-&gt;writeDataset(Rcpp::wrap(corr_matrix));\n                \n                if (compute_pvalues && dsPval) {\n                    Eigen::MatrixXd pvalues_matrix = compute_pvalues_optimized(corr_matrix, n_rows, true);\n                    dsPval-&gt;createDataset(pvalues_matrix.rows(), pvalues_matrix.cols(), \"real\");\n                    dsPval-&gt;writeDataset(Rcpp::wrap(pvalues_matrix));\n                }\n            }\n            \n            \n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsCorr, dsPval);\n            Rcpp::Rcerr &lt;&lt; \"C++ exception RcppbdCorr_hdf5_Block_single: \" &lt;&lt; ex.what() &lt;&lt; std::endl;\n            throw;\n        }\n    }",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.html#usage-example",
    "title": "RcppbdCorr_hdf5_Block_single",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdCorr_hdf5_Block_single(...);",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_Block_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html",
    "title": "RcppbdCorr_hdf5_single",
    "section": "",
    "text": "Rcpp::List BigDataStatMeth::RcppbdCorr_hdf5_single(const std::string &filename, const std::string &strsubgroup, const std::string &strdataset, const std::string &method, bool use_complete_obs, bool compute_pvalues, int block_size, bool bforce, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_group, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_dataset_corr, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_dataset_pval, bool trans_x, const Rcpp::Nullable&lt; int &gt; &threads)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#signature",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#signature",
    "title": "RcppbdCorr_hdf5_single",
    "section": "",
    "text": "Rcpp::List BigDataStatMeth::RcppbdCorr_hdf5_single(const std::string &filename, const std::string &strsubgroup, const std::string &strdataset, const std::string &method, bool use_complete_obs, bool compute_pvalues, int block_size, bool bforce, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_group, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_dataset_corr, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_dataset_pval, bool trans_x, const Rcpp::Nullable&lt; int &gt; &threads)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#description",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#description",
    "title": "RcppbdCorr_hdf5_single",
    "section": "2 Description",
    "text": "2 Description\nImplementation of single matrix correlation computation for HDF5 matrices - OPTIMIZED.",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#parameters",
    "title": "RcppbdCorr_hdf5_single",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfilename (const std::string &): Path to HDF5 file containing the matrix\nstrsubgroup (const std::string &): Group path within HDF5 file\nstrdataset (const std::string &): Dataset name within the group\nmethod (const std::string &): Correlation method (“pearson” or “spearman”)\nuse_complete_obs (bool): Whether to use only complete observations\ncompute_pvalues (bool): Whether to compute p-values (performance impact)\nblock_size (int): Block size for large matrix processing (default: 1000)\nbforce (bool): Whether to overwrite existing results\noutput_group (const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &): Custom output group name (nullable)\noutput_dataset_corr (const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &): Custom correlation dataset name (nullable)\noutput_dataset_pval (const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &): Custom p-values dataset name (nullable)\ntrans_x (bool): Whether to transpose the matrix (samples vs variables correlation)\nthreads (const Rcpp::Nullable&lt; int &gt; &): Number of threads for parallel computation (nullable)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#returns",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#returns",
    "title": "RcppbdCorr_hdf5_single",
    "section": "4 Returns",
    "text": "4 Returns\nRcpp::List containing dataset locations and computation metadata",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#details",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#details",
    "title": "RcppbdCorr_hdf5_single",
    "section": "5 Details",
    "text": "5 Details\nOptimized implementation that computes correlation matrix for a single HDF5 dataset. Uses intelligent method selection between direct computation and block-wise processing based on matrix size and available memory.",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#call-graph",
    "title": "RcppbdCorr_hdf5_single",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#source-code",
    "title": "RcppbdCorr_hdf5_single",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixCorrelation.hpp • Lines 1403-1562\ninline Rcpp::List RcppbdCorr_hdf5_single(const std::string& filename, \n                                             const std::string& strsubgroup, \n                                             const std::string& strdataset,\n                                             const std::string& method, \n                                             bool use_complete_obs,\n                                             bool compute_pvalues, \n                                             int block_size, \n                                             bool bforce,\n                                             const Rcpp::Nullable&lt;Rcpp::CharacterVector&gt;& output_group,\n                                             const Rcpp::Nullable&lt;Rcpp::CharacterVector&gt;& output_dataset_corr,\n                                             const Rcpp::Nullable&lt;Rcpp::CharacterVector&gt;& output_dataset_pval,\n                                             bool trans_x,\n                                             const Rcpp::Nullable&lt;int&gt;& threads) {\n        \n        BigDataStatMeth::hdf5Dataset* dsA = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsCorr = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsPval = nullptr;\n        \n        try {\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1}, block = {1, 1}, offset = {0, 0}, count = {0, 0};\n            \n            // Open input dataset\n            dsA = new BigDataStatMeth::hdf5Dataset(filename, strsubgroup, strdataset, false);\n            dsA-&gt;openDataset();\n            \n            if (dsA-&gt;getDatasetptr() == nullptr) {\n                checkClose_file(dsA);\n                throw std::runtime_error(\"Failed to open input dataset\");\n            }\n            \n            // Determine output group and dataset names\n            std::string stroutgroup;\n            std::string corr_dataset_name;\n            std::string pval_dataset_name;\n            \n            if (output_group.isNull()) {\n                stroutgroup = \"CORR/\" + strdataset;\n            } else {\n                Rcpp::CharacterVector group_vec = Rcpp::as&lt;Rcpp::CharacterVector&gt;(output_group);\n                stroutgroup = Rcpp::as&lt;std::string&gt;(group_vec[0]);\n            }\n            \n            if (output_dataset_corr.isNull()) {\n                corr_dataset_name = \"correlation\";\n            } else {\n                Rcpp::CharacterVector corr_vec = Rcpp::as&lt;Rcpp::CharacterVector&gt;(output_dataset_corr);\n                corr_dataset_name = Rcpp::as&lt;std::string&gt;(corr_vec[0]);\n            }\n            \n            if (output_dataset_pval.isNull()) {\n                pval_dataset_name = \"pvalues\";\n            } else {\n                Rcpp::CharacterVector pval_vec = Rcpp::as&lt;Rcpp::CharacterVector&gt;(output_dataset_pval);\n                pval_dataset_name = Rcpp::as&lt;std::string&gt;(pval_vec[0]);\n            }\n            \n            // Get matrix dimensions\n            hsize_t n_rows_orig = dsA-&gt;nrows();\n            hsize_t n_cols_orig = dsA-&gt;ncols();\n            \n            // Effective dimensions after transposition\n            hsize_t n_rows = trans_x ? n_cols_orig : n_rows_orig;\n            hsize_t n_cols = trans_x ? n_rows_orig : n_cols_orig;\n            count = {n_rows_orig, n_cols_orig};\n            \n            // Create output datasets\n            try {\n                dsCorr = new BigDataStatMeth::hdf5Dataset(filename, stroutgroup, corr_dataset_name, bforce);\n                if (compute_pvalues) {\n                    dsPval = new BigDataStatMeth::hdf5Dataset(filename, stroutgroup, pval_dataset_name, bforce);\n                }\n            } catch (const std::exception& e) {\n                Rcpp::Rcerr &lt;&lt; \"Error creating output datasets: \" &lt;&lt; e.what() &lt;&lt; std::endl;\n                checkClose_file(dsA, dsCorr, dsPval);\n                return R_NilValue;\n            }\n            \n            // Automatic method selection based on matrix size\n            const hsize_t DIRECT_COMPUTATION_THRESHOLD = MAXELEMSINBLOCK / 4;\n            \n            if (n_rows * n_cols &lt; DIRECT_COMPUTATION_THRESHOLD) {\n                \n                // Direct computation for small matrices\n                std::vector&lt;double&gt; vdA(count[0] * count[1]);\n                dsA-&gt;readDatasetBlock({offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdA.data());\n                \n                // Convert to Eigen matrix (row-major)\n                Eigen::MatrixXd X = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt;(\n                    vdA.data(), count[0], count[1]);\n                \n                // Compute correlation\n                corr_result result = RcppbdCorr_matrix_single(X, method, use_complete_obs, compute_pvalues, trans_x, threads);\n                \n                if (!result.bcomputed) {\n                    checkClose_file(dsA, dsCorr, dsPval);\n                    throw std::runtime_error(\"Single matrix correlation computation failed\");\n                }\n                \n                // Write correlation matrix\n                if (dsCorr-&gt;getDatasetptr() == nullptr) {\n                    dsCorr-&gt;createDataset(result.correlation_matrix.rows(), result.correlation_matrix.cols(), \"real\");\n                }\n                dsCorr-&gt;writeDataset(Rcpp::wrap(result.correlation_matrix));\n\n                \n                // Write p-values matrix if computed\n                if (compute_pvalues && result.has_pvalues && dsPval && result.pvalues.rows() &gt; 0 && result.pvalues.cols() &gt; 0) {\n                    if (dsPval-&gt;getDatasetptr() == nullptr) {\n                        dsPval-&gt;createDataset(result.pvalues.rows(), result.pvalues.cols(), \"real\");\n                    }\n                    dsPval-&gt;writeDataset(Rcpp::wrap(result.pvalues));\n                }                \n                \n            } else {\n                \n                // Block-wise computation for large matrices - CRITICAL path for big-omics\n                if (dsA-&gt;getDatasetptr() != nullptr) {\n                    RcppbdCorr_hdf5_Block_single(dsA, dsCorr, dsPval, method, use_complete_obs, \n                                                           compute_pvalues, trans_x, block_size, threads);\n                }\n            }\n            \n            // Clean up datasets\n            delete dsA; dsA = nullptr;\n            delete dsCorr; dsCorr = nullptr;\n            delete dsPval; dsPval = nullptr;\n            \n            // Return comprehensive result list\n            return Rcpp::List::create(\n                Rcpp::Named(\"filename\") = filename,\n                Rcpp::Named(\"group\") = stroutgroup,\n                Rcpp::Named(\"correlation\") = corr_dataset_name,\n                Rcpp::Named(\"method\") = method,\n                Rcpp::Named(\"correlation_type\") = \"single\",\n                Rcpp::Named(\"n_variables\") = (int)n_cols,\n                Rcpp::Named(\"n_observations\") = (int)n_rows,\n                Rcpp::Named(\"use_complete_obs\") = use_complete_obs,\n                Rcpp::Named(\"pvalues\") = compute_pvalues ? pval_dataset_name : \"\",\n                Rcpp::Named(\"has_pvalues\") = compute_pvalues\n            );\n            \n        } catch(H5::FileIException& error) {\n            checkClose_file(dsA, dsCorr, dsPval);\n            Rcpp::Rcerr &lt;&lt; \"\\nC++ exception RcppbdCorr_hdf5_single (File IException): \" &lt;&lt; error.getDetailMsg() &lt;&lt; std::endl;\n            return R_NilValue;\n        } catch(H5::DataSetIException& error) {\n            checkClose_file(dsA, dsCorr, dsPval);\n            Rcpp::Rcerr &lt;&lt; \"\\nC++ exception RcppbdCorr_hdf5_single (DataSet IException): \" &lt;&lt; error.getDetailMsg() &lt;&lt; std::endl;\n            return R_NilValue;\n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsCorr, dsPval);\n            Rcpp::Rcerr &lt;&lt; \"C++ exception RcppbdCorr_hdf5_single: \" &lt;&lt; ex.what() &lt;&lt; std::endl;\n            return R_NilValue;\n        } catch (...) {\n            checkClose_file(dsA, dsCorr, dsPval);\n            Rcpp::Rcerr &lt;&lt; \"\\nC++ exception RcppbdCorr_hdf5_single (unknown reason)\" &lt;&lt; std::endl;\n            return R_NilValue;\n        }\n    }",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdCorr_hdf5_single.html#usage-example",
    "title": "RcppbdCorr_hdf5_single",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdCorr_hdf5_single(...);",
    "crumbs": [
      "Functions",
      "RcppbdCorr_hdf5_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html",
    "title": "RcppbdCorr_matrix_single",
    "section": "",
    "text": "corr_result BigDataStatMeth::RcppbdCorr_matrix_single(Eigen::MatrixXd &X, const std::string &method=\"pearson\", bool use_complete_obs=true, bool compute_pvalues=false, bool trans_x=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html#signature",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html#signature",
    "title": "RcppbdCorr_matrix_single",
    "section": "",
    "text": "corr_result BigDataStatMeth::RcppbdCorr_matrix_single(Eigen::MatrixXd &X, const std::string &method=\"pearson\", bool use_complete_obs=true, bool compute_pvalues=false, bool trans_x=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html#parameters",
    "title": "RcppbdCorr_matrix_single",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nX (Eigen::MatrixXd &)\nmethod (const std::string &)\nuse_complete_obs (bool)\ncompute_pvalues (bool)\ntrans_x (bool)\nthreads (Rcpp::Nullable&lt; int &gt;)",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html#returns",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html#returns",
    "title": "RcppbdCorr_matrix_single",
    "section": "3 Returns",
    "text": "3 Returns\nType: corr_result",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html#call-graph",
    "title": "RcppbdCorr_matrix_single",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html#source-code",
    "title": "RcppbdCorr_matrix_single",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixCorrelation.hpp • Lines 728-943\ninline corr_result RcppbdCorr_matrix_single(Eigen::MatrixXd& X,\n                                        const std::string& method = \"pearson\",\n                                        bool use_complete_obs = true,\n                                        bool compute_pvalues = false,\n                                        bool trans_x = false,\n                                        Rcpp::Nullable&lt;int&gt; threads = R_NilValue) {\n        \n        corr_result result;\n        result.trans_x = trans_x;\n        \n        try {\n            \n            Eigen::MatrixXd Xt;\n            \n            // Determine dimensions after potential transposition\n            int n_obs, n_vars;\n            if (trans_x) {\n                n_obs = X.cols();  // Original variables become observations\n                n_vars = X.rows(); // Original observations become variables\n            } else {\n                n_obs = X.rows();  // Standard: rows are observations\n                n_vars = X.cols(); // Standard: columns are variables\n            }\n            \n            result.correlation_matrix = Eigen::MatrixXd::Identity(n_vars, n_vars);\n            result.method = method;\n            result.n_obs = n_obs;\n            result.n_vars_x = n_vars;\n            result.n_vars_y = n_vars;\n            result.has_pvalues = compute_pvalues; // Disabled for performance\n            \n            \n            if (method == \"pearson\") {\n                // Skip expensive allFinite check if user says use_complete_obs=false\n                bool has_missing = use_complete_obs ? !X.allFinite() : false;\n                \n                if (!has_missing) {\n\n                    if (trans_x) {\n                        // For transpose case, work with rows (correlate observations)\n                        Eigen::MatrixXd X_rows_centered = X.colwise() - X.rowwise().mean();\n                        Eigen::MatrixXd cov = (X_rows_centered * X_rows_centered.transpose()) / (n_obs - 1);\n                        \n                        // Convert covariance to correlation\n                        Eigen::VectorXd inv_std_devs = cov.diagonal().array().sqrt().inverse();\n                        \n                        // Handle near-zero standard deviations\n                        for (int i = 0; i &lt; n_vars; ++i) {\n                            if (cov(i, i) &lt;= 1e-28) { // std_dev^2 threshold\n                                inv_std_devs(i) = 0.0;\n                            }\n                        }\n                        \n                        // Vectorized correlation: R = D^(-1) * Cov * D^(-1)\n                        result.correlation_matrix = inv_std_devs.asDiagonal() * cov * inv_std_devs.asDiagonal();\n                    } else {\n                        // Original path for normal case (correlate variables)\n                        Eigen::MatrixXd X_centered = X.rowwise() - X.colwise().mean();\n                        Eigen::MatrixXd cov = (X_centered.transpose() * X_centered) / (n_obs - 1);\n                        \n                        // Convert covariance to correlation\n                        Eigen::VectorXd inv_std_devs = cov.diagonal().array().sqrt().inverse();\n                        \n                        // Handle near-zero standard deviations\n                        for (int i = 0; i &lt; n_vars; ++i) {\n                            if (cov(i, i) &lt;= 1e-28) { // std_dev^2 threshold\n                                inv_std_devs(i) = 0.0;\n                            }\n                        }\n                        \n                        // Vectorized correlation: R = D^(-1) * Cov * D^(-1)\n                        result.correlation_matrix = inv_std_devs.asDiagonal() * cov * inv_std_devs.asDiagonal();\n                    }\n                    \n                    // Set diagonal to 1.0 and fix rows/cols with zero std dev\n                    result.correlation_matrix.diagonal().setOnes();\n                    for (int i = 0; i &lt; n_vars; ++i) {\n                        Eigen::VectorXd inv_std_devs = result.correlation_matrix.diagonal().array().sqrt().inverse();\n                        if (inv_std_devs(i) == 0.0) {\n                            result.correlation_matrix.row(i).setZero();\n                            result.correlation_matrix.col(i).setZero();\n                            result.correlation_matrix(i, i) = 1.0;\n                        }\n                    }\n                    \n                    // Compute p-values using optimized vectorized function\n                    if (compute_pvalues) {\n                        result.pvalues = compute_pvalues_optimized(result.correlation_matrix, n_obs, true);\n                    }\n                    \n                    result.bcomputed = true;\n                    return result;\n                }\n            }\n            \n            if (n_vars &lt;= 500) {\n                // Direct approach - faster than blocks for small/medium matrices\n                \n#pragma omp parallel for schedule(dynamic) if(n_vars &gt; 100)\n                for (int i = 0; i &lt; n_vars; ++i) {\n                    for (int j = i + 1; j &lt; n_vars; ++j) {\n                        // OPTIMIZED: Intelligent access pattern - no matrix copy for transpose\n                        Eigen::VectorXd vec_i, vec_j;\n                        if (trans_x) {\n                            vec_i = X.row(i);  // Row access for transposed case\n                            vec_j = X.row(j);\n                        } else {\n                            vec_i = X.col(i);  // Column access for normal case\n                            vec_j = X.col(j);\n                        }\n                        \n                        double corr_val;\n                        if (method == \"spearman\") {\n                            corr_val = spearman_correlation(vec_i, vec_j, use_complete_obs);\n                        } else {\n                            corr_val = pearson_correlation(vec_i, vec_j, use_complete_obs);\n                        }\n                        \n                        result.correlation_matrix(i, j) = corr_val;\n                        result.correlation_matrix(j, i) = corr_val;\n                    }\n                }\n                \n                // Compute p-values using optimized vectorized function\n                if (compute_pvalues) {\n                    result.pvalues = compute_pvalues_optimized(result.correlation_matrix, n_obs, true);\n                }\n                \n                result.bcomputed = true;\n                return result;\n            }\n            \n            \n            // FALLBACK: Block-wise computation for large matrices (n_vars &gt; 500)\n                int block_size = std::min(128, std::max(32, n_vars / 8));\n            int num_blocks = (n_vars + block_size - 1) / block_size;\n            \n#pragma omp parallel for schedule(dynamic) if(n_vars &gt; 1000)\n            for (int i_block = 0; i_block &lt; num_blocks; ++i_block) {\n                for (int j_block = i_block; j_block &lt; num_blocks; ++j_block) {\n                    int i_start = i_block * block_size;\n                    int i_end = std::min(i_start + block_size, n_vars);\n                    int i_size = i_end - i_start;\n                    \n                    int j_start = j_block * block_size;\n                    int j_end = std::min(j_start + block_size, n_vars);\n                    int j_size = j_end - j_start;\n                    \n                    if (i_block == j_block) {\n                        // Diagonal block - compute upper triangle only\n                        for (int i = 0; i &lt; i_size; ++i) {\n                            for (int j = i + 1; j &lt; j_size; ++j) {\n                                // OPTIMIZED: Direct access with transpose-aware pattern (no middleCols copies)\n                                Eigen::VectorXd vec_i, vec_j;\n                                if (trans_x) {\n                                    vec_i = X.row(i_start + i);  // Row access for transpose\n                                    vec_j = X.row(j_start + j);\n                                } else {\n                                    vec_i = X.col(i_start + i);  // Column access for normal\n                                    vec_j = X.col(j_start + j);\n                                }\n                                \n                                double corr_val;\n                                if (method == \"spearman\") {\n                                    corr_val = spearman_correlation(vec_i, vec_j, use_complete_obs);\n                                } else {\n                                    corr_val = pearson_correlation(vec_i, vec_j, use_complete_obs);\n                                }\n                                \n                                result.correlation_matrix(i_start + i, j_start + j) = corr_val;\n                                result.correlation_matrix(j_start + j, i_start + i) = corr_val;\n                            }\n                        }\n                    } else {\n                        // Off-diagonal block - compute all pairs\n                        for (int i = 0; i &lt; i_size; ++i) {\n                            for (int j = 0; j &lt; j_size; ++j) {\n                                // OPTIMIZED: Direct access with transpose-aware pattern (no middleCols copies)\n                                Eigen::VectorXd vec_i, vec_j;\n                                if (trans_x) {\n                                    vec_i = X.row(i_start + i);  // Row access for transpose\n                                    vec_j = X.row(j_start + j);\n                                } else {\n                                    vec_i = X.col(i_start + i);  // Column access for normal\n                                    vec_j = X.col(j_start + j);\n                                }\n                                \n                                double corr_val;\n                                if (method == \"spearman\") {\n                                    corr_val = spearman_correlation(vec_i, vec_j, use_complete_obs);\n                                } else {\n                                    corr_val = pearson_correlation(vec_i, vec_j, use_complete_obs);\n                                }\n                                \n                                result.correlation_matrix(i_start + i, j_start + j) = corr_val;\n                                result.correlation_matrix(j_start + j, i_start + i) = corr_val;\n                            }\n                        }\n                    }\n                }\n            }\n            \n            // Compute p-values using optimized vectorized function\n            if (compute_pvalues) {\n                result.pvalues = compute_pvalues_optimized(result.correlation_matrix, n_obs, true);\n            }\n            \n            result.bcomputed = true;\n            \n        } catch (std::exception &ex) {\n            Rcpp::Rcerr &lt;&lt; \"C++ exception RcppbdCorr_matrix_single: \" &lt;&lt; ex.what() &lt;&lt; std::endl;\n            result.bcomputed = false;\n        }\n        \n        return result;\n    }",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdCorr_matrix_single.html#usage-example",
    "title": "RcppbdCorr_matrix_single",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdCorr_matrix_single(...);",
    "crumbs": [
      "Functions",
      "RcppbdCorr_matrix_single"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html",
    "title": "RcppbdEigen_hdf5_Block",
    "section": "",
    "text": "void BigDataStatMeth::RcppbdEigen_hdf5_Block(T *dsA, BigDataStatMeth::hdf5Dataset *dsd, BigDataStatMeth::hdf5Dataset *dsu, int k, const std::string &which, int ncv, bool bcenter, bool bscale, double tol, int max_iter, bool compute_vectors, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#signature",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#signature",
    "title": "RcppbdEigen_hdf5_Block",
    "section": "",
    "text": "void BigDataStatMeth::RcppbdEigen_hdf5_Block(T *dsA, BigDataStatMeth::hdf5Dataset *dsd, BigDataStatMeth::hdf5Dataset *dsu, int k, const std::string &which, int ncv, bool bcenter, bool bscale, double tol, int max_iter, bool compute_vectors, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#description",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#description",
    "title": "RcppbdEigen_hdf5_Block",
    "section": "2 Description",
    "text": "2 Description\nBlock-wise eigendecomposition for large HDF5 matrices using Spectra.",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#parameters",
    "title": "RcppbdEigen_hdf5_Block",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (T *): Input hdf5Dataset containing the matrix\ndsd (BigDataStatMeth::hdf5Dataset *): Output hdf5Dataset for eigenvalues\ndsu (BigDataStatMeth::hdf5Dataset *): Output hdf5Dataset for eigenvectors\nk (int): Number of eigenvalues to compute\nwhich (const std::string &): Which eigenvalues to compute\nncv (int): Number of Arnoldi vectors\nbcenter (bool): Whether to center the data\nbscale (bool): Whether to scale the data\ntol (double): Convergence tolerance\nmax_iter (int): Maximum iterations\ncompute_vectors (bool): Whether to compute eigenvectors\nthreads (Rcpp::Nullable&lt; int &gt;): Number of parallel threads",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#returns",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#returns",
    "title": "RcppbdEigen_hdf5_Block",
    "section": "4 Returns",
    "text": "4 Returns\nType: class T",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#details",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#details",
    "title": "RcppbdEigen_hdf5_Block",
    "section": "5 Details",
    "text": "5 Details\nThis function performs eigenvalue decomposition on large matrices stored in HDF5 using Spectra library for consistency with RSpectra results.",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#call-graph",
    "title": "RcppbdEigen_hdf5_Block",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#source-code",
    "title": "RcppbdEigen_hdf5_Block",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixEigenDecomposition.hpp • Lines 310-422\ninline void RcppbdEigen_hdf5_Block(T* dsA, \n                                       BigDataStatMeth::hdf5Dataset* dsd,\n                                       BigDataStatMeth::hdf5Dataset* dsu, \n                                       int k, const std::string& which, int ncv,\n                                       bool bcenter, bool bscale, double tol, int max_iter,\n                                       bool compute_vectors, Rcpp::Nullable&lt;int&gt; threads = R_NilValue) {\n        \n        static_assert(std::is_same&lt;T*, BigDataStatMeth::hdf5Dataset*&gt;::value ||\n                      std::is_same&lt;T*, BigDataStatMeth::hdf5DatasetInternal*&gt;::value,\n                      \"Error - type not allowed\");\n        \n        BigDataStatMeth::hdf5Dataset* dsnormalizedData = nullptr;\n        \n        try {\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1}, block = {1, 1};\n            eigdecomp reteig;\n            \n            // Get matrix dimensions\n            hsize_t n_rows = dsA-&gt;nrows();\n            hsize_t n_cols = dsA-&gt;ncols();\n            \n            // Check if matrix is square\n            if (n_rows != n_cols) {\n                Rf_error(\"Matrix must be square for eigendecomposition\");\n                return;\n            }\n            \n            hsize_t n = n_rows;\n            \n            // Use parameter validation function\n            std::tie(k, ncv) = validateSpectraParams((int)n, k, ncv);\n            \n            // Read matrix data\n            Eigen::MatrixXd X;\n            std::vector&lt;double&gt; vdA(n * n);\n            \n            dsA-&gt;readDatasetBlock({0, 0}, {n, n}, stride, block, vdA.data());\n            X = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt;(vdA.data(), n, n);\n            \n            // Handle normalization if needed\n            if (bcenter || bscale) {\n                \n                // For large matrices, we could implement block-wise normalization here\n                // For now, since we already read the matrix, apply normalization directly\n                if (n * n &lt; MAXELEMSINBLOCK) {\n                    X = RcppNormalize_Data(X, bcenter, bscale, false);\n                } else {\n                    // For very large matrices, implement block-wise normalization\n                    Rcpp::warning(\"Large matrix normalization not yet optimized - applying direct normalization\");\n                    X = RcppNormalize_Data(X, bcenter, bscale, false);\n                }\n            }\n            \n            // Use Spectra for eigendecomposition - same pattern as SVD\n            reteig = RcppbdEigen_spectra(X, k, which, ncv, false, false, tol, max_iter);\n            \n            if (!reteig.bconv) {\n                Rf_error(\"Eigendecomposition failed to converge\");\n                return;\n            }\n            \n            // Write eigenvalues to HDF5\n            dsd-&gt;createDataset(1, reteig.eigenvalues_real.size(), \"real\");\n            dsd-&gt;writeDataset(Rcpp::wrap(reteig.eigenvalues_real));\n            \n            // Write eigenvectors to HDF5 if computed\n            if (compute_vectors && reteig.bcomputevectors) {\n                dsu-&gt;createDataset(reteig.eigenvectors_real.rows(), reteig.eigenvectors_real.cols(), \"real\");\n                dsu-&gt;writeDataset(Rcpp::wrap(reteig.eigenvectors_real));\n            }\n            \n            // For non-symmetric matrices, also save imaginary parts if non-zero\n            if (!reteig.is_symmetric && reteig.eigenvalues_imag.cwiseAbs().maxCoeff() &gt; 1e-14) {\n                \n                // Create additional datasets for imaginary parts\n                BigDataStatMeth::hdf5Dataset* dsd_imag = new BigDataStatMeth::hdf5Dataset(\n                    dsA-&gt;getFileName(), \"EIGEN/\" + dsA-&gt;getDatasetName(), \"values_imag\", true);\n                dsd_imag-&gt;createDataset(1, reteig.eigenvalues_imag.size(), \"real\");\n                dsd_imag-&gt;writeDataset(Rcpp::wrap(reteig.eigenvalues_imag));\n                delete dsd_imag;\n                \n                if (compute_vectors && reteig.bcomputevectors && reteig.eigenvectors_imag.cwiseAbs().maxCoeff() &gt; 1e-14) {\n                    BigDataStatMeth::hdf5Dataset* dsu_imag = new BigDataStatMeth::hdf5Dataset(\n                        dsA-&gt;getFileName(), \"EIGEN/\" + dsA-&gt;getDatasetName(), \"vectors_imag\", true);\n                    dsu_imag-&gt;createDataset(reteig.eigenvectors_imag.rows(), reteig.eigenvectors_imag.cols(), \"real\");\n                    dsu_imag-&gt;writeDataset(Rcpp::wrap(reteig.eigenvectors_imag));\n                    delete dsu_imag;\n                }\n            }\n            \n        } catch(H5::FileIException& error) {\n            checkClose_file(dsA, dsd, dsu, dsnormalizedData);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception RcppbdEigen_hdf5_Block (File IException)\\n\";\n            return;\n        } catch(H5::DataSetIException& error) {\n            checkClose_file(dsA, dsd, dsu, dsnormalizedData);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception RcppbdEigen_hdf5_Block (DataSet IException)\\n\";\n            return;\n        } catch(H5::GroupIException& error) {\n            checkClose_file(dsA, dsd, dsu, dsnormalizedData);\n            Rcpp::Rcerr &lt;&lt; \"\\nc++ exception RcppbdEigen_hdf5_Block (Group IException)\\n\";\n            return;\n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsd, dsu, dsnormalizedData);\n            Rcpp::Rcerr &lt;&lt; \"C++ exception RcppbdEigen_hdf5_Block: \" &lt;&lt; ex.what();\n            return;\n        } catch (...) {\n            checkClose_file(dsA, dsd, dsu, dsnormalizedData);\n            Rcpp::Rcerr &lt;&lt; \"\\nC++ exception RcppbdEigen_hdf5_Block (unknown reason)\";\n            return;\n        }\n    }",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdEigen_hdf5_Block.html#usage-example",
    "title": "RcppbdEigen_hdf5_Block",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdEigen_hdf5_Block(...);",
    "crumbs": [
      "Functions",
      "RcppbdEigen_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD.html",
    "href": "api-reference/cpp/functions/RcppbdSVD.html",
    "title": "RcppbdSVD",
    "section": "",
    "text": "svdeig BigDataStatMeth::RcppbdSVD(Eigen::MatrixXd &X, int k, int ncv, bool bcenter, bool bscale)",
    "crumbs": [
      "Functions",
      "RcppbdSVD"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD.html#signature",
    "href": "api-reference/cpp/functions/RcppbdSVD.html#signature",
    "title": "RcppbdSVD",
    "section": "",
    "text": "svdeig BigDataStatMeth::RcppbdSVD(Eigen::MatrixXd &X, int k, int ncv, bool bcenter, bool bscale)",
    "crumbs": [
      "Functions",
      "RcppbdSVD"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD.html#description",
    "href": "api-reference/cpp/functions/RcppbdSVD.html#description",
    "title": "RcppbdSVD",
    "section": "2 Description",
    "text": "2 Description\nCompute SVD decomposition using Spectra eigenvalue solver.",
    "crumbs": [
      "Functions",
      "RcppbdSVD"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdSVD.html#parameters",
    "title": "RcppbdSVD",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (Eigen::MatrixXd &): Input matrix for SVD computation\nk (int): Number of singular values/vectors to compute (0 = auto-select)\nncv (int): Number of Arnoldi vectors to use (0 = auto-select)\nbcenter (bool): Whether to center the data before computation\nbscale (bool): Whether to scale the data before computation",
    "crumbs": [
      "Functions",
      "RcppbdSVD"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD.html#returns",
    "href": "api-reference/cpp/functions/RcppbdSVD.html#returns",
    "title": "RcppbdSVD",
    "section": "4 Returns",
    "text": "4 Returns\nsvdeig Structure containing U, S, V matrices and computation status",
    "crumbs": [
      "Functions",
      "RcppbdSVD"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD.html#details",
    "href": "api-reference/cpp/functions/RcppbdSVD.html#details",
    "title": "RcppbdSVD",
    "section": "5 Details",
    "text": "5 Details\nPerforms Singular Value Decomposition of a matrix using the Spectra library for eigenvalue computation. This function computes SVD by solving the eigenvalue problems XTX and XXT.",
    "crumbs": [
      "Functions",
      "RcppbdSVD"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdSVD.html#call-graph",
    "title": "RcppbdSVD",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdSVD"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdSVD.html#source-code",
    "title": "RcppbdSVD",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSvd.hpp • Lines 68-136\ninline svdeig RcppbdSVD( Eigen::MatrixXd& X, int k, int ncv, bool bcenter, bool bscale )\n    {\n        \n        svdeig retsvd;\n        Eigen::MatrixXd nX;\n        int nconv [[maybe_unused]];\n        \n        if( k==0 )    k = (std::min(X.rows(), X.cols()))-1;\n        else if (k &gt; (std::min(X.rows(), X.cols()))-1 ) k = (std::min(X.rows(), X.cols()))-1;\n        \n        if(ncv == 0)  ncv = k + 1 ;\n        if(ncv&lt;k) ncv = k + 1;\n        \n        {\n            Eigen::MatrixXd Xtcp;\n            if(bcenter ==true || bscale == true)  {\n                nX = RcppNormalize_Data(X, bcenter, bscale, false);\n                Xtcp =  bdtcrossproduct(nX);\n            } else {\n                Xtcp =  bdtcrossproduct(X);\n            }\n            \n            Spectra::DenseSymMatProd&lt;double&gt; op(Xtcp);\n            // Updated for Spectra 1.0.1: removed template parameters, pass object by reference\n            Spectra::SymEigsSolver&lt;Spectra::DenseSymMatProd&lt;double&gt;&gt; eigs(op, k, ncv);\n            \n            // Initialize and compute\n            eigs.init();\n            // Updated for Spectra 1.0.1: SortRule as runtime parameter\n            nconv = eigs.compute(Spectra::SortRule::LargestAlge);\n            \n            // Updated for Spectra 1.0.1: enum class for status check\n            if(eigs.info() == Spectra::CompInfo::Successful) {\n                retsvd.d = eigs.eigenvalues().cwiseSqrt();\n                retsvd.u = eigs.eigenvectors();\n                retsvd.bokuv = true;\n            } else {\n                retsvd.bokuv = false;\n            }\n        }\n        if(retsvd.bokuv == true)\n        {\n            Eigen::MatrixXd Xcp;\n            if(bcenter ==true || bscale==true ) {\n                Xcp =  bdcrossproduct(nX);  \n            } else {\n                Xcp =  bdcrossproduct(X);\n            }  \n            \n            Spectra::DenseSymMatProd&lt;double&gt; opv(Xcp);\n            // Updated for Spectra 1.0.1: removed template parameters, pass object by reference\n            Spectra::SymEigsSolver&lt;Spectra::DenseSymMatProd&lt;double&gt;&gt; eigsv(opv, k, ncv);\n            \n            // Initialize and compute\n            eigsv.init();\n            // Updated for Spectra 1.0.1: SortRule as runtime parameter\n            nconv = eigsv.compute(Spectra::SortRule::LargestAlge);\n            \n            // Retrieve results\n            // Updated for Spectra 1.0.1: enum class for status check\n            if(eigsv.info() == Spectra::CompInfo::Successful) {\n                retsvd.v = eigsv.eigenvectors();\n            } else {\n                retsvd.bokd = false;\n            }\n        }\n        \n        return retsvd;\n    }",
    "crumbs": [
      "Functions",
      "RcppbdSVD"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdSVD.html#usage-example",
    "title": "RcppbdSVD",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdSVD(...);",
    "crumbs": [
      "Functions",
      "RcppbdSVD"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html",
    "title": "RcppbdSVD_hdf5_Block",
    "section": "",
    "text": "void BigDataStatMeth::RcppbdSVD_hdf5_Block(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsu, BigDataStatMeth::hdf5Dataset *dsv, BigDataStatMeth::hdf5Dataset *dsd, int k, int q, int nev, bool bcenter, bool bscale, int irows, int icols, double dthreshold, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#signature",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#signature",
    "title": "RcppbdSVD_hdf5_Block",
    "section": "",
    "text": "void BigDataStatMeth::RcppbdSVD_hdf5_Block(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsu, BigDataStatMeth::hdf5Dataset *dsv, BigDataStatMeth::hdf5Dataset *dsd, int k, int q, int nev, bool bcenter, bool bscale, int irows, int icols, double dthreshold, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#description",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#description",
    "title": "RcppbdSVD_hdf5_Block",
    "section": "2 Description",
    "text": "2 Description\nBlock-wise SVD decomposition for large HDF5 matrices.",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#parameters",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#parameters",
    "title": "RcppbdSVD_hdf5_Block",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): Input HDF5 dataset containing the matrix\ndsu (BigDataStatMeth::hdf5Dataset *): Output HDF5 dataset for left singular vectors (U)\ndsv (BigDataStatMeth::hdf5Dataset *): Output HDF5 dataset for right singular vectors (V)\ndsd (BigDataStatMeth::hdf5Dataset *): Output HDF5 dataset for singular values (S)\nk (int): Number of local SVDs to concatenate at each level\nq (int): Number of decomposition levels\nnev (int): Number of eigenvalues per block\nbcenter (bool): Whether to center the data\nbscale (bool): Whether to scale the data\nirows (int): Number of rows in input matrix\nicols (int): Number of columns in input matrix\ndthreshold (double): Threshold for numerical computations\nthreads (Rcpp::Nullable&lt; int &gt;): Number of parallel threads (optional)",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#details",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#details",
    "title": "RcppbdSVD_hdf5_Block",
    "section": "4 Details",
    "text": "4 Details\nPerforms SVD computation on large matrices using block-wise decomposition strategy. This function handles matrices too large to fit in memory by processing them in blocks and combining results.",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#call-graph",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#call-graph",
    "title": "RcppbdSVD_hdf5_Block",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#source-code",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#source-code",
    "title": "RcppbdSVD_hdf5_Block",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSvd.hpp • Lines 171-332\ninline void RcppbdSVD_hdf5_Block( BigDataStatMeth::hdf5Dataset* dsA, \n                                      BigDataStatMeth::hdf5Dataset* dsu, BigDataStatMeth::hdf5Dataset* dsv, \n                                      BigDataStatMeth::hdf5Dataset* dsd, int k, int q, int nev, bool bcenter, bool bscale, \n                                      int irows, int icols, double dthreshold, Rcpp::Nullable&lt;int&gt; threads = R_NilValue )\n    {\n        \n        BigDataStatMeth::hdf5Dataset* dsnormalizedData = nullptr;\n        BigDataStatMeth::hdf5Dataset* dsJoined = nullptr;\n        BigDataStatMeth::hdf5DatasetInternal* dsnormalizedData_i = nullptr;\n        \n        try{\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1},\n                block = {1, 1};\n            \n            svdeig retsvd;\n            // Eigen::MatrixXd nX;\n            Eigen::MatrixXd matlast;\n            Eigen::MatrixXd v;    \n            bool transp = false;\n            std::string strGroupName  = \"tmpgroup\",\n                strPrefix; // Outpu group name for temporal data\n            \n            Rcpp::CharacterVector strvmatnames = {\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\n                                                  \"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\n                                                  \"W\",\"X\",\"Y\",\"Z\"};\n            strPrefix = strvmatnames[q-1];\n            \n            if(irows &gt;= icols) {\n                transp = true;\n            }\n            \n            First_level_SvdBlock_decomposition_hdf5( dsA, strGroupName, k, q, nev, bcenter, bscale, dthreshold, threads);\n            \n            for(int j = 1; j &lt; q; j++) { // For each decomposition level :\n                Next_level_SvdBlock_decomposition_hdf5( dsA, strGroupName, k, j, dthreshold, threads);\n            }\n            \n            // Get dataset names\n            Rcpp::StringVector joindata =  dsA-&gt;getDatasetNames(strGroupName, strPrefix, \"\");\n            \n            // 1.- Join matrix and remove parts from file\n            std::string strnewdataset = std::string((joindata[0])).substr(0,1);\n            \n            dsJoined = new hdf5DatasetInternal(dsA-&gt;getFullPath(), strGroupName, strnewdataset, true);\n            \n            join_datasets( dsJoined, strGroupName, joindata, false, true );\n            \n            // 2.- Get SVD from Blocks full mattrix\n            hsize_t* dims_out = dsJoined-&gt;dim();\n            \n            std::vector&lt;double&gt; vdreaded( dims_out[0] * dims_out[1] );\n            dsJoined-&gt;readDatasetBlock( {0, 0}, {dims_out[0], dims_out[1]}, {1, 1}, {1, 1}, vdreaded.data() );\n            \n            matlast = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; (vdreaded.data(), dims_out[0], dims_out[1] );\n            delete dsJoined; dsJoined = nullptr;\n            \n            retsvd = RcppbdSVD_lapack(matlast, false, false, false);\n            \n            \n            // Write results to hdf5 file : in folder \"SVD\" and dataset \"SVD\".&lt;name input dataset&gt;\n            dsd-&gt;createDataset( 1, retsvd.d.size(), \"real\");\n            dsd-&gt;writeDataset( Rcpp::wrap(retsvd.d) );\n            \n            // 3.- crossprod initial matrix and svdA$u\n            \n            if( bcenter == true || bscale == true || (dsA-&gt;getGroupName().find(\"NORMALIZED_T\") != std::string::npos) ) {\n                \n                if(bcenter == true || bscale == true) {\n                    \n                    dsnormalizedData = new BigDataStatMeth::hdf5Dataset( dsA-&gt;getFullPath(), strGroupName, \"normalmatrix\", false);\n                    dsnormalizedData-&gt;openDataset();\n                    \n                    dims_out = dsnormalizedData-&gt;dim();\n                    \n                    Eigen::MatrixXd A = Eigen::MatrixXd::Zero(dims_out[1], dims_out[0]);\n                    dsnormalizedData-&gt;readDatasetBlock( {0, 0}, {dims_out[0], dims_out[1]}, stride, block, A.data() );\n                    \n                    delete dsnormalizedData; dsnormalizedData = nullptr;\n                    \n                    if(transp == false) {\n                        v = A * retsvd.u;\n                    } else {\n                        v = A.transpose() * retsvd.u;\n                        // v = Rcpp_block_matrix_mul_parallel(A,v retsvd.u, false, false, R_NilValue, threads); // multiplication\n                    }\n                    \n                    // v = Rcpp_block_matrix_mul_parallel(A, retsvd.u, false, false, R_NilValue, threads);\n                    \n                } else {\n                    \n                    dsnormalizedData_i = new BigDataStatMeth::hdf5DatasetInternal( dsA-&gt;getFullPath(), dsA-&gt;getGroupName(), dsA-&gt;getDatasetName(), false);\n                    dsnormalizedData_i-&gt;openDataset();\n                    \n                    dims_out = dsnormalizedData_i-&gt;dim();\n                    \n                    std::vector&lt;double&gt; vdA( dims_out[0] * dims_out[1] );\n                    dsnormalizedData_i-&gt;readDatasetBlock( {0, 0}, {dims_out[0], dims_out[1]}, stride, block, vdA.data() );\n                    Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; A (vdA.data(), dims_out[0], dims_out[1]);\n                    delete dsnormalizedData_i; dsnormalizedData_i= nullptr;\n                    \n                    v = Rcpp_block_matrix_mul_parallel(A, retsvd.u, false, false, R_NilValue, threads);\n                    \n                }\n                \n            } else {\n                \n                dims_out = dsA-&gt;dim();\n                Eigen::MatrixXd A;\n                std::vector&lt;double&gt; vdA( dims_out[0] * dims_out[1] );\n                dsA-&gt;readDatasetBlock( {0,0}, {dims_out[0], dims_out[1] }, stride, block, vdA.data() );\n                \n                A = Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; (vdA.data(), dims_out[1], dims_out[0] );\n                \n                if(transp == false) {\n                    v = Rcpp_block_matrix_mul_parallel(A, retsvd.u, true, false, R_NilValue, threads); // crossprod\n                } else {\n                    v = Rcpp_block_matrix_mul_parallel(A, retsvd.u, false, false, R_NilValue, threads); // multiplication\n                }\n            }\n            \n            // 4.- resuls / svdA$d\n            // v = v.array().rowwise()/(retsvd.d).transpose().array();\n            v = v.array().rowwise() / Eigen::Map&lt;Eigen::RowVectorXd&gt;(retsvd.d.data(), (retsvd.d).size()).array();\n            \n            if (transp == true)  {\n                dsu-&gt;createDataset( v.rows(), v.cols(), \"real\");\n                dsv-&gt;createDataset( retsvd.u.rows(), retsvd.u.cols(), \"real\");\n                \n                dsu-&gt;writeDataset(Rcpp::wrap(v));\n                dsv-&gt;writeDataset(Rcpp::wrap(retsvd.u));\n            } else {\n                dsu-&gt;createDataset( retsvd.u.rows(), retsvd.u.cols(), \"real\");\n                dsv-&gt;createDataset( v.rows(), v.cols(), \"real\");\n                \n                dsu-&gt;writeDataset(Rcpp::wrap(retsvd.u));\n                dsv-&gt;writeDataset(Rcpp::wrap(v));\n            }\n            \n            remove_elements(dsA-&gt;getFileptr(), strGroupName);\n            \n            \n        }  catch( H5::FileIException& error ) { // catch failure caused by the H5File operations\n            checkClose_file(dsA, dsd, dsu, dsv, dsnormalizedData, dsJoined, dsnormalizedData_i);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppbdSVD_hdf5_Block (File IException)\\n\";\n            return void();\n        } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n            checkClose_file(dsA, dsd, dsu, dsv, dsnormalizedData, dsJoined, dsnormalizedData_i);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception RcppbdSVD_hdf5_Block (DataSet IException)\\n\";\n            return void();\n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsd, dsu, dsv, dsnormalizedData, dsJoined, dsnormalizedData_i);\n            Rcpp::Rcerr&lt;&lt; \"C++ exception RcppbdSVD_hdf5_Block : \"&lt;&lt; ex.what();\n            return void();\n        } catch (...) {\n            checkClose_file(dsA, dsd, dsu, dsv, dsnormalizedData, dsJoined, dsnormalizedData_i);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception RcppbdSVD_hdf5_Block (unknown reason)\";\n            return void();\n        }\n        \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#usage-example",
    "href": "api-reference/cpp/functions/RcppbdSVD_hdf5_Block.html#usage-example",
    "title": "RcppbdSVD_hdf5_Block",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = RcppbdSVD_hdf5_Block(...);",
    "crumbs": [
      "Functions",
      "RcppbdSVD_hdf5_Block"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/SplitElementName.html",
    "href": "api-reference/cpp/functions/SplitElementName.html",
    "title": "SplitElementName",
    "section": "",
    "text": "fullpath BigDataStatMeth::SplitElementName(std::string str)",
    "crumbs": [
      "Functions",
      "SplitElementName"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/SplitElementName.html#signature",
    "href": "api-reference/cpp/functions/SplitElementName.html#signature",
    "title": "SplitElementName",
    "section": "",
    "text": "fullpath BigDataStatMeth::SplitElementName(std::string str)",
    "crumbs": [
      "Functions",
      "SplitElementName"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/SplitElementName.html#description",
    "href": "api-reference/cpp/functions/SplitElementName.html#description",
    "title": "SplitElementName",
    "section": "2 Description",
    "text": "2 Description\nSplits a full file path into directory path and filename.",
    "crumbs": [
      "Functions",
      "SplitElementName"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/SplitElementName.html#parameters",
    "href": "api-reference/cpp/functions/SplitElementName.html#parameters",
    "title": "SplitElementName",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nstr (std::string): Full file path",
    "crumbs": [
      "Functions",
      "SplitElementName"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/SplitElementName.html#returns",
    "href": "api-reference/cpp/functions/SplitElementName.html#returns",
    "title": "SplitElementName",
    "section": "4 Returns",
    "text": "4 Returns\nfullpath Structure containing separated path and filename",
    "crumbs": [
      "Functions",
      "SplitElementName"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/SplitElementName.html#details",
    "href": "api-reference/cpp/functions/SplitElementName.html#details",
    "title": "SplitElementName",
    "section": "5 Details",
    "text": "5 Details\nstrFull file path fullpath Structure containing separated path and filename Handles both forward and backward slashes for cross-platform compatibility",
    "crumbs": [
      "Functions",
      "SplitElementName"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/SplitElementName.html#call-graph",
    "href": "api-reference/cpp/functions/SplitElementName.html#call-graph",
    "title": "SplitElementName",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "SplitElementName"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/SplitElementName.html#source-code",
    "href": "api-reference/cpp/functions/SplitElementName.html#source-code",
    "title": "SplitElementName",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/Utilities.hpp • Lines 94-107\ninline fullpath SplitElementName (std::string str)\n    {\n        fullpath currentpath;\n        std::size_t found = str.find_last_of(\"/\\\\\");\n        \n        if( found&lt; str.length() ) {\n            currentpath.filename =  str.substr(found+1);\n            currentpath.path = str.substr(0,found);\n        }else {\n            currentpath.filename = str;\n            currentpath.path = \"\";\n        }\n        return(currentpath);\n    }",
    "crumbs": [
      "Functions",
      "SplitElementName"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/SplitElementName.html#usage-example",
    "href": "api-reference/cpp/functions/SplitElementName.html#usage-example",
    "title": "SplitElementName",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = SplitElementName(...);",
    "crumbs": [
      "Functions",
      "SplitElementName"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xw.html",
    "href": "api-reference/cpp/functions/Xw.html",
    "title": "Xw",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Xw(const Eigen::MatrixXd &X, const Eigen::MatrixXd &w)",
    "crumbs": [
      "Functions",
      "Xw"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xw.html#signature",
    "href": "api-reference/cpp/functions/Xw.html#signature",
    "title": "Xw",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Xw(const Eigen::MatrixXd &X, const Eigen::MatrixXd &w)",
    "crumbs": [
      "Functions",
      "Xw"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xw.html#description",
    "href": "api-reference/cpp/functions/Xw.html#description",
    "title": "Xw",
    "section": "2 Description",
    "text": "2 Description\nCompute matrix-diagonal product Xw.",
    "crumbs": [
      "Functions",
      "Xw"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xw.html#parameters",
    "href": "api-reference/cpp/functions/Xw.html#parameters",
    "title": "Xw",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (const Eigen::MatrixXd &): Input matrix\nw (const Eigen::MatrixXd &): Matrix representing diagonal matrix",
    "crumbs": [
      "Functions",
      "Xw"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xw.html#returns",
    "href": "api-reference/cpp/functions/Xw.html#returns",
    "title": "Xw",
    "section": "4 Returns",
    "text": "4 Returns\nMatrix-diagonal product Xw",
    "crumbs": [
      "Functions",
      "Xw"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xw.html#details",
    "href": "api-reference/cpp/functions/Xw.html#details",
    "title": "Xw",
    "section": "5 Details",
    "text": "5 Details\nComputes the product of a matrix with a diagonal matrix represented as a matrix.",
    "crumbs": [
      "Functions",
      "Xw"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xw.html#call-graph",
    "href": "api-reference/cpp/functions/Xw.html#call-graph",
    "title": "Xw",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "Xw"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xw.html#source-code",
    "href": "api-reference/cpp/functions/Xw.html#source-code",
    "title": "Xw",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memOptimizedProducts.hpp • Lines 232-236\ninline Eigen::MatrixXd Xw(const Eigen::MatrixXd& X, const Eigen::MatrixXd& w) \n{\n    Eigen::MatrixXd Xw = X * w.array().matrix().asDiagonal();\n    return (Xw);\n}",
    "crumbs": [
      "Functions",
      "Xw"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xw.html#usage-example",
    "href": "api-reference/cpp/functions/Xw.html#usage-example",
    "title": "Xw",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Xw(...);",
    "crumbs": [
      "Functions",
      "Xw"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd_parallel.html",
    "href": "api-reference/cpp/functions/Xwd_parallel.html",
    "title": "Xwd_parallel",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Xwd_parallel(const Eigen::MatrixXd &X, const Eigen::VectorXd &w, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Xwd_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd_parallel.html#signature",
    "href": "api-reference/cpp/functions/Xwd_parallel.html#signature",
    "title": "Xwd_parallel",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::Xwd_parallel(const Eigen::MatrixXd &X, const Eigen::VectorXd &w, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "Xwd_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd_parallel.html#description",
    "href": "api-reference/cpp/functions/Xwd_parallel.html#description",
    "title": "Xwd_parallel",
    "section": "2 Description",
    "text": "2 Description\nCompute parallel matrix-diagonal product Xw.",
    "crumbs": [
      "Functions",
      "Xwd_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd_parallel.html#parameters",
    "href": "api-reference/cpp/functions/Xwd_parallel.html#parameters",
    "title": "Xwd_parallel",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (const Eigen::MatrixXd &): Input matrix\nw (const Eigen::VectorXd &): Vector representing diagonal matrix\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel computation",
    "crumbs": [
      "Functions",
      "Xwd_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd_parallel.html#returns",
    "href": "api-reference/cpp/functions/Xwd_parallel.html#returns",
    "title": "Xwd_parallel",
    "section": "4 Returns",
    "text": "4 Returns\nMatrix-diagonal product Xw",
    "crumbs": [
      "Functions",
      "Xwd_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd_parallel.html#details",
    "href": "api-reference/cpp/functions/Xwd_parallel.html#details",
    "title": "Xwd_parallel",
    "section": "5 Details",
    "text": "5 Details\nParallel implementation of matrix-diagonal product computation.",
    "crumbs": [
      "Functions",
      "Xwd_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd_parallel.html#source-code",
    "href": "api-reference/cpp/functions/Xwd_parallel.html#source-code",
    "title": "Xwd_parallel",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memOptimizedProducts.hpp • Lines 275-308\ninline Eigen::MatrixXd Xwd_parallel(const Eigen::MatrixXd& X, const Eigen::VectorXd& w, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n{\n    int n = X.rows();\n    // unsigned int ithreads;\n    Eigen::MatrixXd C = Eigen::MatrixXd::Zero(n,X.cols()) ; \n    \n    \n    // ithreads = get_number_threads(threads, R_NilValue);\n    \n    // if(threads.isNotNull()) {\n    //     if (Rcpp::as&lt;int&gt; (threads) &lt;= std::thread::hardware_concurrency()){\n    //         ithreads = Rcpp::as&lt;int&gt; (threads);\n    //     } else {\n    //         ithreads = getDTthreads(0, true);\n    //         //.11-04-2022.// ithreads = std::thread::hardware_concurrency()/2;}\n    //     }\n    // } else {\n    //     ithreads = getDTthreads(0, true);\n    //     //.11-04-2022.// ithreads = std::thread::hardware_concurrency()/2;\n    // }\n    \n    //.OpenMP.//omp_set_num_threads(ithreads);\n    \n    //.OpenMP.//#pragma omp parallel shared(X, w, C) \n    #pragma omp parallel num_threads( get_number_threads(threads, R_NilValue) ) shared(X, w, C) \n    {\n    #pragma omp for schedule (dynamic)\n        for (int i=0; i&lt;n; i++)\n        {\n            C.col(i) = X.col(i)*w(i);\n        }  \n    }\n    return(C);\n}",
    "crumbs": [
      "Functions",
      "Xwd_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/Xwd_parallel.html#usage-example",
    "href": "api-reference/cpp/functions/Xwd_parallel.html#usage-example",
    "title": "Xwd_parallel",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = Xwd_parallel(...);",
    "crumbs": [
      "Functions",
      "Xwd_parallel"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/after_fork.html",
    "href": "api-reference/cpp/functions/after_fork.html",
    "title": "after_fork",
    "section": "",
    "text": "void after_fork()",
    "crumbs": [
      "Functions",
      "after_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/after_fork.html#signature",
    "href": "api-reference/cpp/functions/after_fork.html#signature",
    "title": "after_fork",
    "section": "",
    "text": "void after_fork()",
    "crumbs": [
      "Functions",
      "after_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/after_fork.html#call-graph",
    "href": "api-reference/cpp/functions/after_fork.html#call-graph",
    "title": "after_fork",
    "section": "2 Call Graph",
    "text": "2 Call Graph",
    "crumbs": [
      "Functions",
      "after_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/after_fork.html#source-code",
    "href": "api-reference/cpp/functions/after_fork.html#source-code",
    "title": "after_fork",
    "section": "3 Source Code",
    "text": "3 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/openme-utils.hpp • Lines 258-260\ninline void after_fork() {\n        if (RestoreAfterFork) DTthreads = pre_fork_DTthreads;\n    }",
    "crumbs": [
      "Functions",
      "after_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/after_fork.html#usage-example",
    "href": "api-reference/cpp/functions/after_fork.html#usage-example",
    "title": "after_fork",
    "section": "4 Usage Example",
    "text": "4 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = after_fork(...);",
    "crumbs": [
      "Functions",
      "after_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdcrossproduct.html",
    "href": "api-reference/cpp/functions/bdcrossproduct.html",
    "title": "bdcrossproduct",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::bdcrossproduct(T X)",
    "crumbs": [
      "Functions",
      "bdcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdcrossproduct.html#signature",
    "href": "api-reference/cpp/functions/bdcrossproduct.html#signature",
    "title": "bdcrossproduct",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::bdcrossproduct(T X)",
    "crumbs": [
      "Functions",
      "bdcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdcrossproduct.html#description",
    "href": "api-reference/cpp/functions/bdcrossproduct.html#description",
    "title": "bdcrossproduct",
    "section": "2 Description",
    "text": "2 Description\nCompute matrix cross-product X’X.",
    "crumbs": [
      "Functions",
      "bdcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdcrossproduct.html#parameters",
    "href": "api-reference/cpp/functions/bdcrossproduct.html#parameters",
    "title": "bdcrossproduct",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (T): Input matrix",
    "crumbs": [
      "Functions",
      "bdcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdcrossproduct.html#returns",
    "href": "api-reference/cpp/functions/bdcrossproduct.html#returns",
    "title": "bdcrossproduct",
    "section": "4 Returns",
    "text": "4 Returns\nCross-product matrix X’X",
    "crumbs": [
      "Functions",
      "bdcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdcrossproduct.html#details",
    "href": "api-reference/cpp/functions/bdcrossproduct.html#details",
    "title": "bdcrossproduct",
    "section": "5 Details",
    "text": "5 Details\nEfficiently computes the cross-product of a matrix with its transpose (X’X) using optimized matrix operations.",
    "crumbs": [
      "Functions",
      "bdcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdcrossproduct.html#call-graph",
    "href": "api-reference/cpp/functions/bdcrossproduct.html#call-graph",
    "title": "bdcrossproduct",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "bdcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdcrossproduct.html#source-code",
    "href": "api-reference/cpp/functions/bdcrossproduct.html#source-code",
    "title": "bdcrossproduct",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memOptimizedProducts.hpp • Lines 162-176\ninline Eigen::MatrixXd bdcrossproduct ( T X )\n{\n    \n    static_assert(std::is_same&lt;T, Eigen::MatrixXd &gt;::value || \n                  std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt;::value || \n                  std::is_same&lt;T, Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt;::value ||\n                  std::is_same&lt;T, Eigen::Transpose&lt;Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; &gt; &gt;::value ||\n                  std::is_same&lt;T, Eigen::Transpose&lt;Eigen::Map&lt; Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::ColMajor&gt;&gt; &gt; &gt;::value,\n                  \"Error - type not allowed\");\n    \n    Eigen::MatrixXd Xem = X;\n    size_t nc(Xem.cols());\n    Eigen::MatrixXd XtX(Eigen::MatrixXd(nc, nc).setZero().selfadjointView&lt;Eigen::Lower&gt;().rankUpdate(Xem.adjoint()));\n    return(XtX);\n}",
    "crumbs": [
      "Functions",
      "bdcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/bdcrossproduct.html#usage-example",
    "href": "api-reference/cpp/functions/bdcrossproduct.html#usage-example",
    "title": "bdcrossproduct",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = bdcrossproduct(...);",
    "crumbs": [
      "Functions",
      "bdcrossproduct"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calc_freq.html",
    "href": "api-reference/cpp/functions/calc_freq.html",
    "title": "calc_freq",
    "section": "",
    "text": "double BigDataStatMeth::calc_freq(Rcpp::NumericVector x)",
    "crumbs": [
      "Functions",
      "calc_freq"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calc_freq.html#signature",
    "href": "api-reference/cpp/functions/calc_freq.html#signature",
    "title": "calc_freq",
    "section": "",
    "text": "double BigDataStatMeth::calc_freq(Rcpp::NumericVector x)",
    "crumbs": [
      "Functions",
      "calc_freq"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calc_freq.html#description",
    "href": "api-reference/cpp/functions/calc_freq.html#description",
    "title": "calc_freq",
    "section": "2 Description",
    "text": "2 Description\nCalculates the Minor Allele Frequency (MAF) from binary genotype data.",
    "crumbs": [
      "Functions",
      "calc_freq"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calc_freq.html#parameters",
    "href": "api-reference/cpp/functions/calc_freq.html#parameters",
    "title": "calc_freq",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nx (Rcpp::NumericVector): NumericVector containing binary genotype data (0/1 encoded)",
    "crumbs": [
      "Functions",
      "calc_freq"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calc_freq.html#returns",
    "href": "api-reference/cpp/functions/calc_freq.html#returns",
    "title": "calc_freq",
    "section": "4 Returns",
    "text": "4 Returns\ndouble MAF value between 0 and 0.5",
    "crumbs": [
      "Functions",
      "calc_freq"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calc_freq.html#details",
    "href": "api-reference/cpp/functions/calc_freq.html#details",
    "title": "calc_freq",
    "section": "5 Details",
    "text": "5 Details\nxNumericVector containing binary genotype data (0/1 encoded) double MAF value between 0 and 0.5 This function calculates MAF for a vector of binary genotypes:Counts occurrences of reference (0) and alternate (1) allelesCalculates frequency as (n0 + 0.5*n1)/total_lengthEnsures MAF is ≤ 0.5 by taking complement if necessary",
    "crumbs": [
      "Functions",
      "calc_freq"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calc_freq.html#call-graph",
    "href": "api-reference/cpp/functions/calc_freq.html#call-graph",
    "title": "calc_freq",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "calc_freq"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calc_freq.html#source-code",
    "href": "api-reference/cpp/functions/calc_freq.html#source-code",
    "title": "calc_freq",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Omics/hdf5OmicsUtils.hpp • Lines 61-79\ninline double calc_freq(Rcpp::NumericVector x)\n    {\n        \n        int len = x.size();\n        \n        std::vector&lt;double&gt; xc = Rcpp::as&lt;std::vector&lt;double&gt; &gt;(x);\n        \n        int n0 = std::count (xc.begin(), xc.end(), 0);\n        int n1 = std::count (xc.begin(), xc.end(), 1);\n        \n        double maf = (double(n0)/len) + 0.5*(double(n1)/len);\n        \n        if( maf &gt; 0.5 ) { \n            maf = 1 - maf;\n        }\n        \n        return maf;\n        \n    }",
    "crumbs": [
      "Functions",
      "calc_freq"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/calc_freq.html#usage-example",
    "href": "api-reference/cpp/functions/calc_freq.html#usage-example",
    "title": "calc_freq",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = calc_freq(...);",
    "crumbs": [
      "Functions",
      "calc_freq"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/checkClose_file.html",
    "href": "api-reference/cpp/functions/checkClose_file.html",
    "title": "checkClose_file",
    "section": "",
    "text": "void BigDataStatMeth::checkClose_file(std::initializer_list&lt; BigDataStatMeth::hdf5Dataset * &gt; list) noexcept",
    "crumbs": [
      "Functions",
      "checkClose_file"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/checkClose_file.html#signature",
    "href": "api-reference/cpp/functions/checkClose_file.html#signature",
    "title": "checkClose_file",
    "section": "",
    "text": "void BigDataStatMeth::checkClose_file(std::initializer_list&lt; BigDataStatMeth::hdf5Dataset * &gt; list) noexcept",
    "crumbs": [
      "Functions",
      "checkClose_file"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/checkClose_file.html#description",
    "href": "api-reference/cpp/functions/checkClose_file.html#description",
    "title": "checkClose_file",
    "section": "2 Description",
    "text": "2 Description\nSafely close a list of dataset handles (initializer-list form).",
    "crumbs": [
      "Functions",
      "checkClose_file"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/checkClose_file.html#parameters",
    "href": "api-reference/cpp/functions/checkClose_file.html#parameters",
    "title": "checkClose_file",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nlist (std::initializer_list&lt; BigDataStatMeth::hdf5Dataset * &gt;): Initializer list of dataset wrapper pointers (nullable).",
    "crumbs": [
      "Functions",
      "checkClose_file"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/checkClose_file.html#details",
    "href": "api-reference/cpp/functions/checkClose_file.html#details",
    "title": "checkClose_file",
    "section": "4 Details",
    "text": "4 Details\nlistInitializer list of dataset wrapper pointers (nullable). Convenience overload to allow brace-initializer syntax: checkClose_file({ds1, ds2, ds3});",
    "crumbs": [
      "Functions",
      "checkClose_file"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/checkClose_file.html#call-graph",
    "href": "api-reference/cpp/functions/checkClose_file.html#call-graph",
    "title": "checkClose_file",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "checkClose_file"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/checkClose_file.html#source-code",
    "href": "api-reference/cpp/functions/checkClose_file.html#source-code",
    "title": "checkClose_file",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5CheckClose.hpp • Lines 92-99\ninline void checkClose_file(\n            std::initializer_list&lt;BigDataStatMeth::hdf5Dataset*&gt; list) noexcept {\n        for (auto* p : list) {\n            try {\n                if (p && p-&gt;getDatasetptr() != nullptr) p-&gt;close_file();\n            } catch (...) {}\n        }\n    }",
    "crumbs": [
      "Functions",
      "checkClose_file"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/checkClose_file.html#usage-example",
    "href": "api-reference/cpp/functions/checkClose_file.html#usage-example",
    "title": "checkClose_file",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = checkClose_file(...);",
    "crumbs": [
      "Functions",
      "checkClose_file"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cleanup_temp_datasets.html",
    "href": "api-reference/cpp/functions/cleanup_temp_datasets.html",
    "title": "cleanup_temp_datasets",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::cleanup_temp_datasets(BigDataStatMeth::hdf5Dataset *tempA, BigDataStatMeth::hdf5Dataset *tempB)",
    "crumbs": [
      "Functions",
      "cleanup_temp_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cleanup_temp_datasets.html#signature",
    "href": "api-reference/cpp/functions/cleanup_temp_datasets.html#signature",
    "title": "cleanup_temp_datasets",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::cleanup_temp_datasets(BigDataStatMeth::hdf5Dataset *tempA, BigDataStatMeth::hdf5Dataset *tempB)",
    "crumbs": [
      "Functions",
      "cleanup_temp_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cleanup_temp_datasets.html#description",
    "href": "api-reference/cpp/functions/cleanup_temp_datasets.html#description",
    "title": "cleanup_temp_datasets",
    "section": "2 Description",
    "text": "2 Description\nClean up temporary datasets created during operations.",
    "crumbs": [
      "Functions",
      "cleanup_temp_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cleanup_temp_datasets.html#parameters",
    "href": "api-reference/cpp/functions/cleanup_temp_datasets.html#parameters",
    "title": "cleanup_temp_datasets",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ntempA (BigDataStatMeth::hdf5Dataset *): Temporary dataset A (can be nullptr)\ntempB (BigDataStatMeth::hdf5Dataset *): Temporary dataset B (can be nullptr)",
    "crumbs": [
      "Functions",
      "cleanup_temp_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cleanup_temp_datasets.html#details",
    "href": "api-reference/cpp/functions/cleanup_temp_datasets.html#details",
    "title": "cleanup_temp_datasets",
    "section": "4 Details",
    "text": "4 Details\nSafely deletes temporary datasets and releases memory. Used internally to ensure proper resource cleanup even when exceptions occur.",
    "crumbs": [
      "Functions",
      "cleanup_temp_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cleanup_temp_datasets.html#call-graph",
    "href": "api-reference/cpp/functions/cleanup_temp_datasets.html#call-graph",
    "title": "cleanup_temp_datasets",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "cleanup_temp_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cleanup_temp_datasets.html#source-code",
    "href": "api-reference/cpp/functions/cleanup_temp_datasets.html#source-code",
    "title": "cleanup_temp_datasets",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5DiagonalMethods.hpp • Lines 100-115\ninline void cleanup_temp_datasets(BigDataStatMeth::hdf5Dataset* tempA, \n                                          BigDataStatMeth::hdf5Dataset* tempB)\n        {\n            try {\n                if (tempA != nullptr) {\n                    delete tempA;\n                    tempA = nullptr;\n                }\n                if (tempB != nullptr) {\n                    delete tempB;\n                    tempB = nullptr;\n                }\n            } catch(...) {\n                // Silent cleanup\n            }\n        }",
    "crumbs": [
      "Functions",
      "cleanup_temp_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/cleanup_temp_datasets.html#usage-example",
    "href": "api-reference/cpp/functions/cleanup_temp_datasets.html#usage-example",
    "title": "cleanup_temp_datasets",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = cleanup_temp_datasets(...);",
    "crumbs": [
      "Functions",
      "cleanup_temp_datasets"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/correlation_pvalue.html",
    "href": "api-reference/cpp/functions/correlation_pvalue.html",
    "title": "correlation_pvalue",
    "section": "",
    "text": "double BigDataStatMeth::correlation_pvalue(double r, int n)",
    "crumbs": [
      "Functions",
      "correlation_pvalue"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/correlation_pvalue.html#signature",
    "href": "api-reference/cpp/functions/correlation_pvalue.html#signature",
    "title": "correlation_pvalue",
    "section": "",
    "text": "double BigDataStatMeth::correlation_pvalue(double r, int n)",
    "crumbs": [
      "Functions",
      "correlation_pvalue"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/correlation_pvalue.html#description",
    "href": "api-reference/cpp/functions/correlation_pvalue.html#description",
    "title": "correlation_pvalue",
    "section": "2 Description",
    "text": "2 Description\nCompute p-value for correlation coefficient - R equivalent implementation.",
    "crumbs": [
      "Functions",
      "correlation_pvalue"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/correlation_pvalue.html#parameters",
    "href": "api-reference/cpp/functions/correlation_pvalue.html#parameters",
    "title": "correlation_pvalue",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nr (double): Correlation coefficient [-1, 1]\nn (int): Sample size (number of observations)",
    "crumbs": [
      "Functions",
      "correlation_pvalue"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/correlation_pvalue.html#returns",
    "href": "api-reference/cpp/functions/correlation_pvalue.html#returns",
    "title": "correlation_pvalue",
    "section": "4 Returns",
    "text": "4 Returns\nTwo-tailed p-value [0, 1], or NaN if invalid input",
    "crumbs": [
      "Functions",
      "correlation_pvalue"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/correlation_pvalue.html#details",
    "href": "api-reference/cpp/functions/correlation_pvalue.html#details",
    "title": "correlation_pvalue",
    "section": "5 Details",
    "text": "5 Details\nImplements the exact equivalent of R’s correlation test: t&lt;-(rsqrt(n-2))/sqrt(1-r^2)p&lt;-2(pt(abs(t),(n-2),lower.tail=FALSE))",
    "crumbs": [
      "Functions",
      "correlation_pvalue"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/correlation_pvalue.html#call-graph",
    "href": "api-reference/cpp/functions/correlation_pvalue.html#call-graph",
    "title": "correlation_pvalue",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "correlation_pvalue"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/correlation_pvalue.html#source-code",
    "href": "api-reference/cpp/functions/correlation_pvalue.html#source-code",
    "title": "correlation_pvalue",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixCorrelation.hpp • Lines 355-372\ninline double correlation_pvalue(double r, int n) {\n        if (std::isnan(r) || n &lt; 3 || std::abs(r) &gt;= 1.0) {\n            return std::numeric_limits&lt;double&gt;::quiet_NaN();\n        }\n        \n        // Calculate t-statistic exactly as in R: t &lt;- (r*sqrt(n-2))/sqrt(1-r^2)\n        double df = n - 2;\n        double t_stat = (r * std::sqrt(df)) / std::sqrt(1.0 - r * r);\n        \n        // Calculate p-value exactly as in R: p &lt;- 2*(pt(abs(t),(n-2), lower.tail=FALSE))\n        // pt(abs(t), df, lower.tail=FALSE) = 1 - pt(abs(t), df, lower.tail=TRUE)\n        double abs_t = std::abs(t_stat);\n        double cdf_value = t_distribution_cdf(abs_t, df);\n        double p_value = 2.0 * (1.0 - cdf_value);\n        \n        // Ensure p-value is in valid range [0, 1]\n        return std::max(0.0, std::min(1.0, p_value));\n    }",
    "crumbs": [
      "Functions",
      "correlation_pvalue"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/correlation_pvalue.html#usage-example",
    "href": "api-reference/cpp/functions/correlation_pvalue.html#usage-example",
    "title": "correlation_pvalue",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = correlation_pvalue(...);",
    "crumbs": [
      "Functions",
      "correlation_pvalue"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/crossprod.html",
    "href": "api-reference/cpp/functions/crossprod.html",
    "title": "crossprod",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::crossprod(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool isSymmetric, hsize_t hdf5_block, hsize_t mem_block_size, bool bparal, bool browmajor, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "crossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/crossprod.html#signature",
    "href": "api-reference/cpp/functions/crossprod.html#signature",
    "title": "crossprod",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::crossprod(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool isSymmetric, hsize_t hdf5_block, hsize_t mem_block_size, bool bparal, bool browmajor, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "crossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/crossprod.html#description",
    "href": "api-reference/cpp/functions/crossprod.html#description",
    "title": "crossprod",
    "section": "2 Description",
    "text": "2 Description\nComputes the cross-product of two matrices stored in HDF5 format.",
    "crumbs": [
      "Functions",
      "crossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/crossprod.html#parameters",
    "href": "api-reference/cpp/functions/crossprod.html#parameters",
    "title": "crossprod",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input matrix dataset (A)\ndsB (BigDataStatMeth::hdf5Dataset *): Second input matrix dataset (B)\ndsC (BigDataStatMeth::hdf5Dataset *): Output matrix dataset for result\nhdf5_block (hsize_t): Block size for HDF5 operations\nmem_block_size (hsize_t): Memory block size for computations\nbparal (bool): Whether to use parallel processing\nbrowmajor (bool): Whether matrices are stored in row-major order\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing (optional)",
    "crumbs": [
      "Functions",
      "crossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/crossprod.html#returns",
    "href": "api-reference/cpp/functions/crossprod.html#returns",
    "title": "crossprod",
    "section": "4 Returns",
    "text": "4 Returns\nBigDataStatMeth::hdf5Dataset* Pointer to the result dataset",
    "crumbs": [
      "Functions",
      "crossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/crossprod.html#details",
    "href": "api-reference/cpp/functions/crossprod.html#details",
    "title": "crossprod",
    "section": "5 Details",
    "text": "5 Details\ndsAFirst input matrix dataset (A) dsBSecond input matrix dataset (B) dsCOutput matrix dataset for result hdf5_blockBlock size for HDF5 operations mem_block_sizeMemory block size for computations bparalWhether to use parallel processing browmajorWhether matrices are stored in row-major order threadsNumber of threads for parallel processing (optional) BigDataStatMeth::hdf5Dataset* Pointer to the result dataset Computes C = A^T * B using block-wise operations:Divides matrices into blocks for memory-efficient processingProcesses blocks using optimized Eigen operationsAccumulates results in the output dataset",
    "crumbs": [
      "Functions",
      "crossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/crossprod.html#call-graph",
    "href": "api-reference/cpp/functions/crossprod.html#call-graph",
    "title": "crossprod",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "crossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/crossprod.html#source-code",
    "href": "api-reference/cpp/functions/crossprod.html#source-code",
    "title": "crossprod",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/crossprod.hpp • Lines 77-272\ninline BigDataStatMeth::hdf5Dataset* crossprod( \n            BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, \n            BigDataStatMeth::hdf5Dataset* dsC, bool isSymmetric, \n            hsize_t hdf5_block, hsize_t mem_block_size, bool bparal, \n            bool browmajor, Rcpp::Nullable&lt;int&gt; threads = R_NilValue) \n\n    {\n        \n        try {\n            \n            hsize_t N = dsA-&gt;nrows();\n            hsize_t K = dsA-&gt;ncols();\n            hsize_t M = dsB-&gt;nrows();\n            hsize_t L = dsB-&gt;ncols();\n            \n            if( K == L)\n            {\n                // hsize_t isize = hdf5_block + 1,\n                //     ksize = hdf5_block + 1,\n                //     jsize = hdf5_block + 1;\n                \n                std::vector&lt;hsize_t&gt; stride = {1, 1};\n                std::vector&lt;hsize_t&gt; block = {1, 1};\n                \n                if (isSymmetric) {\n                    if (N != M) {\n                        throw std::range_error(\"Symmetric crossprod requires square result matrix\");\n                    }\n                    if (dsA-&gt;getFileName() != dsB-&gt;getFileName() || \n                        dsA-&gt;getGroup() != dsB-&gt;getGroup() || \n                        dsA-&gt;getDatasetName() != dsB-&gt;getDatasetName()) {\n                        Rcpp::warning(\"isSymmetric=TRUE but different datasets provided. Results may be incorrect.\");\n                    }\n                }\n                \n                dsC-&gt;createDataset( N, M, \"real\");\n/** 2025/11/25                \n                // Configure parallel processing\n                int num_threads = 1;\n                if (bparal) {\n                    num_threads = get_number_threads(threads, Rcpp::wrap(bparal));  \n#ifdef _OPENMP\n                    omp_set_num_threads(num_threads);\n#endif\n                }\n Fi 2025/11/25 **/\n\n#ifdef _OPENMP // Configure parallel processing\n                int num_threads = 1;\n                if (bparal) {\n                    num_threads = get_number_threads(threads, Rcpp::wrap(bparal));  \n                    omp_set_num_threads(num_threads);\n                }\n#endif                \n                                \n//                 // HDF5 thread safety: Initialize lock for I/O serialization\n// #ifdef _OPENMP\n//                 static omp_lock_t hdf5_lock;\n//                 static bool hdf5_lock_initialized = false;\n//                 \n//                 if (bparal && !hdf5_lock_initialized) {\n//                     omp_init_lock(&hdf5_lock);\n//                     hdf5_lock_initialized = true;\n//                 }\n// #endif\n                \n                // Calculate total blocks for parallelization\n                hsize_t blocks_i = (N + hdf5_block - 1) / hdf5_block;\n                hsize_t blocks_j = (M + hdf5_block - 1) / hdf5_block;\n                hsize_t total_blocks;\n                \n                if (isSymmetric) {\n                    // For symmetric case: only upper triangle blocks\n                    total_blocks = (blocks_i * (blocks_i + 1)) / 2;\n                } else {\n                    // For general case: all blocks\n                    total_blocks = blocks_i * blocks_j;\n                }\n                \n#ifdef _OPENMP\n#pragma omp parallel for if(bparal) schedule(dynamic)\n#endif\n                for (hsize_t block_idx = 0; block_idx &lt; total_blocks; ++block_idx)\n                {\n                    // Convert linear index to (ii_idx, jj_idx)\n                    hsize_t ii_idx = 0, \n                            jj_idx = 0;\n                    \n                    if (isSymmetric) {\n                        // Convert to upper triangle coordinates\n                        hsize_t remaining = block_idx;\n                        for (hsize_t i = 0; i &lt; blocks_i; ++i) {\n                            hsize_t blocks_in_row = blocks_i - i;\n                            if (remaining &lt; blocks_in_row) {\n                                ii_idx = i;\n                                jj_idx = i + remaining;\n                                break;\n                            }\n                            remaining -= blocks_in_row;\n                        }\n                    } else {\n                        // Convert to regular coordinates\n                        ii_idx = block_idx / blocks_j;\n                        jj_idx = block_idx % blocks_j;\n                    }\n                    \n                    // Convert block indices to matrix indices\n                    hsize_t ii = ii_idx * hdf5_block;\n                    hsize_t jj = jj_idx * hdf5_block;\n                    \n                    // Boundary checks\n                    if (ii &gt;= N || jj &gt;= M) continue;\n                    \n                    // Thread-local variables (each thread needs its own)\n                    hsize_t local_isize = hdf5_block + 1;\n                    hsize_t local_jsize = hdf5_block + 1;\n                    hsize_t local_ksize = hdf5_block + 1;\n                    \n                    hsize_t iRowsA = getOptimBlockSize( N, hdf5_block, ii, local_isize);\n                    if( ii + hdf5_block &gt; N ) local_isize = N - ii;\n                    \n                    hsize_t iRowsB = getOptimBlockSize( M, hdf5_block, jj, local_jsize);\n                    if( jj + hdf5_block &gt; M) local_jsize = M - jj;\n                    \n                    Eigen::MatrixXd C_accum = Eigen::MatrixXd::Zero(iRowsA, iRowsB); \n                    \n                    std::vector&lt;hsize_t&gt; offset = {jj, ii};        \n                    std::vector&lt;hsize_t&gt; count = {iRowsB, iRowsA}; \n                    \n                    for(hsize_t kk = 0; kk &lt; K; kk += hdf5_block)\n                    {\n                        if( kk + hdf5_block &gt; K ) local_ksize = K - kk;\n                        \n                        hsize_t iColsA = getOptimBlockSize( K, hdf5_block, kk, local_ksize),\n                            iColsB = iColsA;\n                        \n                        // Pre-allocate data vectors outside critical section\n                        std::vector&lt;double&gt; vdA(iRowsA * iColsA);\n                        std::vector&lt;double&gt; vdB(iRowsB * iColsB);\n                        \n//                         // Thread-safe I/O: Serialize all HDF5 operations\n// #ifdef _OPENMP\n//                         if (bparal) omp_set_lock(&hdf5_lock);\n// #endif\n                        \n                        // HDF5 read operations (inside critical section)\n                        dsA-&gt;readDatasetBlock( {ii, kk}, {iRowsA,iColsA}, stride, block, vdA.data() );\n                        dsB-&gt;readDatasetBlock( {jj, kk}, {iRowsB, iColsB}, stride, block, vdB.data() );\n                        \n// #ifdef _OPENMP\n//                         if (bparal) omp_unset_lock(&hdf5_lock);\n// #endif\n                        \n                        // Parallel computation (outside critical section)\n                        Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; A (vdA.data(), iRowsA, iColsA );\n                        Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; tmp_B (vdB.data(), iRowsB, iColsB);   \n                        Eigen::MatrixXd B = tmp_B.transpose();\n                        \n                        C_accum.noalias() += A * B;\n                        \n                        // Readjust counters\n                        if( kk + hdf5_block &gt; K ) local_ksize = hdf5_block + 1;\n                        if( iColsA &gt; hdf5_block ) {\n                            kk = kk - hdf5_block + iColsA; \n                        }\n                    }\n                    \n//                     // Thread-safe I/O: Serialize all HDF5 write operations\n// #ifdef _OPENMP\n//                     if (bparal) omp_set_lock(&hdf5_lock);\n// #endif\n                    \n                    dsC-&gt;writeDatasetBlock(Rcpp::wrap(C_accum), offset, count, stride, block, false);\n                    \n                    if (isSymmetric && ii != jj) {\n                        std::vector&lt;hsize_t&gt; offset_sym = {ii, jj};\n                        std::vector&lt;hsize_t&gt; count_sym = {iRowsA, iRowsB};\n                        dsC-&gt;writeDatasetBlock(Rcpp::wrap(C_accum.transpose()), offset_sym, count_sym, stride, block, false);\n                    }\n                    \n// #ifdef _OPENMP\n//                     if (bparal) omp_unset_lock(&hdf5_lock);\n// #endif\n                }\n                \n            } else {\n                throw std::range_error(\"non-conformable arguments\");\n            }\n            \n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception crossprod: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n            return(dsC);\n        }\n        \n        return(dsC);\n    }",
    "crumbs": [
      "Functions",
      "crossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/crossprod.html#usage-example",
    "href": "api-reference/cpp/functions/crossprod.html#usage-example",
    "title": "crossprod",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = crossprod(...);",
    "crumbs": [
      "Functions",
      "crossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgemm_.html",
    "href": "api-reference/cpp/functions/dgemm_.html",
    "title": "dgemm_",
    "section": "",
    "text": "void BigDataStatMeth::dgemm_(char *, char *, int *, int *, int *, double *, double *, int *, double *, int *, double *, double *, int *)",
    "crumbs": [
      "Functions",
      "dgemm_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgemm_.html#signature",
    "href": "api-reference/cpp/functions/dgemm_.html#signature",
    "title": "dgemm_",
    "section": "",
    "text": "void BigDataStatMeth::dgemm_(char *, char *, int *, int *, int *, double *, double *, int *, double *, int *, double *, double *, int *)",
    "crumbs": [
      "Functions",
      "dgemm_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgemm_.html#description",
    "href": "api-reference/cpp/functions/dgemm_.html#description",
    "title": "dgemm_",
    "section": "2 Description",
    "text": "2 Description\nLAPACK DGEMM matrix multiplication.",
    "crumbs": [
      "Functions",
      "dgemm_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgemm_.html#details",
    "href": "api-reference/cpp/functions/dgemm_.html#details",
    "title": "dgemm_",
    "section": "3 Details",
    "text": "3 Details\nExternal LAPACK function for matrix-matrix multiplication: C := alphaop(A)op(B) + beta*C",
    "crumbs": [
      "Functions",
      "dgemm_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgemm_.html#call-graph",
    "href": "api-reference/cpp/functions/dgemm_.html#call-graph",
    "title": "dgemm_",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "dgemm_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgemm_.html#usage-example",
    "href": "api-reference/cpp/functions/dgemm_.html#usage-example",
    "title": "dgemm_",
    "section": "5 Usage Example",
    "text": "5 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = dgemm_(...);",
    "crumbs": [
      "Functions",
      "dgemm_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesv_.html",
    "href": "api-reference/cpp/functions/dgesv_.html",
    "title": "dgesv_",
    "section": "",
    "text": "int BigDataStatMeth::dgesv_(int *, int *, double *, int *, int *, double *, int *, int *)",
    "crumbs": [
      "Functions",
      "dgesv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesv_.html#signature",
    "href": "api-reference/cpp/functions/dgesv_.html#signature",
    "title": "dgesv_",
    "section": "",
    "text": "int BigDataStatMeth::dgesv_(int *, int *, double *, int *, int *, double *, int *, int *)",
    "crumbs": [
      "Functions",
      "dgesv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesv_.html#description",
    "href": "api-reference/cpp/functions/dgesv_.html#description",
    "title": "dgesv_",
    "section": "2 Description",
    "text": "2 Description\nLAPACK routine for solving general linear equations AX = B.",
    "crumbs": [
      "Functions",
      "dgesv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesv_.html#parameters",
    "href": "api-reference/cpp/functions/dgesv_.html#parameters",
    "title": "dgesv_",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nn (``): Pointer to matrix order\nnrhs (``): Pointer to number of right-hand sides\na (``): Pointer to matrix A\nlda (``): Pointer to leading dimension of A\nipiv (``): Pointer to pivot indices\nb (``): Pointer to matrix B\nldb (``): Pointer to leading dimension of B\ninfo (``): Pointer to status information",
    "crumbs": [
      "Functions",
      "dgesv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesv_.html#returns",
    "href": "api-reference/cpp/functions/dgesv_.html#returns",
    "title": "dgesv_",
    "section": "4 Returns",
    "text": "4 Returns\nType: int",
    "crumbs": [
      "Functions",
      "dgesv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesv_.html#details",
    "href": "api-reference/cpp/functions/dgesv_.html#details",
    "title": "dgesv_",
    "section": "5 Details",
    "text": "5 Details\nnPointer to matrix order nrhsPointer to number of right-hand sides aPointer to matrix A ldaPointer to leading dimension of A ipivPointer to pivot indices bPointer to matrix B ldbPointer to leading dimension of B infoPointer to status information",
    "crumbs": [
      "Functions",
      "dgesv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesv_.html#call-graph",
    "href": "api-reference/cpp/functions/dgesv_.html#call-graph",
    "title": "dgesv_",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "dgesv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dgesv_.html#usage-example",
    "href": "api-reference/cpp/functions/dgesv_.html#usage-example",
    "title": "dgesv_",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = dgesv_(...);",
    "crumbs": [
      "Functions",
      "dgesv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/divideDiagonals.html",
    "href": "api-reference/cpp/functions/divideDiagonals.html",
    "title": "divideDiagonals",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::divideDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "divideDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/divideDiagonals.html#signature",
    "href": "api-reference/cpp/functions/divideDiagonals.html#signature",
    "title": "divideDiagonals",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::divideDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "divideDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/divideDiagonals.html#description",
    "href": "api-reference/cpp/functions/divideDiagonals.html#description",
    "title": "divideDiagonals",
    "section": "2 Description",
    "text": "2 Description\nDivide diagonal elements from two matrices or vectors.",
    "crumbs": [
      "Functions",
      "divideDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/divideDiagonals.html#parameters",
    "href": "api-reference/cpp/functions/divideDiagonals.html#parameters",
    "title": "divideDiagonals",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input dataset (dividend)\ndsB (BigDataStatMeth::hdf5Dataset *): Second input dataset (divisor)\ndsResult (BigDataStatMeth::hdf5Dataset *): Result dataset (will be created)\ntarget (std::string): Where to write result: “A”, “B”, or “new”\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "divideDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/divideDiagonals.html#details",
    "href": "api-reference/cpp/functions/divideDiagonals.html#details",
    "title": "divideDiagonals",
    "section": "4 Details",
    "text": "4 Details\nPerforms optimized diagonal division C_diag = A_diag / B_diag. Same optimization strategy as addDiagonals but for element-wise division. Division by zero follows IEEE 754 standard (results in infinity).",
    "crumbs": [
      "Functions",
      "divideDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/divideDiagonals.html#call-graph",
    "href": "api-reference/cpp/functions/divideDiagonals.html#call-graph",
    "title": "divideDiagonals",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "divideDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/divideDiagonals.html#source-code",
    "href": "api-reference/cpp/functions/divideDiagonals.html#source-code",
    "title": "divideDiagonals",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5DiagonalMethods.hpp • Lines 509-528\ninline void divideDiagonals(BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB,\n                                    BigDataStatMeth::hdf5Dataset* dsResult, std::string target = \"new\",\n                                    bool bparal = false, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n        {\n            try {\n                bool isVectorA = isDiagonalVector(dsA);\n                bool isVectorB = isDiagonalVector(dsB);\n                \n                if (isVectorA && isVectorB && (target == \"A\" || target == \"B\")) {\n                    BigDataStatMeth::hdf5Dataset* targetDataset = (target == \"A\") ? dsA : dsB;\n                    Rcpp_vector_divide_hdf5(dsA, dsB, targetDataset, bparal, threads);\n                } else if (isVectorA && isVectorB && target == \"new\") {\n                    Rcpp_vector_divide_hdf5(dsA, dsB, dsResult, bparal, threads);\n                } else {\n                    performMatrixDiagonalOperation(dsA, dsB, dsResult, 3, target, bparal, threads);\n                }\n            } catch(std::exception& ex) {\n                Rf_error(\"Error in divideDiagonals: %s\", ex.what());\n            }\n        }",
    "crumbs": [
      "Functions",
      "divideDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/divideDiagonals.html#usage-example",
    "href": "api-reference/cpp/functions/divideDiagonals.html#usage-example",
    "title": "divideDiagonals",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = divideDiagonals(...);",
    "crumbs": [
      "Functions",
      "divideDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dsysv_.html",
    "href": "api-reference/cpp/functions/dsysv_.html",
    "title": "dsysv_",
    "section": "",
    "text": "int BigDataStatMeth::dsysv_(char *, int *, int *, double *, int *, int *, double *, int *, double *, int *, int *)",
    "crumbs": [
      "Functions",
      "dsysv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dsysv_.html#signature",
    "href": "api-reference/cpp/functions/dsysv_.html#signature",
    "title": "dsysv_",
    "section": "",
    "text": "int BigDataStatMeth::dsysv_(char *, int *, int *, double *, int *, int *, double *, int *, double *, int *, int *)",
    "crumbs": [
      "Functions",
      "dsysv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dsysv_.html#description",
    "href": "api-reference/cpp/functions/dsysv_.html#description",
    "title": "dsysv_",
    "section": "2 Description",
    "text": "2 Description\nLAPACK routine for solving symmetric linear equations AX = B.",
    "crumbs": [
      "Functions",
      "dsysv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dsysv_.html#parameters",
    "href": "api-reference/cpp/functions/dsysv_.html#parameters",
    "title": "dsysv_",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nuplo (``): Pointer to upper/lower triangular indicator\nn (``): Pointer to matrix order\nnrhs (``): Pointer to number of right-hand sides\na (``): Pointer to matrix A\nlda (``): Pointer to leading dimension of A\nipiv (``): Pointer to pivot indices\nb (``): Pointer to matrix B\nldb (``): Pointer to leading dimension of B\nwork (``): Pointer to workspace array\nlwork (``): Pointer to workspace size\ninfo (``): Pointer to status information",
    "crumbs": [
      "Functions",
      "dsysv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dsysv_.html#returns",
    "href": "api-reference/cpp/functions/dsysv_.html#returns",
    "title": "dsysv_",
    "section": "4 Returns",
    "text": "4 Returns\nType: int",
    "crumbs": [
      "Functions",
      "dsysv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dsysv_.html#details",
    "href": "api-reference/cpp/functions/dsysv_.html#details",
    "title": "dsysv_",
    "section": "5 Details",
    "text": "5 Details\nuploPointer to upper/lower triangular indicator nPointer to matrix order nrhsPointer to number of right-hand sides aPointer to matrix A ldaPointer to leading dimension of A ipivPointer to pivot indices bPointer to matrix B ldbPointer to leading dimension of B workPointer to workspace array lworkPointer to workspace size infoPointer to status information",
    "crumbs": [
      "Functions",
      "dsysv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dsysv_.html#call-graph",
    "href": "api-reference/cpp/functions/dsysv_.html#call-graph",
    "title": "dsysv_",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "dsysv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/dsysv_.html#usage-example",
    "href": "api-reference/cpp/functions/dsysv_.html#usage-example",
    "title": "dsysv_",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = dsysv_(...);",
    "crumbs": [
      "Functions",
      "dsysv_"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/exists_HDF5_element.html",
    "href": "api-reference/cpp/functions/exists_HDF5_element.html",
    "title": "exists_HDF5_element",
    "section": "",
    "text": "bool BigDataStatMeth::exists_HDF5_element(H5::H5File *file, std::string element)",
    "crumbs": [
      "Functions",
      "exists_HDF5_element"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/exists_HDF5_element.html#signature",
    "href": "api-reference/cpp/functions/exists_HDF5_element.html#signature",
    "title": "exists_HDF5_element",
    "section": "",
    "text": "bool BigDataStatMeth::exists_HDF5_element(H5::H5File *file, std::string element)",
    "crumbs": [
      "Functions",
      "exists_HDF5_element"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/exists_HDF5_element.html#description",
    "href": "api-reference/cpp/functions/exists_HDF5_element.html#description",
    "title": "exists_HDF5_element",
    "section": "2 Description",
    "text": "2 Description\nChecks if an HDF5 element (group or dataset) exists.",
    "crumbs": [
      "Functions",
      "exists_HDF5_element"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/exists_HDF5_element.html#parameters",
    "href": "api-reference/cpp/functions/exists_HDF5_element.html#parameters",
    "title": "exists_HDF5_element",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfile (H5::H5File *): Pointer to HDF5 file\nelement (std::string): Path to the element to check",
    "crumbs": [
      "Functions",
      "exists_HDF5_element"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/exists_HDF5_element.html#returns",
    "href": "api-reference/cpp/functions/exists_HDF5_element.html#returns",
    "title": "exists_HDF5_element",
    "section": "4 Returns",
    "text": "4 Returns\nbool True if element exists, false otherwise",
    "crumbs": [
      "Functions",
      "exists_HDF5_element"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/exists_HDF5_element.html#details",
    "href": "api-reference/cpp/functions/exists_HDF5_element.html#details",
    "title": "exists_HDF5_element",
    "section": "5 Details",
    "text": "5 Details\nfilePointer to HDF5 file elementPath to the element to check bool True if element exists, false otherwiseH5::FileIExceptionon file operation errorsHandles trailing slashes in group paths",
    "crumbs": [
      "Functions",
      "exists_HDF5_element"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/exists_HDF5_element.html#call-graph",
    "href": "api-reference/cpp/functions/exists_HDF5_element.html#call-graph",
    "title": "exists_HDF5_element",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "exists_HDF5_element"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/exists_HDF5_element.html#source-code",
    "href": "api-reference/cpp/functions/exists_HDF5_element.html#source-code",
    "title": "exists_HDF5_element",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5Utilities.hpp • Lines 68-95\ninline bool exists_HDF5_element(H5::H5File* file, std::string element)\n    {\n        bool bexists = false;\n        try\n        {\n            // H5::Exception::dontPrint();\n         \n             if( element.substr(element.length(), element.length()) == \"/\" ) {\n                 element = element.substr( 0, element.length()-1);\n             } \n             \n            // Search dataset\n            if(pathExists( file-&gt;getId(), element)) \n                bexists = true;\n            \n        } catch(H5::FileIException& error) { // catch failure caused by the H5File operations\n            file-&gt;close();\n            Rcpp::Rcerr&lt;&lt;\"c++ exception exists_HDF5_element (File IException)\" &lt;&lt; std::endl;\n            return bexists;\n        } catch(std::exception &ex) {\n            Rcpp::Rcerr &lt;&lt; \"c++ exception exists_HDF5_element: \" &lt;&lt; ex.what();\n            return(bexists);\n        }  catch (...) {\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception exists_HDF5_element (unknown reason)\";\n            return(bexists);\n        }   \n        return bexists;\n    }",
    "crumbs": [
      "Functions",
      "exists_HDF5_element"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/exists_HDF5_element.html#usage-example",
    "href": "api-reference/cpp/functions/exists_HDF5_element.html#usage-example",
    "title": "exists_HDF5_element",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = exists_HDF5_element(...);",
    "crumbs": [
      "Functions",
      "exists_HDF5_element"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/find_all.html",
    "href": "api-reference/cpp/functions/find_all.html",
    "title": "find_all",
    "section": "",
    "text": "std::vector&lt; _InputIterator &gt; BigDataStatMeth::find_all(_InputIterator begin, _InputIterator end, const T &val)",
    "crumbs": [
      "Functions",
      "find_all"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/find_all.html#signature",
    "href": "api-reference/cpp/functions/find_all.html#signature",
    "title": "find_all",
    "section": "",
    "text": "std::vector&lt; _InputIterator &gt; BigDataStatMeth::find_all(_InputIterator begin, _InputIterator end, const T &val)",
    "crumbs": [
      "Functions",
      "find_all"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/find_all.html#description",
    "href": "api-reference/cpp/functions/find_all.html#description",
    "title": "find_all",
    "section": "2 Description",
    "text": "2 Description\nFinds all occurrences of a value in a range.",
    "crumbs": [
      "Functions",
      "find_all"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/find_all.html#parameters",
    "href": "api-reference/cpp/functions/find_all.html#parameters",
    "title": "find_all",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nbegin (_InputIterator): Iterator to the start of the range\nend (_InputIterator): Iterator to the end of the range\nval (const T &): Value to search for",
    "crumbs": [
      "Functions",
      "find_all"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/find_all.html#returns",
    "href": "api-reference/cpp/functions/find_all.html#returns",
    "title": "find_all",
    "section": "4 Returns",
    "text": "4 Returns\nstd::vector&lt;_InputIterator&gt; Vector of iterators pointing to matches",
    "crumbs": [
      "Functions",
      "find_all"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/find_all.html#details",
    "href": "api-reference/cpp/functions/find_all.html#details",
    "title": "find_all",
    "section": "5 Details",
    "text": "5 Details\nTemplate function that searches for all occurrences of a value in a range defined by iterators and returns their positions.",
    "crumbs": [
      "Functions",
      "find_all"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/find_all.html#caller-graph",
    "href": "api-reference/cpp/functions/find_all.html#caller-graph",
    "title": "find_all",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "find_all"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/find_all.html#source-code",
    "href": "api-reference/cpp/functions/find_all.html#source-code",
    "title": "find_all",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5SortDataset.hpp • Lines 47-58\nfind_all(_InputIterator begin, _InputIterator end, const T& val)\n    {\n        std::vector&lt;_InputIterator&gt; matches;\n        while(begin != end)\n        {\n            if((*begin) == val)\n                matches.push_back(begin);\n            ++begin;\n        }\n        \n        return matches;\n    }",
    "crumbs": [
      "Functions",
      "find_all"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/find_all.html#usage-example",
    "href": "api-reference/cpp/functions/find_all.html#usage-example",
    "title": "find_all",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = find_all(...);",
    "crumbs": [
      "Functions",
      "find_all"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes.html",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes.html",
    "title": "getBlockPositionsSizes",
    "section": "",
    "text": "void BigDataStatMeth::getBlockPositionsSizes(hsize_t maxPosition, hsize_t blockSize, std::vector&lt; hsize_t &gt; &starts, std::vector&lt; hsize_t &gt; &sizes)",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes.html#signature",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes.html#signature",
    "title": "getBlockPositionsSizes",
    "section": "",
    "text": "void BigDataStatMeth::getBlockPositionsSizes(hsize_t maxPosition, hsize_t blockSize, std::vector&lt; hsize_t &gt; &starts, std::vector&lt; hsize_t &gt; &sizes)",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes.html#description",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes.html#description",
    "title": "getBlockPositionsSizes",
    "section": "2 Description",
    "text": "2 Description\nCalculate block positions and sizes for matrix operations.",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes.html#parameters",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes.html#parameters",
    "title": "getBlockPositionsSizes",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nmaxPosition (hsize_t): Maximum position to process\nblockSize (hsize_t): Size of each block\nstarts (std::vector&lt; hsize_t &gt; &): Vector to store starting positions of blocks\nsizes (std::vector&lt; hsize_t &gt; &): Vector to store sizes of blocks",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes.html#details",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes.html#details",
    "title": "getBlockPositionsSizes",
    "section": "4 Details",
    "text": "4 Details\nDetermines optimal block positions and sizes for block-based matrix operations, ensuring efficient memory usage and processing.",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes.html#call-graph",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes.html#call-graph",
    "title": "getBlockPositionsSizes",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes.html#source-code",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes.html#source-code",
    "title": "getBlockPositionsSizes",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memOtherFunctions.hpp • Lines 25-44\ninline void getBlockPositionsSizes( hsize_t maxPosition, hsize_t blockSize, std::vector&lt;hsize_t&gt;& starts, std::vector&lt;hsize_t&gt;& sizes ){\n        \n        hsize_t isize = blockSize + 1;\n        \n        for (hsize_t ii = 0; ii &lt; maxPosition; ii += blockSize)\n        {\n            if( ii + blockSize &gt; maxPosition ) {\n                isize = maxPosition - ii; }\n            \n            hsize_t sizetoRead = getOptimBlockSize( maxPosition, blockSize, ii, isize);\n            \n            starts.push_back(ii);\n            sizes.push_back(sizetoRead);\n            \n            if( ii + blockSize &gt; maxPosition ) isize = blockSize + 1;\n            if( sizetoRead &gt; blockSize ) {\n                ii = ii - blockSize + sizetoRead; }\n        }\n        \n    }",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes.html#usage-example",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes.html#usage-example",
    "title": "getBlockPositionsSizes",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getBlockPositionsSizes(...);",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html",
    "title": "getBlockPositionsSizes_mat",
    "section": "",
    "text": "void BigDataStatMeth::getBlockPositionsSizes_mat(hsize_t maxPosition, hsize_t blockSize, std::vector&lt; hsize_t &gt; &starts, std::vector&lt; hsize_t &gt; &sizes)",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_mat"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#signature",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#signature",
    "title": "getBlockPositionsSizes_mat",
    "section": "",
    "text": "void BigDataStatMeth::getBlockPositionsSizes_mat(hsize_t maxPosition, hsize_t blockSize, std::vector&lt; hsize_t &gt; &starts, std::vector&lt; hsize_t &gt; &sizes)",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_mat"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#description",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#description",
    "title": "getBlockPositionsSizes_mat",
    "section": "2 Description",
    "text": "2 Description\nCalculate block positions and sizes for matrix operations.",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_mat"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#parameters",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#parameters",
    "title": "getBlockPositionsSizes_mat",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nmaxPosition (hsize_t): Maximum position to process\nblockSize (hsize_t): Size of each block\nstarts (std::vector&lt; hsize_t &gt; &): Vector to store starting positions of blocks\nsizes (std::vector&lt; hsize_t &gt; &): Vector to store sizes of blocks",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_mat"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#details",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#details",
    "title": "getBlockPositionsSizes_mat",
    "section": "4 Details",
    "text": "4 Details\nDetermines optimal block positions and sizes for block-based matrix operations, ensuring efficient memory usage and processing.",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_mat"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#call-graph",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#call-graph",
    "title": "getBlockPositionsSizes_mat",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_mat"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#source-code",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#source-code",
    "title": "getBlockPositionsSizes_mat",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memMultiplication.hpp • Lines 85-103\ninline void getBlockPositionsSizes_mat( hsize_t maxPosition, hsize_t blockSize, std::vector&lt;hsize_t&gt;& starts, std::vector&lt;hsize_t&gt;& sizes ){\n        \n        hsize_t isize = blockSize + 1;\n        \n        for (hsize_t ii = 0; ii &lt; maxPosition; ii += blockSize)\n        {\n            if( ii + blockSize &gt; maxPosition ) {\n                isize = maxPosition - ii; }\n            \n            hsize_t sizetoRead = getOptimBlockSize( maxPosition, blockSize, ii, isize);\n            \n            starts.push_back(ii);\n            sizes.push_back(sizetoRead);\n            \n            if( sizetoRead &gt; blockSize ) {\n                ii = ii - blockSize + sizetoRead; }\n        }\n        \n    }",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_mat"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#usage-example",
    "href": "api-reference/cpp/functions/getBlockPositionsSizes_mat.html#usage-example",
    "title": "getBlockPositionsSizes_mat",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getBlockPositionsSizes_mat(...);",
    "crumbs": [
      "Functions",
      "getBlockPositionsSizes_mat"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads_R.html",
    "href": "api-reference/cpp/functions/getDTthreads_R.html",
    "title": "getDTthreads_R",
    "section": "",
    "text": "SEXP getDTthreads_R(SEXP verbose)",
    "crumbs": [
      "Functions",
      "getDTthreads_R"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads_R.html#signature",
    "href": "api-reference/cpp/functions/getDTthreads_R.html#signature",
    "title": "getDTthreads_R",
    "section": "",
    "text": "SEXP getDTthreads_R(SEXP verbose)",
    "crumbs": [
      "Functions",
      "getDTthreads_R"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads_R.html#description",
    "href": "api-reference/cpp/functions/getDTthreads_R.html#description",
    "title": "getDTthreads_R",
    "section": "2 Description",
    "text": "2 Description\nR interface for thread configuration inspection.",
    "crumbs": [
      "Functions",
      "getDTthreads_R"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads_R.html#parameters",
    "href": "api-reference/cpp/functions/getDTthreads_R.html#parameters",
    "title": "getDTthreads_R",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nverbose (SEXP): Whether to print detailed configuration",
    "crumbs": [
      "Functions",
      "getDTthreads_R"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads_R.html#returns",
    "href": "api-reference/cpp/functions/getDTthreads_R.html#returns",
    "title": "getDTthreads_R",
    "section": "4 Returns",
    "text": "4 Returns\nSEXP Integer SEXP with thread count",
    "crumbs": [
      "Functions",
      "getDTthreads_R"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads_R.html#details",
    "href": "api-reference/cpp/functions/getDTthreads_R.html#details",
    "title": "getDTthreads_R",
    "section": "5 Details",
    "text": "5 Details\nverboseWhether to print detailed configuration SEXP Integer SEXP with thread count When verbose:Prints OpenMP versionShows processor countLists all relevant environment variablesDisplays current thread settings",
    "crumbs": [
      "Functions",
      "getDTthreads_R"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads_R.html#call-graph",
    "href": "api-reference/cpp/functions/getDTthreads_R.html#call-graph",
    "title": "getDTthreads_R",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "getDTthreads_R"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads_R.html#source-code",
    "href": "api-reference/cpp/functions/getDTthreads_R.html#source-code",
    "title": "getDTthreads_R",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/openme-utils.hpp • Lines 218-239\ninline SEXP getDTthreads_R(SEXP verbose) {\n    if(!IS_TRUE_OR_FALSE(verbose))\n        Rf_error((\"%s must be TRUE or FALSE\"), \"verbose\");\n    if (LOGICAL(verbose)[0]) {\n#ifndef _OPENMP\n        Rprintf((\"This installation of BigDataStatMeth has not been compiled with OpenMP support.\\n\"));\n#else\n        Rprintf((\"  OpenMP version (_OPENMP)       %d\\n\"), _OPENMP); // user can use Google to map 201511 to 4.5; it's odd that OpenMP API does not provide 4.5\n#endif\n        Rprintf((\"  omp_get_num_procs()            %d\\n\"), omp_get_num_procs());\n        Rprintf((\"  R_DATATABLE_NUM_PROCS_PERCENT  %s\\n\"), mygetenv(\"R_DATATABLE_NUM_PROCS_PERCENT\", \"unset (default 50)\"));\n        Rprintf((\"  R_DATATABLE_NUM_THREADS        %s\\n\"), mygetenv(\"R_DATATABLE_NUM_THREADS\", \"unset\"));\n        Rprintf((\"  R_DATATABLE_THROTTLE           %s\\n\"), mygetenv(\"R_DATATABLE_THROTTLE\", \"unset (default 1024)\"));\n        Rprintf((\"  omp_get_thread_limit()         %d\\n\"), omp_get_thread_limit());\n        Rprintf((\"  omp_get_max_threads()          %d\\n\"), omp_get_max_threads());\n        Rprintf((\"  OMP_THREAD_LIMIT               %s\\n\"), mygetenv(\"OMP_THREAD_LIMIT\", \"unset\"));  // CRAN sets to 2\n        Rprintf((\"  OMP_NUM_THREADS                %s\\n\"), mygetenv(\"OMP_NUM_THREADS\", \"unset\"));\n        Rprintf((\"  RestoreAfterFork               %s\\n\"), RestoreAfterFork ? \"true\" : \"false\");\n        Rprintf((\"  BigDataStatMeth is using %d threads with throttle==%d.\\n\"), getDTthreads(INT_MAX, false), DTthrottle);\n    }\n    return Rf_ScalarInteger(getDTthreads(INT_MAX, false));\n}",
    "crumbs": [
      "Functions",
      "getDTthreads_R"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getDTthreads_R.html#usage-example",
    "href": "api-reference/cpp/functions/getDTthreads_R.html#usage-example",
    "title": "getDTthreads_R",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getDTthreads_R(...);",
    "crumbs": [
      "Functions",
      "getDTthreads_R"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getGeneralSortRule.html",
    "href": "api-reference/cpp/functions/getGeneralSortRule.html",
    "title": "getGeneralSortRule",
    "section": "",
    "text": "Spectra::SortRule BigDataStatMeth::getGeneralSortRule(const std::string &which)",
    "crumbs": [
      "Functions",
      "getGeneralSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getGeneralSortRule.html#signature",
    "href": "api-reference/cpp/functions/getGeneralSortRule.html#signature",
    "title": "getGeneralSortRule",
    "section": "",
    "text": "Spectra::SortRule BigDataStatMeth::getGeneralSortRule(const std::string &which)",
    "crumbs": [
      "Functions",
      "getGeneralSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getGeneralSortRule.html#description",
    "href": "api-reference/cpp/functions/getGeneralSortRule.html#description",
    "title": "getGeneralSortRule",
    "section": "2 Description",
    "text": "2 Description\nConvert ‘which’ string to Spectra SortRule for general matrices.",
    "crumbs": [
      "Functions",
      "getGeneralSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getGeneralSortRule.html#parameters",
    "href": "api-reference/cpp/functions/getGeneralSortRule.html#parameters",
    "title": "getGeneralSortRule",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nwhich (const std::string &): String indicating which eigenvalues to compute",
    "crumbs": [
      "Functions",
      "getGeneralSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getGeneralSortRule.html#returns",
    "href": "api-reference/cpp/functions/getGeneralSortRule.html#returns",
    "title": "getGeneralSortRule",
    "section": "4 Returns",
    "text": "4 Returns\nCorresponding Spectra SortRule for general eigenproblems",
    "crumbs": [
      "Functions",
      "getGeneralSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getGeneralSortRule.html#details",
    "href": "api-reference/cpp/functions/getGeneralSortRule.html#details",
    "title": "getGeneralSortRule",
    "section": "5 Details",
    "text": "5 Details\nwhichString indicating which eigenvalues to compute Corresponding Spectra SortRule for general eigenproblems",
    "crumbs": [
      "Functions",
      "getGeneralSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getGeneralSortRule.html#caller-graph",
    "href": "api-reference/cpp/functions/getGeneralSortRule.html#caller-graph",
    "title": "getGeneralSortRule",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "getGeneralSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getGeneralSortRule.html#source-code",
    "href": "api-reference/cpp/functions/getGeneralSortRule.html#source-code",
    "title": "getGeneralSortRule",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixEigenDecomposition.hpp • Lines 54-63\ninline Spectra::SortRule getGeneralSortRule(const std::string& which) {\n        if (which == \"LM\") return Spectra::SortRule::LargestMagn;\n        if (which == \"SM\") return Spectra::SortRule::SmallestMagn;\n        if (which == \"LR\") return Spectra::SortRule::LargestReal;\n        if (which == \"SR\") return Spectra::SortRule::SmallestReal;\n        if (which == \"LI\") return Spectra::SortRule::LargestImag;\n        if (which == \"SI\") return Spectra::SortRule::SmallestImag;\n        // Default to largest magnitude for general matrices\n        return Spectra::SortRule::LargestMagn;\n    }",
    "crumbs": [
      "Functions",
      "getGeneralSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getGeneralSortRule.html#usage-example",
    "href": "api-reference/cpp/functions/getGeneralSortRule.html#usage-example",
    "title": "getGeneralSortRule",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getGeneralSortRule(...);",
    "crumbs": [
      "Functions",
      "getGeneralSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getIntEnv.html",
    "href": "api-reference/cpp/functions/getIntEnv.html",
    "title": "getIntEnv",
    "section": "",
    "text": "static int getIntEnv(const char *name, int def)",
    "crumbs": [
      "Functions",
      "getIntEnv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getIntEnv.html#signature",
    "href": "api-reference/cpp/functions/getIntEnv.html#signature",
    "title": "getIntEnv",
    "section": "",
    "text": "static int getIntEnv(const char *name, int def)",
    "crumbs": [
      "Functions",
      "getIntEnv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getIntEnv.html#description",
    "href": "api-reference/cpp/functions/getIntEnv.html#description",
    "title": "getIntEnv",
    "section": "2 Description",
    "text": "2 Description\nRetrieves integer value from environment variable.",
    "crumbs": [
      "Functions",
      "getIntEnv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getIntEnv.html#parameters",
    "href": "api-reference/cpp/functions/getIntEnv.html#parameters",
    "title": "getIntEnv",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nname (const char *): Environment variable name\ndef (int): Default value if not found or invalid",
    "crumbs": [
      "Functions",
      "getIntEnv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getIntEnv.html#returns",
    "href": "api-reference/cpp/functions/getIntEnv.html#returns",
    "title": "getIntEnv",
    "section": "4 Returns",
    "text": "4 Returns\nint Parsed value or default",
    "crumbs": [
      "Functions",
      "getIntEnv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getIntEnv.html#details",
    "href": "api-reference/cpp/functions/getIntEnv.html#details",
    "title": "getIntEnv",
    "section": "5 Details",
    "text": "5 Details\nnameEnvironment variable name defDefault value if not found or invalid int Parsed value or defaultHandles empty or missing environment variablesValidates numeric formatEnsures positive integer valuesIssues warning for invalid values",
    "crumbs": [
      "Functions",
      "getIntEnv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getIntEnv.html#call-graph",
    "href": "api-reference/cpp/functions/getIntEnv.html#call-graph",
    "title": "getIntEnv",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "getIntEnv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getIntEnv.html#source-code",
    "href": "api-reference/cpp/functions/getIntEnv.html#source-code",
    "title": "getIntEnv",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/openme-utils.hpp • Lines 66-82\nstatic int getIntEnv(const char *name, int def)\n{\n    const char *val = getenv(name);\n    if (val==NULL) return def;\n    size_t nchar = strlen(val);\n    if (nchar==0) return def;\n    char *end;\n    errno = 0;\n    long int ans = strtol(val, &end, 10);  // ignores leading whitespace. If it fully consumed the string, *end=='\\0' and isspace('\\0')==false\n    while (isspace(*end)) end++;  // ignore trailing whitespace\n    if (errno || (size_t)(end-val)!=nchar || ans&lt;1 || ans&gt;INT_MAX) {\n        Rcpp::warning((\"Ignoring invalid %s==\\\"%s\\\". Not an integer &gt;= 1. Please remove any characters that are not a digit [0-9].\"), name, val);\n        return def;\n        \n    }\n    return (int)ans;\n}",
    "crumbs": [
      "Functions",
      "getIntEnv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getIntEnv.html#usage-example",
    "href": "api-reference/cpp/functions/getIntEnv.html#usage-example",
    "title": "getIntEnv",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getIntEnv(...);",
    "crumbs": [
      "Functions",
      "getIntEnv"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMaxBlockSize.html",
    "href": "api-reference/cpp/functions/getMaxBlockSize.html",
    "title": "getMaxBlockSize",
    "section": "",
    "text": "int BigDataStatMeth::getMaxBlockSize(int nRowsA, int nColsA, int nRowsB, int nColsB, int ifactor, Rcpp::Nullable&lt; int &gt; block_size=R_NilValue)",
    "crumbs": [
      "Functions",
      "getMaxBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMaxBlockSize.html#signature",
    "href": "api-reference/cpp/functions/getMaxBlockSize.html#signature",
    "title": "getMaxBlockSize",
    "section": "",
    "text": "int BigDataStatMeth::getMaxBlockSize(int nRowsA, int nColsA, int nRowsB, int nColsB, int ifactor, Rcpp::Nullable&lt; int &gt; block_size=R_NilValue)",
    "crumbs": [
      "Functions",
      "getMaxBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMaxBlockSize.html#description",
    "href": "api-reference/cpp/functions/getMaxBlockSize.html#description",
    "title": "getMaxBlockSize",
    "section": "2 Description",
    "text": "2 Description\nCalculates optimal block size for matrix operations.",
    "crumbs": [
      "Functions",
      "getMaxBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMaxBlockSize.html#parameters",
    "href": "api-reference/cpp/functions/getMaxBlockSize.html#parameters",
    "title": "getMaxBlockSize",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nnRowsA (int): Number of rows in matrix A\nnColsA (int): Number of columns in matrix A\nnRowsB (int): Number of rows in matrix B\nnColsB (int): Number of columns in matrix B\nifactor (int): Block size scaling factor\nblock_size (Rcpp::Nullable&lt; int &gt;): Optional user-specified block size",
    "crumbs": [
      "Functions",
      "getMaxBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMaxBlockSize.html#returns",
    "href": "api-reference/cpp/functions/getMaxBlockSize.html#returns",
    "title": "getMaxBlockSize",
    "section": "4 Returns",
    "text": "4 Returns\nint Optimal block size",
    "crumbs": [
      "Functions",
      "getMaxBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMaxBlockSize.html#details",
    "href": "api-reference/cpp/functions/getMaxBlockSize.html#details",
    "title": "getMaxBlockSize",
    "section": "5 Details",
    "text": "5 Details\nnRowsANumber of rows in matrix A nColsANumber of columns in matrix A nRowsBNumber of rows in matrix B nColsBNumber of columns in matrix B ifactorBlock size scaling factor block_sizeOptional user-specified block size int Optimal block size Block size determination:Considers matrix dimensionsRespects maximum block size limitsAllows user override with warningsOptimizes for memory usage",
    "crumbs": [
      "Functions",
      "getMaxBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMaxBlockSize.html#source-code",
    "href": "api-reference/cpp/functions/getMaxBlockSize.html#source-code",
    "title": "getMaxBlockSize",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/Utilities.hpp • Lines 251-279\ninline int getMaxBlockSize ( int nRowsA, int nColsA, int nRowsB, int nColsB, int ifactor, Rcpp::Nullable&lt;int&gt; block_size = R_NilValue) \n    {\n        \n        int iblock_size;\n        \n        try\n        {\n            \n            iblock_size = std::min( std::min( nRowsA, nColsA), std::min( nRowsB, nColsB) );\n\n            if (block_size.isNotNull()) {\n                // if( Rcpp::as&lt;int&gt; (block_size) &lt; iblock_size ) {\n                //     iblock_size = Rcpp::as&lt;int&gt; (block_size); }\n                iblock_size = Rcpp::as&lt;int&gt; (block_size);\n                if( (unsigned)iblock_size &gt; (MAXBLOCKSIZE / ifactor) ) {\n                    Rcpp::warning(\"Warning: block size %i is bigger than the maximum recomended %i.\", iblock_size, (MAXBLOCKSIZE / ifactor));\n                }\n            } else {\n                //..// iblock_size = std::min(  std::min(dsA-&gt;nrows(),dsA-&gt;ncols()),  std::min(dsB-&gt;nrows(), dsB-&gt;ncols()));\n                if ((unsigned)iblock_size &gt; (MAXBLOCKSIZE / ifactor))\n                    iblock_size = MAXBLOCKSIZE / ifactor;\n            }\n                \n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception getObjecDataType: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n        }\n        \n        return(iblock_size);\n    }",
    "crumbs": [
      "Functions",
      "getMaxBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getMaxBlockSize.html#usage-example",
    "href": "api-reference/cpp/functions/getMaxBlockSize.html#usage-example",
    "title": "getMaxBlockSize",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getMaxBlockSize(...);",
    "crumbs": [
      "Functions",
      "getMaxBlockSize"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjectDims.html",
    "href": "api-reference/cpp/functions/getObjectDims.html",
    "title": "getObjectDims",
    "section": "",
    "text": "Rcpp::IntegerVector BigDataStatMeth::getObjectDims(Rcpp::RObject obj, std::string strtype)",
    "crumbs": [
      "Functions",
      "getObjectDims"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjectDims.html#signature",
    "href": "api-reference/cpp/functions/getObjectDims.html#signature",
    "title": "getObjectDims",
    "section": "",
    "text": "Rcpp::IntegerVector BigDataStatMeth::getObjectDims(Rcpp::RObject obj, std::string strtype)",
    "crumbs": [
      "Functions",
      "getObjectDims"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjectDims.html#description",
    "href": "api-reference/cpp/functions/getObjectDims.html#description",
    "title": "getObjectDims",
    "section": "2 Description",
    "text": "2 Description\nGets dimensions of an R object.",
    "crumbs": [
      "Functions",
      "getObjectDims"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjectDims.html#parameters",
    "href": "api-reference/cpp/functions/getObjectDims.html#parameters",
    "title": "getObjectDims",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nobj (Rcpp::RObject): R object to measure\nstrtype (std::string): Optional pre-determined object type",
    "crumbs": [
      "Functions",
      "getObjectDims"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjectDims.html#returns",
    "href": "api-reference/cpp/functions/getObjectDims.html#returns",
    "title": "getObjectDims",
    "section": "4 Returns",
    "text": "4 Returns\nRcpp::IntegerVector Two-element vector with dimensions",
    "crumbs": [
      "Functions",
      "getObjectDims"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjectDims.html#details",
    "href": "api-reference/cpp/functions/getObjectDims.html#details",
    "title": "getObjectDims",
    "section": "5 Details",
    "text": "5 Details\nobjR object to measure strtypeOptional pre-determined object type Rcpp::IntegerVector Two-element vector with dimensionsReturns [1, length] for vectorsReturns [rows, cols] for matricesReturns [nrow, ncol] for data framesReturns [0, 0] for unsupported types",
    "crumbs": [
      "Functions",
      "getObjectDims"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjectDims.html#call-graph",
    "href": "api-reference/cpp/functions/getObjectDims.html#call-graph",
    "title": "getObjectDims",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "getObjectDims"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjectDims.html#source-code",
    "href": "api-reference/cpp/functions/getObjectDims.html#source-code",
    "title": "getObjectDims",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/Utilities.hpp • Lines 179-228\ninline Rcpp::IntegerVector getObjectDims(Rcpp::RObject obj, std::string strtype) \n    {\n        \n        Rcpp::IntegerVector dims(2);\n        \n        try \n        {\n            if(strtype ==\"\") {\n                strtype = getObjecDataType(obj);    \n            }\n            \n            if( strtype == \"numeric\"  || strtype == \"int\" || strtype == \"factor\" ){\n                if( Rf_isMatrix(obj)) {\n                    dims[0] = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(obj).rows();\n                    dims[1] = Rcpp::as&lt;Rcpp::NumericMatrix&gt;(obj).cols();\n                } else {\n                    dims[0] = 1;\n                    dims[1] = Rcpp::as&lt;Rcpp::NumericVector&gt;(obj).length();\n                }\n            }else if( strtype == \"logic\" ) {\n                if( Rf_isMatrix(obj)) {\n                    dims[0] = Rcpp::as&lt;Rcpp::LogicalMatrix&gt;(obj).rows();\n                    dims[1] = Rcpp::as&lt;Rcpp::LogicalMatrix&gt;(obj).cols();\n                } else {\n                    dims[0] = 1;\n                    dims[1] = Rcpp::as&lt;Rcpp::LogicalVector&gt;(obj).length();\n                }\n            } else if( strtype == \"char\" ){\n                if( Rf_isMatrix(obj)) {\n                    dims[0] = Rcpp::as&lt;Rcpp::CharacterMatrix&gt;(obj).rows();\n                    dims[1] = Rcpp::as&lt;Rcpp::CharacterMatrix&gt;(obj).cols();\n                } else {\n                    dims[0] = 1;\n                    dims[1] = Rcpp::as&lt;Rcpp::CharacterVector&gt;(obj).length();\n                }\n            } else if(strtype == \"dataframe\"){\n                dims[0] = Rcpp::as&lt;Rcpp::DataFrame&gt;(obj).nrows();\n                dims[1] = Rcpp::as&lt;Rcpp::DataFrame&gt;(obj).length();\n                \n            } else {\n                dims[0] = 0;\n                dims[1] = 0;\n            }\n            \n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception getObjecDataType: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n        }\n        \n        return(dims);\n    }",
    "crumbs": [
      "Functions",
      "getObjectDims"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getObjectDims.html#usage-example",
    "href": "api-reference/cpp/functions/getObjectDims.html#usage-example",
    "title": "getObjectDims",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getObjectDims(...);",
    "crumbs": [
      "Functions",
      "getObjectDims"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimalBlockElements.html",
    "href": "api-reference/cpp/functions/getOptimalBlockElements.html",
    "title": "getOptimalBlockElements",
    "section": "",
    "text": "size_t BigDataStatMeth::getOptimalBlockElements()",
    "crumbs": [
      "Functions",
      "getOptimalBlockElements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimalBlockElements.html#signature",
    "href": "api-reference/cpp/functions/getOptimalBlockElements.html#signature",
    "title": "getOptimalBlockElements",
    "section": "",
    "text": "size_t BigDataStatMeth::getOptimalBlockElements()",
    "crumbs": [
      "Functions",
      "getOptimalBlockElements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimalBlockElements.html#description",
    "href": "api-reference/cpp/functions/getOptimalBlockElements.html#description",
    "title": "getOptimalBlockElements",
    "section": "2 Description",
    "text": "2 Description\nCalculates optimal block size for memory-efficient matrix operations.",
    "crumbs": [
      "Functions",
      "getOptimalBlockElements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimalBlockElements.html#returns",
    "href": "api-reference/cpp/functions/getOptimalBlockElements.html#returns",
    "title": "getOptimalBlockElements",
    "section": "3 Returns",
    "text": "3 Returns\nsize_t Optimal number of matrix elements per processing block",
    "crumbs": [
      "Functions",
      "getOptimalBlockElements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimalBlockElements.html#details",
    "href": "api-reference/cpp/functions/getOptimalBlockElements.html#details",
    "title": "getOptimalBlockElements",
    "section": "4 Details",
    "text": "4 Details\nDetermines optimal block element count for BigDataStatMeth algorithms based on available system memory. Provides adaptive block sizing that scales performance while maintaining compatibility across diverse hardware. Includes responsible resource usage for shared HPC environments.",
    "crumbs": [
      "Functions",
      "getOptimalBlockElements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimalBlockElements.html#call-graph",
    "href": "api-reference/cpp/functions/getOptimalBlockElements.html#call-graph",
    "title": "getOptimalBlockElements",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "getOptimalBlockElements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimalBlockElements.html#source-code",
    "href": "api-reference/cpp/functions/getOptimalBlockElements.html#source-code",
    "title": "getOptimalBlockElements",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/system-utils.hpp • Lines 102-116\ninline size_t getOptimalBlockElements() {\n    size_t availableMB = getAvailableMemoryMB();\n    \n    if (availableMB &gt; 64000) {        // &gt;64GB available (HPC/Server systems)\n        // Responsible usage: limit to ~4GB blocks even on 1TB systems\n        // Allows other users to share resources effectively\n        return 500000000;             // ~4GB blocks (500M * 8 bytes)\n    } else if (availableMB &gt; 16000) { // &gt;16GB available memory\n        return 200000000;             // ~1.6GB blocks (200M * 8 bytes)\n    } else if (availableMB &gt; 8000) {  // &gt;8GB available memory\n        return 125000000;             // ~1GB blocks (125M * 8 bytes)\n    } else {                          // Conservative for &lt;8GB systems\n        return 75000000;              // ~600MB blocks (75M * 8 bytes)\n    }\n}",
    "crumbs": [
      "Functions",
      "getOptimalBlockElements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getOptimalBlockElements.html#usage-example",
    "href": "api-reference/cpp/functions/getOptimalBlockElements.html#usage-example",
    "title": "getOptimalBlockElements",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getOptimalBlockElements(...);",
    "crumbs": [
      "Functions",
      "getOptimalBlockElements"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSymmetricSortRule.html",
    "href": "api-reference/cpp/functions/getSymmetricSortRule.html",
    "title": "getSymmetricSortRule",
    "section": "",
    "text": "Spectra::SortRule BigDataStatMeth::getSymmetricSortRule(const std::string &which)",
    "crumbs": [
      "Functions",
      "getSymmetricSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSymmetricSortRule.html#signature",
    "href": "api-reference/cpp/functions/getSymmetricSortRule.html#signature",
    "title": "getSymmetricSortRule",
    "section": "",
    "text": "Spectra::SortRule BigDataStatMeth::getSymmetricSortRule(const std::string &which)",
    "crumbs": [
      "Functions",
      "getSymmetricSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSymmetricSortRule.html#description",
    "href": "api-reference/cpp/functions/getSymmetricSortRule.html#description",
    "title": "getSymmetricSortRule",
    "section": "2 Description",
    "text": "2 Description\nConvert ‘which’ string to Spectra SortRule for symmetric matrices.",
    "crumbs": [
      "Functions",
      "getSymmetricSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSymmetricSortRule.html#parameters",
    "href": "api-reference/cpp/functions/getSymmetricSortRule.html#parameters",
    "title": "getSymmetricSortRule",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nwhich (const std::string &): String indicating which eigenvalues to compute",
    "crumbs": [
      "Functions",
      "getSymmetricSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSymmetricSortRule.html#returns",
    "href": "api-reference/cpp/functions/getSymmetricSortRule.html#returns",
    "title": "getSymmetricSortRule",
    "section": "4 Returns",
    "text": "4 Returns\nCorresponding Spectra SortRule for symmetric eigenproblems",
    "crumbs": [
      "Functions",
      "getSymmetricSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSymmetricSortRule.html#details",
    "href": "api-reference/cpp/functions/getSymmetricSortRule.html#details",
    "title": "getSymmetricSortRule",
    "section": "5 Details",
    "text": "5 Details\nwhichString indicating which eigenvalues to compute Corresponding Spectra SortRule for symmetric eigenproblems",
    "crumbs": [
      "Functions",
      "getSymmetricSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSymmetricSortRule.html#caller-graph",
    "href": "api-reference/cpp/functions/getSymmetricSortRule.html#caller-graph",
    "title": "getSymmetricSortRule",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "getSymmetricSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSymmetricSortRule.html#source-code",
    "href": "api-reference/cpp/functions/getSymmetricSortRule.html#source-code",
    "title": "getSymmetricSortRule",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixEigenDecomposition.hpp • Lines 40-47\ninline Spectra::SortRule getSymmetricSortRule(const std::string& which) {\n        if (which == \"LA\") return Spectra::SortRule::LargestAlge;\n        if (which == \"SA\") return Spectra::SortRule::SmallestAlge;\n        if (which == \"LM\") return Spectra::SortRule::LargestMagn;\n        if (which == \"SM\") return Spectra::SortRule::SmallestMagn;\n        // Default to largest magnitude for symmetric matrices\n        return Spectra::SortRule::LargestMagn;\n    }",
    "crumbs": [
      "Functions",
      "getSymmetricSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/getSymmetricSortRule.html#usage-example",
    "href": "api-reference/cpp/functions/getSymmetricSortRule.html#usage-example",
    "title": "getSymmetricSortRule",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = getSymmetricSortRule(...);",
    "crumbs": [
      "Functions",
      "getSymmetricSortRule"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html",
    "title": "get_HDF5_mean_sd_by_column",
    "section": "",
    "text": "void BigDataStatMeth::get_HDF5_mean_sd_by_column(BigDataStatMeth::hdf5Dataset *dsA, Eigen::MatrixXd &normalize, bool bsd, bool bmean, Rcpp::Nullable&lt; int &gt; wsize)",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_column"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#signature",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#signature",
    "title": "get_HDF5_mean_sd_by_column",
    "section": "",
    "text": "void BigDataStatMeth::get_HDF5_mean_sd_by_column(BigDataStatMeth::hdf5Dataset *dsA, Eigen::MatrixXd &normalize, bool bsd, bool bmean, Rcpp::Nullable&lt; int &gt; wsize)",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_column"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#description",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#description",
    "title": "get_HDF5_mean_sd_by_column",
    "section": "2 Description",
    "text": "2 Description\nCalculate column-wise mean and standard deviation.",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_column"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#parameters",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#parameters",
    "title": "get_HDF5_mean_sd_by_column",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): Input matrix dataset\nnormalize (Eigen::MatrixXd &): Output matrix for mean and std values\nbsd (bool): compute sd\nbmean (bool): compute mean\nwsize (Rcpp::Nullable&lt; int &gt;): Block size for processing",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_column"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#details",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#details",
    "title": "get_HDF5_mean_sd_by_column",
    "section": "4 Details",
    "text": "4 Details\nComputes mean and standard deviation for each column of the matrix using block-based processing for memory efficiency. Optimized for cases where n &lt;&lt; m (rows much fewer than columns).",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_column"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#call-graph",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#call-graph",
    "title": "get_HDF5_mean_sd_by_column",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_column"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#source-code",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#source-code",
    "title": "get_HDF5_mean_sd_by_column",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSdMean.hpp • Lines 175-245\ninline void get_HDF5_mean_sd_by_column( BigDataStatMeth::hdf5Dataset* dsA,\n                                        Eigen::MatrixXd& normalize, \n                                        bool bsd, bool bmean, \n                                        Rcpp::Nullable&lt;int&gt; wsize )\n{\n    \n    // IntegerVector dims_out = get_HDF5_dataset_size(*dataset);\n    \n    try\n    {\n\n        hsize_t block_size = 0;\n        hsize_t* dims_out = dsA-&gt;dim();\n        \n        std::vector&lt;hsize_t&gt; stride = {1, 1},\n                             block = {1, 1},\n                             offset = {0, 0},\n                             count = {0, 0};\n        \n        \n        block_size = get_block_size(wsize, dims_out[1], dims_out[0]);\n\n        count[1] = dims_out[1];\n        if( block_size &lt; dims_out[0] )\n            count[0] = block_size;\n        else\n            count[0] = dims_out[0];\n        \n        // Read data in blocks of 500 columns\n        for(hsize_t i=0; (i &lt;= floor(dims_out[0]/block_size)) || i==0; i++)\n        {\n\n            if( offset[0] + block_size &lt;= dims_out[0] ) {\n                count[0] = block_size;\n            }else {\n                count[0] = dims_out[0] - offset[0];\n            }\n            \n            std::vector&lt;double&gt; vdA( count[0] * count[1] ); \n            dsA-&gt;readDatasetBlock( {offset[0], offset[1]}, {count[0], count[1]}, stride, block, vdA.data() );\n            Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; X (vdA.data(), count[0], count[1] );\n\n            Eigen::VectorXd mean = X.rowwise().mean();\n            normalize.block( 0, offset[0], 1, mean.size()) = mean.transpose();\n            \n            if(bsd) {\n                Eigen::VectorXd sd = ((X.colwise() - mean).array().square().rowwise().sum() / (X.cols() - 1)).sqrt();\n                normalize.block( 1, offset[0], 1, sd.size()) = sd.transpose();\n            }\n            \n            offset[0] = offset[0] + block_size;\n\n        }\n        \n    } catch( H5::FileIException& error ) { // catch failure caused by the H5File operations\n        // error.printErrorStack();\n        checkClose_file(dsA);\n        Rf_error(\"c++ exception get_HDF5_mean_sd_by_column (File IException)\");\n    } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n        // error.printErrorStack();\n        checkClose_file(dsA);\n        Rf_error(\"c++ exception get_HDF5_mean_sd_by_column (DataSet IException)\");\n    } catch(std::exception& error) {\n        checkClose_file(dsA);\n        Rf_error(\"c++ exception get_HDF5_mean_sd_by_column function: %s\",error.what());\n        // return void();\n    }\n    \n    return void();  // successfully terminated\n    \n}",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_column"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#usage-example",
    "href": "api-reference/cpp/functions/get_HDF5_mean_sd_by_column.html#usage-example",
    "title": "get_HDF5_mean_sd_by_column",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = get_HDF5_mean_sd_by_column(...);",
    "crumbs": [
      "Functions",
      "get_HDF5_mean_sd_by_column"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_NewLineEnding.html",
    "href": "api-reference/cpp/functions/get_NewLineEnding.html",
    "title": "get_NewLineEnding",
    "section": "",
    "text": "bool BigDataStatMeth::get_NewLineEnding(const char *filename)",
    "crumbs": [
      "Functions",
      "get_NewLineEnding"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_NewLineEnding.html#signature",
    "href": "api-reference/cpp/functions/get_NewLineEnding.html#signature",
    "title": "get_NewLineEnding",
    "section": "",
    "text": "bool BigDataStatMeth::get_NewLineEnding(const char *filename)",
    "crumbs": [
      "Functions",
      "get_NewLineEnding"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_NewLineEnding.html#description",
    "href": "api-reference/cpp/functions/get_NewLineEnding.html#description",
    "title": "get_NewLineEnding",
    "section": "2 Description",
    "text": "2 Description\nChecks if a file ends with a newline character.",
    "crumbs": [
      "Functions",
      "get_NewLineEnding"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_NewLineEnding.html#parameters",
    "href": "api-reference/cpp/functions/get_NewLineEnding.html#parameters",
    "title": "get_NewLineEnding",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfilename (const char *): Path to the file to check",
    "crumbs": [
      "Functions",
      "get_NewLineEnding"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_NewLineEnding.html#returns",
    "href": "api-reference/cpp/functions/get_NewLineEnding.html#returns",
    "title": "get_NewLineEnding",
    "section": "4 Returns",
    "text": "4 Returns\nbool True if file ends with newline, false otherwise",
    "crumbs": [
      "Functions",
      "get_NewLineEnding"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_NewLineEnding.html#details",
    "href": "api-reference/cpp/functions/get_NewLineEnding.html#details",
    "title": "get_NewLineEnding",
    "section": "5 Details",
    "text": "5 Details\nfilenamePath to the file to check bool True if file ends with newline, false otherwiseOpens file in binary mode to ensure consistent behavior across platforms",
    "crumbs": [
      "Functions",
      "get_NewLineEnding"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_NewLineEnding.html#caller-graph",
    "href": "api-reference/cpp/functions/get_NewLineEnding.html#caller-graph",
    "title": "get_NewLineEnding",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "get_NewLineEnding"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_NewLineEnding.html#source-code",
    "href": "api-reference/cpp/functions/get_NewLineEnding.html#source-code",
    "title": "get_NewLineEnding",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5ImportFiles.hpp • Lines 93-102\ninline bool get_NewLineEnding(const char *filename) {\n        const int LINE_FEED = '\\x0A';\n        FILE *f = fopen(filename, \"rb\");  /* binary mode */\n        if (f == NULL) return false;\n        const bool empty_file = fseek(f, 0, SEEK_END) == 0 && ftell(f) == 0;\n        const bool result = !empty_file ||\n            (fseek(f, -1, SEEK_END) == 0 && fgetc(f) == LINE_FEED);\n        fclose(f);\n        return result;\n    }",
    "crumbs": [
      "Functions",
      "get_NewLineEnding"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_NewLineEnding.html#usage-example",
    "href": "api-reference/cpp/functions/get_NewLineEnding.html#usage-example",
    "title": "get_NewLineEnding",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = get_NewLineEnding(...);",
    "crumbs": [
      "Functions",
      "get_NewLineEnding"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_block_size.html",
    "href": "api-reference/cpp/functions/get_block_size.html",
    "title": "get_block_size",
    "section": "",
    "text": "hsize_t BigDataStatMeth::get_block_size(Rcpp::Nullable&lt; int &gt; wsize, hsize_t reference_size, hsize_t alternative_size)",
    "crumbs": [
      "Functions",
      "get_block_size"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_block_size.html#signature",
    "href": "api-reference/cpp/functions/get_block_size.html#signature",
    "title": "get_block_size",
    "section": "",
    "text": "hsize_t BigDataStatMeth::get_block_size(Rcpp::Nullable&lt; int &gt; wsize, hsize_t reference_size, hsize_t alternative_size)",
    "crumbs": [
      "Functions",
      "get_block_size"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_block_size.html#description",
    "href": "api-reference/cpp/functions/get_block_size.html#description",
    "title": "get_block_size",
    "section": "2 Description",
    "text": "2 Description\nCalculate optimal block size for processing.",
    "crumbs": [
      "Functions",
      "get_block_size"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_block_size.html#parameters",
    "href": "api-reference/cpp/functions/get_block_size.html#parameters",
    "title": "get_block_size",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nwsize (Rcpp::Nullable&lt; int &gt;): User-specified block size (optional)\nreference_size (hsize_t): Primary dimension size\nalternative_size (hsize_t): Secondary dimension size",
    "crumbs": [
      "Functions",
      "get_block_size"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_block_size.html#returns",
    "href": "api-reference/cpp/functions/get_block_size.html#returns",
    "title": "get_block_size",
    "section": "4 Returns",
    "text": "4 Returns\nOptimal block size for processing",
    "crumbs": [
      "Functions",
      "get_block_size"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_block_size.html#details",
    "href": "api-reference/cpp/functions/get_block_size.html#details",
    "title": "get_block_size",
    "section": "5 Details",
    "text": "5 Details\nDetermines the optimal block size for processing based on matrix dimensions and memory constraints.",
    "crumbs": [
      "Functions",
      "get_block_size"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_block_size.html#caller-graph",
    "href": "api-reference/cpp/functions/get_block_size.html#caller-graph",
    "title": "get_block_size",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "get_block_size"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_block_size.html#source-code",
    "href": "api-reference/cpp/functions/get_block_size.html#source-code",
    "title": "get_block_size",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSdMean.hpp • Lines 56-77\ninline hsize_t get_block_size( Rcpp::Nullable&lt;int&gt; wsize, hsize_t reference_size, hsize_t alternative_size) {\n    \n    hsize_t bsize = 0;\n    \n    if( wsize.isNull()) {\n        if( reference_size &gt; MAXELEMSINBLOCK ){\n            bsize = 1;\n        } else {\n            hsize_t maxsize = std::max( alternative_size, reference_size);\n            bsize = std::ceil( MAXELEMSINBLOCK / maxsize);\n        }\n    } else {\n        if(reference_size &gt; MAXELEMSINBLOCK){\n            bsize = 1;\n        } else {\n            bsize = Rcpp::as&lt;int&gt; (wsize);\n        }\n    }\n    \n    return(bsize);\n    \n}",
    "crumbs": [
      "Functions",
      "get_block_size"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_block_size.html#usage-example",
    "href": "api-reference/cpp/functions/get_block_size.html#usage-example",
    "title": "get_block_size",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = get_block_size(...);",
    "crumbs": [
      "Functions",
      "get_block_size"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_number_threads.html",
    "href": "api-reference/cpp/functions/get_number_threads.html",
    "title": "get_number_threads",
    "section": "",
    "text": "unsigned int get_number_threads(Rcpp::Nullable&lt; int &gt; threads, Rcpp::Nullable&lt; bool &gt; bparal)",
    "crumbs": [
      "Functions",
      "get_number_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_number_threads.html#signature",
    "href": "api-reference/cpp/functions/get_number_threads.html#signature",
    "title": "get_number_threads",
    "section": "",
    "text": "unsigned int get_number_threads(Rcpp::Nullable&lt; int &gt; threads, Rcpp::Nullable&lt; bool &gt; bparal)",
    "crumbs": [
      "Functions",
      "get_number_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_number_threads.html#parameters",
    "href": "api-reference/cpp/functions/get_number_threads.html#parameters",
    "title": "get_number_threads",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nthreads (Rcpp::Nullable&lt; int &gt;)\nbparal (Rcpp::Nullable&lt; bool &gt;)",
    "crumbs": [
      "Functions",
      "get_number_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_number_threads.html#returns",
    "href": "api-reference/cpp/functions/get_number_threads.html#returns",
    "title": "get_number_threads",
    "section": "3 Returns",
    "text": "3 Returns\nType: unsigned int",
    "crumbs": [
      "Functions",
      "get_number_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_number_threads.html#call-graph",
    "href": "api-reference/cpp/functions/get_number_threads.html#call-graph",
    "title": "get_number_threads",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "get_number_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_number_threads.html#source-code",
    "href": "api-reference/cpp/functions/get_number_threads.html#source-code",
    "title": "get_number_threads",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/openme-utils.hpp • Lines 277-298\ninline unsigned int get_number_threads(Rcpp::Nullable&lt;int&gt; threads, Rcpp::Nullable&lt;bool&gt; bparal) {\n        \n        // unsigned int ithreads = std::thread::hardware_concurrency();\n        unsigned int ithreads = getDTthreads(INT_MAX, false);\n        \n        if( bparal.isNotNull() ) {\n            if(Rcpp::as&lt;bool&gt;(bparal) == false) {\n                ithreads = 1;\n                return(ithreads);\n            }\n        }\n        \n        if(threads.isNotNull()) {\n            if (Rcpp::as&lt;int&gt; (threads) &lt;= (int)ithreads){\n                ithreads = Rcpp::as&lt;int&gt; (threads);\n            }\n        } else {\n            ithreads =  getDTthreads(0, false);\n        }    \n        \n        return(ithreads);\n    }",
    "crumbs": [
      "Functions",
      "get_number_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_number_threads.html#usage-example",
    "href": "api-reference/cpp/functions/get_number_threads.html#usage-example",
    "title": "get_number_threads",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = get_number_threads(...);",
    "crumbs": [
      "Functions",
      "get_number_threads"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_value_to_impute_discrete.html",
    "href": "api-reference/cpp/functions/get_value_to_impute_discrete.html",
    "title": "get_value_to_impute_discrete",
    "section": "",
    "text": "int BigDataStatMeth::get_value_to_impute_discrete(std::map&lt; double, double &gt; probMap)",
    "crumbs": [
      "Functions",
      "get_value_to_impute_discrete"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_value_to_impute_discrete.html#signature",
    "href": "api-reference/cpp/functions/get_value_to_impute_discrete.html#signature",
    "title": "get_value_to_impute_discrete",
    "section": "",
    "text": "int BigDataStatMeth::get_value_to_impute_discrete(std::map&lt; double, double &gt; probMap)",
    "crumbs": [
      "Functions",
      "get_value_to_impute_discrete"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_value_to_impute_discrete.html#description",
    "href": "api-reference/cpp/functions/get_value_to_impute_discrete.html#description",
    "title": "get_value_to_impute_discrete",
    "section": "2 Description",
    "text": "2 Description\nGenerates a discrete value for imputation based on probability distribution.",
    "crumbs": [
      "Functions",
      "get_value_to_impute_discrete"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_value_to_impute_discrete.html#parameters",
    "href": "api-reference/cpp/functions/get_value_to_impute_discrete.html#parameters",
    "title": "get_value_to_impute_discrete",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nprobMap (std::map&lt; double, double &gt;): Map containing value-frequency pairs",
    "crumbs": [
      "Functions",
      "get_value_to_impute_discrete"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_value_to_impute_discrete.html#returns",
    "href": "api-reference/cpp/functions/get_value_to_impute_discrete.html#returns",
    "title": "get_value_to_impute_discrete",
    "section": "4 Returns",
    "text": "4 Returns\nint Generated value for imputation",
    "crumbs": [
      "Functions",
      "get_value_to_impute_discrete"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_value_to_impute_discrete.html#details",
    "href": "api-reference/cpp/functions/get_value_to_impute_discrete.html#details",
    "title": "get_value_to_impute_discrete",
    "section": "5 Details",
    "text": "5 Details\nThis function takes a map of value-probability pairs and generates a random value according to the probability distribution, excluding NA values (3).",
    "crumbs": [
      "Functions",
      "get_value_to_impute_discrete"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_value_to_impute_discrete.html#caller-graph",
    "href": "api-reference/cpp/functions/get_value_to_impute_discrete.html#caller-graph",
    "title": "get_value_to_impute_discrete",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "get_value_to_impute_discrete"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_value_to_impute_discrete.html#source-code",
    "href": "api-reference/cpp/functions/get_value_to_impute_discrete.html#source-code",
    "title": "get_value_to_impute_discrete",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5ImputeData.hpp • Lines 49-82\ninline int get_value_to_impute_discrete(std::map&lt;double, double&gt; probMap)\n    {\n        try\n        {\n            std::vector &lt;double&gt; probs;\n        \n            // Get values and counts for each map element\n            for( auto it = probMap.begin(); it != probMap.end(); ++it )\n                probs.push_back( it-&gt;second );\n            \n            // remove last element (corresponds to 3=&lt;NA&gt;)\n            probs.erase(probs.end() - 1);\n            \n            // Get total count\n            double totalSNPS = std::accumulate(probs.begin(), probs.end(), decltype(probs)::value_type(0));\n            \n            // Get probabilities without &lt;NA&gt;\n            for (std::vector&lt;double&gt;::iterator it = probs.begin() ; it != probs.end(); ++it)\n                *it = *it/totalSNPS;\n            \n            // Generate value with given probabilities\n            std::random_device rd;\n            std::mt19937 gen(rd());\n            \n            std::discrete_distribution&lt;&gt; d(probs.begin(), probs.end());\n            \n            return (d(gen));\n        } catch(const std::exception& e) {\n            Rf_error( \"c++ exception get_value_to_impute_discrete : %s\", e.what());\n            // std::cerr &lt;&lt; e.what() &lt;&lt; '\\n';\n            return(3);\n        }\n        \n    }",
    "crumbs": [
      "Functions",
      "get_value_to_impute_discrete"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/get_value_to_impute_discrete.html#usage-example",
    "href": "api-reference/cpp/functions/get_value_to_impute_discrete.html#usage-example",
    "title": "get_value_to_impute_discrete",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = get_value_to_impute_discrete(...);",
    "crumbs": [
      "Functions",
      "get_value_to_impute_discrete"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imax.html",
    "href": "api-reference/cpp/functions/imax.html",
    "title": "imax",
    "section": "",
    "text": "static int imax(int a, int b)",
    "crumbs": [
      "Functions",
      "imax"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imax.html#signature",
    "href": "api-reference/cpp/functions/imax.html#signature",
    "title": "imax",
    "section": "",
    "text": "static int imax(int a, int b)",
    "crumbs": [
      "Functions",
      "imax"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imax.html#description",
    "href": "api-reference/cpp/functions/imax.html#description",
    "title": "imax",
    "section": "2 Description",
    "text": "2 Description\nMaximum of two integers.",
    "crumbs": [
      "Functions",
      "imax"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imax.html#parameters",
    "href": "api-reference/cpp/functions/imax.html#parameters",
    "title": "imax",
    "section": "3 Parameters",
    "text": "3 Parameters\n\na (int): First integer\nb (int): Second integer",
    "crumbs": [
      "Functions",
      "imax"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imax.html#returns",
    "href": "api-reference/cpp/functions/imax.html#returns",
    "title": "imax",
    "section": "4 Returns",
    "text": "4 Returns\nint Larger value",
    "crumbs": [
      "Functions",
      "imax"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imax.html#details",
    "href": "api-reference/cpp/functions/imax.html#details",
    "title": "imax",
    "section": "5 Details",
    "text": "5 Details\naFirst integer bSecond integer int Larger value",
    "crumbs": [
      "Functions",
      "imax"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imax.html#call-graph",
    "href": "api-reference/cpp/functions/imax.html#call-graph",
    "title": "imax",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "imax"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imax.html#source-code",
    "href": "api-reference/cpp/functions/imax.html#source-code",
    "title": "imax",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/openme-utils.hpp • Line 98\nstatic inline int imax(int a, int b) { return a &gt; b ? a : b; }",
    "crumbs": [
      "Functions",
      "imax"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/imax.html#usage-example",
    "href": "api-reference/cpp/functions/imax.html#usage-example",
    "title": "imax",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = imax(...);",
    "crumbs": [
      "Functions",
      "imax"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/index.html",
    "href": "api-reference/cpp/functions/index.html",
    "title": "C++ Functions",
    "section": "",
    "text": "Documentation for standalone utility functions.\n\naddDiagonals* after_fork* avoid_openmp_hang_within_fork* Bblock_matrix_mul_parallel* bdcrossproduct* bdtcrossproduct* calc_freq* calculate_multiplication_blocks* checkClose_file* Cholesky_decomposition_hdf5* Cholesky_decomposition_intermediate_hdf5* Cholesky_decomposition_outofcore_hdf5* classify_matrix_type* cleanup_temp_datasets* compute_pvalues_optimized* correlation_pvalue* createHardLink* crossprod* cumsum* dgemm_* dgesdd_* dgesv_* dgesvd_* divideDiagonals* dscal_* dsysv_* estimateOptimalBlockSize* exists_HDF5_element* extractDiagonalToVector* find_all* First_level_SvdBlock_decomposition_hdf5* get_block_size* get_data_as_Matrix* get_HDF5_mean_sd_by_column* get_HDF5_mean_sd_by_row* get_NewLineEnding* get_number_threads* get_SplitData_in_vectorString* get_threads* get_value_to_impute_discrete* getAvailableMemoryMB* getBlockPositionsSizes_hdf5* getBlockPositionsSizes_mat* getBlockPositionsSizes* getDiagonalfromMatrix* getDTthreads_R* getDTthreads* getGeneralSortRule* getInitialPosition* getIntEnv* getMatrixBlockSize* getMaxBlockSize* getObjecDataType* getObjectDims* getOptimalBlockElements* getOptimBlockSize* getSizetoRead* getSymmetricSortRule* getVectorBlockSize* hdf5_matrixVector_calculus* imax* imin* initDTthreads* Inverse_Matrix_Cholesky_hdf5* Inverse_Matrix_Cholesky_intermediate_hdf5* Inverse_Matrix_Cholesky_outofcore_hdf5* Inverse_of_Cholesky_decomposition_hdf5* Inverse_of_Cholesky_decomposition_intermediate_hdf5* Inverse_of_Cholesky_decomposition_outofcore_hdf5* is_number* isDiagonalVector* isMatrixSymmetric* join_datasets* multiplication* multiplicationSparse* multiplyDiagonals* mygetenv* Next_level_SvdBlock_decomposition_hdf5* pathExists* pearson_correlation* performMatrixDiagonalOperation* powerDiagonals* prepareForParallelization* Rcpp_block_matrix_mul_parallel* Rcpp_block_matrix_mul* Rcpp_block_matrix_substract_hdf5* Rcpp_block_matrix_sum_hdf5* Rcpp_block_matrix_vector_substract_hdf5* Rcpp_block_matrix_vector_substract* Rcpp_block_matrix_vector_sum_hdf5* Rcpp_FileExist* Rcpp_Import_File_to_hdf5* Rcpp_Impute_snps_hdf5* Rcpp_InvCholesky_hdf5* Rcpp_matrix_blockSubstract* Rcpp_matrix_blockSum* Rcpp_matrix_substract* Rcpp_matrix_sum* Rcpp_matrix_vect_mult* Rcpp_matrix_vect_substract* Rcpp_matrix_vect_sum* Rcpp_matrix_vector_blockMult* Rcpp_matrix_vector_blockSubstract* Rcpp_matrix_vector_blockSum* Rcpp_matrixVectorDivision_byCol* Rcpp_matrixVectorDivision_byRow* Rcpp_matrixVectorMultiplication_byCol* Rcpp_matrixVectorMultiplication_byRow* Rcpp_matrixVectorPow_byRow* Rcpp_matrixVectorPower_byCol* Rcpp_matrixVectorSubstract_byCol* Rcpp_matrixVectorSubstract_byRow* Rcpp_matrixVectorSum_byCol* Rcpp_matrixVectorSum_byRow* Rcpp_Remove_Low_Data_hdf5* Rcpp_Remove_MAF_hdf5* Rcpp_vector_add_hdf5* Rcpp_vector_divide_hdf5* Rcpp_vector_mult* Rcpp_vector_multiply_hdf5* Rcpp_vector_power_hdf5* Rcpp_vector_substract* Rcpp_vector_subtract_hdf5* Rcpp_vector_sum* RcppApplyFunctionHdf5* RcppbdCorr_hdf5_Block_cross* RcppbdCorr_hdf5_Block_single* RcppbdCorr_hdf5_cross* RcppbdCorr_hdf5_single* RcppbdCorr_matrix_cross* RcppbdCorr_matrix_single* RcppbdEigen_hdf5_Block* RcppbdEigen_hdf5* RcppbdEigen_spectra* RcppbdSVD_hdf5_Block* RcppbdSVD_hdf5* RcppbdSVD_lapack* RcppbdSVD* RcppBind_datasets_hdf5* RcppGetPCAIndividualsHdf5* RcppGetPCAVariablesHdf5* RcppNormalize_Data_R_hdf5* RcppNormalize_Data* RcppNormalizeColwise* RcppNormalizeHdf5* RcppNormalizeRowwise* RcppPCAHdf5* RcppPseudoinv* RcppPseudoinvHdf5* RcppQR* RcppQRHdf5* RcppReduce_dataset_hdf5* RcppRemove_hdf5_elements* RcppSolve* RcppSolveHdf5* RcppSort_dataset_hdf5* RcppSplit_matrix_hdf5_internal* RcppSplit_matrix_hdf5* RcppTypifyNormalizeHdf5* remove_elements* removeColumn* removeRow* renameElement* scalarOperation* setDiagonalMatrix* setLowerTriangularMatrix* setUpperTriangularMatrix* spearman_correlation* SplitElementName* subtractDiagonals* t_distribution_cdf* tcrossprod* transpose* validateSpectraParams* validateVector* validateVectorDataset* VectortoOrderedMap_SNP_counts* wdX_parallel* wdX* when_fork* writeDiagonalFromVector* wX* xtwx* Xw* Xwd_parallel* Xwd* xwxt",
    "crumbs": [
      "Functions"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/index.html#standalone-functions",
    "href": "api-reference/cpp/functions/index.html#standalone-functions",
    "title": "C++ Functions",
    "section": "",
    "text": "Documentation for standalone utility functions.\n\naddDiagonals* after_fork* avoid_openmp_hang_within_fork* Bblock_matrix_mul_parallel* bdcrossproduct* bdtcrossproduct* calc_freq* calculate_multiplication_blocks* checkClose_file* Cholesky_decomposition_hdf5* Cholesky_decomposition_intermediate_hdf5* Cholesky_decomposition_outofcore_hdf5* classify_matrix_type* cleanup_temp_datasets* compute_pvalues_optimized* correlation_pvalue* createHardLink* crossprod* cumsum* dgemm_* dgesdd_* dgesv_* dgesvd_* divideDiagonals* dscal_* dsysv_* estimateOptimalBlockSize* exists_HDF5_element* extractDiagonalToVector* find_all* First_level_SvdBlock_decomposition_hdf5* get_block_size* get_data_as_Matrix* get_HDF5_mean_sd_by_column* get_HDF5_mean_sd_by_row* get_NewLineEnding* get_number_threads* get_SplitData_in_vectorString* get_threads* get_value_to_impute_discrete* getAvailableMemoryMB* getBlockPositionsSizes_hdf5* getBlockPositionsSizes_mat* getBlockPositionsSizes* getDiagonalfromMatrix* getDTthreads_R* getDTthreads* getGeneralSortRule* getInitialPosition* getIntEnv* getMatrixBlockSize* getMaxBlockSize* getObjecDataType* getObjectDims* getOptimalBlockElements* getOptimBlockSize* getSizetoRead* getSymmetricSortRule* getVectorBlockSize* hdf5_matrixVector_calculus* imax* imin* initDTthreads* Inverse_Matrix_Cholesky_hdf5* Inverse_Matrix_Cholesky_intermediate_hdf5* Inverse_Matrix_Cholesky_outofcore_hdf5* Inverse_of_Cholesky_decomposition_hdf5* Inverse_of_Cholesky_decomposition_intermediate_hdf5* Inverse_of_Cholesky_decomposition_outofcore_hdf5* is_number* isDiagonalVector* isMatrixSymmetric* join_datasets* multiplication* multiplicationSparse* multiplyDiagonals* mygetenv* Next_level_SvdBlock_decomposition_hdf5* pathExists* pearson_correlation* performMatrixDiagonalOperation* powerDiagonals* prepareForParallelization* Rcpp_block_matrix_mul_parallel* Rcpp_block_matrix_mul* Rcpp_block_matrix_substract_hdf5* Rcpp_block_matrix_sum_hdf5* Rcpp_block_matrix_vector_substract_hdf5* Rcpp_block_matrix_vector_substract* Rcpp_block_matrix_vector_sum_hdf5* Rcpp_FileExist* Rcpp_Import_File_to_hdf5* Rcpp_Impute_snps_hdf5* Rcpp_InvCholesky_hdf5* Rcpp_matrix_blockSubstract* Rcpp_matrix_blockSum* Rcpp_matrix_substract* Rcpp_matrix_sum* Rcpp_matrix_vect_mult* Rcpp_matrix_vect_substract* Rcpp_matrix_vect_sum* Rcpp_matrix_vector_blockMult* Rcpp_matrix_vector_blockSubstract* Rcpp_matrix_vector_blockSum* Rcpp_matrixVectorDivision_byCol* Rcpp_matrixVectorDivision_byRow* Rcpp_matrixVectorMultiplication_byCol* Rcpp_matrixVectorMultiplication_byRow* Rcpp_matrixVectorPow_byRow* Rcpp_matrixVectorPower_byCol* Rcpp_matrixVectorSubstract_byCol* Rcpp_matrixVectorSubstract_byRow* Rcpp_matrixVectorSum_byCol* Rcpp_matrixVectorSum_byRow* Rcpp_Remove_Low_Data_hdf5* Rcpp_Remove_MAF_hdf5* Rcpp_vector_add_hdf5* Rcpp_vector_divide_hdf5* Rcpp_vector_mult* Rcpp_vector_multiply_hdf5* Rcpp_vector_power_hdf5* Rcpp_vector_substract* Rcpp_vector_subtract_hdf5* Rcpp_vector_sum* RcppApplyFunctionHdf5* RcppbdCorr_hdf5_Block_cross* RcppbdCorr_hdf5_Block_single* RcppbdCorr_hdf5_cross* RcppbdCorr_hdf5_single* RcppbdCorr_matrix_cross* RcppbdCorr_matrix_single* RcppbdEigen_hdf5_Block* RcppbdEigen_hdf5* RcppbdEigen_spectra* RcppbdSVD_hdf5_Block* RcppbdSVD_hdf5* RcppbdSVD_lapack* RcppbdSVD* RcppBind_datasets_hdf5* RcppGetPCAIndividualsHdf5* RcppGetPCAVariablesHdf5* RcppNormalize_Data_R_hdf5* RcppNormalize_Data* RcppNormalizeColwise* RcppNormalizeHdf5* RcppNormalizeRowwise* RcppPCAHdf5* RcppPseudoinv* RcppPseudoinvHdf5* RcppQR* RcppQRHdf5* RcppReduce_dataset_hdf5* RcppRemove_hdf5_elements* RcppSolve* RcppSolveHdf5* RcppSort_dataset_hdf5* RcppSplit_matrix_hdf5_internal* RcppSplit_matrix_hdf5* RcppTypifyNormalizeHdf5* remove_elements* removeColumn* removeRow* renameElement* scalarOperation* setDiagonalMatrix* setLowerTriangularMatrix* setUpperTriangularMatrix* spearman_correlation* SplitElementName* subtractDiagonals* t_distribution_cdf* tcrossprod* transpose* validateSpectraParams* validateVector* validateVectorDataset* VectortoOrderedMap_SNP_counts* wdX_parallel* wdX* when_fork* writeDiagonalFromVector* wX* xtwx* Xw* Xwd_parallel* Xwd* xwxt",
    "crumbs": [
      "Functions"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isDiagonalVector.html",
    "href": "api-reference/cpp/functions/isDiagonalVector.html",
    "title": "isDiagonalVector",
    "section": "",
    "text": "bool BigDataStatMeth::DiagonalOps::isDiagonalVector(BigDataStatMeth::hdf5Dataset *ds)",
    "crumbs": [
      "Functions",
      "isDiagonalVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isDiagonalVector.html#signature",
    "href": "api-reference/cpp/functions/isDiagonalVector.html#signature",
    "title": "isDiagonalVector",
    "section": "",
    "text": "bool BigDataStatMeth::DiagonalOps::isDiagonalVector(BigDataStatMeth::hdf5Dataset *ds)",
    "crumbs": [
      "Functions",
      "isDiagonalVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isDiagonalVector.html#description",
    "href": "api-reference/cpp/functions/isDiagonalVector.html#description",
    "title": "isDiagonalVector",
    "section": "2 Description",
    "text": "2 Description\nCheck if dataset represents a diagonal vector.",
    "crumbs": [
      "Functions",
      "isDiagonalVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isDiagonalVector.html#parameters",
    "href": "api-reference/cpp/functions/isDiagonalVector.html#parameters",
    "title": "isDiagonalVector",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nds (BigDataStatMeth::hdf5Dataset *): Dataset to check",
    "crumbs": [
      "Functions",
      "isDiagonalVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isDiagonalVector.html#returns",
    "href": "api-reference/cpp/functions/isDiagonalVector.html#returns",
    "title": "isDiagonalVector",
    "section": "4 Returns",
    "text": "4 Returns\ntrue if dataset is a valid vector, false if matrix",
    "crumbs": [
      "Functions",
      "isDiagonalVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isDiagonalVector.html#details",
    "href": "api-reference/cpp/functions/isDiagonalVector.html#details",
    "title": "isDiagonalVector",
    "section": "5 Details",
    "text": "5 Details\nValidates dataset dimensions to determine if it’s a vector (1×N or N×1). A dataset is considered a diagonal vector if it has vector dimensions. Future enhancements could include .diag suffix validation.",
    "crumbs": [
      "Functions",
      "isDiagonalVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isDiagonalVector.html#call-graph",
    "href": "api-reference/cpp/functions/isDiagonalVector.html#call-graph",
    "title": "isDiagonalVector",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "isDiagonalVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isDiagonalVector.html#source-code",
    "href": "api-reference/cpp/functions/isDiagonalVector.html#source-code",
    "title": "isDiagonalVector",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5DiagonalMethods.hpp • Lines 72-80\ninline bool isDiagonalVector(BigDataStatMeth::hdf5Dataset* ds)\n        {\n            hsize_t rows = ds-&gt;nrows();\n            hsize_t cols = ds-&gt;ncols();\n            \n            return (rows == 1 && cols &gt; 1) ||     // Row vector\n                (cols == 1 && rows &gt; 1) ||     // Column vector\n                (rows == 1 && cols == 1);      // Scalar\n        }",
    "crumbs": [
      "Functions",
      "isDiagonalVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/isDiagonalVector.html#usage-example",
    "href": "api-reference/cpp/functions/isDiagonalVector.html#usage-example",
    "title": "isDiagonalVector",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = isDiagonalVector(...);",
    "crumbs": [
      "Functions",
      "isDiagonalVector"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/is_number.html",
    "href": "api-reference/cpp/functions/is_number.html",
    "title": "is_number",
    "section": "",
    "text": "bool BigDataStatMeth::is_number(const std::string &s)",
    "crumbs": [
      "Functions",
      "is_number"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/is_number.html#signature",
    "href": "api-reference/cpp/functions/is_number.html#signature",
    "title": "is_number",
    "section": "",
    "text": "bool BigDataStatMeth::is_number(const std::string &s)",
    "crumbs": [
      "Functions",
      "is_number"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/is_number.html#description",
    "href": "api-reference/cpp/functions/is_number.html#description",
    "title": "is_number",
    "section": "2 Description",
    "text": "2 Description\nTests if a string represents a valid number.",
    "crumbs": [
      "Functions",
      "is_number"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/is_number.html#parameters",
    "href": "api-reference/cpp/functions/is_number.html#parameters",
    "title": "is_number",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ns (const std::string &): String to test",
    "crumbs": [
      "Functions",
      "is_number"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/is_number.html#returns",
    "href": "api-reference/cpp/functions/is_number.html#returns",
    "title": "is_number",
    "section": "4 Returns",
    "text": "4 Returns\nbool True if string represents a valid number, false otherwise",
    "crumbs": [
      "Functions",
      "is_number"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/is_number.html#details",
    "href": "api-reference/cpp/functions/is_number.html#details",
    "title": "is_number",
    "section": "5 Details",
    "text": "5 Details\nsString to test bool True if string represents a valid number, false otherwiseHandles both integer and floating-point representations Also validates against overflow conditions",
    "crumbs": [
      "Functions",
      "is_number"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/is_number.html#source-code",
    "href": "api-reference/cpp/functions/is_number.html#source-code",
    "title": "is_number",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5ImportFiles.hpp • Lines 113-119\ninline bool is_number(const std::string& s)\n    {\n        char* end = nullptr;\n        double val = strtod(s.c_str(), &end);\n\n        return end != s.c_str() && *end == '\\0' && val != HUGE_VAL;\n    }",
    "crumbs": [
      "Functions",
      "is_number"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/is_number.html#usage-example",
    "href": "api-reference/cpp/functions/is_number.html#usage-example",
    "title": "is_number",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = is_number(...);",
    "crumbs": [
      "Functions",
      "is_number"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplication.html",
    "href": "api-reference/cpp/functions/multiplication.html",
    "title": "multiplication",
    "section": "",
    "text": "void BigDataStatMeth::multiplication(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool transpose_A, bool transpose_B, Rcpp::Nullable&lt; bool &gt; bparal, Rcpp::Nullable&lt; int &gt; hdf5_block, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "multiplication"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplication.html#signature",
    "href": "api-reference/cpp/functions/multiplication.html#signature",
    "title": "multiplication",
    "section": "",
    "text": "void BigDataStatMeth::multiplication(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool transpose_A, bool transpose_B, Rcpp::Nullable&lt; bool &gt; bparal, Rcpp::Nullable&lt; int &gt; hdf5_block, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "multiplication"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplication.html#description",
    "href": "api-reference/cpp/functions/multiplication.html#description",
    "title": "multiplication",
    "section": "2 Description",
    "text": "2 Description\nMain matrix multiplication function for HDF5 matrices.",
    "crumbs": [
      "Functions",
      "multiplication"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplication.html#parameters",
    "href": "api-reference/cpp/functions/multiplication.html#parameters",
    "title": "multiplication",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input matrix dataset\ndsB (BigDataStatMeth::hdf5Dataset *): Second input matrix dataset\ndsC (BigDataStatMeth::hdf5Dataset *): Output matrix dataset\ntranspose_A (bool): Whether to transpose matrix A\ntranspose_B (bool): Whether to transpose matrix B\nbparal (Rcpp::Nullable&lt; bool &gt;): Whether to use parallel processing\nhdf5_block (Rcpp::Nullable&lt; int &gt;): Block size for HDF5 I/O operations\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "multiplication"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplication.html#details",
    "href": "api-reference/cpp/functions/multiplication.html#details",
    "title": "multiplication",
    "section": "4 Details",
    "text": "4 Details\nPerforms matrix multiplication C = A * B where A, B, and C are HDF5 datasets. Supports parallel processing and block-based computation for memory efficiency.",
    "crumbs": [
      "Functions",
      "multiplication"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplication.html#call-graph",
    "href": "api-reference/cpp/functions/multiplication.html#call-graph",
    "title": "multiplication",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "multiplication"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplication.html#source-code",
    "href": "api-reference/cpp/functions/multiplication.html#source-code",
    "title": "multiplication",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/multiplication.hpp • Lines 242-455\ninline void multiplication( BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, BigDataStatMeth::hdf5Dataset* dsC,\n                                bool transpose_A, bool transpose_B, Rcpp::Nullable&lt;bool&gt; bparal, Rcpp::Nullable&lt;int&gt; hdf5_block, Rcpp::Nullable&lt;int&gt; threads = R_NilValue) \n    {\n        \n        try {\n        \n            // int ihdf5_block;\n            // hsize_t K = dsA-&gt;nrows();\n            // hsize_t N = dsA-&gt;ncols();\n            // hsize_t L = dsB-&gt;ncols();\n            // hsize_t M = dsB-&gt;nrows();\n            // \n             \n             hsize_t K, N, L, M;\n            \n            if (transpose_A) {\n                K = dsA-&gt;ncols();  // t(A): filas de A se vuelven columnas\n                N = dsA-&gt;nrows();  // t(A): columnas de A se vuelven filas\n            } else {\n                K = dsA-&gt;nrows();  // A normal\n                N = dsA-&gt;ncols();\n            }\n            \n            if (transpose_B) {\n                L = dsB-&gt;nrows();  // t(B): columnas de B se vuelven filas\n                M = dsB-&gt;ncols();  // t(B): filas de B se vuelven columnas\n            } else {\n                L = dsB-&gt;ncols();  // B normal\n                M = dsB-&gt;nrows();\n            }\n            \n             // if( hdf5_block.isNotNull()) {\n             //     ihdf5_block =  Rcpp::as&lt;int&gt;(hdf5_block);\n             // } else {\n             //     ihdf5_block =  MAXBLOCKSIZE/3;\n             // }\n\n             \n             int ihdf5_block_N, ihdf5_block_M, ihdf5_block_K;\n             \n             if( hdf5_block.isNotNull()) {\n                 ihdf5_block_N = ihdf5_block_M = ihdf5_block_K = Rcpp::as&lt;int&gt;(hdf5_block);\n             } else {\n                 BlockSizes blocks = calculate_multiplication_blocks(N, M, K);\n                 ihdf5_block_N = ihdf5_block_M = blocks.output_block;\n                 ihdf5_block_K = blocks.inner_block;\n             }\n             \n             \n            if( K == L )\n            {\n\n                std::vector&lt;hsize_t&gt; stride = {1, 1},\n                                     block = {1, 1},\n                                     vsizetoRead, vstart,\n                                     vsizetoReadM, vstartM,\n                                     vsizetoReadK, vstartK;\n                \n                dsC-&gt;createDataset( N, M, \"real\");\n                \n                if( dsC-&gt;getDatasetptr() != nullptr) {\n\n                    \n                    getBlockPositionsSizes_hdf5( N, ihdf5_block_N, vstart, vsizetoRead );\n                    getBlockPositionsSizes_hdf5( M, ihdf5_block_M, vstartM, vsizetoReadM );\n                    getBlockPositionsSizes_hdf5( K, ihdf5_block_K, vstartK, vsizetoReadK );\n                    \n                    // getBlockPositionsSizes_hdf5( N, ihdf5_block, vstart, vsizetoRead );\n                    // getBlockPositionsSizes_hdf5( M, ihdf5_block, vstartM, vsizetoReadM );\n                    // getBlockPositionsSizes_hdf5( K, ihdf5_block, vstartK, vsizetoReadK );\n                    \n                    \n                    // int ithreads = get_number_threads(threads, R_NilValue);\n                    // int chunks = vstart.size()/ithreads;\n                    \n                    #pragma omp parallel num_threads( get_number_threads(threads, R_NilValue) ) shared(dsA, dsB, dsC, vstart, vsizetoRead) // chunks\n                    {\n                        \n                        #pragma omp for schedule(dynamic) nowait\n                        for (hsize_t ii = 0; ii &lt; vstart.size(); ii++)\n                        {\n                            for (hsize_t jj = 0; jj &lt; vstartM.size(); jj++)\n                            {\n                                \n                                Eigen::MatrixXd C_accumulator = Eigen::MatrixXd::Zero(vsizetoReadM[jj], vsizetoRead[ii]);\n                                \n                                for (hsize_t kk = 0; kk &lt; vstartK.size(); kk++)\n                                {\n                                    hsize_t iColsA = vsizetoReadK[kk],\n                                            iRowsA = vsizetoRead[ii],\n                                            iColsB = vsizetoReadM[jj],\n                                            iRowsB = vsizetoReadK[kk];\n                                    \n                                    std::vector&lt;double&gt; vdA( iRowsA * iColsA );\n                                    #pragma omp critical(accessFile)\n                                    {\n                                        dsA-&gt;readDatasetBlock( {vstartK[kk], vstart[ii]}, {vsizetoReadK[kk], vsizetoRead[ii]}, stride, block, vdA.data() );\n                                    }\n                                    \n                                    Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; A (vdA.data(), vsizetoReadK[kk], vsizetoRead[ii] );\n                                    \n                                    std::vector&lt;double&gt; vdB( iRowsB * iColsB );\n                                    #pragma omp critical(accessFile)\n                                    {\n                                        dsB-&gt;readDatasetBlock( {vstartM[jj], vstartK[kk]}, {vsizetoReadM[jj], vsizetoReadK[kk]}, stride, block, vdB.data() );\n                                    }\n                                    \n                                    Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; B (vdB.data(), vsizetoReadM[jj], vsizetoReadK[kk] );\n                                    \n                                    // C_accumulator += B * A;\n                                    // Operación según flags de transposición\n                                    if (!transpose_A && !transpose_B) {\n                                        C_accumulator += B * A;                           // A * B\n                                    } else if (transpose_A && !transpose_B) {\n                                        C_accumulator += B * A.transpose();               // t(A) * B\n                                    } else if (!transpose_A && transpose_B) {\n                                        C_accumulator += B.transpose() * A;               // A * t(B)\n                                    } else {\n                                        C_accumulator += B.transpose() * A.transpose();   // t(A) * t(B)\n                                    }\n                                }\n                            \n                                std::vector&lt;double&gt; vdC_final(vsizetoReadM[jj] * vsizetoRead[ii]);\n                                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; C_final_map(vdC_final.data(), vsizetoReadM[jj], vsizetoRead[ii]);\n                                C_final_map = C_accumulator;\n                                \n                                std::vector&lt;hsize_t&gt; offset = {vstartM[jj], vstart[ii]};\n                                std::vector&lt;hsize_t&gt; count = {vsizetoReadM[jj], vsizetoRead[ii]};\n                                                                \n                                #pragma omp critical(accessFile)\n                                {\n                                    dsC-&gt;writeDatasetBlock(vdC_final, offset, count, stride, block);\n                                }\n                            }\n                        }\n                        \n                    // #pragma omp for schedule(dynamic) nowait\n                    //     for (hsize_t ii = 0; ii &lt; vstart.size(); ii++)\n                    //     {\n                    //         \n                    //         for (hsize_t jj = 0; jj &lt; vstartM.size(); jj++)\n                    //         {\n                    //             \n                    //             for (hsize_t kk = 0; kk &lt; vstartK.size(); kk++)\n                    //             {\n                    //                 \n                    //                 hsize_t iColsA = vsizetoReadK[kk],\n                    //                         iRowsA = vsizetoRead[ii],\n                    //                         iColsB = vsizetoReadM[jj],\n                    //                         iRowsB = vsizetoReadK[kk];\n                    //                 \n                    //                 std::vector&lt;double&gt; vdA( iRowsA * iColsA );\n                    //                 #pragma omp critical(accessFile)\n                    //                 {\n                    //                     dsA-&gt;readDatasetBlock( {vstartK[kk], vstart[ii]}, {vsizetoReadK[kk], vsizetoRead[ii]}, stride, block, vdA.data() );\n                    //                 }\n                    //                 \n                    //                 Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; A (vdA.data(), vsizetoReadK[kk], vsizetoRead[ii] );\n                    //                 \n                    //                 std::vector&lt;double&gt; vdB( iRowsB * iColsB );\n                    //                 #pragma omp critical(accessFile)\n                    //                 {\n                    //                     dsB-&gt;readDatasetBlock( {vstartM[jj], vstartK[kk]}, {vsizetoReadM[jj], vsizetoReadK[kk]}, stride, block, vdB.data() );\n                    //                 }\n                    //                 \n                    //                 Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; B (vdB.data(), vsizetoReadM[jj], vsizetoReadK[kk] );\n                    //                 \n                    //                 std::vector&lt;double&gt; vdC( vsizetoReadM[jj] * vsizetoRead[ii] );\n                    //                 #pragma omp critical(accessFile)\n                    //                 {\n                    //                     dsC-&gt;readDatasetBlock( {vstartM[jj], vstart[ii]}, {vsizetoReadM[jj],  vsizetoRead[ii]}, stride, block, vdC.data() );\n                    //                 }\n                    // \n                    //                 Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; C (vdC.data(), B.rows(), A.cols() );\n                    // \n                    //                 C = C + B * A;\n                    // \n                    //                 std::vector&lt;hsize_t&gt; offset = {vstartM[jj], vstart[ii]};\n                    //                 std::vector&lt;hsize_t&gt; count = {vsizetoReadM[jj], vsizetoRead[ii] };\n                    //                 \n                    //                 #pragma omp critical(accessFile)\n                    //                 {\n                    //                     dsC-&gt;writeDatasetBlock(vdC, offset, count, stride, block);\n                    //                 }\n                    //             }\n                    //         }\n                    //     }\n                    }\n                } \n                \n            } else {\n                throw std::range_error(\"multiplication error: non-conformable arguments\");\n            }\n\n        }  catch( H5::FileIException& error ) { // catch failure caused by the H5File operations\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ c++ exception multiplication (File IException)\\n\";\n            // return void();\n        } catch( H5::DataSetIException& error ) { // catch failure caused by the DataSet operations\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception multiplication (DataSet IException)\\n\";\n            // return void();\n        } catch(std::exception &ex) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nc++ exception multiplication \" &lt;&lt; ex.what();\n            // return void();\n        }  catch (...) {\n            checkClose_file(dsA, dsB, dsC);\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception multiplication (unknown reason)\";\n            // return void();\n        }\n\n        return void();\n    }",
    "crumbs": [
      "Functions",
      "multiplication"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplication.html#usage-example",
    "href": "api-reference/cpp/functions/multiplication.html#usage-example",
    "title": "multiplication",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = multiplication(...);",
    "crumbs": [
      "Functions",
      "multiplication"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplyDiagonals.html",
    "href": "api-reference/cpp/functions/multiplyDiagonals.html",
    "title": "multiplyDiagonals",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::multiplyDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "multiplyDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplyDiagonals.html#signature",
    "href": "api-reference/cpp/functions/multiplyDiagonals.html#signature",
    "title": "multiplyDiagonals",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::multiplyDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "multiplyDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplyDiagonals.html#description",
    "href": "api-reference/cpp/functions/multiplyDiagonals.html#description",
    "title": "multiplyDiagonals",
    "section": "2 Description",
    "text": "2 Description\nMultiply diagonal elements from two matrices or vectors.",
    "crumbs": [
      "Functions",
      "multiplyDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplyDiagonals.html#parameters",
    "href": "api-reference/cpp/functions/multiplyDiagonals.html#parameters",
    "title": "multiplyDiagonals",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input dataset\ndsB (BigDataStatMeth::hdf5Dataset *): Second input dataset\ndsResult (BigDataStatMeth::hdf5Dataset *): Result dataset (will be created)\ntarget (std::string): Where to write result: “A”, “B”, or “new”\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "multiplyDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplyDiagonals.html#details",
    "href": "api-reference/cpp/functions/multiplyDiagonals.html#details",
    "title": "multiplyDiagonals",
    "section": "4 Details",
    "text": "4 Details\nPerforms optimized diagonal multiplication C_diag = A_diag * B_diag. Same optimization strategy as addDiagonals but for element-wise multiplication.",
    "crumbs": [
      "Functions",
      "multiplyDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplyDiagonals.html#call-graph",
    "href": "api-reference/cpp/functions/multiplyDiagonals.html#call-graph",
    "title": "multiplyDiagonals",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "multiplyDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplyDiagonals.html#source-code",
    "href": "api-reference/cpp/functions/multiplyDiagonals.html#source-code",
    "title": "multiplyDiagonals",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5DiagonalMethods.hpp • Lines 469-488\ninline void multiplyDiagonals(BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB,\n                                      BigDataStatMeth::hdf5Dataset* dsResult, std::string target = \"new\",\n                                      bool bparal = false, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n        {\n            try {\n                bool isVectorA = isDiagonalVector(dsA);\n                bool isVectorB = isDiagonalVector(dsB);\n                \n                if (isVectorA && isVectorB && (target == \"A\" || target == \"B\")) {\n                    BigDataStatMeth::hdf5Dataset* targetDataset = (target == \"A\") ? dsA : dsB;\n                    Rcpp_vector_multiply_hdf5(dsA, dsB, targetDataset, bparal, threads);\n                } else if (isVectorA && isVectorB && target == \"new\") {\n                    Rcpp_vector_multiply_hdf5(dsA, dsB, dsResult, bparal, threads);\n                } else {\n                    performMatrixDiagonalOperation(dsA, dsB, dsResult, 2, target, bparal, threads);\n                }\n            } catch(std::exception& ex) {\n                Rf_error(\"Error in multiplyDiagonals: %s\", ex.what());\n            }\n        }",
    "crumbs": [
      "Functions",
      "multiplyDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/multiplyDiagonals.html#usage-example",
    "href": "api-reference/cpp/functions/multiplyDiagonals.html#usage-example",
    "title": "multiplyDiagonals",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = multiplyDiagonals(...);",
    "crumbs": [
      "Functions",
      "multiplyDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pathExists.html",
    "href": "api-reference/cpp/functions/pathExists.html",
    "title": "pathExists",
    "section": "",
    "text": "bool BigDataStatMeth::pathExists(hid_t id, const std::string &path)",
    "crumbs": [
      "Functions",
      "pathExists"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pathExists.html#signature",
    "href": "api-reference/cpp/functions/pathExists.html#signature",
    "title": "pathExists",
    "section": "",
    "text": "bool BigDataStatMeth::pathExists(hid_t id, const std::string &path)",
    "crumbs": [
      "Functions",
      "pathExists"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pathExists.html#description",
    "href": "api-reference/cpp/functions/pathExists.html#description",
    "title": "pathExists",
    "section": "2 Description",
    "text": "2 Description\nChecks if a path exists in an HDF5 file.",
    "crumbs": [
      "Functions",
      "pathExists"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pathExists.html#parameters",
    "href": "api-reference/cpp/functions/pathExists.html#parameters",
    "title": "pathExists",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nid (hid_t): HDF5 file or group identifier\npath (const std::string &): Path to check",
    "crumbs": [
      "Functions",
      "pathExists"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pathExists.html#returns",
    "href": "api-reference/cpp/functions/pathExists.html#returns",
    "title": "pathExists",
    "section": "4 Returns",
    "text": "4 Returns\nbool True if path exists, false otherwise",
    "crumbs": [
      "Functions",
      "pathExists"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pathExists.html#details",
    "href": "api-reference/cpp/functions/pathExists.html#details",
    "title": "pathExists",
    "section": "5 Details",
    "text": "5 Details\nidHDF5 file or group identifier pathPath to check bool True if path exists, false otherwiseH5::FileIExceptionon file operation errors",
    "crumbs": [
      "Functions",
      "pathExists"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pathExists.html#call-graph",
    "href": "api-reference/cpp/functions/pathExists.html#call-graph",
    "title": "pathExists",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "pathExists"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pathExists.html#source-code",
    "href": "api-reference/cpp/functions/pathExists.html#source-code",
    "title": "pathExists",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5Utilities.hpp • Lines 40-55\ninline bool pathExists(hid_t id, const std::string& path)\n    {\n        try {\n            return H5Lexists( id, path.c_str(), H5P_DEFAULT ) &gt; 0;    \n        } catch(H5::FileIException& error) { // catch failure caused by the H5File operations\n            Rcpp::Rcerr&lt;&lt;\"c++ exception pathExists (File IException)\" &lt;&lt; std::endl;\n            return false;\n        } catch(std::exception &ex) {\n            Rcpp::Rcerr &lt;&lt; \"c++ exception pathExists: \" &lt;&lt; ex.what();\n            return false;\n        }  catch (...) {\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception pathExists (unknown reason)\";\n            return false;\n        }\n        \n    }",
    "crumbs": [
      "Functions",
      "pathExists"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/pathExists.html#usage-example",
    "href": "api-reference/cpp/functions/pathExists.html#usage-example",
    "title": "pathExists",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = pathExists(...);",
    "crumbs": [
      "Functions",
      "pathExists"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/performMatrixDiagonalOperation.html",
    "href": "api-reference/cpp/functions/performMatrixDiagonalOperation.html",
    "title": "performMatrixDiagonalOperation",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::performMatrixDiagonalOperation(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, int operation, std::string target, bool bparal, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "performMatrixDiagonalOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#signature",
    "href": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#signature",
    "title": "performMatrixDiagonalOperation",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::performMatrixDiagonalOperation(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, int operation, std::string target, bool bparal, Rcpp::Nullable&lt; int &gt; threads)",
    "crumbs": [
      "Functions",
      "performMatrixDiagonalOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#description",
    "href": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#description",
    "title": "performMatrixDiagonalOperation",
    "section": "2 Description",
    "text": "2 Description\nPerform diagonal operations on matrices using extract-operate-write strategy.",
    "crumbs": [
      "Functions",
      "performMatrixDiagonalOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#parameters",
    "href": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#parameters",
    "title": "performMatrixDiagonalOperation",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input dataset\ndsB (BigDataStatMeth::hdf5Dataset *): Second input dataset\ndsResult (BigDataStatMeth::hdf5Dataset *): Result dataset (only used if target=“new”)\noperation (int): Operation type: 0=add, 1=subtract, 2=multiply, 3=divide\ntarget (std::string): Where to write result: “A”, “B”, or “new”\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads",
    "crumbs": [
      "Functions",
      "performMatrixDiagonalOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#details",
    "href": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#details",
    "title": "performMatrixDiagonalOperation",
    "section": "4 Details",
    "text": "4 Details\nImplements the extract-operate-write pattern for matrix diagonal operations. It processes only diagonal elements instead of full matrices.",
    "crumbs": [
      "Functions",
      "performMatrixDiagonalOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#call-graph",
    "href": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#call-graph",
    "title": "performMatrixDiagonalOperation",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "performMatrixDiagonalOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#source-code",
    "href": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#source-code",
    "title": "performMatrixDiagonalOperation",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5DiagonalMethods.hpp • Lines 259-361\ninline void performMatrixDiagonalOperation(BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB,\n                                                   BigDataStatMeth::hdf5Dataset* dsResult, int operation, std::string target,\n                                                   bool bparal, Rcpp::Nullable&lt;int&gt; threads)\n        {\n            BigDataStatMeth::hdf5Dataset* tempA = nullptr;\n            BigDataStatMeth::hdf5Dataset* tempB = nullptr;\n            BigDataStatMeth::hdf5Dataset* tempResult = nullptr;\n            \n            try {\n                bool isVectorA = isDiagonalVector(dsA);\n                bool isVectorB = isDiagonalVector(dsB);\n                \n                BigDataStatMeth::hdf5Dataset* finalA = dsA;\n                BigDataStatMeth::hdf5Dataset* finalB = dsB;\n                \n                // Extract diagonal from A if it's a matrix\n                if (!isVectorA) {\n                    if (dsA-&gt;nrows() != dsA-&gt;ncols()) {\n                        checkClose_file(tempA, tempB, tempResult);\n                        Rf_error(\"Matrix A must be square for diagonal operations\");\n                        return;\n                    }\n                    std::string tempNameA = dsA-&gt;getDatasetName() + \"_temp_diag_A\";\n                    tempA = new BigDataStatMeth::hdf5Dataset(dsA-&gt;getFileptr(), dsA-&gt;getGroup(), tempNameA, true);\n                    extractDiagonalToVector(dsA, tempA);\n                    finalA = tempA;\n                }\n                \n                // Extract diagonal from B if it's a matrix\n                if (!isVectorB) {\n                    if (dsB-&gt;nrows() != dsB-&gt;ncols()) {\n                        checkClose_file(tempA, tempB, tempResult);\n                        Rf_error(\"Matrix B must be square for diagonal operations\");\n                        cleanup_temp_datasets(tempA, tempB);\n                        return;\n                    }\n                    std::string tempNameB = dsB-&gt;getDatasetName() + \"_temp_diag_B\";\n                    tempB = new BigDataStatMeth::hdf5Dataset(dsB-&gt;getFileptr(), dsB-&gt;getGroup(), tempNameB, true);\n                    extractDiagonalToVector(dsB, tempB);\n                    finalB = tempB;\n                }\n                \n                // Validate dimensions\n                hsize_t sizeA = validateVectorDataset(finalA);\n                hsize_t sizeB = validateVectorDataset(finalB);\n                \n                if (sizeA == 0 || sizeB == 0 || sizeA != sizeB) {\n                    checkClose_file(tempA, tempB, tempResult);\n                    Rf_error(\"Invalid or incompatible diagonal dimensions: %llu vs %llu\", sizeA, sizeB);\n                    cleanup_temp_datasets(tempA, tempB);\n                    return;\n                }\n                \n                // Determine target for operation result\n                BigDataStatMeth::hdf5Dataset* operationTarget = nullptr;\n                \n                if (target == \"new\") {\n                    operationTarget = dsResult;\n                } else if (target == \"A\") {\n                    if (isVectorA) {\n                        operationTarget = dsA;\n                    } else {\n                        std::string tempNameResult = dsA-&gt;getDatasetName() + \"_temp_result\";\n                        tempResult = new BigDataStatMeth::hdf5Dataset(dsA-&gt;getFileptr(), dsA-&gt;getGroup(), tempNameResult, true);\n                        operationTarget = tempResult;\n                    }\n                } else if (target == \"B\") {\n                    if (isVectorB) {\n                        operationTarget = dsB;\n                    } else {\n                        std::string tempNameResult = dsB-&gt;getDatasetName() + \"_temp_result\";\n                        tempResult = new BigDataStatMeth::hdf5Dataset(dsB-&gt;getFileptr(), dsB-&gt;getGroup(), tempNameResult, true);\n                        operationTarget = tempResult;\n                    }\n                }\n                \n                // Perform vector operation\n                switch (operation) {\n                case 0: Rcpp_vector_add_hdf5(finalA, finalB, operationTarget, bparal, threads); break;\n                case 1: Rcpp_vector_subtract_hdf5(finalA, finalB, operationTarget, bparal, threads); break;\n                case 2: Rcpp_vector_multiply_hdf5(finalA, finalB, operationTarget, bparal, threads); break;\n                case 3: Rcpp_vector_divide_hdf5(finalA, finalB, operationTarget, bparal, threads); break;\n                default: Rf_error(\"Unknown diagonal operation: %d\", operation);\n                }\n                \n                // Write result back to matrix diagonal if needed\n                if (target == \"A\" && !isVectorA) {\n                    writeDiagonalFromVector(tempResult, dsA);\n                } else if (target == \"B\" && !isVectorB) {\n                    writeDiagonalFromVector(tempResult, dsB);\n                }\n                \n                // Cleanup\n                cleanup_temp_datasets(tempA, tempB);\n                if (tempResult) { delete tempResult; tempResult = nullptr; }\n                \n            } catch(std::exception& ex) {\n                checkClose_file(tempA, tempB, tempResult);\n                cleanup_temp_datasets(tempA, tempB);\n                if (tempResult) { delete tempResult; tempResult = nullptr; }\n                Rf_error(\"Error in performMatrixDiagonalOperation: %s\", ex.what());\n            }\n        }",
    "crumbs": [
      "Functions",
      "performMatrixDiagonalOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#usage-example",
    "href": "api-reference/cpp/functions/performMatrixDiagonalOperation.html#usage-example",
    "title": "performMatrixDiagonalOperation",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = performMatrixDiagonalOperation(...);",
    "crumbs": [
      "Functions",
      "performMatrixDiagonalOperation"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/prepareForParallelization.html",
    "href": "api-reference/cpp/functions/prepareForParallelization.html",
    "title": "prepareForParallelization",
    "section": "",
    "text": "std::vector&lt; svdPositions &gt; BigDataStatMeth::prepareForParallelization(T *dsA, int M, int k, bool transp, int block_size, std::string strDatasetName)",
    "crumbs": [
      "Functions",
      "prepareForParallelization"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/prepareForParallelization.html#signature",
    "href": "api-reference/cpp/functions/prepareForParallelization.html#signature",
    "title": "prepareForParallelization",
    "section": "",
    "text": "std::vector&lt; svdPositions &gt; BigDataStatMeth::prepareForParallelization(T *dsA, int M, int k, bool transp, int block_size, std::string strDatasetName)",
    "crumbs": [
      "Functions",
      "prepareForParallelization"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/prepareForParallelization.html#parameters",
    "href": "api-reference/cpp/functions/prepareForParallelization.html#parameters",
    "title": "prepareForParallelization",
    "section": "2 Parameters",
    "text": "2 Parameters\n\ndsA (T *)\nM (int)\nk (int)\ntransp (bool)\nblock_size (int)\nstrDatasetName (std::string)",
    "crumbs": [
      "Functions",
      "prepareForParallelization"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/prepareForParallelization.html#returns",
    "href": "api-reference/cpp/functions/prepareForParallelization.html#returns",
    "title": "prepareForParallelization",
    "section": "3 Returns",
    "text": "3 Returns\nType: class T",
    "crumbs": [
      "Functions",
      "prepareForParallelization"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/prepareForParallelization.html#call-graph",
    "href": "api-reference/cpp/functions/prepareForParallelization.html#call-graph",
    "title": "prepareForParallelization",
    "section": "4 Call Graph",
    "text": "4 Call Graph",
    "crumbs": [
      "Functions",
      "prepareForParallelization"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/prepareForParallelization.html#source-code",
    "href": "api-reference/cpp/functions/prepareForParallelization.html#source-code",
    "title": "prepareForParallelization",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixSvdBlock.hpp • Lines 136-230\nstd::vector&lt;svdPositions&gt; prepareForParallelization( T* dsA, int M, int k, bool transp, int block_size, std::string strDatasetName)\n{\n\n    static_assert(std::is_same&lt;T*, BigDataStatMeth::hdf5Dataset* &gt;::value ||\n                  std::is_same&lt;T*, BigDataStatMeth::hdf5DatasetInternal* &gt;::value,\n                  \"Error - type not allowed\");\n\n\n    std::vector&lt;svdPositions&gt; pos;\n    BigDataStatMeth::hdf5Dataset* unlimDataset = nullptr;\n\n    // Rcpp::Rcout&lt;&lt;\"\\nAnem a preparar per paralelitzar... a veure que fem per aquí perquè no m'agrada massa... \\n\";\n\n    try{\n\n        // int realsizeread, cummoffset;\n        int realsizeread;\n        int irows = dsA-&gt;ncols();\n        int icols = dsA-&gt;nrows();\n\n        \n        if(transp == false) {\n            irows = dsA-&gt;ncols();\n            icols = dsA-&gt;nrows();    \n        } else {\n            irows = dsA-&gt;nrows();\n            icols = dsA-&gt;ncols();    \n        }\n        \n\n        for( int i = 0; i&lt; M ; i++)\n        {\n            int maxsizetoread = block_size;\n\n            pos.push_back(svdPositions());\n\n            pos[i].strDatasetName = strDatasetName + std::to_string(i/(M/k));\n            pos[i].totOffset = getInitialPosition( transp, (unsigned long long)(i*block_size) ); // Initial read position\n\n            // Get max block size to read - for blocks smaller than default block size\n            if( ((i+1)*block_size) &gt; icols)\n                maxsizetoread = icols - (i*block_size);\n\n            if( i+1 == M && icols - maxsizetoread!=0) {\n                realsizeread = icols - (i*block_size);\n            } else{\n                realsizeread = maxsizetoread;\n            }\n\n            pos[i].count = getSizetoRead(transp, (unsigned long long)(realsizeread), icols, irows );\n\n            if( i%(M/k) == 0 || ( (i%(M/k) &gt; 0 &&  !exists_HDF5_element(dsA-&gt;getFileptr(),  pos[i].strDatasetName)) ) )\n            {\n                pos[i].cummoffset = 0;\n            } else {\n                pos[i].cummoffset = pos[i-1].cummoffset + pos[i-1].count[0];\n            }\n\n            pos[i].partOffset[1] = pos[i].cummoffset;\n        }\n\n\n    } catch( H5::FileIException& error ) {\n        checkClose_file(dsA, unlimDataset);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception prepareForParallelization (File IException)\\n\";\n        return(pos);\n    } catch( H5::DataSetIException& error ) {\n        checkClose_file(dsA, unlimDataset);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception prepareForParallelization (DataSet IException)\\n\";\n        return(pos);\n    } catch( H5::GroupIException& error ) {\n        checkClose_file(dsA, unlimDataset);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception prepareForParallelization (Group IException)\\n\";\n        return(pos);\n    } catch( H5::DataTypeIException& error ) {\n        checkClose_file(dsA, unlimDataset);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception prepareForParallelization (DataType IException)\\n\";\n        return(pos);\n    } catch( H5::DataSpaceIException& error ) {\n        checkClose_file(dsA, unlimDataset);\n        Rcpp::Rcerr&lt;&lt;\"\\nc++ exception prepareForParallelization (DataSpace IException)\\n\";\n        return(pos);\n    } catch(std::exception &ex) {\n        checkClose_file(dsA, unlimDataset);\n        Rcpp::Rcerr&lt;&lt;\"c++ exception prepareForParallelization \\n\"&lt;&lt; ex.what();\n        return(pos);\n    } catch (...) {\n        checkClose_file(dsA, unlimDataset);\n        Rcpp::Rcerr&lt;&lt;\"\\nC++ exception prepareForParallelization (unknown reason)\";\n        return(pos);\n    }\n\n\n    return(pos);\n}",
    "crumbs": [
      "Functions",
      "prepareForParallelization"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/prepareForParallelization.html#usage-example",
    "href": "api-reference/cpp/functions/prepareForParallelization.html#usage-example",
    "title": "prepareForParallelization",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = prepareForParallelization(...);",
    "crumbs": [
      "Functions",
      "prepareForParallelization"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeRow.html",
    "href": "api-reference/cpp/functions/removeRow.html",
    "title": "removeRow",
    "section": "",
    "text": "void BigDataStatMeth::removeRow(Eigen::MatrixXd &matrix, unsigned int rowToRemove)",
    "crumbs": [
      "Functions",
      "removeRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeRow.html#signature",
    "href": "api-reference/cpp/functions/removeRow.html#signature",
    "title": "removeRow",
    "section": "",
    "text": "void BigDataStatMeth::removeRow(Eigen::MatrixXd &matrix, unsigned int rowToRemove)",
    "crumbs": [
      "Functions",
      "removeRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeRow.html#description",
    "href": "api-reference/cpp/functions/removeRow.html#description",
    "title": "removeRow",
    "section": "2 Description",
    "text": "2 Description\nRemoves a row from an Eigen matrix.",
    "crumbs": [
      "Functions",
      "removeRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeRow.html#parameters",
    "href": "api-reference/cpp/functions/removeRow.html#parameters",
    "title": "removeRow",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nmatrix (Eigen::MatrixXd &): Reference to matrix to modify\nrowToRemove (unsigned int): Index of row to remove",
    "crumbs": [
      "Functions",
      "removeRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeRow.html#details",
    "href": "api-reference/cpp/functions/removeRow.html#details",
    "title": "removeRow",
    "section": "4 Details",
    "text": "4 Details\nmatrixReference to matrix to modify rowToRemoveIndex of row to remove Implementation details:Shifts remaining rows upResizes matrix to remove last row",
    "crumbs": [
      "Functions",
      "removeRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeRow.html#caller-graph",
    "href": "api-reference/cpp/functions/removeRow.html#caller-graph",
    "title": "removeRow",
    "section": "5 Caller Graph",
    "text": "5 Caller Graph",
    "crumbs": [
      "Functions",
      "removeRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeRow.html#source-code",
    "href": "api-reference/cpp/functions/removeRow.html#source-code",
    "title": "removeRow",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5Utilities.hpp • Lines 345-354\ninline void removeRow(Eigen::MatrixXd& matrix, unsigned int rowToRemove)\n    {\n        unsigned int numRows = matrix.rows()-1;\n        unsigned int numCols = matrix.cols();\n        \n        if( rowToRemove &lt; numRows )\n            matrix.block(rowToRemove,0,numRows-rowToRemove,numCols) = matrix.bottomRows(numRows-rowToRemove).eval();\n        \n        matrix.conservativeResize(numRows,numCols);\n    }",
    "crumbs": [
      "Functions",
      "removeRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/removeRow.html#usage-example",
    "href": "api-reference/cpp/functions/removeRow.html#usage-example",
    "title": "removeRow",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = removeRow(...);",
    "crumbs": [
      "Functions",
      "removeRow"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/renameElement.html",
    "href": "api-reference/cpp/functions/renameElement.html",
    "title": "renameElement",
    "section": "",
    "text": "void BigDataStatMeth::renameElement(H5::H5File *file, std::string original, std::string link)",
    "crumbs": [
      "Functions",
      "renameElement"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/renameElement.html#signature",
    "href": "api-reference/cpp/functions/renameElement.html#signature",
    "title": "renameElement",
    "section": "",
    "text": "void BigDataStatMeth::renameElement(H5::H5File *file, std::string original, std::string link)",
    "crumbs": [
      "Functions",
      "renameElement"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/renameElement.html#description",
    "href": "api-reference/cpp/functions/renameElement.html#description",
    "title": "renameElement",
    "section": "2 Description",
    "text": "2 Description\nRenames an element in an HDF5 file.",
    "crumbs": [
      "Functions",
      "renameElement"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/renameElement.html#parameters",
    "href": "api-reference/cpp/functions/renameElement.html#parameters",
    "title": "renameElement",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nfile (H5::H5File *): Pointer to HDF5 file\noriginal (std::string): Current path of element\nlink (std::string): New path for element",
    "crumbs": [
      "Functions",
      "renameElement"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/renameElement.html#details",
    "href": "api-reference/cpp/functions/renameElement.html#details",
    "title": "renameElement",
    "section": "4 Details",
    "text": "4 Details\nfilePointer to HDF5 file originalCurrent path of element linkNew path for elementH5::FileIExceptionon file operation errors H5::DataSetIExceptionon dataset operation errors H5::GroupIExceptionon group operation errors std::exceptionon other errors",
    "crumbs": [
      "Functions",
      "renameElement"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/renameElement.html#source-code",
    "href": "api-reference/cpp/functions/renameElement.html#source-code",
    "title": "renameElement",
    "section": "5 Source Code",
    "text": "5 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5Utilities.hpp • Lines 291-329\ninline void renameElement( H5::H5File* file, std::string original, std::string link)\n    {\n        \n        try{\n            \n            H5::Exception::dontPrint();\n            \n            const char * charOriginal = original.c_str();\n            const char * charLink = link.c_str();\n            \n            // Rcpp::Rcout&lt;&lt;\"original: \"&lt;&lt; original&lt;&lt;\" - Desti: \"&lt;&lt;link&lt;&lt;\"\\n\";\n            \n            herr_t status = H5Lmove(file-&gt;getId(), charOriginal, file-&gt;getId(), charLink, H5P_DEFAULT, H5P_DEFAULT);\n            \n            if(status&lt;0) {\n                Rcpp::Rcerr&lt;&lt;\"c++ exception renameElement (rename_element IException)\" &lt;&lt; std::endl;\n                return void();\n            } \n            \n            \n        } catch(H5::FileIException& error) { \n            Rcpp::Rcerr&lt;&lt;\"c++ exception renameElement (File IException)\" &lt;&lt; std::endl;\n            return void();\n        } catch(H5::DataSetIException& error) { \n            Rcpp::Rcerr&lt;&lt;\"c++ exception renameElement (DataSet IException)\" &lt;&lt; std::endl;\n            return void();\n        } catch(H5::GroupIException& error) { \n            Rcpp::Rcerr&lt;&lt;\"c++ exception renameElement (Group IException)\" &lt;&lt; std::endl;\n            return void();\n        } catch(std::exception &ex) {\n            Rcpp::Rcerr &lt;&lt; \"c++ exception renameElement: \" &lt;&lt; ex.what();\n            return void();\n        }  catch (...) {\n            Rcpp::Rcerr&lt;&lt;\"\\nC++ exception renameElement (unknown reason)\";\n            return void();\n        }\n        \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "renameElement"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/renameElement.html#usage-example",
    "href": "api-reference/cpp/functions/renameElement.html#usage-example",
    "title": "renameElement",
    "section": "6 Usage Example",
    "text": "6 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = renameElement(...);",
    "crumbs": [
      "Functions",
      "renameElement"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setDiagonalMatrix.html",
    "href": "api-reference/cpp/functions/setDiagonalMatrix.html",
    "title": "setDiagonalMatrix",
    "section": "",
    "text": "void BigDataStatMeth::setDiagonalMatrix(BigDataStatMeth::hdf5Dataset *dsMat, Rcpp::NumericVector intNewDiagonal)",
    "crumbs": [
      "Functions",
      "setDiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setDiagonalMatrix.html#signature",
    "href": "api-reference/cpp/functions/setDiagonalMatrix.html#signature",
    "title": "setDiagonalMatrix",
    "section": "",
    "text": "void BigDataStatMeth::setDiagonalMatrix(BigDataStatMeth::hdf5Dataset *dsMat, Rcpp::NumericVector intNewDiagonal)",
    "crumbs": [
      "Functions",
      "setDiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setDiagonalMatrix.html#description",
    "href": "api-reference/cpp/functions/setDiagonalMatrix.html#description",
    "title": "setDiagonalMatrix",
    "section": "2 Description",
    "text": "2 Description\nSets the diagonal elements of a matrix stored in HDF5 format.",
    "crumbs": [
      "Functions",
      "setDiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setDiagonalMatrix.html#parameters",
    "href": "api-reference/cpp/functions/setDiagonalMatrix.html#parameters",
    "title": "setDiagonalMatrix",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsMat (BigDataStatMeth::hdf5Dataset *): Target HDF5 dataset containing the matrix\nintNewDiagonal (Rcpp::NumericVector): Vector of new diagonal values to set",
    "crumbs": [
      "Functions",
      "setDiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setDiagonalMatrix.html#details",
    "href": "api-reference/cpp/functions/setDiagonalMatrix.html#details",
    "title": "setDiagonalMatrix",
    "section": "4 Details",
    "text": "4 Details\ndsMatTarget HDF5 dataset containing the matrix intNewDiagonalVector of new diagonal values to set Implementation approach:Writes diagonal elements one at a timeUses HDF5 block writing for efficient accessPreserves existing non-diagonal elements",
    "crumbs": [
      "Functions",
      "setDiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setDiagonalMatrix.html#call-graph",
    "href": "api-reference/cpp/functions/setDiagonalMatrix.html#call-graph",
    "title": "setDiagonalMatrix",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "setDiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setDiagonalMatrix.html#source-code",
    "href": "api-reference/cpp/functions/setDiagonalMatrix.html#source-code",
    "title": "setDiagonalMatrix",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixDiagonal.hpp • Lines 104-136\ninline void setDiagonalMatrix(BigDataStatMeth::hdf5Dataset* dsMat, Rcpp::NumericVector intNewDiagonal)\n    {\n        try {\n            const hsize_t DIAG_BLOCK_SIZE = 256;\n            hsize_t matrix_size = intNewDiagonal.size();\n            std::vector&lt;hsize_t&gt; stride = {1, 1}, block = {1, 1};\n            \n            for (hsize_t block_start = 0; block_start &lt; matrix_size; block_start += DIAG_BLOCK_SIZE) {\n                hsize_t current_block_size = std::min(DIAG_BLOCK_SIZE, matrix_size - block_start);\n                \n                // Read square block starting from diagonal position\n                std::vector&lt;hsize_t&gt; offset = {block_start, block_start};\n                std::vector&lt;hsize_t&gt; count = {current_block_size, current_block_size};\n                \n                std::vector&lt;double&gt; block_data(current_block_size * current_block_size);\n                dsMat-&gt;readDatasetBlock(offset, count, stride, block, block_data.data());\n                \n                // Map to Eigen for correct R/HDF5 layout handling\n                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; \n                    block_matrix(block_data.data(), current_block_size, current_block_size);\n                \n                // Use Eigen diagonal assignment - NO LOOP NEEDED\n                Eigen::Map&lt;Eigen::VectorXd&gt; diagonal_segment(REAL(intNewDiagonal) + block_start, current_block_size);\n                block_matrix.diagonal() = diagonal_segment;\n                \n                // Write modified block back\n                dsMat-&gt;writeDatasetBlock(Rcpp::wrap(block_matrix), offset, count, stride, block, false);\n            }\n            \n        } catch(std::exception& ex) {\n            Rcpp::stop(\"c++ exception setDiagonalMatrix: \"+ std::string(ex.what()));\n        }\n    }",
    "crumbs": [
      "Functions",
      "setDiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setDiagonalMatrix.html#usage-example",
    "href": "api-reference/cpp/functions/setDiagonalMatrix.html#usage-example",
    "title": "setDiagonalMatrix",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = setDiagonalMatrix(...);",
    "crumbs": [
      "Functions",
      "setDiagonalMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setUpperTriangularMatrix.html",
    "href": "api-reference/cpp/functions/setUpperTriangularMatrix.html",
    "title": "setUpperTriangularMatrix",
    "section": "",
    "text": "void BigDataStatMeth::setUpperTriangularMatrix(BigDataStatMeth::hdf5Dataset *dsMat, hsize_t dElementsBlock)",
    "crumbs": [
      "Functions",
      "setUpperTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setUpperTriangularMatrix.html#signature",
    "href": "api-reference/cpp/functions/setUpperTriangularMatrix.html#signature",
    "title": "setUpperTriangularMatrix",
    "section": "",
    "text": "void BigDataStatMeth::setUpperTriangularMatrix(BigDataStatMeth::hdf5Dataset *dsMat, hsize_t dElementsBlock)",
    "crumbs": [
      "Functions",
      "setUpperTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setUpperTriangularMatrix.html#description",
    "href": "api-reference/cpp/functions/setUpperTriangularMatrix.html#description",
    "title": "setUpperTriangularMatrix",
    "section": "2 Description",
    "text": "2 Description\nSet the upper triangular matrix using block-based approach.",
    "crumbs": [
      "Functions",
      "setUpperTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setUpperTriangularMatrix.html#parameters",
    "href": "api-reference/cpp/functions/setUpperTriangularMatrix.html#parameters",
    "title": "setUpperTriangularMatrix",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsMat (BigDataStatMeth::hdf5Dataset *): The dataset to set the upper triangular matrix of\ndElementsBlock (hsize_t): The block size to use for the matrix",
    "crumbs": [
      "Functions",
      "setUpperTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setUpperTriangularMatrix.html#details",
    "href": "api-reference/cpp/functions/setUpperTriangularMatrix.html#details",
    "title": "setUpperTriangularMatrix",
    "section": "4 Details",
    "text": "4 Details\nReads blocks, modifies them in memory to create upper triangular structure, and writes complete modified blocks instead of individual elements",
    "crumbs": [
      "Functions",
      "setUpperTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setUpperTriangularMatrix.html#call-graph",
    "href": "api-reference/cpp/functions/setUpperTriangularMatrix.html#call-graph",
    "title": "setUpperTriangularMatrix",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "setUpperTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setUpperTriangularMatrix.html#source-code",
    "href": "api-reference/cpp/functions/setUpperTriangularMatrix.html#source-code",
    "title": "setUpperTriangularMatrix",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixTriangular.hpp • Lines 51-135\ninline void setUpperTriangularMatrix( BigDataStatMeth::hdf5Dataset* dsMat, hsize_t dElementsBlock)\n    {\n        \n        try {\n            \n            std::vector&lt;hsize_t&gt; stride = {1, 1},\n                block = {1, 1};\n            \n            hsize_t readedRows = 0,\n                rowstoRead,\n                minimumBlockSize;\n            \n            // Optimized block size calculation\n            minimumBlockSize = std::max(static_cast&lt;hsize_t&gt;(1024), \n                                        std::min(dElementsBlock, dsMat-&gt;nrows()));\n            \n            while ( readedRows &lt; dsMat-&gt;nrows() ) {\n                \n                rowstoRead = ( -2 * readedRows - 1 + std::sqrt( pow(2*readedRows, 2) - 4 * readedRows + 8 * minimumBlockSize + 1) ) / 2;\n                \n                if( readedRows + rowstoRead &gt; dsMat-&gt;nrows()) {\n                    rowstoRead = dsMat-&gt;nrows() - readedRows;\n                }\n                \n                // Read square block from diagonal position\n                std::vector&lt;hsize_t&gt; offset = {readedRows, readedRows};\n                std::vector&lt;hsize_t&gt; count = {rowstoRead, rowstoRead};\n                \n                // Read the current square block\n                std::vector&lt;double&gt; block_data(rowstoRead * rowstoRead);\n                dsMat-&gt;readDatasetBlock(offset, count, stride, block, block_data.data());\n                \n                // Map to Eigen matrix for easier manipulation\n                Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; \n                    block_matrix(block_data.data(), rowstoRead, rowstoRead);\n                \n                // Create upper triangular: copy lower triangle to upper triangle\n                for (hsize_t i = 0; i &lt; rowstoRead; i++) {\n                    for (hsize_t j = i + 1; j &lt; rowstoRead; j++) {\n                        block_matrix(i, j) = block_matrix(j, i);  // Copy lower to upper\n                    }\n                }\n                \n                // Write the complete modified block back - SINGLE WRITE OPERATION\n                dsMat-&gt;writeDatasetBlock(Rcpp::wrap(block_matrix), offset, count, stride, block, false);\n                \n                // Handle rectangular regions outside the diagonal blocks\n                if (readedRows + rowstoRead &lt; dsMat-&gt;nrows()) {\n                    // Write lower-right rectangular region (below diagonal block)\n                    hsize_t remaining_rows = dsMat-&gt;nrows() - readedRows - rowstoRead;\n                    std::vector&lt;hsize_t&gt; rect_offset = {readedRows + rowstoRead, readedRows};\n                    std::vector&lt;hsize_t&gt; rect_count = {remaining_rows, rowstoRead};\n                    \n                    // Read lower-left block (source for transpose)\n                    std::vector&lt;double&gt; rect_data(remaining_rows * rowstoRead);\n                    dsMat-&gt;readDatasetBlock(rect_offset, rect_count, stride, block, rect_data.data());\n                    \n                    Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; \n                        rect_matrix(rect_data.data(), remaining_rows, rowstoRead);\n                    \n                    // Write transposed data to upper-right position\n                    std::vector&lt;hsize_t&gt; upper_offset = {readedRows, readedRows + rowstoRead};\n                    std::vector&lt;hsize_t&gt; upper_count = {rowstoRead, remaining_rows};\n                    \n                    Eigen::MatrixXd transposed = rect_matrix.transpose();\n                    dsMat-&gt;writeDatasetBlock(Rcpp::wrap(transposed), upper_offset, upper_count, stride, block, false);\n                }\n                \n                readedRows = readedRows + rowstoRead; \n            }\n            \n        }\n        catch( H5::FileIException& error ) {\n            Rcpp::Rcout&lt;&lt;\"c++ exception setUpperTriangularMatrix (File IException)\";\n            return void();\n        } catch( H5::DataSetIException& error ) {\n            Rcpp::Rcout &lt;&lt; \"c++ exception setUpperTriangularMatrix (DataSet IException)\";\n            return void();\n        } catch(std::exception& ex) {\n            Rcpp::Rcout &lt;&lt; \"c++ exception setUpperTriangularMatrix: \" &lt;&lt; ex.what();\n            return void();\n        }\n        \n        return void();\n    }",
    "crumbs": [
      "Functions",
      "setUpperTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/setUpperTriangularMatrix.html#usage-example",
    "href": "api-reference/cpp/functions/setUpperTriangularMatrix.html#usage-example",
    "title": "setUpperTriangularMatrix",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = setUpperTriangularMatrix(...);",
    "crumbs": [
      "Functions",
      "setUpperTriangularMatrix"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/subtractDiagonals.html",
    "href": "api-reference/cpp/functions/subtractDiagonals.html",
    "title": "subtractDiagonals",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::subtractDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "subtractDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/subtractDiagonals.html#signature",
    "href": "api-reference/cpp/functions/subtractDiagonals.html#signature",
    "title": "subtractDiagonals",
    "section": "",
    "text": "void BigDataStatMeth::DiagonalOps::subtractDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "subtractDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/subtractDiagonals.html#description",
    "href": "api-reference/cpp/functions/subtractDiagonals.html#description",
    "title": "subtractDiagonals",
    "section": "2 Description",
    "text": "2 Description\nSubtract diagonal elements from two matrices or vectors.",
    "crumbs": [
      "Functions",
      "subtractDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/subtractDiagonals.html#parameters",
    "href": "api-reference/cpp/functions/subtractDiagonals.html#parameters",
    "title": "subtractDiagonals",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): First input dataset (minuend)\ndsB (BigDataStatMeth::hdf5Dataset *): Second input dataset (subtrahend)\ndsResult (BigDataStatMeth::hdf5Dataset *): Result dataset (will be created)\ntarget (std::string): Where to write result: “A”, “B”, or “new”\nbparal (bool): Whether to use parallel processing\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "subtractDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/subtractDiagonals.html#details",
    "href": "api-reference/cpp/functions/subtractDiagonals.html#details",
    "title": "subtractDiagonals",
    "section": "4 Details",
    "text": "4 Details\nPerforms optimized diagonal subtraction C_diag = A_diag - B_diag. Same optimization strategy as addDiagonals but for subtraction operation.",
    "crumbs": [
      "Functions",
      "subtractDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/subtractDiagonals.html#call-graph",
    "href": "api-reference/cpp/functions/subtractDiagonals.html#call-graph",
    "title": "subtractDiagonals",
    "section": "5 Call Graph",
    "text": "5 Call Graph",
    "crumbs": [
      "Functions",
      "subtractDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/subtractDiagonals.html#source-code",
    "href": "api-reference/cpp/functions/subtractDiagonals.html#source-code",
    "title": "subtractDiagonals",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5DiagonalMethods.hpp • Lines 430-449\ninline void subtractDiagonals(BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB,\n                                      BigDataStatMeth::hdf5Dataset* dsResult, std::string target = \"new\",\n                                      bool bparal = false, Rcpp::Nullable&lt;int&gt; threads = R_NilValue)\n        {\n            try {\n                bool isVectorA = isDiagonalVector(dsA);\n                bool isVectorB = isDiagonalVector(dsB);\n                \n                if (isVectorA && isVectorB && (target == \"A\" || target == \"B\")) {\n                    BigDataStatMeth::hdf5Dataset* targetDataset = (target == \"A\") ? dsA : dsB;\n                    Rcpp_vector_subtract_hdf5(dsA, dsB, targetDataset, bparal, threads);\n                } else if (isVectorA && isVectorB && target == \"new\") {\n                    Rcpp_vector_subtract_hdf5(dsA, dsB, dsResult, bparal, threads);\n                } else {\n                    performMatrixDiagonalOperation(dsA, dsB, dsResult, 1, target, bparal, threads);\n                }\n            } catch(std::exception& ex) {\n                Rf_error(\"Error in subtractDiagonals: %s\", ex.what());\n            }\n        }",
    "crumbs": [
      "Functions",
      "subtractDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/subtractDiagonals.html#usage-example",
    "href": "api-reference/cpp/functions/subtractDiagonals.html#usage-example",
    "title": "subtractDiagonals",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = subtractDiagonals(...);",
    "crumbs": [
      "Functions",
      "subtractDiagonals"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/tcrossprod.html",
    "href": "api-reference/cpp/functions/tcrossprod.html",
    "title": "tcrossprod",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::tcrossprod(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool isSymmetric, hsize_t hdf5_block, hsize_t mem_block_size, bool bparal, bool browmajor, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "tcrossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/tcrossprod.html#signature",
    "href": "api-reference/cpp/functions/tcrossprod.html#signature",
    "title": "tcrossprod",
    "section": "",
    "text": "BigDataStatMeth::hdf5Dataset * BigDataStatMeth::tcrossprod(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool isSymmetric, hsize_t hdf5_block, hsize_t mem_block_size, bool bparal, bool browmajor, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)",
    "crumbs": [
      "Functions",
      "tcrossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/tcrossprod.html#description",
    "href": "api-reference/cpp/functions/tcrossprod.html#description",
    "title": "tcrossprod",
    "section": "2 Description",
    "text": "2 Description\nTransposed cross-product for HDF5 matrices.",
    "crumbs": [
      "Functions",
      "tcrossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/tcrossprod.html#parameters",
    "href": "api-reference/cpp/functions/tcrossprod.html#parameters",
    "title": "tcrossprod",
    "section": "3 Parameters",
    "text": "3 Parameters\n\ndsA (BigDataStatMeth::hdf5Dataset *): Input matrix dataset\ndsB (BigDataStatMeth::hdf5Dataset *): Input matrix dataset\ndsC (BigDataStatMeth::hdf5Dataset *): Output matrix dataset\nhdf5_block (hsize_t): Block size for HDF5 I/O operations\nmem_block_size (hsize_t): Block size for in-memory operations\nbparal (bool): Whether to use parallel processing\nbrowmajor (bool): Whether matrices are stored in row-major order\nthreads (Rcpp::Nullable&lt; int &gt;): Number of threads for parallel processing",
    "crumbs": [
      "Functions",
      "tcrossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/tcrossprod.html#returns",
    "href": "api-reference/cpp/functions/tcrossprod.html#returns",
    "title": "tcrossprod",
    "section": "4 Returns",
    "text": "4 Returns\nType: BigDataStatMeth::hdf5Dataset *",
    "crumbs": [
      "Functions",
      "tcrossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/tcrossprod.html#details",
    "href": "api-reference/cpp/functions/tcrossprod.html#details",
    "title": "tcrossprod",
    "section": "5 Details",
    "text": "5 Details\nComputes the transposed cross-product of matrices stored in HDF5 format. Supports both A’A and AA’ computations with block-based processing.",
    "crumbs": [
      "Functions",
      "tcrossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/tcrossprod.html#call-graph",
    "href": "api-reference/cpp/functions/tcrossprod.html#call-graph",
    "title": "tcrossprod",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "tcrossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/tcrossprod.html#source-code",
    "href": "api-reference/cpp/functions/tcrossprod.html#source-code",
    "title": "tcrossprod",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/tcrossprod.hpp • Lines 58-211\ninline BigDataStatMeth::hdf5Dataset* tcrossprod( \n            BigDataStatMeth::hdf5Dataset* dsA, BigDataStatMeth::hdf5Dataset* dsB, \n            BigDataStatMeth::hdf5Dataset* dsC, bool isSymmetric, hsize_t hdf5_block, \n            hsize_t mem_block_size, bool bparal, bool browmajor, \n            Rcpp::Nullable&lt;int&gt; threads = R_NilValue) \n\n    {\n        \n        try {\n            \n            hsize_t N = dsA-&gt;ncols();  \n            hsize_t K = dsA-&gt;nrows();  \n            hsize_t M = dsB-&gt;ncols();  \n            hsize_t L = dsB-&gt;nrows();  \n            \n            if (K != L) {\n                throw std::range_error(\"non-conformable arguments\");\n            }\n            \n            if( K == L)\n            {\n                if (isSymmetric) {\n                    if (N != M) {\n                        throw std::range_error(\"Symmetric tcrossprod requires square result matrix\");\n                    }\n                    if (dsA-&gt;getFileName() != dsB-&gt;getFileName() || \n                        dsA-&gt;getGroup() != dsB-&gt;getGroup() || \n                        dsA-&gt;getDatasetName() != dsB-&gt;getDatasetName()) {\n                        Rcpp::warning(\"isSymmetric=TRUE but different datasets provided. Results may be incorrect.\");\n                    }\n                }\n\n                \n/** 2025/11/25\n                // Configure parallel processing\n                int num_threads = 1;\n                if (bparal) {\n                    num_threads = get_number_threads(threads, Rcpp::wrap(bparal));\n#ifdef _OPENMP\n                    omp_set_num_threads(num_threads);\n#endif\n                }\n Fi 2025/11/25 **/\n\n#ifdef _OPENMP  // Configure parallel processing\n                int num_threads = 1;\n                if (bparal) {\n                    num_threads = get_number_threads(threads, Rcpp::wrap(bparal));\n                    omp_set_num_threads(num_threads);\n                }\n#endif\n                \n                // Calculate total blocks for parallelization\n                hsize_t blocks_i = (N + hdf5_block - 1) / hdf5_block;\n                hsize_t blocks_j = isSymmetric ? blocks_i : (M + hdf5_block - 1) / hdf5_block;\n                hsize_t total_blocks;\n                \n                if (isSymmetric) {\n                    // For symmetric case: only upper triangle blocks\n                    total_blocks = (blocks_i * (blocks_i + 1)) / 2;\n                } else {\n                    // For general case: all blocks\n                    total_blocks = blocks_i * blocks_j;\n                }\n                \n                dsC-&gt;createDataset( N, M, \"real\");\n                \n#ifdef _OPENMP\n#pragma omp parallel for if(bparal) schedule(dynamic)\n#endif\n                for (hsize_t block_idx = 0; block_idx &lt; total_blocks; ++block_idx)\n                {\n                    // Convert linear index to (ii_idx, jj_idx)\n                    hsize_t ii_idx = 0, \n                            jj_idx = 0;\n                    \n                    if (isSymmetric) {\n                        // Convert to upper triangle coordinates\n                        hsize_t remaining = block_idx;\n                        for (hsize_t i = 0; i &lt; blocks_i; ++i) {\n                            hsize_t blocks_in_row = blocks_i - i;\n                            if (remaining &lt; blocks_in_row) {\n                                ii_idx = i;\n                                jj_idx = i + remaining;\n                                break;\n                            }\n                            remaining -= blocks_in_row;\n                        }\n                    } else {\n                        // Convert to regular coordinates\n                        ii_idx = block_idx / blocks_j;\n                        jj_idx = block_idx % blocks_j;\n                    }\n                    \n                    // Convert block indices to matrix indices\n                    hsize_t ii = ii_idx * hdf5_block;\n                    hsize_t jj = jj_idx * hdf5_block;\n                    \n                    // Boundary checks\n                    if (ii &gt;= N || jj &gt;= M) continue;\n                    \n                    hsize_t Nii = std::min(hdf5_block, N - ii);\n                    hsize_t Mjj = std::min(hdf5_block, M - jj);\n                    \n                    // Thread-local variables\n                    std::vector&lt;hsize_t&gt; stride = {1, 1};\n                    std::vector&lt;hsize_t&gt; block = {1, 1};\n                    \n                    Eigen::MatrixXd C_accum = Eigen::MatrixXd::Zero(Nii, Mjj);\n                    \n                    std::vector&lt;hsize_t&gt; offset = {jj, ii};\n                    std::vector&lt;hsize_t&gt; count = {Mjj, Nii};\n                    \n                    for(hsize_t kk = 0; kk &lt; K; kk += hdf5_block)\n                    {\n                        hsize_t Kkk = std::min(hdf5_block, K - kk); \n                        \n                        Eigen::MatrixXd A;\n                        \n                        {\n                            std::vector&lt;double&gt; vdA( Nii * Kkk);\n                            dsA-&gt;readDatasetBlock( {kk, ii}, {Kkk, Nii}, stride, block, vdA.data() );\n                            Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; tmp_A (vdA.data(), Kkk, Nii );    \n                            A = tmp_A.transpose();\n                        }\n                        \n                        std::vector&lt;double&gt; vdB( Mjj * Kkk);\n                        dsB-&gt;readDatasetBlock( {kk, jj}, { Kkk, Mjj}, stride, block, vdB.data() );\n                        Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor&gt;&gt; B (vdB.data(), Kkk, Mjj);   \n                        \n                        C_accum.noalias() += A * B;\n                    }\n                    \n                    dsC-&gt;writeDatasetBlock(Rcpp::wrap(C_accum), offset, count, stride, block, false);\n                    \n                    // Symetric matrices\n                    if (isSymmetric && ii != jj) {\n                        std::vector&lt;hsize_t&gt; offset_sym = {ii, jj};\n                        std::vector&lt;hsize_t&gt; count_sym = {Nii, Mjj};\n                        dsC-&gt;writeDatasetBlock(Rcpp::wrap(C_accum.transpose()), offset_sym, count_sym, stride, block, false);\n                    }\n                }\n                \n            } else {\n                throw std::range_error(\"non-conformable arguments\");\n            }\n            \n        } catch(std::exception& ex) {\n            Rcpp::Rcout&lt;&lt; \"c++ exception tcrossprod: \"&lt;&lt;ex.what()&lt;&lt; \" \\n\";\n            return(dsC);\n        }\n        \n        return(dsC);\n    }",
    "crumbs": [
      "Functions",
      "tcrossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/tcrossprod.html#usage-example",
    "href": "api-reference/cpp/functions/tcrossprod.html#usage-example",
    "title": "tcrossprod",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = tcrossprod(...);",
    "crumbs": [
      "Functions",
      "tcrossprod"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateSpectraParams.html",
    "href": "api-reference/cpp/functions/validateSpectraParams.html",
    "title": "validateSpectraParams",
    "section": "",
    "text": "std::tuple&lt; int, int &gt; BigDataStatMeth::validateSpectraParams(int n, int k, int ncv)",
    "crumbs": [
      "Functions",
      "validateSpectraParams"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateSpectraParams.html#signature",
    "href": "api-reference/cpp/functions/validateSpectraParams.html#signature",
    "title": "validateSpectraParams",
    "section": "",
    "text": "std::tuple&lt; int, int &gt; BigDataStatMeth::validateSpectraParams(int n, int k, int ncv)",
    "crumbs": [
      "Functions",
      "validateSpectraParams"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateSpectraParams.html#description",
    "href": "api-reference/cpp/functions/validateSpectraParams.html#description",
    "title": "validateSpectraParams",
    "section": "2 Description",
    "text": "2 Description\nValidate and adjust Spectra parameters for convergence.",
    "crumbs": [
      "Functions",
      "validateSpectraParams"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateSpectraParams.html#parameters",
    "href": "api-reference/cpp/functions/validateSpectraParams.html#parameters",
    "title": "validateSpectraParams",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nn (int): Matrix size\nk (int): Number of eigenvalues requested\nncv (int): Number of Arnoldi vectors",
    "crumbs": [
      "Functions",
      "validateSpectraParams"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateSpectraParams.html#returns",
    "href": "api-reference/cpp/functions/validateSpectraParams.html#returns",
    "title": "validateSpectraParams",
    "section": "4 Returns",
    "text": "4 Returns\nAdjusted parameters that satisfy Spectra constraints",
    "crumbs": [
      "Functions",
      "validateSpectraParams"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateSpectraParams.html#details",
    "href": "api-reference/cpp/functions/validateSpectraParams.html#details",
    "title": "validateSpectraParams",
    "section": "5 Details",
    "text": "5 Details\nnMatrix size kNumber of eigenvalues requested ncvNumber of Arnoldi vectors Adjusted parameters that satisfy Spectra constraints",
    "crumbs": [
      "Functions",
      "validateSpectraParams"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateSpectraParams.html#caller-graph",
    "href": "api-reference/cpp/functions/validateSpectraParams.html#caller-graph",
    "title": "validateSpectraParams",
    "section": "6 Caller Graph",
    "text": "6 Caller Graph",
    "crumbs": [
      "Functions",
      "validateSpectraParams"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateSpectraParams.html#source-code",
    "href": "api-reference/cpp/functions/validateSpectraParams.html#source-code",
    "title": "validateSpectraParams",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Algebra/matrixEigenDecomposition.hpp • Lines 72-92\ninline std::tuple&lt;int, int&gt; validateSpectraParams(int n, int k, int ncv) {\n        // Ensure k is reasonable\n        if (k &lt;= 0) k = std::min(n, 6);\n        if (k &gt;= n) k = n - 1;\n        \n        // Calculate optimal ncv following RSpectra defaults\n        if (ncv &lt;= 0) {\n            ncv = std::min(n, std::max(2 * k + 1, k + 2));\n        }\n        \n        // Enforce Spectra constraints: k + 2 &lt;= ncv &lt;= n\n        ncv = std::max(ncv, k + 2);\n        ncv = std::min(ncv, n);\n        \n        // Additional safety: ensure we have enough space for convergence\n        if (ncv - k &lt; 2) {\n            ncv = std::min(n, k + 2);\n        }\n        \n        return std::make_tuple(k, ncv);\n    }",
    "crumbs": [
      "Functions",
      "validateSpectraParams"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateSpectraParams.html#usage-example",
    "href": "api-reference/cpp/functions/validateSpectraParams.html#usage-example",
    "title": "validateSpectraParams",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = validateSpectraParams(...);",
    "crumbs": [
      "Functions",
      "validateSpectraParams"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVectorDataset.html",
    "href": "api-reference/cpp/functions/validateVectorDataset.html",
    "title": "validateVectorDataset",
    "section": "",
    "text": "hsize_t BigDataStatMeth::DiagonalOps::validateVectorDataset(BigDataStatMeth::hdf5Dataset *ds)",
    "crumbs": [
      "Functions",
      "validateVectorDataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVectorDataset.html#signature",
    "href": "api-reference/cpp/functions/validateVectorDataset.html#signature",
    "title": "validateVectorDataset",
    "section": "",
    "text": "hsize_t BigDataStatMeth::DiagonalOps::validateVectorDataset(BigDataStatMeth::hdf5Dataset *ds)",
    "crumbs": [
      "Functions",
      "validateVectorDataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVectorDataset.html#description",
    "href": "api-reference/cpp/functions/validateVectorDataset.html#description",
    "title": "validateVectorDataset",
    "section": "2 Description",
    "text": "2 Description\nValidate vector dataset and return its size.",
    "crumbs": [
      "Functions",
      "validateVectorDataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVectorDataset.html#parameters",
    "href": "api-reference/cpp/functions/validateVectorDataset.html#parameters",
    "title": "validateVectorDataset",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nds (BigDataStatMeth::hdf5Dataset *): Dataset to validate",
    "crumbs": [
      "Functions",
      "validateVectorDataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVectorDataset.html#returns",
    "href": "api-reference/cpp/functions/validateVectorDataset.html#returns",
    "title": "validateVectorDataset",
    "section": "4 Returns",
    "text": "4 Returns\nVector size if valid vector, 0 if not a vector",
    "crumbs": [
      "Functions",
      "validateVectorDataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVectorDataset.html#details",
    "href": "api-reference/cpp/functions/validateVectorDataset.html#details",
    "title": "validateVectorDataset",
    "section": "5 Details",
    "text": "5 Details\nChecks dataset dimensions and returns the number of elements in the vector. Used for dimension validation before performing vector operations.",
    "crumbs": [
      "Functions",
      "validateVectorDataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVectorDataset.html#call-graph",
    "href": "api-reference/cpp/functions/validateVectorDataset.html#call-graph",
    "title": "validateVectorDataset",
    "section": "6 Call Graph",
    "text": "6 Call Graph",
    "crumbs": [
      "Functions",
      "validateVectorDataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVectorDataset.html#source-code",
    "href": "api-reference/cpp/functions/validateVectorDataset.html#source-code",
    "title": "validateVectorDataset",
    "section": "7 Source Code",
    "text": "7 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/hdf5Utilities/hdf5DiagonalMethods.hpp • Lines 38-51\ninline hsize_t validateVectorDataset(BigDataStatMeth::hdf5Dataset* ds)\n        {\n            hsize_t rows = ds-&gt;nrows();\n            hsize_t cols = ds-&gt;ncols();\n            \n            if (rows == 1 && cols &gt; 1) {\n                return cols;  // Row vector 1×N\n            } else if (cols == 1 && rows &gt; 1) {\n                return rows;  // Column vector N×1\n            } else if (rows == 1 && cols == 1) {\n                return 1;     // Scalar 1×1\n            }\n            return 0;  // Not a vector (matrix N×M)\n        }",
    "crumbs": [
      "Functions",
      "validateVectorDataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/validateVectorDataset.html#usage-example",
    "href": "api-reference/cpp/functions/validateVectorDataset.html#usage-example",
    "title": "validateVectorDataset",
    "section": "8 Usage Example",
    "text": "8 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = validateVectorDataset(...);",
    "crumbs": [
      "Functions",
      "validateVectorDataset"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX.html",
    "href": "api-reference/cpp/functions/wdX.html",
    "title": "wdX",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::wdX(const Eigen::MatrixXd &X, const Eigen::VectorXd &w)",
    "crumbs": [
      "Functions",
      "wdX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX.html#signature",
    "href": "api-reference/cpp/functions/wdX.html#signature",
    "title": "wdX",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::wdX(const Eigen::MatrixXd &X, const Eigen::VectorXd &w)",
    "crumbs": [
      "Functions",
      "wdX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX.html#description",
    "href": "api-reference/cpp/functions/wdX.html#description",
    "title": "wdX",
    "section": "2 Description",
    "text": "2 Description\nCompute diagonal-matrix product wX.",
    "crumbs": [
      "Functions",
      "wdX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX.html#parameters",
    "href": "api-reference/cpp/functions/wdX.html#parameters",
    "title": "wdX",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (const Eigen::MatrixXd &): Input matrix\nw (const Eigen::VectorXd &): Vector representing diagonal matrix",
    "crumbs": [
      "Functions",
      "wdX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX.html#returns",
    "href": "api-reference/cpp/functions/wdX.html#returns",
    "title": "wdX",
    "section": "4 Returns",
    "text": "4 Returns\nDiagonal-matrix product wX",
    "crumbs": [
      "Functions",
      "wdX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX.html#details",
    "href": "api-reference/cpp/functions/wdX.html#details",
    "title": "wdX",
    "section": "5 Details",
    "text": "5 Details\nComputes the product of a diagonal matrix (represented as a vector) with a matrix using sequential processing.",
    "crumbs": [
      "Functions",
      "wdX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX.html#source-code",
    "href": "api-reference/cpp/functions/wdX.html#source-code",
    "title": "wdX",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memOptimizedProducts.hpp • Lines 262-271\ninline Eigen::MatrixXd wdX(const Eigen::MatrixXd& X, const Eigen::VectorXd& w)\n{\n    int n = X.cols();\n    Eigen::MatrixXd C = Eigen::MatrixXd::Zero(X.rows(),n) ; \n    \n    for (int i=0; i&lt;n; i++) {\n        C.row(i) = w(i)*X.row(i);\n    }\n    return(C);\n}",
    "crumbs": [
      "Functions",
      "wdX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/wdX.html#usage-example",
    "href": "api-reference/cpp/functions/wdX.html#usage-example",
    "title": "wdX",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = wdX(...);",
    "crumbs": [
      "Functions",
      "wdX"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/when_fork.html",
    "href": "api-reference/cpp/functions/when_fork.html",
    "title": "when_fork",
    "section": "",
    "text": "void when_fork()",
    "crumbs": [
      "Functions",
      "when_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/when_fork.html#signature",
    "href": "api-reference/cpp/functions/when_fork.html#signature",
    "title": "when_fork",
    "section": "",
    "text": "void when_fork()",
    "crumbs": [
      "Functions",
      "when_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/when_fork.html#call-graph",
    "href": "api-reference/cpp/functions/when_fork.html#call-graph",
    "title": "when_fork",
    "section": "2 Call Graph",
    "text": "2 Call Graph",
    "crumbs": [
      "Functions",
      "when_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/when_fork.html#source-code",
    "href": "api-reference/cpp/functions/when_fork.html#source-code",
    "title": "when_fork",
    "section": "3 Source Code",
    "text": "3 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/Utilities/openme-utils.hpp • Lines 250-253\ninline void when_fork() {\n        pre_fork_DTthreads = DTthreads;\n        DTthreads = 1;\n    }",
    "crumbs": [
      "Functions",
      "when_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/when_fork.html#usage-example",
    "href": "api-reference/cpp/functions/when_fork.html#usage-example",
    "title": "when_fork",
    "section": "4 Usage Example",
    "text": "4 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = when_fork(...);",
    "crumbs": [
      "Functions",
      "when_fork"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xtwx.html",
    "href": "api-reference/cpp/functions/xtwx.html",
    "title": "xtwx",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::xtwx(const Eigen::MatrixXd &X, const Eigen::MatrixXd &w)",
    "crumbs": [
      "Functions",
      "xtwx"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xtwx.html#signature",
    "href": "api-reference/cpp/functions/xtwx.html#signature",
    "title": "xtwx",
    "section": "",
    "text": "Eigen::MatrixXd BigDataStatMeth::xtwx(const Eigen::MatrixXd &X, const Eigen::MatrixXd &w)",
    "crumbs": [
      "Functions",
      "xtwx"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xtwx.html#description",
    "href": "api-reference/cpp/functions/xtwx.html#description",
    "title": "xtwx",
    "section": "2 Description",
    "text": "2 Description\nCompute transposed weighted cross-product X’wX.",
    "crumbs": [
      "Functions",
      "xtwx"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xtwx.html#parameters",
    "href": "api-reference/cpp/functions/xtwx.html#parameters",
    "title": "xtwx",
    "section": "3 Parameters",
    "text": "3 Parameters\n\nX (const Eigen::MatrixXd &): Input matrix\nw (const Eigen::MatrixXd &): Weight matrix",
    "crumbs": [
      "Functions",
      "xtwx"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xtwx.html#returns",
    "href": "api-reference/cpp/functions/xtwx.html#returns",
    "title": "xtwx",
    "section": "4 Returns",
    "text": "4 Returns\nTransposed weighted cross-product X’wX",
    "crumbs": [
      "Functions",
      "xtwx"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xtwx.html#details",
    "href": "api-reference/cpp/functions/xtwx.html#details",
    "title": "xtwx",
    "section": "5 Details",
    "text": "5 Details\nComputes the transposed weighted cross-product of a matrix, where w is a diagonal weight matrix.",
    "crumbs": [
      "Functions",
      "xtwx"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xtwx.html#source-code",
    "href": "api-reference/cpp/functions/xtwx.html#source-code",
    "title": "xtwx",
    "section": "6 Source Code",
    "text": "6 Source Code\n\n\n\n\n\n\nNoteImplementation\n\n\n\n\n\nFile: inst/include/memAlgebra/memOptimizedProducts.hpp • Lines 222-228\ninline Eigen::MatrixXd xtwx(const Eigen::MatrixXd& X, const Eigen::MatrixXd& w)\n{\n    const int n(X.cols());\n    Eigen::MatrixXd XtwX(Eigen::MatrixXd(n, n).setZero().\n                             selfadjointView&lt;Eigen::Lower&gt;().rankUpdate(X.adjoint() * w.array().sqrt().matrix().asDiagonal()));\n    return (XtwX);\n}",
    "crumbs": [
      "Functions",
      "xtwx"
    ]
  },
  {
    "objectID": "api-reference/cpp/functions/xtwx.html#usage-example",
    "href": "api-reference/cpp/functions/xtwx.html#usage-example",
    "title": "xtwx",
    "section": "7 Usage Example",
    "text": "7 Usage Example\n#include \"BigDataStatMeth.hpp\"\n\n// Example usage\nauto result = xtwx(...);",
    "crumbs": [
      "Functions",
      "xtwx"
    ]
  },
  {
    "objectID": "api-reference/cpp/index.html",
    "href": "api-reference/cpp/index.html",
    "title": "C++ API Reference",
    "section": "",
    "text": "Complete reference for the BigDataStatMeth C++ header-only library.",
    "crumbs": [
      "C++ API Reference"
    ]
  },
  {
    "objectID": "api-reference/cpp/index.html#overview",
    "href": "api-reference/cpp/index.html#overview",
    "title": "C++ API Reference",
    "section": "",
    "text": "Complete reference for the BigDataStatMeth C++ header-only library.",
    "crumbs": [
      "C++ API Reference"
    ]
  },
  {
    "objectID": "api-reference/cpp/index.html#classes",
    "href": "api-reference/cpp/index.html#classes",
    "title": "C++ API Reference",
    "section": "2 Classes",
    "text": "2 Classes\nBrowse the available classes:\n\nClasses overview",
    "crumbs": [
      "C++ API Reference"
    ]
  },
  {
    "objectID": "api-reference/cpp/index.html#functions",
    "href": "api-reference/cpp/index.html#functions",
    "title": "C++ API Reference",
    "section": "3 Functions",
    "text": "3 Functions\nBrowse standalone functions:\n\nFunctions overview",
    "crumbs": [
      "C++ API Reference"
    ]
  },
  {
    "objectID": "api-reference/cpp/index.html#getting-started",
    "href": "api-reference/cpp/index.html#getting-started",
    "title": "C++ API Reference",
    "section": "4 Getting Started",
    "text": "4 Getting Started\nInclude the headers in your C++ code:\n#include &lt;BigDataStatMeth/your_header.hpp&gt;\nAll classes and functions are documented with their public and private interfaces.",
    "crumbs": [
      "C++ API Reference"
    ]
  },
  {
    "objectID": "api-reference/r-functions.html",
    "href": "api-reference/r-functions.html",
    "title": "R Functions",
    "section": "",
    "text": "NoteAuto-Generated Documentation\n\n\n\nThis section will contain auto-generated API documentation from your existing scripts."
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract_hdf5.html",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract_hdf5.html",
    "title": "bdblockSubstract_hdf5",
    "section": "",
    "text": "BLOCKWISE_OPS",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract_hdf5.html#usage",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract_hdf5.html#usage",
    "title": "bdblockSubstract_hdf5",
    "section": "1 Usage",
    "text": "1 Usage\nbdblockSubstract_hdf5(filename, group, A, B, groupB = NULL, block_size = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract_hdf5.html#arguments",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract_hdf5.html#arguments",
    "title": "bdblockSubstract_hdf5",
    "section": "2 Arguments",
    "text": "2 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString indicating the HDF5 file path\n\n\ngroup\nString indicating the group containing matrix A\n\n\nA\nString specifying the dataset name for matrix A\n\n\nB\nString specifying the dataset name for matrix B\n\n\ngroupB\nOptional string indicating group containing matrix B. If NULL, uses same group as A\n\n\nblock_size\nOptional integer specifying block size for processing. If NULL, automatically determined based on matrix dimensions\n\n\nparal\nOptional boolean indicating whether to use parallel processing. Default is false\n\n\nthreads\nOptional integer specifying number of threads for parallel processing. If NULL, uses maximum available threads\n\n\noutgroup\nOptional string specifying output group. Default is “OUTPUT”\n\n\noutdataset\nOptional string specifying output dataset name. Default is “A_-_B”\n\n\noverwrite\nOptional boolean indicating whether to overwrite existing datasets. Default is false",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract_hdf5.html#value",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract_hdf5.html#value",
    "title": "bdblockSubstract_hdf5",
    "section": "3 Value",
    "text": "3 Value\n\nA list containing the location of the subtraction result:\n\nfn: Character string. Path to the HDF5 file containing the result\nds: Character string. Full dataset path to the subtraction result (A - B) within the HDF5 file",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract_hdf5.html#details",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract_hdf5.html#details",
    "title": "bdblockSubstract_hdf5",
    "section": "4 Details",
    "text": "4 Details\nThe function implements optimized subtraction through:\nOperation modes: - Matrix-matrix subtraction (A - B) - Matrix-vector subtraction - Vector-matrix subtraction\nBlock processing: - Automatic block size selection - Memory-efficient operations - Parallel computation support\nBlock size optimization based on: - Matrix dimensions - Available memory - Operation type (matrix/vector)\nError handling: - Dimension validation - Resource management - Exception handling",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSubstract_hdf5.html#examples",
    "href": "api-reference/r/blockwise_ops/bdblockSubstract_hdf5.html#examples",
    "title": "bdblockSubstract_hdf5",
    "section": "5 Examples",
    "text": "5 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test matrices\nN &lt;- 1500\nM &lt;- 1500\nset.seed(555)\na &lt;- matrix(rnorm(N*M), N, M)\nb &lt;- matrix(rnorm(N*M), N, M)\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", a, \"data\", \"A\",\n                     overwriteFile = TRUE)\nbdCreate_hdf5_matrix(\"test.hdf5\", b, \"data\", \"B\",\n                     overwriteFile = FALSE)\n\n# Perform subtraction\nbdblockSubstract_hdf5(\"test.hdf5\", \"data\", \"A\", \"B\",\n                      outgroup = \"results\",\n                      outdataset = \"diff\",\n                      block_size = 1024,\n                      paral = TRUE)",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSubstract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum_hdf5.html",
    "href": "api-reference/r/blockwise_ops/bdblockSum_hdf5.html",
    "title": "bdblockSum_hdf5",
    "section": "",
    "text": "BLOCKWISE_OPS",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum_hdf5.html#usage",
    "href": "api-reference/r/blockwise_ops/bdblockSum_hdf5.html#usage",
    "title": "bdblockSum_hdf5",
    "section": "1 Usage",
    "text": "1 Usage\nbdblockSum_hdf5(filename, group, A, B, groupB = NULL, block_size = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum_hdf5.html#arguments",
    "href": "api-reference/r/blockwise_ops/bdblockSum_hdf5.html#arguments",
    "title": "bdblockSum_hdf5",
    "section": "2 Arguments",
    "text": "2 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString indicating the HDF5 file path\n\n\ngroup\nString indicating the group containing matrix A\n\n\nA\nString specifying the dataset name for matrix A\n\n\nB\nString specifying the dataset name for matrix B\n\n\ngroupB\nOptional string indicating group containing matrix B. If NULL, uses same group as A\n\n\nblock_size\nOptional integer specifying block size for processing. If NULL, automatically determined based on matrix dimensions\n\n\nparal\nOptional boolean indicating whether to use parallel processing. Default is false\n\n\nthreads\nOptional integer specifying number of threads for parallel processing. If NULL, uses maximum available threads\n\n\noutgroup\nOptional string specifying output group. Default is “OUTPUT”\n\n\noutdataset\nOptional string specifying output dataset name. Default is “A_+_B”\n\n\noverwrite\nOptional boolean indicating whether to overwrite existing datasets. Default is false",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum_hdf5.html#value",
    "href": "api-reference/r/blockwise_ops/bdblockSum_hdf5.html#value",
    "title": "bdblockSum_hdf5",
    "section": "3 Value",
    "text": "3 Value\n\nA list containing the location of the addition result:\n\nfn: Character string. Path to the HDF5 file containing the result\nds: Character string. Full dataset path to the addition result (A + B) within the HDF5 file",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum_hdf5.html#details",
    "href": "api-reference/r/blockwise_ops/bdblockSum_hdf5.html#details",
    "title": "bdblockSum_hdf5",
    "section": "4 Details",
    "text": "4 Details\nThe function implements optimized addition through:\nOperation modes: - Matrix-matrix addition (A + B) - Matrix-vector addition - Vector-matrix addition\nBlock processing: - Automatic block size selection - Memory-efficient operations - Parallel computation support\nBlock size optimization based on: - Matrix dimensions - Available memory - Operation type (matrix/vector)\nError handling: - Dimension validation - Resource management - Exception handling",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockSum_hdf5.html#examples",
    "href": "api-reference/r/blockwise_ops/bdblockSum_hdf5.html#examples",
    "title": "bdblockSum_hdf5",
    "section": "5 Examples",
    "text": "5 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test matrices\nN &lt;- 1500\nM &lt;- 1500\nset.seed(555)\na &lt;- matrix(rnorm(N*M), N, M)\nb &lt;- matrix(rnorm(N*M), N, M)\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", a, \"data\", \"A\",\n                     overwriteFile = TRUE)\nbdCreate_hdf5_matrix(\"test.hdf5\", b, \"data\", \"B\",\n                     overwriteFile = FALSE)\n\n# Perform addition\nbdblockSum_hdf5(\"test.hdf5\", \"data\", \"A\", \"B\",\n                outgroup = \"results\",\n                outdataset = \"sum\",\n                block_size = 1024,\n                paral = TRUE)",
    "crumbs": [
      "Block-wise Operations",
      "bdblockSum_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.html",
    "href": "api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.html",
    "title": "bdblockmult_sparse_hdf5",
    "section": "",
    "text": "BLOCKWISE_OPS",
    "crumbs": [
      "Block-wise Operations",
      "bdblockmult_sparse_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.html#usage",
    "href": "api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.html#usage",
    "title": "bdblockmult_sparse_hdf5",
    "section": "1 Usage",
    "text": "1 Usage\nbdblockmult_sparse_hdf5(filename, group, A, B, groupB = NULL, block_size = NULL, mixblock_size = NULL, paral = NULL, threads = NULL, outgroup = NULL, outdataset = NULL, overwrite = NULL)",
    "crumbs": [
      "Block-wise Operations",
      "bdblockmult_sparse_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.html#arguments",
    "href": "api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.html#arguments",
    "title": "bdblockmult_sparse_hdf5",
    "section": "2 Arguments",
    "text": "2 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString indicating the HDF5 file path\n\n\ngroup\nString indicating the group path for matrix A\n\n\nA\nString specifying the dataset name for matrix A\n\n\nB\nString specifying the dataset name for matrix B\n\n\ngroupB\nOptional string indicating group path for matrix B. If NULL, uses same group as A\n\n\nblock_size\nOptional integer specifying block size for processing. If NULL, automatically determined based on matrix dimensions\n\n\nmixblock_size\nOptional integer for memory block size in parallel processing\n\n\nparal\nOptional boolean indicating whether to use parallel processing. Default is false\n\n\nthreads\nOptional integer specifying number of threads for parallel processing. If NULL, uses maximum available threads\n\n\noutgroup\nOptional string specifying output group. Default is “OUTPUT”\n\n\noutdataset\nOptional string specifying output dataset name. Default is “A_x_B”\n\n\noverwrite\nOptional boolean indicating whether to overwrite existing datasets. Default is false",
    "crumbs": [
      "Block-wise Operations",
      "bdblockmult_sparse_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.html#value",
    "href": "api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.html#value",
    "title": "bdblockmult_sparse_hdf5",
    "section": "3 Value",
    "text": "3 Value\n\nModifies the HDF5 file in place, adding the multiplication result",
    "crumbs": [
      "Block-wise Operations",
      "bdblockmult_sparse_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.html#details",
    "href": "api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.html#details",
    "title": "bdblockmult_sparse_hdf5",
    "section": "4 Details",
    "text": "4 Details\nThe function implements optimized sparse matrix multiplication through: - Block-wise processing to manage memory usage - Automatic block size optimization - Parallel processing support - Efficient sparse matrix storage\nBlock size optimization considers: - Available system memory - Matrix dimensions and sparsity - Parallel processing requirements\nMemory efficiency is achieved through: - Sparse matrix storage format - Block-wise processing - Minimal temporary storage - Proper resource cleanup",
    "crumbs": [
      "Block-wise Operations",
      "bdblockmult_sparse_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.html#examples",
    "href": "api-reference/r/blockwise_ops/bdblockmult_sparse_hdf5.html#examples",
    "title": "bdblockmult_sparse_hdf5",
    "section": "5 Examples",
    "text": "5 Examples\n\n\nCode\nlibrary(Matrix)\nlibrary(BigDataStatMeth)\n\n# Create sparse test matrices\nk &lt;- 1e3\nset.seed(1)\nx_sparse &lt;- sparseMatrix(\n    i = sample(x = k, size = k),\n    j = sample(x = k, size = k),\n    x = rnorm(n = k)\n)\n\nset.seed(2)\ny_sparse &lt;- sparseMatrix(\n    i = sample(x = k, size = k),\n    j = sample(x = k, size = k),\n    x = rnorm(n = k)\n)\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", as.matrix(x_sparse), \"SPARSE\", \"x_sparse\")\nbdCreate_hdf5_matrix(\"test.hdf5\", as.matrix(y_sparse), \"SPARSE\", \"y_sparse\")\n\n# Perform multiplication\nbdblockmult_sparse_hdf5(\"test.hdf5\", \"SPARSE\", \"x_sparse\", \"y_sparse\",\n                        block_size = 1024,\n                        paral = TRUE,\n                        threads = 4)",
    "crumbs": [
      "Block-wise Operations",
      "bdblockmult_sparse_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/index.html",
    "href": "api-reference/r/blockwise_ops/index.html",
    "title": "Block-wise Ops",
    "section": "",
    "text": "Low-level block-wise primitives (multiplication, sums, differences, vector–matrix) for scaling and performance.",
    "crumbs": [
      "Block-wise Operations"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/index.html#overview",
    "href": "api-reference/r/blockwise_ops/index.html#overview",
    "title": "Block-wise Ops",
    "section": "",
    "text": "Low-level block-wise primitives (multiplication, sums, differences, vector–matrix) for scaling and performance.",
    "crumbs": [
      "Block-wise Operations"
    ]
  },
  {
    "objectID": "api-reference/r/blockwise_ops/index.html#functions",
    "href": "api-reference/r/blockwise_ops/index.html#functions",
    "title": "Block-wise Ops",
    "section": "2 Functions",
    "text": "2 Functions\n\n2.1 bdblockSum_hdf5\nbdblockSum_hdf5\n\n\n2.2 bdblockmult_hdf5\nbdblockmult_hdf5\n\n\n2.3 bdblockmult_sparse_hdf5\nbdblockmult_sparse_hdf5\n\n\n2.4 bdblockSubstract_hdf5\nbdblockSubstract_hdf5\n\n\n2.5 bdcomputeMatrixVector_hdf5\nPerforms element-wise operations between a matrix and a vector stored in HDF5 format. The function supports addition, subtraction, multiplication, division and power operations, with options for row-wise or column-wise application and parallel processing.\n\n\n2.6 bdblockSubstract\nPerforms efficient matrix subtraction using block-based algorithms. The function supports various input combinations (matrix-matrix, matrix-vector, vector-vector) and provides options for parallel processing and block-based computation.\n\n\n2.7 bdblockSum\nPerforms efficient matrix addition using block-based algorithms. The function supports various input combinations (matrix-matrix, matrix-vector, vector-vector) and provides options for parallel processing and block-based computation.",
    "crumbs": [
      "Block-wise Operations"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html",
    "title": "bdCreate_diagonal_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdCreate_diagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html#description",
    "title": "bdCreate_diagonal_hdf5",
    "section": "1 Description",
    "text": "1 Description\nCreates a diagonal matrix or vector directly in an HDF5 file using block-wise processing to minimize memory usage. This unified function replaces separate diagonal and identity matrix creation functions, providing flexible diagonal creation with automatic parameter detection.",
    "crumbs": [
      "HDF5 Algebra",
      "bdCreate_diagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html#usage",
    "title": "bdCreate_diagonal_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdCreate_diagonal_hdf5(filename, group, dataset, size = NULL, scalar = 1.0, diagonal_values = NULL, output_type = \"matrix\", block_size = 0L, compression = 6L, overwriteFile = NULL, overwriteDataset = NULL, threads = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdCreate_diagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html#arguments",
    "title": "bdCreate_diagonal_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter. Path to HDF5 file\n\n\ngroup\nCharacter. Group path in HDF5 file (default: “/”)\n\n\ndataset\nCharacter. Name of dataset to create\n\n\nsize\nInteger. Size of diagonal (auto-detected if diagonal_values provided)\n\n\nscalar\nNumeric. Scalar multiplier for diagonal elements (default: 1.0)\n\n\ndiagonal_values\nNumeric vector. Custom diagonal values (optional)\n\n\noutput_type\nCharacter. Output format: “matrix” or “vector” (default: “matrix”)\n\n\nblock_size\nInteger. Block size for processing (default: auto-estimate)\n\n\ncompression\nInteger. Compression level 0-9 (default: 6)\n\n\noverwriteFile\nLogical. Overwrite file if exists (default: FALSE)\n\n\noverwriteDataset\nLogical. Overwrite dataset if exists (default: FALSE)\n\n\nthreads\nInteger. Number of threads to use (default: auto-detect)",
    "crumbs": [
      "HDF5 Algebra",
      "bdCreate_diagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html#value",
    "title": "bdCreate_diagonal_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the diagonal matrix (group/dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdCreate_diagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html#details",
    "title": "bdCreate_diagonal_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides flexible diagonal creation with two main modes: - Vector mode: Provide custom diagonal values - Size is automatically detected from vector length - Scalar acts as additional multiplier - Ideal for custom diagonal patterns - Scalar mode: Provide size and scalar value\n- Creates uniform diagonal with specified scalar - scalar=1.0 creates identity matrix/vector - Ideal for identity or uniform diagonal matrices - Output formats: - “matrix”: Creates full N×N matrix (sparse, only diagonal populated) - “vector”: Creates efficient 1×N vector with diagonal values only - Performance features: - Block-wise processing for memory efficiency - Optional compression with configurable levels - Parallel processing support for large datasets - Automatic block size optimization",
    "crumbs": [
      "HDF5 Algebra",
      "bdCreate_diagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdCreate_diagonal_hdf5.html#examples",
    "title": "bdCreate_diagonal_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create identity matrix (1M x 1M)\nbdCreate_diagonal_hdf5(\"identity.h5\", \"/\", \"I_matrix\", \n                      size = 1000000, scalar = 1.0)\n\n# Create scaled identity vector (more efficient)\nbdCreate_diagonal_hdf5(\"scaled_id.h5\", \"/\", \"scaled_I\", \n                      size = 500000, scalar = 3.14, \n                      output_type = \"vector\")\n\n# Create custom diagonal matrix\ncustom_diag &lt;- runif(10000)\nbdCreate_diagonal_hdf5(\"custom.h5\", \"/\", \"my_diag\",\n                      diagonal_values = custom_diag,\n                      scalar = 2.0, output_type = \"matrix\")\n\n# Create custom diagonal vector (most efficient)\nbdCreate_diagonal_hdf5(\"custom_vec.h5\", \"/\", \"my_diag_vec\",\n                      diagonal_values = custom_diag,\n                      output_type = \"vector\")",
    "crumbs": [
      "HDF5 Algebra",
      "bdCreate_diagonal_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_add_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdDiag_add_hdf5.html",
    "title": "bdDiag_add_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_add_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdDiag_add_hdf5.html#description",
    "title": "bdDiag_add_hdf5",
    "section": "1 Description",
    "text": "1 Description\nPerforms optimized diagonal addition between two datasets stored in HDF5 format. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct operation) and uses the most efficient approach.",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_add_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdDiag_add_hdf5.html#usage",
    "title": "bdDiag_add_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdDiag_add_hdf5(filename, group, A, B, groupB = NULL, target = NULL, outgroup = NULL, outdataset = NULL, paral = NULL, threads = NULL, overwrite = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_add_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdDiag_add_hdf5.html#arguments",
    "title": "bdDiag_add_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString. Path to the HDF5 file containing the datasets.\n\n\ngroup\nString. Group path containing the first dataset (A).\n\n\nA\nString. Name of the first dataset (matrix or vector).\n\n\nB\nString. Name of the second dataset (matrix or vector).\n\n\ngroupB\nOptional string. Group path containing dataset B.\n\n\ntarget\nOptional string. Where to write result: “A”, “B”, or “new” (default: “new”).\n\n\noutgroup\nOptional string. Output group path (only used if target=“new”).\n\n\noutdataset\nOptional string. Output dataset name (only used if target=“new”).\n\n\nparal\nOptional logical. Whether to use parallel processing.\n\n\nthreads\nOptional integer. Number of threads for parallel processing.\n\n\noverwrite\nOptional logical. Whether to overwrite existing datasets.",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_add_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdDiag_add_hdf5.html#value",
    "title": "bdDiag_add_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the diagonal addition result (group/dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_add_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html",
    "title": "bdDiag_multiply_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html#description",
    "title": "bdDiag_multiply_hdf5",
    "section": "1 Description",
    "text": "1 Description\nPerforms optimized diagonal multiplication between two datasets stored in HDF5 format. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct operation) and uses the most efficient approach. This function performs element-wise multiplication and is ~50-250x faster than traditional matrix operations.",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html#usage",
    "title": "bdDiag_multiply_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdDiag_multiply_hdf5(filename, group, A, B, groupB = NULL, target = NULL, outgroup = NULL, outdataset = NULL, paral = NULL, threads = NULL, overwrite = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html#arguments",
    "title": "bdDiag_multiply_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString. Path to the HDF5 file containing the datasets.\n\n\ngroup\nString. Group path containing the first dataset (A).\n\n\nA\nString. Name of the first dataset (matrix or vector).\n\n\nB\nString. Name of the second dataset (matrix or vector).\n\n\ngroupB\nOptional string. Group path containing dataset B. If NULL, uses same group as A.\n\n\ntarget\nOptional string. Where to write result: “A”, “B”, or “new” (default: “new”).\n\n\noutgroup\nOptional string. Output group path. Default is “OUTPUT”.\n\n\noutdataset\nOptional string. Output dataset name. Default is “A_*_B” with .diag suffix if appropriate.\n\n\nparal\nOptional logical. Whether to use parallel processing. Default is FALSE.\n\n\nthreads\nOptional integer. Number of threads for parallel processing. If NULL, uses maximum available threads.\n\n\noverwrite\nOptional logical. Whether to overwrite existing datasets. Default is FALSE.",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html#value",
    "title": "bdDiag_multiply_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the diagonal multiplication result (group/dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html#details",
    "title": "bdDiag_multiply_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides flexible diagonal multiplication with automatic optimization: - Operation modes: - Matrix * Matrix: Extract diagonals → vector multiplication → save as vector - Matrix * Vector: Extract diagonal → vector multiplication → save as vector\n- Vector * Vector: Direct vector multiplication (most efficient) - Performance features: - Uses optimized vector operations for maximum efficiency - Automatic type detection and dimension validation - Memory-efficient processing for large datasets - Parallel processing support for improved performance - Mathematical properties: - Element-wise multiplication (not matrix multiplication) - Commutative operation: A * B = B * A - Handles overflow according to IEEE 754 standards - Preserves sign information correctly",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdDiag_multiply_hdf5.html#examples",
    "title": "bdDiag_multiply_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test matrices\nN &lt;- 1000\nset.seed(123)\nA &lt;- matrix(rnorm(N*N), N, N)\nB &lt;- matrix(rnorm(N*N), N, N)\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", A, \"data\", \"matrixA\",\n                     overwriteFile = TRUE)\nbdCreate_hdf5_matrix(\"test.hdf5\", B, \"data\", \"matrixB\",\n                     overwriteFile = FALSE)\n\n# Multiply diagonals (element-wise)\nresult &lt;- bdDiag_multiply_hdf5(\"test.hdf5\", \"data\", \"matrixA\", \"matrixB\",\n                              outgroup = \"results\",\n                              outdataset = \"diagonal_product\",\n                              paral = TRUE)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_multiply_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html",
    "title": "bdDiag_subtract_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html#description",
    "title": "bdDiag_subtract_hdf5",
    "section": "1 Description",
    "text": "1 Description\nPerforms optimized diagonal subtraction between two datasets stored in HDF5 format. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct operation) and uses the most efficient approach. This function is ~50-250x faster than traditional matrix operations for diagonal computations.",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html#usage",
    "title": "bdDiag_subtract_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdDiag_subtract_hdf5(filename, group, A, B, groupB = NULL, target = NULL, outgroup = NULL, outdataset = NULL, paral = NULL, threads = NULL, overwrite = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html#arguments",
    "title": "bdDiag_subtract_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString. Path to the HDF5 file containing the datasets.\n\n\ngroup\nString. Group path containing the first dataset (A, minuend).\n\n\nA\nString. Name of the first dataset (minuend).\n\n\nB\nString. Name of the second dataset (subtrahend).\n\n\ngroupB\nOptional string. Group path containing dataset B. If NULL, uses same group as A.\n\n\ntarget\nOptional string. Where to write result: “A”, “B”, or “new” (default: “new”).\n\n\noutgroup\nOptional string. Output group path. Default is “OUTPUT”.\n\n\noutdataset\nOptional string. Output dataset name. Default is “A_-_B” with .diag suffix if appropriate.\n\n\nparal\nOptional logical. Whether to use parallel processing. Default is FALSE.\n\n\nthreads\nOptional integer. Number of threads for parallel processing. If NULL, uses maximum available threads.\n\n\noverwrite\nOptional logical. Whether to overwrite existing datasets. Default is FALSE.",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html#value",
    "title": "bdDiag_subtract_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the diagonal subtraction result (group/dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html#details",
    "title": "bdDiag_subtract_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides flexible diagonal subtraction with automatic optimization: - Operation modes: - Matrix - Matrix: Extract diagonals → vector subtraction → save as vector - Matrix - Vector: Extract diagonal → vector subtraction → save as vector\n- Vector - Vector: Direct vector subtraction (most efficient) - Performance features: - Uses optimized vector operations for maximum efficiency - Automatic type detection and dimension validation - Memory-efficient processing for large datasets - Parallel processing support for improved performance - Validation checks: - Matrix inputs must be square (N×N) - Vector inputs must have compatible dimensions - Automatic dimension matching between operands",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdDiag_subtract_hdf5.html#examples",
    "title": "bdDiag_subtract_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test matrices\nN &lt;- 1000\nset.seed(123)\nA &lt;- matrix(rnorm(N*N), N, N)\nB &lt;- matrix(rnorm(N*N), N, N)\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test.hdf5\", A, \"data\", \"matrixA\",\n                     overwriteFile = TRUE)\nbdCreate_hdf5_matrix(\"test.hdf5\", B, \"data\", \"matrixB\",\n                     overwriteFile = FALSE)\n\n# Subtract diagonals\nresult &lt;- bdDiag_subtract_hdf5(\"test.hdf5\", \"data\", \"matrixA\", \"matrixB\",\n                              outgroup = \"results\",\n                              outdataset = \"diagonal_diff\",\n                              paral = TRUE)",
    "crumbs": [
      "HDF5 Algebra",
      "bdDiag_subtract_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html",
    "title": "bdInvCholesky_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdInvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#description",
    "title": "bdInvCholesky_hdf5",
    "section": "1 Description",
    "text": "1 Description\nComputes the inverse of a symmetric positive-definite matrix stored in an HDF5 file using the Cholesky decomposition method. This approach is more efficient and numerically stable than general matrix inversion methods for symmetric positive-definite matrices.",
    "crumbs": [
      "HDF5 Algebra",
      "bdInvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#usage",
    "title": "bdInvCholesky_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdInvCholesky_hdf5(filename, group, dataset, outdataset, outgroup = NULL, fullMatrix = NULL, overwrite = NULL, threads = 2L, elementsBlock = 1000000L)",
    "crumbs": [
      "HDF5 Algebra",
      "bdInvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#arguments",
    "title": "bdInvCholesky_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file containing the input matrix.\n\n\ngroup\nCharacter string. Path to the group containing the input dataset.\n\n\ndataset\nCharacter string. Name of the input dataset to invert.\n\n\noutdataset\nCharacter string. Name for the output dataset.\n\n\noutgroup\nCharacter string. Optional output group path. If not provided, results are stored in the input group.\n\n\nfullMatrix\nLogical. If TRUE, stores the complete inverse matrix. If FALSE (default), stores only the lower triangular part to save space.\n\n\noverwrite\nLogical. If TRUE, allows overwriting existing results.\n\n\nthreads\nInteger. Number of threads for parallel computation (default = 2).\n\n\nelementsBlock\nInteger. Maximum number of elements to process in each block (default = 1,000,000). For matrices larger than 5000x5000, automatically adjusted to number of rows or columns * 2.",
    "crumbs": [
      "HDF5 Algebra",
      "bdInvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#value",
    "title": "bdInvCholesky_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the inverse Cholesky decomposition A^(-1) result (group/dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdInvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#details",
    "title": "bdInvCholesky_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function implements an efficient matrix inversion algorithm that leverages the special properties of symmetric positive-definite matrices. Key features: - Uses Cholesky decomposition for improved numerical stability - Block-based computation for large matrices - Optional storage formats (full or triangular) - Parallel processing support - Memory-efficient block algorithm\nThe algorithm proceeds in two main steps: 1. Compute the Cholesky decomposition A = LL’ 2. Solve the system LL’X = I for X = A^(-1)\nAdvantages of this method: - More efficient than general matrix inversion - Better numerical stability - Preserves matrix symmetry - Exploits positive-definiteness for efficiency",
    "crumbs": [
      "HDF5 Algebra",
      "bdInvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#examples",
    "title": "bdInvCholesky_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(rhdf5)\n\n# Create a symmetric positive-definite matrix\nset.seed(1234)\nX &lt;- matrix(rnorm(100), 10, 10)\nA &lt;- crossprod(X)  # A = X'X is symmetric positive-definite\n\n# Save to HDF5\nh5createFile(\"matrix.h5\")\nh5write(A, \"matrix.h5\", \"data/matrix\")\n\n# Compute inverse using Cholesky decomposition\nbdInvCholesky_hdf5(\"matrix.h5\", \"data\", \"matrix\",\n                   outdataset = \"inverse\",\n                   outgroup = \"results\",\n                   fullMatrix = TRUE,\n                   threads = 4)\n\n# Verify the inverse\nAinv &lt;- h5read(\"matrix.h5\", \"results/inverse\")\nmax(abs(A %*% Ainv - diag(nrow(A))))  # Should be very small",
    "crumbs": [
      "HDF5 Algebra",
      "bdInvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_algebra/bdInvCholesky_hdf5.html#see-also",
    "title": "bdInvCholesky_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdCholesky_hdf5 for the underlying Cholesky decomposition\nbdSolve_hdf5 for solving linear systems",
    "crumbs": [
      "HDF5 Algebra",
      "bdInvCholesky_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdQR_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdQR_hdf5.html",
    "title": "bdQR_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdQR_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#description",
    "title": "bdQR_hdf5",
    "section": "1 Description",
    "text": "1 Description\nComputes the QR decomposition of a matrix stored in an HDF5 file, factoring it into a product A = QR where Q is an orthogonal matrix and R is an upper triangular matrix. Results are stored back in the HDF5 file.",
    "crumbs": [
      "HDF5 Algebra",
      "bdQR_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#usage",
    "title": "bdQR_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdQR_hdf5(filename, group, dataset, outgroup = NULL, outdataset = NULL, thin = NULL, block_size = NULL, overwrite = NULL, threads = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdQR_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#arguments",
    "title": "bdQR_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file containing the input matrix.\n\n\ngroup\nCharacter string. Path to the group containing the input dataset.\n\n\ndataset\nCharacter string. Name of the input dataset to decompose.\n\n\noutgroup\nCharacter string. Optional output group path where results will be stored. If not provided, results are stored in &lt;input_group&gt;/QRDec.\n\n\noutdataset\nCharacter string. Optional base name for output datasets. Results will be stored as Q.'outdataset' and R.'outdataset'.\n\n\nthin\nLogical. If TRUE, computes the reduced (thin) QR decomposition. If FALSE (default), computes the full decomposition.\n\n\nblock_size\nInteger. Optional block size for blocked computation.\n\n\noverwrite\nLogical. If TRUE, allows overwriting existing datasets. Default is FALSE.\n\n\nthreads\nInteger. Optional number of threads for parallel computation. If NULL, uses all available threads.",
    "crumbs": [
      "HDF5 Algebra",
      "bdQR_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#value",
    "title": "bdQR_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string with the HDF5 filename\nds_Q: Character string with the full dataset path to the Q matrix (orthogonal matrix). Results are written to the HDF5 file as “Q.’outdataset’” within the specified group\nds_R: Character string with the full dataset path to the R matrix (upper triangular matrix). Results are written to the HDF5 file as “R.’outdataset’” within the specified group",
    "crumbs": [
      "HDF5 Algebra",
      "bdQR_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#details",
    "title": "bdQR_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function performs QR decomposition on large matrices stored in HDF5 format, which is particularly useful for matrices too large to fit in memory. Features include: - Support for both thin and full QR decomposition - Blocked computation for improved performance - Parallel processing capabilities - Flexible output location specification - Optional overwriting of existing datasets",
    "crumbs": [
      "HDF5 Algebra",
      "bdQR_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#examples",
    "title": "bdQR_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\n# Create a sample HDF5 file with a matrix\nlibrary(rhdf5)\nA &lt;- matrix(rnorm(1000), 100, 10)\nh5createFile(\"example.h5\")\nh5write(A, \"example.h5\", \"mygroup/mymatrix\")\n\n# Compute QR decomposition\nbdQR_hdf5(\"example.h5\", \"mygroup\", \"mymatrix\",\n          outgroup = \"mygroup/results\",\n          outdataset = \"qr_result\",\n          thin = TRUE)",
    "crumbs": [
      "HDF5 Algebra",
      "bdQR_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_algebra/bdQR_hdf5.html#see-also",
    "title": "bdQR_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\nbdQR for QR decomposition of in-memory matrices",
    "crumbs": [
      "HDF5 Algebra",
      "bdQR_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html",
    "title": "bdSolve_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdSolve_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#description",
    "title": "bdSolve_hdf5",
    "section": "1 Description",
    "text": "1 Description\nSolves the linear system AX = B where matrices A and B are stored in HDF5 format. The solution X is written back to the HDF5 file.",
    "crumbs": [
      "HDF5 Algebra",
      "bdSolve_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#usage",
    "title": "bdSolve_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdSolve_hdf5(filename, groupA, datasetA, groupB, datasetB, outgroup = NULL, outdataset = NULL, overwrite = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdSolve_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#arguments",
    "title": "bdSolve_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString. Path to the HDF5 file.\n\n\ngroupA\nString. Group containing matrix A.\n\n\ndatasetA\nString. Dataset name for matrix A.\n\n\ngroupB\nString. Group containing matrix B.\n\n\ndatasetB\nString. Dataset name for matrix B.\n\n\noutgroup\nOptional string. Output group name (defaults to “Solved”).\n\n\noutdataset\nOptional string. Output dataset name (defaults to “A_B”).\n\n\noverwrite\nLogical. Whether to overwrite existing results.",
    "crumbs": [
      "HDF5 Algebra",
      "bdSolve_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#value",
    "title": "bdSolve_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the solution of the linear system (group/dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdSolve_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#details",
    "title": "bdSolve_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides an HDF5-based implementation for solving large linear systems. Key features: - HDF5 Integration: - Efficient reading of input matrices - Memory-efficient processing - Direct output to HDF5 format - Implementation Features: - Automatic solver selection - Support for large matrices - Flexible output options - Memory-efficient processing\nThe function handles: - Data validation - Memory management - Error handling - HDF5 file operations",
    "crumbs": [
      "HDF5 Algebra",
      "bdSolve_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#examples",
    "title": "bdSolve_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\nlibrary(BigDataStatMeth)\n\n# Create test matrices\nN &lt;- 1000\nM &lt;- 1000\nfn &lt;- \"test_temp.hdf5\"\n\nset.seed(555)\nY &lt;- matrix(rnorm(N*M), N, M)\nX &lt;- matrix(rnorm(N), N, 1)\nYcp &lt;- crossprod(Y)\n\n# Compare with in-memory solution\nresm &lt;- bdSolve(Ycp, X)\nresr &lt;- solve(Ycp, X)\nall.equal(resm, resr)\n\n# Save matrices to HDF5\nbdCreate_hdf5_matrix(filename = fn,\n                     object = Ycp,\n                     group = \"data\",\n                     dataset = \"A\",\n                     transp = FALSE,\n                     overwriteFile = TRUE,\n                     overwriteDataset = TRUE,\n                     unlimited = FALSE)\n\nbdCreate_hdf5_matrix(filename = fn,\n                     object = X,\n                     group = \"data\",\n                     dataset = \"B\",\n                     transp = FALSE,\n                     overwriteFile = FALSE,\n                     overwriteDataset = TRUE,\n                     unlimited = FALSE)\n\n# Solve using HDF5-stored matrices\nbdSolve_hdf5(filename = fn,\n             groupA = \"data\",\n             datasetA = \"A\",\n             groupB = \"data\",\n             datasetB = \"B\",\n             outgroup = \"Solved\",\n             outdataset = \"A_B\",\n             overwrite = TRUE)\n\n# Cleanup\nif (file.exists(fn)) {\n    file.remove(fn)\n}",
    "crumbs": [
      "HDF5 Algebra",
      "bdSolve_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_algebra/bdSolve_hdf5.html#see-also",
    "title": "bdSolve_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdSolve for in-memory matrix solving\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 Algebra",
      "bdSolve_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html",
    "title": "bdWriteOppsiteTriangularMatrix_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteOppsiteTriangularMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#description",
    "title": "bdWriteOppsiteTriangularMatrix_hdf5",
    "section": "1 Description",
    "text": "1 Description\nCreates a symmetric matrix by mirroring values from one triangular part to the other in an HDF5-stored matrix. This function modifies the matrix in-place, either copying the upper triangular values to the lower triangular part or vice versa.",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteOppsiteTriangularMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#usage",
    "title": "bdWriteOppsiteTriangularMatrix_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdWriteOppsiteTriangularMatrix_hdf5(filename, group, dataset, copytolower = NULL, elementsBlock = 1000000L)",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteOppsiteTriangularMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#arguments",
    "title": "bdWriteOppsiteTriangularMatrix_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string specifying the path to an existing HDF5 file\n\n\ngroup\nCharacter string indicating the input group containing the dataset\n\n\ndataset\nCharacter string specifying the dataset to be modified\n\n\ncopytolower\nLogical. If TRUE, copies upper triangular to lower triangular. If FALSE (default), copies lower triangular to upper triangular.\n\n\nelementsBlock\nInteger defining the maximum number of elements to process in each block. Default is 1,000,000. For matrices larger than 5000x5000, automatically adjusted to number of rows or columns * 2.",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteOppsiteTriangularMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#value",
    "title": "bdWriteOppsiteTriangularMatrix_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the modified matrix. The opposite triangular part is written to the same input dataset, completing the symmetric matrix (group/dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteOppsiteTriangularMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#details",
    "title": "bdWriteOppsiteTriangularMatrix_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides an efficient way to create symmetric matrices from triangular data. It operates directly on HDF5 datasets using block processing for memory efficiency. The function: - Validates that the input matrix is square - Processes the matrix in blocks for memory efficiency - Performs in-place modification of the dataset - Preserves the original values in the source triangular part - Supports both upper-to-lower and lower-to-upper mirroring\nThe implementation uses block processing to handle large matrices efficiently, making it suitable for big data applications. The block size can be adjusted based on available memory and performance requirements.",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteOppsiteTriangularMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#examples",
    "title": "bdWriteOppsiteTriangularMatrix_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\nlibrary(BigDataStatMeth)\n\n# Create a matrix with upper triangular values\nX &lt;- matrix(rnorm(100), 10, 10)\nX.1 &lt;- X\nX[lower.tri(X)] &lt;- 0\n\n# Save to HDF5\nbdCreate_hdf5_matrix(\"test_file.hdf5\", X, \"data\", \"X\", \n                     overwriteFile = TRUE, \n                     overwriteDataset = FALSE, \n                     unlimited = FALSE)\n                     \n# Mirror upper triangular to lower\nbdWriteOppsiteTriangularMatrix_hdf5(\n  filename = \"test_file.hdf5\", \n  group = \"data\",\n  dataset = \"X\",\n  copytolower = TRUE,\n  elementsBlock = 10\n)\n\n# Create a matrix with lower triangular values\nX &lt;- X.1\nX[upper.tri(X)] &lt;- 0\n\n# Add to HDF5 file\nbdCreate_hdf5_matrix(\"test_file.hdf5\", X, \"data\", \"Y\", \n                     overwriteFile = FALSE, \n                     overwriteDataset = FALSE, \n                     unlimited = FALSE)\n                     \n# Mirror lower triangular to upper\nbdWriteOppsiteTriangularMatrix_hdf5(\n  filename = \"test_file.hdf5\", \n  group = \"data\",\n  dataset = \"Y\",\n  copytolower = FALSE,\n  elementsBlock = 10\n)\n\n# Cleanup\nif (file.exists(\"test_file.hdf5\")) {\n  file.remove(\"test_file.hdf5\")\n}",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteOppsiteTriangularMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_algebra/bdWriteOppsiteTriangularMatrix_hdf5.html#see-also",
    "title": "bdWriteOppsiteTriangularMatrix_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 Algebra",
      "bdWriteOppsiteTriangularMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html",
    "href": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html",
    "title": "bdpseudoinv_hdf5",
    "section": "",
    "text": "HDF5_ALGEBRA",
    "crumbs": [
      "HDF5 Algebra",
      "bdpseudoinv_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#description",
    "href": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#description",
    "title": "bdpseudoinv_hdf5",
    "section": "1 Description",
    "text": "1 Description\nComputes the Moore-Penrose pseudoinverse of a matrix stored in HDF5 format. The implementation is designed for large matrices, using block-based processing and efficient I/O operations.",
    "crumbs": [
      "HDF5 Algebra",
      "bdpseudoinv_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#usage",
    "href": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#usage",
    "title": "bdpseudoinv_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdpseudoinv_hdf5(filename, group, dataset, outgroup = NULL, outdataset = NULL, overwrite = NULL, threads = NULL)",
    "crumbs": [
      "HDF5 Algebra",
      "bdpseudoinv_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#arguments",
    "title": "bdpseudoinv_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nString. Path to the HDF5 file.\n\n\ngroup\nString. Group containing the input matrix.\n\n\ndataset\nString. Dataset name for the input matrix.\n\n\noutgroup\nOptional string. Output group name (defaults to “PseudoInverse”).\n\n\noutdataset\nOptional string. Output dataset name (defaults to input dataset name).\n\n\noverwrite\nLogical. Whether to overwrite existing results.\n\n\nthreads\nOptional integer. Number of threads for parallel computation.",
    "crumbs": [
      "HDF5 Algebra",
      "bdpseudoinv_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#value",
    "href": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#value",
    "title": "bdpseudoinv_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the pseudoinverse matrix (group/dataset)",
    "crumbs": [
      "HDF5 Algebra",
      "bdpseudoinv_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#details",
    "href": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#details",
    "title": "bdpseudoinv_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides an HDF5-based implementation for computing pseudoinverses of large matrices. Key features: - HDF5 Integration: - Efficient reading of input matrix - Block-based processing for large matrices - Memory-efficient computation - Direct output to HDF5 format - Implementation Features: - SVD-based computation - Parallel processing support - Automatic memory management - Flexible output options\nThe function handles: - Data validation - Memory management - Error handling - HDF5 file operations",
    "crumbs": [
      "HDF5 Algebra",
      "bdpseudoinv_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#examples",
    "href": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#examples",
    "title": "bdpseudoinv_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\nlibrary(BigDataStatMeth)\n\n# Create a singular matrix\nX &lt;- matrix(c(1,2,3,2,4,6), 2, 3)\nfn &lt;- \"test.hdf5\"\n\n# Save to HDF5\nbdCreate_hdf5_matrix(filename = fn,\n                     object = X,\n                     group = \"data\",\n                     dataset = \"X\",\n                     overwriteFile = TRUE)\n\n# Compute pseudoinverse\nbdpseudoinv_hdf5(filename = fn,\n                 group = \"data\",\n                 dataset = \"X\",\n                 outgroup = \"results\",\n                 outdataset = \"X_pinv\",\n                 overwrite = TRUE)\n\n# Cleanup\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "HDF5 Algebra",
      "bdpseudoinv_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_algebra/bdpseudoinv_hdf5.html#see-also",
    "title": "bdpseudoinv_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdpseudoinv for in-memory computation\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 Algebra",
      "bdpseudoinv_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/index.html",
    "href": "api-reference/r/hdf5_algebra/index.html",
    "title": "HDF5 Algebra",
    "section": "",
    "text": "Algebra and matrix factorizations over HDF5 datasets (PCA, SVD, QR, Cholesky, eigen, solve, pseudoinverse).",
    "crumbs": [
      "HDF5 Algebra"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/index.html#overview",
    "href": "api-reference/r/hdf5_algebra/index.html#overview",
    "title": "HDF5 Algebra",
    "section": "",
    "text": "Algebra and matrix factorizations over HDF5 datasets (PCA, SVD, QR, Cholesky, eigen, solve, pseudoinverse).",
    "crumbs": [
      "HDF5 Algebra"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_algebra/index.html#functions",
    "href": "api-reference/r/hdf5_algebra/index.html#functions",
    "title": "HDF5 Algebra",
    "section": "2 Functions",
    "text": "2 Functions\n\n2.1 bdCholesky_hdf5\nComputes the Cholesky decomposition of a symmetric positive-definite matrix stored in an HDF5 file. The Cholesky decomposition factors a matrix A into the product A = LL’ where L is a lower triangular matrix.\n\n\n2.2 bdQR_hdf5\nComputes the QR decomposition of a matrix stored in an HDF5 file, factoring it into a product A = QR where Q is an orthogonal matrix and R is an upper triangular matrix. Results are stored back in the HDF5 file.\n\n\n2.3 bdSVD_hdf5\nComputes the Singular Value Decomposition (SVD) of a large matrix stored in an HDF5 file. The SVD decomposes a matrix A into a product A = UDV’ where U and V are orthogonal matrices and D is a diagonal matrix containing the singular values.\n\n\n2.4 bdSolve_hdf5\nSolves the linear system AX = B where matrices A and B are stored in HDF5 format. The solution X is written back to the HDF5 file.\n\n\n2.5 bdPCA_hdf5\nPerforms Principal Component Analysis (PCA) on a large matrix stored in an HDF5 file. PCA reduces the dimensionality of the data while preserving as much variance as possible. The implementation uses SVD internally for efficient and numerically stable computation.\n\n\n2.6 bdCrossprod_hdf5\nbdCrossprod_hdf5\n\n\n2.7 bdtCrossprod_hdf5\nbdtCrossprod_hdf5\n\n\n2.8 bdCreate_diagonal_hdf5\nCreates a diagonal matrix or vector directly in an HDF5 file using block-wise processing to minimize memory usage. This unified function replaces separate diagonal and identity matrix creation functions, providing flexible diagonal creation with automatic parameter detection.\n\n\n2.9 bdDiag_add_hdf5\nPerforms optimized diagonal addition between two datasets stored in HDF5 format. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct operation) and uses the most efficient approach.\n\n\n2.10 bdDiag_subtract_hdf5\nPerforms optimized diagonal subtraction between two datasets stored in HDF5 format. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct operation) and uses the most efficient approach. This function is ~50-250x faster than traditional matrix operations for diagonal computations.\n\n\n2.11 bdDiag_multiply_hdf5\nPerforms optimized diagonal multiplication between two datasets stored in HDF5 format. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct operation) and uses the most efficient approach. This function performs element-wise multiplication and is ~50-250x faster than traditional matrix operations.\n\n\n2.12 bdDiag_divide_hdf5\nPerforms optimized diagonal division between two datasets stored in HDF5 format. Automatically detects whether inputs are matrices (extracts diagonals) or vectors (direct operation) and uses the most efficient approach. This function is ~50-250x faster than traditional matrix operations for diagonal computations.\n\n\n2.13 bdDiag_scalar_hdf5\nPerforms optimized scalar operations on diagonal elements of matrices or vectors stored in HDF5 format. Automatically detects whether input is a matrix (extracts diagonal) or vector (direct operation) and applies the specified scalar operation.\n\n\n2.14 bdEigen_hdf5\nComputes the eigenvalue decomposition of a large matrix stored in an HDF5 file using the Spectra library. This provides consistent results with the RSpectra package and can handle both symmetric and non-symmetric matrices.\n\n\n2.15 bdInvCholesky_hdf5\nComputes the inverse of a symmetric positive-definite matrix stored in an HDF5 file using the Cholesky decomposition method. This approach is more efficient and numerically stable than general matrix inversion methods for symmetric positive-definite matrices.\n\n\n2.16 bdgetDiagonal_hdf5\nRetrieves the diagonal elements from a matrix stored in an HDF5 file.\n\n\n2.17 bdWriteDiagonal_hdf5\nUpdates the diagonal elements of a matrix stored in an HDF5 file.\n\n\n2.18 bdpseudoinv_hdf5\nComputes the Moore-Penrose pseudoinverse of a matrix stored in HDF5 format. The implementation is designed for large matrices, using block-based processing and efficient I/O operations.\n\n\n2.19 bdWriteOppsiteTriangularMatrix_hdf5\nCreates a symmetric matrix by mirroring values from one triangular part to the other in an HDF5-stored matrix. This function modifies the matrix in-place, either copying the upper triangular values to the lower triangular part or vice versa.",
    "crumbs": [
      "HDF5 Algebra"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.html",
    "href": "api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.html",
    "title": "bdCheckMatrix_hdf5",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCheckMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.html#description",
    "href": "api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.html#description",
    "title": "bdCheckMatrix_hdf5",
    "section": "1 Description",
    "text": "1 Description\nChecks whether a matrix stored in HDF5 format is suitable for eigenvalue decomposition using Spectra. The function verifies that the matrix is square and optionally checks for symmetry to recommend the best solver type.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCheckMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.html#usage",
    "title": "bdCheckMatrix_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdCheckMatrix_hdf5(filename, group = NULL, dataset = NULL, check_symmetry = NULL, tolerance = NULL, sample_size = NULL)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCheckMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.html#arguments",
    "title": "bdCheckMatrix_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file containing the matrix.\n\n\ngroup\nCharacter string. Path to the group containing the dataset.\n\n\ndataset\nCharacter string. Name of the dataset to check.\n\n\ncheck_symmetry\nLogical. Whether to check if the matrix is symmetric (default = TRUE).\n\n\ntolerance\nNumeric. Tolerance for symmetry checking (default = 1e-12).\n\n\nsample_size\nInteger. Number of elements to sample for large matrices (default = 1000).",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCheckMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.html#value",
    "href": "api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.html#value",
    "title": "bdCheckMatrix_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nA list with matrix properties and suitability assessment.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCheckMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdCheckMatrix_hdf5.html#examples",
    "title": "bdCheckMatrix_hdf5",
    "section": "5 Examples",
    "text": "5 Examples\n\n\nCode\n# Check matrix suitability\ncheck_result &lt;- bdEigen_check_matrix(\"data.h5\", \"matrices\", \"my_matrix\")\n\nif (check_result$suitable_for_eigen) {\n  # Use appropriate solver based on recommendation\n  if (check_result$recommended_solver == \"symmetric\") {\n    result &lt;- bdEigen_hdf5(\"data.h5\", \"matrices\", \"my_matrix\", which = \"LA\")\n  } else {\n    result &lt;- bdEigen_hdf5(\"data.h5\", \"matrices\", \"my_matrix\", which = \"LM\")\n  }\n} else {\n  cat(\"Matrix is not suitable for eigendecomposition\\n\")\n}",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCheckMatrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html",
    "title": "bdCreate_hdf5_group",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_group"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#description",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#description",
    "title": "bdCreate_hdf5_group",
    "section": "1 Description",
    "text": "1 Description\nCreate a (nested) group inside an HDF5 file. The operation is idempotent: if the group already exists, no error is raised.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_group"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#usage",
    "title": "bdCreate_hdf5_group",
    "section": "2 Usage",
    "text": "2 Usage\nbdCreate_hdf5_group(filename, group)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_group"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#arguments",
    "title": "bdCreate_hdf5_group",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ngroup\nCharacter string. Group path to create (e.g., \"MGCCA_OUT/scores\").",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_group"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#value",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#value",
    "title": "bdCreate_hdf5_group",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\ngr: Character string with the full group path created within the HDF5 file",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_group"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#details",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#details",
    "title": "bdCreate_hdf5_group",
    "section": "5 Details",
    "text": "5 Details\nIntermediate groups are created when needed. The HDF5 file must exist prior to the call (create it with a writer function).",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_group"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#examples",
    "title": "bdCreate_hdf5_group",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\nfn &lt;- \"test.hdf5\"\n\n# Ensure file exists (e.g., by creating an empty dataset or via a helper)\nmat &lt;- matrix(0, nrow = 1, ncol = 1)\nbdCreate_hdf5_matrix(fn, mat, group = \"tmp\", dataset = \"seed\",\n                     overwriteFile = TRUE)\n\n# Create nested group\nbdCreate_hdf5_group(fn, \"MGCCA_OUT/scores\")",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_group"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#see-also",
    "href": "api-reference/r/hdf5_io_management/bdCreate_hdf5_group.html#see-also",
    "title": "bdCreate_hdf5_group",
    "section": "7 See Also",
    "text": "7 See Also\n\nbdCreate_hdf5_matrix, bdRemove_hdf5_element",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdCreate_hdf5_group"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportData_hdf5.html",
    "href": "api-reference/r/hdf5_io_management/bdImportData_hdf5.html",
    "title": "bdImportData_hdf5",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportData_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportData_hdf5.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdImportData_hdf5.html#usage",
    "title": "bdImportData_hdf5",
    "section": "1 Usage",
    "text": "1 Usage\nbdImportData_hdf5(...)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportData_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportData_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdImportData_hdf5.html#arguments",
    "title": "bdImportData_hdf5",
    "section": "2 Arguments",
    "text": "2 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\ninFile\nCharacter string specifying either a local file path or URL containing the data to import\n\n\ndestFile\nCharacter string specifying the file name and path where the HDF5 file will be stored\n\n\ndestGroup\nCharacter string specifying the group name within the HDF5 file where the dataset will be stored\n\n\ndestDataset\nCharacter string specifying the name for the dataset within the HDF5 file\n\n\nheader\nLogical or character vector. If TRUE, the first row contains column names. If a character vector, use these as column names. Default is TRUE.\n\n\nrownames\nLogical or character vector. If TRUE, first column contains row names. If a character vector, use these as row names. Default is FALSE.\n\n\noverwrite\nLogical indicating if existing datasets should be overwritten. Default is FALSE.\n\n\noverwriteFile\nLogical indicating if the entire HDF5 file should be overwritten if it exists. CAUTION: This will delete all existing data. Default is FALSE.\n\n\nsep\nCharacter string specifying the field separator in the input file. Default is “\\t” (tab).\n\n\nparal\nLogical indicating whether to use parallel computation. Default is TRUE.\n\n\nthreads\nInteger specifying the number of threads to use for parallel computation. Only used if paral=TRUE. If NULL, uses maximum available threads.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportData_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportData_hdf5.html#value",
    "href": "api-reference/r/hdf5_io_management/bdImportData_hdf5.html#value",
    "title": "bdImportData_hdf5",
    "section": "3 Value",
    "text": "3 Value\n\nNo return value. The function writes the data directly to the specified HDF5 file.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportData_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdImportData_hdf5.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdImportData_hdf5.html#examples",
    "title": "bdImportData_hdf5",
    "section": "4 Examples",
    "text": "4 Examples\n\n\nCode\n# Import from local file\nbdImportData_hdf5(\n  inFile = \"data.txt\",\n  destFile = \"output.h5\",\n  destGroup = \"mydata\",\n  destDataset = \"matrix1\",\n  header = TRUE,\n  sep = \"\\t\"\n)\n\n# Import from URL\nbdImportData_hdf5(\n  inFile = \"https://example.com/data.csv\",\n  destFile = \"output.h5\",\n  destGroup = \"downloaded\",\n  destDataset = \"remote_data\",\n  sep = \",\"\n)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdImportData_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html",
    "href": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html",
    "title": "bdIsLocked_hdf5",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdIsLocked_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html#description",
    "href": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html#description",
    "title": "bdIsLocked_hdf5",
    "section": "1 Description",
    "text": "1 Description\nUses HDF5 file locking to check if filename can be opened in read/write mode. If opening fails under locking, the file is treated as “in use” and TRUE is returned. Non-existent files return FALSE.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdIsLocked_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html#usage",
    "title": "bdIsLocked_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdIsLocked_hdf5(filename)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdIsLocked_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html#arguments",
    "title": "bdIsLocked_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter. Path to the HDF5 file.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdIsLocked_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html#value",
    "href": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html#value",
    "title": "bdIsLocked_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nLogical scalar: if locked/in use, otherwise.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdIsLocked_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html#details",
    "href": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html#details",
    "title": "bdIsLocked_hdf5",
    "section": "5 Details",
    "text": "5 Details\nRequires HDF5 file locking (HDF5 &gt;= 1.12 recommended). The function sets HDF5_USE_FILE_LOCKING=TRUE for the process.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdIsLocked_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdIsLocked_hdf5.html#examples",
    "title": "bdIsLocked_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nif (bdIsFileLocked(\"data.h5\")) stop(\"File in use\")",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdIsLocked_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html",
    "href": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html",
    "title": "bdRemove_hdf5_element",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdRemove_hdf5_element"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#description",
    "href": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#description",
    "title": "bdRemove_hdf5_element",
    "section": "1 Description",
    "text": "1 Description\nRemoves specified groups or datasets from an HDF5 file.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdRemove_hdf5_element"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#usage",
    "title": "bdRemove_hdf5_element",
    "section": "2 Usage",
    "text": "2 Usage\nbdRemove_hdf5_element(filename, elements)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdRemove_hdf5_element"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#arguments",
    "title": "bdRemove_hdf5_element",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\nelements\nCharacter vector. Full paths to elements to remove (e.g., “group/dataset” or “group/subgroup”).",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdRemove_hdf5_element"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#value",
    "href": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#value",
    "title": "bdRemove_hdf5_element",
    "section": "4 Value",
    "text": "4 Value\n\nNo return value, called for side effects (element removal).",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdRemove_hdf5_element"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#details",
    "href": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#details",
    "title": "bdRemove_hdf5_element",
    "section": "5 Details",
    "text": "5 Details\nThis function provides safe element removal capabilities with: - Removal options: - Single element removal - Multiple element removal - Groups and datasets removal - Implementation features: - Safe file operations - Memory-efficient implementation - Comprehensive error handling - Path validation\nThe function validates paths and performs safe removal operations.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdRemove_hdf5_element"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#examples",
    "title": "bdRemove_hdf5_element",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test matrices\nmatA &lt;- matrix(1:15, nrow = 3, byrow = TRUE)\nmatB &lt;- matrix(15:1, nrow = 3, byrow = TRUE)\n\n# Save to HDF5\nfn &lt;- \"test.hdf5\"\nbdCreate_hdf5_matrix(fn, matA, \"data\", \"matrix1\",\n                     overwriteFile = TRUE)\nbdCreate_hdf5_matrix(fn, matB, \"data\", \"matrix2\",\n                     overwriteFile = FALSE)\n\n# Remove elements\nbdRemove_hdf5_element(fn, c(\"data/matrix1\", \"data/matrix2\"))\n\n# Cleanup\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdRemove_hdf5_element"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#see-also",
    "href": "api-reference/r/hdf5_io_management/bdRemove_hdf5_element.html#see-also",
    "title": "bdRemove_hdf5_element",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdRemove_hdf5_element"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html",
    "href": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html",
    "title": "bdSplit_matrix_hdf5",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#description",
    "href": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#description",
    "title": "bdSplit_matrix_hdf5",
    "section": "1 Description",
    "text": "1 Description\nSplits a large dataset in an HDF5 file into smaller submatrices, with support for both row-wise and column-wise splitting.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#usage",
    "title": "bdSplit_matrix_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdSplit_matrix_hdf5(filename, group, dataset, outgroup = NULL, outdataset = NULL, nblocks = NULL, blocksize = NULL, bycols = TRUE, overwrite = FALSE)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#arguments",
    "title": "bdSplit_matrix_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ngroup\nCharacter string. Path to the group containing input dataset.\n\n\ndataset\nCharacter string. Name of the dataset to split.\n\n\noutgroup\nCharacter string (optional). Output group path. If NULL, uses input group.\n\n\noutdataset\nCharacter string (optional). Base name for output datasets. If NULL, uses input dataset name with block number suffix.\n\n\nnblocks\nInteger (optional). Number of blocks to split into. Mutually exclusive with blocksize.\n\n\nblocksize\nInteger (optional). Size of each block. Mutually exclusive with nblocks.\n\n\nbycols\nLogical (optional). Whether to split by columns (TRUE) or rows (FALSE). Default is TRUE.\n\n\noverwrite\nLogical (optional). Whether to overwrite existing datasets. Default is FALSE.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#value",
    "href": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#value",
    "title": "bdSplit_matrix_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string with the HDF5 filename\nds: Character string with the output group path where the split datasets are stored. Multiple datasets are created in this location named as &lt;outdataset&gt;.1, &lt;outdataset&gt;.2, etc.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#details",
    "href": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#details",
    "title": "bdSplit_matrix_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides efficient dataset splitting capabilities with: - Splitting options: - Row-wise or column-wise splitting - Fixed block size splitting - Fixed block count splitting - Implementation features: - Memory-efficient processing - Block-based operations - Safe file operations - Progress reporting\nThe function supports two splitting strategies: 1. By number of blocks: Splits the dataset into a specified number of roughly equal-sized blocks 2. By block size: Splits the dataset into blocks of a specified size",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#examples",
    "title": "bdSplit_matrix_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test data\ndata &lt;- matrix(rnorm(1000), 100, 10)\n\n# Save to HDF5\nfn &lt;- \"test.hdf5\"\nbdCreate_hdf5_matrix(fn, data, \"data\", \"matrix1\",\n                     overwriteFile = TRUE)\n\n# Split by number of blocks\nbdSplit_matrix_hdf5(\n  filename = fn,\n  group = \"data\",\n  dataset = \"matrix1\",\n  outgroup = \"data_split\",\n  outdataset = \"block\",\n  nblocks = 4,\n  bycols = TRUE\n)\n\n# Split by block size\nbdSplit_matrix_hdf5(\n  filename = fn,\n  group = \"data\",\n  dataset = \"matrix1\",\n  outgroup = \"data_split2\",\n  outdataset = \"block\",\n  blocksize = 25,\n  bycols = TRUE\n)\n\n# Cleanup\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_io_management/bdSplit_matrix_hdf5.html#see-also",
    "title": "bdSplit_matrix_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdSplit_matrix_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html",
    "href": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html",
    "title": "bdgetDatasetsList_hdf5",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDatasetsList_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#description",
    "href": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#description",
    "title": "bdgetDatasetsList_hdf5",
    "section": "1 Description",
    "text": "1 Description\nRetrieves a list of all datasets within a specified HDF5 group, with optional filtering by prefix or suffix.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDatasetsList_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#usage",
    "title": "bdgetDatasetsList_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdgetDatasetsList_hdf5(filename, group, prefix = NULL)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDatasetsList_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#arguments",
    "title": "bdgetDatasetsList_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ngroup\nCharacter string. Path to the group within the HDF5 file.\n\n\nprefix\nOptional character string. If provided, only returns datasets starting with this prefix.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDatasetsList_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#value",
    "href": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#value",
    "title": "bdgetDatasetsList_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nCharacter vector containing dataset names.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDatasetsList_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#details",
    "href": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#details",
    "title": "bdgetDatasetsList_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides flexible dataset listing capabilities for HDF5 files. Key features: - Listing options: - All datasets in a group - Datasets matching a prefix - Datasets matching a suffix - Implementation features: - Safe HDF5 file operations - Memory-efficient implementation - Comprehensive error handling - Read-only access to files\nThe function opens the HDF5 file in read-only mode to ensure data safety.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDatasetsList_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#examples",
    "title": "bdgetDatasetsList_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create a test HDF5 file\nfn &lt;- \"test.hdf5\"\nX &lt;- matrix(rnorm(100), 10, 10)\nY &lt;- matrix(rnorm(100), 10, 10)\n\n# Save matrices to HDF5\nbdCreate_hdf5_matrix(fn, X, \"data\", \"matrix1\",\n                     overwriteFile = TRUE)\nbdCreate_hdf5_matrix(fn, Y, \"data\", \"matrix2\",\n                     overwriteFile = FALSE)\n\n# List all datasets in group\ndatasets &lt;- bdgetDatasetsList_hdf5(fn, \"data\")\nprint(datasets)\n\n# List datasets with prefix \"matrix\"\nfiltered &lt;- bdgetDatasetsList_hdf5(fn, \"data\", prefix = \"matrix\")\nprint(filtered)\n\n# Cleanup\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDatasetsList_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_io_management/bdgetDatasetsList_hdf5.html#see-also",
    "title": "bdgetDatasetsList_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdgetDatasetsList_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html",
    "href": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html",
    "title": "bdmove_hdf5_dataset",
    "section": "",
    "text": "HDF5_IO_MANAGEMENT",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdmove_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html#description",
    "href": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html#description",
    "title": "bdmove_hdf5_dataset",
    "section": "1 Description",
    "text": "1 Description\nMoves an HDF5 dataset from one location to another within the same HDF5 file. This function automatically handles moving associated rownames and colnames datasets, creates parent groups if needed, and updates all internal references.",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdmove_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html#usage",
    "href": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html#usage",
    "title": "bdmove_hdf5_dataset",
    "section": "2 Usage",
    "text": "2 Usage\nbdmove_hdf5_dataset(filename, source_path, dest_path, overwrite = FALSE)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdmove_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html#arguments",
    "href": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html#arguments",
    "title": "bdmove_hdf5_dataset",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file\n\n\nsource_path\nCharacter string. Current path to the dataset (e.g., “/group1/dataset1”)\n\n\ndest_path\nCharacter string. New path for the dataset (e.g., “/group2/new_name”)\n\n\noverwrite\nLogical. Whether to overwrite destination if it exists (default: FALSE)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdmove_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html#value",
    "href": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html#value",
    "title": "bdmove_hdf5_dataset",
    "section": "4 Value",
    "text": "4 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the moved dataset in its new location (group/dataset)",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdmove_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html#details",
    "href": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html#details",
    "title": "bdmove_hdf5_dataset",
    "section": "5 Details",
    "text": "5 Details\nThis function provides a high-level interface for moving datasets within HDF5 files. The operation is efficient as it uses HDF5’s native linking mechanism without copying actual data.\nKey features:",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdmove_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html#examples",
    "href": "api-reference/r/hdf5_io_management/bdmove_hdf5_dataset.html#examples",
    "title": "bdmove_hdf5_dataset",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\n# Move dataset to a different group\nsuccess &lt;- bdmove_hdf5_dataset(\"data.h5\", \n                         source_path = \"/old_group/my_dataset\",\n                         dest_path = \"/new_group/my_dataset\")\n\n# Rename dataset within the same group\nsuccess &lt;- bdmove_hdf5_dataset(\"data.h5\",\n                         source_path = \"/data/old_name\", \n                         dest_path = \"/data/new_name\",\n                         overwrite = TRUE)\n\n# Move dataset to root level\nsuccess &lt;- bdmove_hdf5_dataset(\"data.h5\",\n                         source_path = \"/deep/nested/dataset\",\n                         dest_path = \"/dataset\")\n\n# Move with automatic group creation\nsuccess &lt;- bdmove_hdf5_dataset(\"data.h5\",\n                         source_path = \"/old_location/dataset\",\n                         dest_path = \"/new/deep/structure/dataset\")",
    "crumbs": [
      "HDF5 I/O & Management",
      "bdmove_hdf5_dataset"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/index.html",
    "href": "api-reference/r/hdf5_io_management/index.html",
    "title": "HDF5 I/O & Management",
    "section": "",
    "text": "Creation, import, layout and management of HDF5 datasets and groups (I/O, metadata, subsetting, binding, moving, reducing).",
    "crumbs": [
      "HDF5 I/O & Management"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/index.html#overview",
    "href": "api-reference/r/hdf5_io_management/index.html#overview",
    "title": "HDF5 I/O & Management",
    "section": "",
    "text": "Creation, import, layout and management of HDF5 datasets and groups (I/O, metadata, subsetting, binding, moving, reducing).",
    "crumbs": [
      "HDF5 I/O & Management"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_io_management/index.html#functions",
    "href": "api-reference/r/hdf5_io_management/index.html#functions",
    "title": "HDF5 I/O & Management",
    "section": "2 Functions",
    "text": "2 Functions\n\n2.1 bdImportData_hdf5\nbdImportData_hdf5\n\n\n2.2 bdBind_hdf5_datasets\nbdBind_hdf5_datasets\n\n\n2.3 bdCreate_hdf5_emptyDataset\nCreates an HDF5 dataset of size inside with name , without writing data (allocation only). Honors file/dataset overwrite flags and supports unlimited datasets.\n\n\n2.4 bdCreate_hdf5_group\nCreate a (nested) group inside an HDF5 file. The operation is idempotent: if the group already exists, no error is raised.\n\n\n2.5 bdCreate_hdf5_matrix\nbdCreate_hdf5_matrix\n\n\n2.6 bdIsLocked_hdf5\nUses HDF5 file locking to check if can be opened in read/write mode. If opening fails under locking, the file is treated as “in use” and is returned. Non-existent files return .\n\n\n2.7 bdgetDim_hdf5\nRetrieves the dimensions (number of rows and columns) of a dataset stored in an HDF5 file.\n\n\n2.8 bdgetDatasetsList_hdf5\nRetrieves a list of all datasets within a specified HDF5 group, with optional filtering by prefix or suffix.\n\n\n2.9 bdCheckMatrix_hdf5\nChecks whether a matrix stored in HDF5 format is suitable for eigenvalue decomposition using Spectra. The function verifies that the matrix is square and optionally checks for symmetry to recommend the best solver type.\n\n\n2.10 bdImportTextFile_hdf5\nConverts a text file (e.g., CSV, TSV) to HDF5 format, providing efficient storage and access capabilities.\n\n\n2.11 bdmove_hdf5_dataset\nMoves an HDF5 dataset from one location to another within the same HDF5 file. This function automatically handles moving associated rownames and colnames datasets, creates parent groups if needed, and updates all internal references.\n\n\n2.12 bdReduce_hdf5_dataset\nReduces multiple datasets within an HDF5 group using arithmetic operations (addition or subtraction).\n\n\n2.13 bdRemove_hdf5_element\nRemoves specified groups or datasets from an HDF5 file.\n\n\n2.14 bdSort_hdf5_dataset\nSorts a dataset in an HDF5 file based on a predefined ordering specified through a list of sorting blocks.\n\n\n2.15 bdSplit_matrix_hdf5\nSplits a large dataset in an HDF5 file into smaller submatrices, with support for both row-wise and column-wise splitting.\n\n\n2.16 bdsubset_hdf5_dataset\nCreates a new HDF5 dataset containing only the specified rows or columns from an existing dataset. This operation is memory efficient as it uses HDF5’s hyperslab selection for direct disk-to-disk copying without loading the entire dataset into memory.\n\n\n2.17 bdWrite_hdf5_dimnames\nWrite row and/or column names metadata for an existing dataset in an HDF5 file. Empty vectors skip the corresponding dimnames.",
    "crumbs": [
      "HDF5 I/O & Management"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html",
    "href": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html",
    "title": "bdImputeSNPs_hdf5",
    "section": "",
    "text": "HDF5_STATISTICS",
    "crumbs": [
      "HDF5 Statistics",
      "bdImputeSNPs_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#description",
    "href": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#description",
    "title": "bdImputeSNPs_hdf5",
    "section": "1 Description",
    "text": "1 Description\nPerforms imputation of missing values in SNP (Single Nucleotide Polymorphism) data stored in HDF5 format.",
    "crumbs": [
      "HDF5 Statistics",
      "bdImputeSNPs_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#usage",
    "href": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#usage",
    "title": "bdImputeSNPs_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdImputeSNPs_hdf5(filename, group, dataset, outgroup = NULL, outdataset = NULL, bycols = TRUE, paral = NULL, threads = NULL, overwrite = NULL)",
    "crumbs": [
      "HDF5 Statistics",
      "bdImputeSNPs_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#arguments",
    "title": "bdImputeSNPs_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ngroup\nCharacter string. Path to the group containing input dataset.\n\n\ndataset\nCharacter string. Name of the dataset to impute.\n\n\noutgroup\nCharacter string (optional). Output group path. If NULL, uses input group.\n\n\noutdataset\nCharacter string (optional). Output dataset name. If NULL, overwrites input dataset.\n\n\nbycols\nLogical (optional). Whether to impute by columns (TRUE) or rows (FALSE). Default is TRUE.\n\n\nparal\nLogical (optional). Whether to use parallel processing.\n\n\nthreads\nInteger (optional). Number of threads for parallel processing.\n\n\noverwrite\nLogical (optional). Whether to overwrite existing dataset.",
    "crumbs": [
      "HDF5 Statistics",
      "bdImputeSNPs_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#value",
    "href": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#value",
    "title": "bdImputeSNPs_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components:\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the imputed data (group/dataset)",
    "crumbs": [
      "HDF5 Statistics",
      "bdImputeSNPs_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#details",
    "href": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#details",
    "title": "bdImputeSNPs_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides efficient imputation capabilities for genomic data with support for: - Imputation options: - Row-wise or column-wise imputation - Parallel processing - Configurable thread count - Output options: - Custom output location - In-place modification - Overwrite protection - Implementation features: - Memory-efficient processing - Safe file operations - Error handling\nThe function supports both in-place modification and creation of new datasets.",
    "crumbs": [
      "HDF5 Statistics",
      "bdImputeSNPs_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#examples",
    "href": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#examples",
    "title": "bdImputeSNPs_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test data with missing values\ndata &lt;- matrix(sample(c(0, 1, 2, NA), 100, replace = TRUE), 10, 10)\n\n# Save to HDF5\nfn &lt;- \"snp_data.hdf5\"\nbdCreate_hdf5_matrix(fn, data, \"genotype\", \"snps\",\n                     overwriteFile = TRUE)\n\n# Impute missing values\nbdImputeSNPs_hdf5(\n  filename = fn,\n  group = \"genotype\",\n  dataset = \"snps\",\n  outgroup = \"genotype_imputed\",\n  outdataset = \"snps_complete\",\n  bycols = TRUE,\n  paral = TRUE\n)\n\n# Cleanup\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "HDF5 Statistics",
      "bdImputeSNPs_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_statistics/bdImputeSNPs_hdf5.html#see-also",
    "title": "bdImputeSNPs_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdCreate_hdf5_matrix for creating HDF5 matrices",
    "crumbs": [
      "HDF5 Statistics",
      "bdImputeSNPs_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html",
    "href": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html",
    "title": "bdRemoveMAF_hdf5",
    "section": "",
    "text": "HDF5_STATISTICS",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemoveMAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#description",
    "href": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#description",
    "title": "bdRemoveMAF_hdf5",
    "section": "1 Description",
    "text": "1 Description\nFilters SNPs (Single Nucleotide Polymorphisms) based on Minor Allele Frequency (MAF) in genomic data stored in HDF5 format.",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemoveMAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#usage",
    "href": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#usage",
    "title": "bdRemoveMAF_hdf5",
    "section": "2 Usage",
    "text": "2 Usage\nbdRemoveMAF_hdf5(filename, group, dataset, outgroup, outdataset, maf, bycols, blocksize, overwrite = NULL)",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemoveMAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#arguments",
    "title": "bdRemoveMAF_hdf5",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter string. Path to the HDF5 file.\n\n\ngroup\nCharacter string. Path to the group containing input dataset.\n\n\ndataset\nCharacter string. Name of the dataset to filter.\n\n\noutgroup\nCharacter string. Output group path for filtered data.\n\n\noutdataset\nCharacter string. Output dataset name for filtered data.\n\n\nmaf\nNumeric (optional). MAF threshold for filtering (0-1). Default is 0.05. SNPs with MAF above this threshold are removed.\n\n\nbycols\nLogical (optional). Whether to process by columns (TRUE) or rows (FALSE). Default is FALSE.\n\n\nblocksize\nInteger (optional). Block size for processing. Default is 100. Larger values use more memory but may be faster.\n\n\noverwrite\nLogical (optional). Whether to overwrite existing dataset. Default is FALSE.",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemoveMAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#value",
    "href": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#value",
    "title": "bdRemoveMAF_hdf5",
    "section": "4 Value",
    "text": "4 Value\n\nList with components. If an error occurs, all string values are returned as empty strings (““):\n\nfn: Character string with the HDF5 filename\nds: Character string with the full dataset path to the filtered dataset (group/dataset)\nnremoved: Integer with the number of SNPs removed due to low Minor Allele Frequency (MAF)",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemoveMAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#details",
    "href": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#details",
    "title": "bdRemoveMAF_hdf5",
    "section": "5 Details",
    "text": "5 Details\nThis function provides efficient MAF-based filtering capabilities with: - Filtering options: - MAF threshold-based filtering - Row-wise or column-wise processing - Block-based processing - Implementation features: - Memory-efficient processing - Block-based operations - Safe file operations - Progress reporting\nThe function supports both in-place modification and creation of new datasets.",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemoveMAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#examples",
    "href": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#examples",
    "title": "bdRemoveMAF_hdf5",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\nlibrary(BigDataStatMeth)\n\n# Create test SNP data\nsnps &lt;- matrix(sample(c(0, 1, 2), 1000, replace = TRUE,\n                     prob = c(0.7, 0.2, 0.1)), 100, 10)\n\n# Save to HDF5\nfn &lt;- \"snp_data.hdf5\"\nbdCreate_hdf5_matrix(fn, snps, \"genotype\", \"raw_snps\",\n                     overwriteFile = TRUE)\n\n# Remove SNPs with high MAF\nbdRemoveMAF_hdf5(\n  filename = fn,\n  group = \"genotype\",\n  dataset = \"raw_snps\",\n  outgroup = \"genotype_filtered\",\n  outdataset = \"filtered_snps\",\n  maf = 0.1,\n  bycols = TRUE,\n  blocksize = 50\n)\n\n# Cleanup\nif (file.exists(fn)) {\n  file.remove(fn)\n}",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemoveMAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#see-also",
    "href": "api-reference/r/hdf5_statistics/bdRemoveMAF_hdf5.html#see-also",
    "title": "bdRemoveMAF_hdf5",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdRemovelowdata_hdf5 for removing low-representation SNPs\nbdImputeSNPs_hdf5 for imputing missing SNP values",
    "crumbs": [
      "HDF5 Statistics",
      "bdRemoveMAF_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdapply_Function_hdf5.html",
    "href": "api-reference/r/hdf5_statistics/bdapply_Function_hdf5.html",
    "title": "bdapply_Function_hdf5",
    "section": "",
    "text": "HDF5_STATISTICS",
    "crumbs": [
      "HDF5 Statistics",
      "bdapply_Function_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdapply_Function_hdf5.html#usage",
    "href": "api-reference/r/hdf5_statistics/bdapply_Function_hdf5.html#usage",
    "title": "bdapply_Function_hdf5",
    "section": "1 Usage",
    "text": "1 Usage\nbdapply_Function_hdf5(filename, group, datasets, outgroup, func, b_group = NULL, b_datasets = NULL, overwrite = FALSE, transp_dataset = FALSE, transp_bdataset = FALSE, fullMatrix = FALSE, byrows = FALSE, threads = 2L)",
    "crumbs": [
      "HDF5 Statistics",
      "bdapply_Function_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdapply_Function_hdf5.html#arguments",
    "href": "api-reference/r/hdf5_statistics/bdapply_Function_hdf5.html#arguments",
    "title": "bdapply_Function_hdf5",
    "section": "2 Arguments",
    "text": "2 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nfilename\nCharacter array, indicating the name of the file to create\n\n\ngroup\nCharacter array, indicating the input group where the data set to be imputed is\n\n\ndatasets\nCharacter array, indicating the input datasets to be used\n\n\noutgroup\nCharacter array, indicating group where the data set will be saved after imputation. If NULL, output dataset is stored in the same input group\n\n\nfunc\nCharacter array, function to be applied: - “QR”: QR decomposition via bdQR() - “CrossProd”: Cross product via bdCrossprod() - “tCrossProd”: Transposed cross product via bdtCrossprod() - “invChol”: Inverse via Cholesky decomposition - “blockmult”: Matrix multiplication - “CrossProd_double”: Cross product with two matrices - “tCrossProd_double”: Transposed cross product with two matrices - “solve”: Matrix equation solving - “sdmean”: Standard deviation and mean computation\n\n\nb_group\nOptional character array indicating the input group for secondary datasets (used in two-matrix operations)\n\n\nb_datasets\nOptional character array indicating the secondary datasets for two-matrix operations\n\n\noverwrite\nOptional boolean. If true, overwrites existing results\n\n\ntransp_dataset\nOptional boolean. If true, transposes first dataset\n\n\ntransp_bdataset\nOptional boolean. If true, transposes second dataset\n\n\nfullMatrix\nOptional boolean for Cholesky operations. If true, stores complete matrix; if false, stores only lower triangular\n\n\nbyrows\nOptional boolean for statistical operations. If true, computes by rows; if false, by columns\n\n\nthreads\nOptional integer specifying number of threads for parallel processing",
    "crumbs": [
      "HDF5 Statistics",
      "bdapply_Function_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdapply_Function_hdf5.html#value",
    "href": "api-reference/r/hdf5_statistics/bdapply_Function_hdf5.html#value",
    "title": "bdapply_Function_hdf5",
    "section": "3 Value",
    "text": "3 Value\n\nModifies the HDF5 file in place, adding computed results",
    "crumbs": [
      "HDF5 Statistics",
      "bdapply_Function_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdapply_Function_hdf5.html#details",
    "href": "api-reference/r/hdf5_statistics/bdapply_Function_hdf5.html#details",
    "title": "bdapply_Function_hdf5",
    "section": "4 Details",
    "text": "4 Details\n//’ For matrix multiplication operations (blockmult, CrossProd_double, tCrossProd_double), the datasets and b_datasets vectors must have the same length. Each operation is performed element-wise between the corresponding pairs of datasets. Specifically, the b_datasets vector defines the second operand for each matrix multiplication. For example, if datasets = {\"A1\", \"A2\", \"A3\"} and b_datasets = {\"B1\", \"B2\", \"B3\"}, the operations executed are: A1 %*% B1, A2 %*% B2, and A3 %*% B3.\nExample: If datasets = {\"A1\", \"A2\", \"A3\"} and b_datasets = {\"B1\", \"B2\", \"B3\"}, the function computes: A1 %*% B1, A2 %*% B2, and A3 %*% B3",
    "crumbs": [
      "HDF5 Statistics",
      "bdapply_Function_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/bdapply_Function_hdf5.html#examples",
    "href": "api-reference/r/hdf5_statistics/bdapply_Function_hdf5.html#examples",
    "title": "bdapply_Function_hdf5",
    "section": "5 Examples",
    "text": "5 Examples\n\n\nCode\n# Create a sample large matrix in HDF5\n# Create hdf5 datasets\nbdCreate_hdf5_matrix(filename = \"test_temp.hdf5\", \n                    object = Y, group = \"data\", dataset = \"Y\",\n                    transp = FALSE,\n                    overwriteFile = TRUE, overwriteDataset = TRUE, \n                    unlimited = FALSE)\n\nbdCreate_hdf5_matrix(filename = \"test_temp.hdf5\", \n                    object = X,  group = \"data\",  dataset = \"X\",\n                    transp = FALSE,\n                    overwriteFile = FALSE, overwriteDataset = TRUE, \n                    unlimited = FALSE)\n\nbdCreate_hdf5_matrix(filename = \"test_temp.hdf5\",\n                    object = Z,  group = \"data\",  dataset = \"Z\",\n                    transp = FALSE,\n                    overwriteFile = FALSE, overwriteDataset = TRUE,\n                    unlimited = FALSE)\n\ndsets &lt;- bdgetDatasetsList_hdf5(\"test_temp.hdf5\", group = \"data\")\ndsets\n\n# Apply function :  QR Decomposition\nbdapply_Function_hdf5(filename = \"test_temp.hdf5\",\n                     group = \"data\",datasets = dsets,\n                     outgroup = \"QR\",func = \"QR\",\n                     overwrite = TRUE)",
    "crumbs": [
      "HDF5 Statistics",
      "bdapply_Function_hdf5"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/index.html",
    "href": "api-reference/r/hdf5_statistics/index.html",
    "title": "HDF5 Statistics",
    "section": "",
    "text": "Descriptive statistics, normalization, correlation, and domain-specific preprocessing over HDF5.",
    "crumbs": [
      "HDF5 Statistics"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/index.html#overview",
    "href": "api-reference/r/hdf5_statistics/index.html#overview",
    "title": "HDF5 Statistics",
    "section": "",
    "text": "Descriptive statistics, normalization, correlation, and domain-specific preprocessing over HDF5.",
    "crumbs": [
      "HDF5 Statistics"
    ]
  },
  {
    "objectID": "api-reference/r/hdf5_statistics/index.html#functions",
    "href": "api-reference/r/hdf5_statistics/index.html#functions",
    "title": "HDF5 Statistics",
    "section": "2 Functions",
    "text": "2 Functions\n\n2.1 bdapply_Function_hdf5\nbdapply_Function_hdf5\n\n\n2.2 bdNormalize_hdf5\nbdNormalize_hdf5\n\n\n2.3 bdImputeSNPs_hdf5\nPerforms imputation of missing values in SNP (Single Nucleotide Polymorphism) data stored in HDF5 format.\n\n\n2.4 bdCorr_hdf5\nThis function computes Pearson or Spearman correlation matrix for matrices stored in HDF5 format. It automatically detects whether to compute: It automatically selects between direct computation for small matrices and block-wise processing for large matrices to optimize memory usage and performance.\nCorrelation types supported: \nFor omics data analysis: \n\n\n2.5 bdgetSDandMean_hdf5\nComputes standard deviation and/or mean statistics for a matrix stored in HDF5 format, with support for row-wise or column-wise computations.\n\n\n2.6 bdRemovelowdata_hdf5\nRemoves SNPs (Single Nucleotide Polymorphisms) with low representation from genomic data stored in HDF5 format.\n\n\n2.7 bdRemoveMAF_hdf5\nFilters SNPs (Single Nucleotide Polymorphisms) based on Minor Allele Frequency (MAF) in genomic data stored in HDF5 format.",
    "crumbs": [
      "HDF5 Statistics"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCorr_matrix.html",
    "href": "api-reference/r/linear_algebra/bdCorr_matrix.html",
    "title": "bdCorr_matrix",
    "section": "",
    "text": "LINEAR_ALGEBRA",
    "crumbs": [
      "Linear Algebra",
      "bdCorr_matrix"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCorr_matrix.html#description",
    "href": "api-reference/r/linear_algebra/bdCorr_matrix.html#description",
    "title": "bdCorr_matrix",
    "section": "1 Description",
    "text": "1 Description\nCompute Pearson or Spearman correlation matrix for matrices that fit in memory. This function automatically detects whether to compute:",
    "crumbs": [
      "Linear Algebra",
      "bdCorr_matrix"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCorr_matrix.html#usage",
    "href": "api-reference/r/linear_algebra/bdCorr_matrix.html#usage",
    "title": "bdCorr_matrix",
    "section": "2 Usage",
    "text": "2 Usage\nbdCorr_matrix(X, Y = NULL, trans_x = NULL, trans_y = NULL, method = NULL, use_complete_obs = NULL, compute_pvalues = NULL, threads = NULL)",
    "crumbs": [
      "Linear Algebra",
      "bdCorr_matrix"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCorr_matrix.html#arguments",
    "href": "api-reference/r/linear_algebra/bdCorr_matrix.html#arguments",
    "title": "bdCorr_matrix",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nX\nFirst numeric matrix (observations in rows, variables in columns)\n\n\nY\nSecond numeric matrix (optional, observations in rows, variables in columns)\n\n\ntrans_x\nLogical, whether to transpose matrix X (default: FALSE)\n\n\ntrans_y\nLogical, whether to transpose matrix Y (default: FALSE, ignored if Y not provided)\n\n\nmethod\nCharacter string indicating correlation method (“pearson” or “spearman”, default: “pearson”)\n\n\nuse_complete_obs\nLogical, whether to use only complete observations (default: TRUE)\n\n\ncompute_pvalues\nLogical, whether to compute p-values for correlations (default: TRUE)\n\n\nthreads\nInteger, number of threads for parallel computation (optional, default: -1 for auto)",
    "crumbs": [
      "Linear Algebra",
      "bdCorr_matrix"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCorr_matrix.html#value",
    "href": "api-reference/r/linear_algebra/bdCorr_matrix.html#value",
    "title": "bdCorr_matrix",
    "section": "4 Value",
    "text": "4 Value\n\nA list containing correlation results",
    "crumbs": [
      "Linear Algebra",
      "bdCorr_matrix"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdCorr_matrix.html#examples",
    "href": "api-reference/r/linear_algebra/bdCorr_matrix.html#examples",
    "title": "bdCorr_matrix",
    "section": "5 Examples",
    "text": "5 Examples\n\n\nCode\n# Backward compatible - existing code unchanged\nset.seed(123)\nX &lt;- matrix(rnorm(1000), ncol = 10)\nresult_original &lt;- bdCorr_matrix(X)\n\n# Create omics-style data\ngene_expr &lt;- matrix(rnorm(5000), nrow = 100, ncol = 50)  # 100 samples × 50 genes\n\n# Gene-gene correlations (variables)\ngene_corr &lt;- bdCorr_matrix(gene_expr, trans_x = FALSE)\n\n# Sample-sample correlations (individuals)  \nsample_corr &lt;- bdCorr_matrix(gene_expr, trans_x = TRUE)\n\n# Cross-correlation examples\nmethylation &lt;- matrix(rnorm(4000), nrow = 100, ncol = 40)  # 100 samples × 40 CpGs\n\n# Variables vs variables (genes vs CpGs)\nvars_vs_vars &lt;- bdCorr_matrix(gene_expr, methylation, \n                             trans_x = FALSE, trans_y = FALSE)\n\n# Samples vs variables (individuals vs CpGs)\nsamples_vs_vars &lt;- bdCorr_matrix(gene_expr, methylation,\n                                trans_x = TRUE, trans_y = FALSE)",
    "crumbs": [
      "Linear Algebra",
      "bdCorr_matrix"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdQR.html",
    "href": "api-reference/r/linear_algebra/bdQR.html",
    "title": "bdQR",
    "section": "",
    "text": "LINEAR_ALGEBRA",
    "crumbs": [
      "Linear Algebra",
      "bdQR"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdQR.html#description",
    "href": "api-reference/r/linear_algebra/bdQR.html#description",
    "title": "bdQR",
    "section": "1 Description",
    "text": "1 Description\nComputes the QR decomposition (also called QR factorization) of a matrix A into a product A = QR where Q is an orthogonal matrix and R is an upper triangular matrix. This function operates on in-memory matrices.",
    "crumbs": [
      "Linear Algebra",
      "bdQR"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdQR.html#usage",
    "href": "api-reference/r/linear_algebra/bdQR.html#usage",
    "title": "bdQR",
    "section": "2 Usage",
    "text": "2 Usage\nbdQR(X, thin = NULL, block_size = NULL, threads = NULL)",
    "crumbs": [
      "Linear Algebra",
      "bdQR"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdQR.html#arguments",
    "href": "api-reference/r/linear_algebra/bdQR.html#arguments",
    "title": "bdQR",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nX\nA real matrix or vector to be decomposed\n\n\nthin\nLogical. If TRUE, returns the reduced (thin) Q matrix. If FALSE (default), returns the full Q matrix. The thin decomposition is more memory efficient.\n\n\nblock_size\nInteger. Optional block size for blocked computation. Larger blocks may improve performance but require more memory.\n\n\nthreads\nInteger. Optional number of threads for parallel computation. If NULL, uses all available threads.",
    "crumbs": [
      "Linear Algebra",
      "bdQR"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdQR.html#value",
    "href": "api-reference/r/linear_algebra/bdQR.html#value",
    "title": "bdQR",
    "section": "4 Value",
    "text": "4 Value\n\nA list containing: * Q: The orthogonal matrix Q * R: The upper triangular matrix R",
    "crumbs": [
      "Linear Algebra",
      "bdQR"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdQR.html#details",
    "href": "api-reference/r/linear_algebra/bdQR.html#details",
    "title": "bdQR",
    "section": "5 Details",
    "text": "5 Details\nThe QR decomposition is a fundamental matrix factorization that decomposes a matrix into an orthogonal matrix Q and an upper triangular matrix R. This implementation: - Supports both thin and full QR decomposition - Can utilize parallel computation for better performance - Handles both matrix and vector inputs",
    "crumbs": [
      "Linear Algebra",
      "bdQR"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdQR.html#examples",
    "href": "api-reference/r/linear_algebra/bdQR.html#examples",
    "title": "bdQR",
    "section": "6 Examples",
    "text": "6 Examples\n\n\nCode\n# Create a random 100x50 matrix\nX &lt;- matrix(rnorm(5000), 100, 50)\n\n# Compute thin QR decomposition\nresult &lt;- bdQR(X, thin = TRUE)\n\n# Verify the decomposition\n# Should be approximately zero\nmax(abs(X - result$Q %*% result$R))",
    "crumbs": [
      "Linear Algebra",
      "bdQR"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdQR.html#see-also",
    "href": "api-reference/r/linear_algebra/bdQR.html#see-also",
    "title": "bdQR",
    "section": "7 See Also",
    "text": "7 See Also\n\nbdQR_hdf5 for QR decomposition of HDF5-stored matrices",
    "crumbs": [
      "Linear Algebra",
      "bdQR"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdSolve.html",
    "href": "api-reference/r/linear_algebra/bdSolve.html",
    "title": "bdSolve",
    "section": "",
    "text": "LINEAR_ALGEBRA",
    "crumbs": [
      "Linear Algebra",
      "bdSolve"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdSolve.html#description",
    "href": "api-reference/r/linear_algebra/bdSolve.html#description",
    "title": "bdSolve",
    "section": "1 Description",
    "text": "1 Description\nSolves the linear system AX = B where A is an N-by-N matrix and X and B are N-by-NRHS matrices. The function automatically detects if A is symmetric and uses the appropriate solver.",
    "crumbs": [
      "Linear Algebra",
      "bdSolve"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdSolve.html#usage",
    "href": "api-reference/r/linear_algebra/bdSolve.html#usage",
    "title": "bdSolve",
    "section": "2 Usage",
    "text": "2 Usage\nbdSolve(A, B)",
    "crumbs": [
      "Linear Algebra",
      "bdSolve"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdSolve.html#arguments",
    "href": "api-reference/r/linear_algebra/bdSolve.html#arguments",
    "title": "bdSolve",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nA\nNumeric matrix. The coefficient matrix (must be square).\n\n\nB\nNumeric matrix. The right-hand side matrix (must have same number of rows as A).",
    "crumbs": [
      "Linear Algebra",
      "bdSolve"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdSolve.html#value",
    "href": "api-reference/r/linear_algebra/bdSolve.html#value",
    "title": "bdSolve",
    "section": "4 Value",
    "text": "4 Value\n\nNumeric matrix X, the solution to AX = B.",
    "crumbs": [
      "Linear Algebra",
      "bdSolve"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdSolve.html#details",
    "href": "api-reference/r/linear_algebra/bdSolve.html#details",
    "title": "bdSolve",
    "section": "5 Details",
    "text": "5 Details\nThis function provides an efficient implementation for solving linear systems using LAPACK routines. Key features: - Automatic detection of matrix properties: - Checks for matrix symmetry - Selects optimal solver based on matrix structure - Solver selection: - Symmetric systems: Uses LAPACK’s dsysv routine - Non-symmetric systems: Uses LAPACK’s dgesv routine - Performance optimizations: - Automatic workspace sizing - Efficient memory management - Support for multiple right-hand sides\nThe implementation ensures: - Robust error handling - Efficient memory usage - Numerical stability - Support for various matrix sizes",
    "crumbs": [
      "Linear Algebra",
      "bdSolve"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdSolve.html#examples",
    "href": "api-reference/r/linear_algebra/bdSolve.html#examples",
    "title": "bdSolve",
    "section": "6 Examples",
    "text": "6 Examples\n\nlibrary(BigDataStatMeth)\n\n# Create test matrices\nn &lt;- 500\nm &lt;- 500\n\nA &lt;- matrix(runif(n*m), nrow = n, ncol = m)\nB &lt;- matrix(runif(n), nrow = n)\nAS &lt;- A %*% t(A)  # Create symmetric matrix\n\n# Solve using bdSolve\nX &lt;- bdSolve(A, B)\n\n# Compare with R's solve\nXR &lt;- solve(A, B)\nall.equal(X, XR, check.attributes=FALSE)",
    "crumbs": [
      "Linear Algebra",
      "bdSolve"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdSolve.html#see-also",
    "href": "api-reference/r/linear_algebra/bdSolve.html#see-also",
    "title": "bdSolve",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdSolve_hdf5 for solving systems with HDF5-stored matrices\nsolve for R’s built-in solver",
    "crumbs": [
      "Linear Algebra",
      "bdSolve"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdpseudoinv.html",
    "href": "api-reference/r/linear_algebra/bdpseudoinv.html",
    "title": "bdpseudoinv",
    "section": "",
    "text": "LINEAR_ALGEBRA",
    "crumbs": [
      "Linear Algebra",
      "bdpseudoinv"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdpseudoinv.html#description",
    "href": "api-reference/r/linear_algebra/bdpseudoinv.html#description",
    "title": "bdpseudoinv",
    "section": "1 Description",
    "text": "1 Description\nComputes the Moore-Penrose pseudoinverse of a matrix using SVD decomposition. This implementation handles both square and rectangular matrices, and provides numerically stable results even for singular or near-singular matrices.",
    "crumbs": [
      "Linear Algebra",
      "bdpseudoinv"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdpseudoinv.html#usage",
    "href": "api-reference/r/linear_algebra/bdpseudoinv.html#usage",
    "title": "bdpseudoinv",
    "section": "2 Usage",
    "text": "2 Usage\nbdpseudoinv(X, threads = NULL)",
    "crumbs": [
      "Linear Algebra",
      "bdpseudoinv"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdpseudoinv.html#arguments",
    "href": "api-reference/r/linear_algebra/bdpseudoinv.html#arguments",
    "title": "bdpseudoinv",
    "section": "3 Arguments",
    "text": "3 Arguments\n\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\nX\nNumeric matrix or vector to be pseudoinverted.\n\n\nthreads\nOptional integer. Number of threads for parallel computation. If NULL, uses maximum available threads.",
    "crumbs": [
      "Linear Algebra",
      "bdpseudoinv"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdpseudoinv.html#value",
    "href": "api-reference/r/linear_algebra/bdpseudoinv.html#value",
    "title": "bdpseudoinv",
    "section": "4 Value",
    "text": "4 Value\n\nThe pseudoinverse matrix of X.",
    "crumbs": [
      "Linear Algebra",
      "bdpseudoinv"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdpseudoinv.html#details",
    "href": "api-reference/r/linear_algebra/bdpseudoinv.html#details",
    "title": "bdpseudoinv",
    "section": "5 Details",
    "text": "5 Details\nThe Moore-Penrose pseudoinverse (denoted A+) of a matrix A is computed using Singular Value Decomposition (SVD).\nFor a matrix A = USigmaV^T (where ^T denotes transpose), the pseudoinverse is computed as:\nwhere Sigma+ is obtained by taking the reciprocal of non-zero singular values.",
    "crumbs": [
      "Linear Algebra",
      "bdpseudoinv"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdpseudoinv.html#examples",
    "href": "api-reference/r/linear_algebra/bdpseudoinv.html#examples",
    "title": "bdpseudoinv",
    "section": "6 Examples",
    "text": "6 Examples\n\nlibrary(BigDataStatMeth)\n\n# Create a singular matrix\nX &lt;- matrix(c(1,2,3,2,4,6), 2, 3)  # rank-deficient matrix\n\n# Compute pseudoinverse\nX_pinv &lt;- bdpseudoinv(X)\n\n# Verify Moore-Penrose conditions\n# 1. X %*% X_pinv %*% X = X\nall.equal(X %*% X_pinv %*% X, X)\n\n# 2. X_pinv %*% X %*% X_pinv = X_pinv\nall.equal(X_pinv %*% X %*% X_pinv, X_pinv)",
    "crumbs": [
      "Linear Algebra",
      "bdpseudoinv"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/bdpseudoinv.html#see-also",
    "href": "api-reference/r/linear_algebra/bdpseudoinv.html#see-also",
    "title": "bdpseudoinv",
    "section": "7 See Also",
    "text": "7 See Also\n\n\nbdpseudoinv_hdf5 for HDF5-stored matrices\nbdSVD_hdf5 for singular value decomposition",
    "crumbs": [
      "Linear Algebra",
      "bdpseudoinv"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/index.html",
    "href": "api-reference/r/linear_algebra/index.html",
    "title": "Linear Algebra",
    "section": "",
    "text": "Linear algebra operations that run in memory (RAM).",
    "crumbs": [
      "Linear Algebra"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/index.html#overview",
    "href": "api-reference/r/linear_algebra/index.html#overview",
    "title": "Linear Algebra",
    "section": "",
    "text": "Linear algebra operations that run in memory (RAM).",
    "crumbs": [
      "Linear Algebra"
    ]
  },
  {
    "objectID": "api-reference/r/linear_algebra/index.html#functions",
    "href": "api-reference/r/linear_algebra/index.html#functions",
    "title": "Linear Algebra",
    "section": "2 Functions",
    "text": "2 Functions\n\n2.1 bdQR\nComputes the QR decomposition (also called QR factorization) of a matrix A into a product A = QR where Q is an orthogonal matrix and R is an upper triangular matrix. This function operates on in-memory matrices.\n\n\n2.2 bdSolve\nSolves the linear system AX = B where A is an N-by-N matrix and X and B are N-by-NRHS matrices. The function automatically detects if A is symmetric and uses the appropriate solver.\n\n\n2.3 bdpseudoinv\nComputes the Moore-Penrose pseudoinverse of a matrix using SVD decomposition. This implementation handles both square and rectangular matrices, and provides numerically stable results even for singular or near-singular matrices.\n\n\n2.4 bdCorr_matrix\nCompute Pearson or Spearman correlation matrix for matrices that fit in memory. This function automatically detects whether to compute: \n\n\n2.5 bdCrossprod\nComputes matrix cross-products efficiently using block-based algorithms and optional parallel processing. Supports both single-matrix (X’X) and two-matrix (X’Y) cross-products.\n\n\n2.6 bdScalarwproduct\nMultiplies a numeric matrix by a scalar weight , returning . The input must be a base R numeric matrix (or convertible to one).\n\n\n2.7 bdtCrossprod\nComputes matrix transposed cross-products efficiently using block-based algorithms and optional parallel processing. Supports both single-matrix (XX’) and two-matrix (XY’) transposed cross-products.\n\n\n2.8 bd_wproduct\nCompute weighted operations using a diagonal weight from : Inputs may be base numeric matrices .",
    "crumbs": [
      "Linear Algebra"
    ]
  },
  {
    "objectID": "appendix/cpp_functions_table.html",
    "href": "appendix/cpp_functions_table.html",
    "title": "C++ Functions Reference Table",
    "section": "",
    "text": "Complete list of C++ functions in BigDataStatMeth for manual API matching."
  },
  {
    "objectID": "appendix/cpp_functions_table.html#overview",
    "href": "appendix/cpp_functions_table.html#overview",
    "title": "C++ Functions Reference Table",
    "section": "",
    "text": "Complete list of C++ functions in BigDataStatMeth for manual API matching."
  },
  {
    "objectID": "appendix/cpp_functions_table.html#c-functions",
    "href": "appendix/cpp_functions_table.html#c-functions",
    "title": "C++ Functions Reference Table",
    "section": "2 C++ Functions",
    "text": "2 C++ Functions\n\n\n\n\n\n\n\n\n\nFunction Name\nSignature\nDescription\nLink\n\n\n\n\naddDiagonals\nvoid BigDataStatMeth::DiagonalOps::addDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nAdd diagonal elements from two matrices or vectors.\n../api-reference/cpp/functions/addDiagonals.qmd\n\n\nafter_fork\nvoid after_fork()\n—\n../api-reference/cpp/functions/after_fork.qmd\n\n\navoid_openmp_hang_within_fork\nvoid avoid_openmp_hang_within_fork()\n—\n../api-reference/cpp/functions/avoid_openmp_hang_within_fork.qmd\n\n\nBblock_matrix_mul_parallel\nEigen::MatrixXd BigDataStatMeth::Bblock_matrix_mul_parallel(Eigen::MatrixXd A, Eigen::MatrixXd B, int block_size, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nParallel block-based matrix multiplication.\n../api-reference/cpp/functions/Bblock_matrix_mul_parallel.qmd\n\n\nbdcrossproduct\nEigen::MatrixXd BigDataStatMeth::bdcrossproduct(T X)\nCompute matrix cross-product X’X.\n../api-reference/cpp/functions/bdcrossproduct.qmd\n\n\nbdtcrossproduct\nEigen::MatrixXd BigDataStatMeth::bdtcrossproduct(T X)\nCompute matrix transposed cross-product XX’.\n../api-reference/cpp/functions/bdtcrossproduct.qmd\n\n\ncalc_freq\ndouble BigDataStatMeth::calc_freq(Rcpp::NumericVector x)\nCalculates the Minor Allele Frequency (MAF) from binary genotype data.\n../api-reference/cpp/functions/calc_freq.qmd\n\n\ncalculate_multiplication_blocks\nBlockSizes BigDataStatMeth::calculate_multiplication_blocks(hsize_t N, hsize_t M, hsize_t K)\nCalculate optimal block sizes specifically for multiplication.\n../api-reference/cpp/functions/calculate_multiplication_blocks.qmd\n\n\ncheckClose_file\nvoid BigDataStatMeth::checkClose_file(std::initializer_list&lt; BigDataStatMeth::hdf5Dataset * &gt; list) noexcept\nSafely close a list of dataset handles (initializer-list form).\n../api-reference/cpp/functions/checkClose_file.qmd\n\n\nCholesky_decomposition_hdf5\nint BigDataStatMeth::Cholesky_decomposition_hdf5(BigDataStatMeth::hdf5Dataset *inDataset, BigDataStatMeth::hdf5Dataset *outDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)\nPerforms Cholesky decomposition with automatic algorithm selection.\n../api-reference/cpp/functions/Cholesky_decomposition_hdf5.qmd\n\n\nCholesky_decomposition_intermediate_hdf5\nint BigDataStatMeth::Cholesky_decomposition_intermediate_hdf5(BigDataStatMeth::hdf5Dataset *inDataset, BigDataStatMeth::hdf5Dataset *outDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)\nPerforms Cholesky decomposition on a matrix stored in HDF5 format.\n../api-reference/cpp/functions/Cholesky_decomposition_intermediate_hdf5.qmd\n\n\nCholesky_decomposition_outofcore_hdf5\nint BigDataStatMeth::Cholesky_decomposition_outofcore_hdf5(BigDataStatMeth::hdf5Dataset *inDataset, BigDataStatMeth::hdf5Dataset *outDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)\nOut-of-core Cholesky decomposition for large matrices using tiled algorithm.\n../api-reference/cpp/functions/Cholesky_decomposition_outofcore_hdf5.qmd\n\n\nclassify_matrix_type\nMatrixType BigDataStatMeth::classify_matrix_type(hsize_t N, hsize_t M, hsize_t K)\nClassify matrix type based on dimensional characteristics.\n../api-reference/cpp/functions/classify_matrix_type.qmd\n\n\ncleanup_temp_datasets\nvoid BigDataStatMeth::DiagonalOps::cleanup_temp_datasets(BigDataStatMeth::hdf5Dataset *tempA, BigDataStatMeth::hdf5Dataset *tempB)\nClean up temporary datasets created during operations.\n../api-reference/cpp/functions/cleanup_temp_datasets.qmd\n\n\ncompute_pvalues_optimized\nEigen::MatrixXd BigDataStatMeth::compute_pvalues_optimized(const Eigen::MatrixXd &correlation_matrix, int n_obs, bool symmetric=true)\nOptimized p-value computation for correlation matrices using Eigen.\n../api-reference/cpp/functions/compute_pvalues_optimized.qmd\n\n\ncorrelation_pvalue\ndouble BigDataStatMeth::correlation_pvalue(double r, int n)\nCompute p-value for correlation coefficient - R equivalent implementation.\n../api-reference/cpp/functions/correlation_pvalue.qmd\n\n\ncreateHardLink\nvoid BigDataStatMeth::createHardLink(H5::H5File *file, std::string original, std::string link)\nCreates a hard link in an HDF5 file.\n../api-reference/cpp/functions/createHardLink.qmd\n\n\ncrossprod\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::crossprod(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool isSymmetric, hsize_t hdf5_block, hsize_t mem_block_size, bool bparal, bool browmajor, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nComputes the cross-product of two matrices stored in HDF5 format.\n../api-reference/cpp/functions/crossprod.qmd\n\n\ncumsum\nEigen::VectorXd BigDataStatMeth::cumsum(Eigen::VectorXd x)\nCompute cumulative sum of a vector.\n../api-reference/cpp/functions/cumsum.qmd\n\n\ndgemm_\nvoid BigDataStatMeth::dgemm_(char *, char *, int *, int *, int *, double *, double *, int *, double *, int *, double *, double *, int *)\nLAPACK DGEMM matrix multiplication.\n../api-reference/cpp/functions/dgemm_.qmd\n\n\ndgesdd_\nvoid BigDataStatMeth::dgesdd_(char *, int *, int *, double *, int *, double *, double *, int *, double *, int *, double *, int *, int *, int *)\n—\n../api-reference/cpp/functions/dgesdd_.qmd\n\n\ndgesv_\nint BigDataStatMeth::dgesv_(int *, int *, double *, int *, int *, double *, int *, int *)\nLAPACK routine for solving general linear equations AX = B.\n../api-reference/cpp/functions/dgesv_.qmd\n\n\ndgesvd_\nvoid BigDataStatMeth::dgesvd_(char *, char *, int *, int *, double *, int *, double *, double *, int *, double *, int *, double *, int *, int *)\nLAPACK DGESVD singular value decomposition.\n../api-reference/cpp/functions/dgesvd_.qmd\n\n\ndivideDiagonals\nvoid BigDataStatMeth::DiagonalOps::divideDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nDivide diagonal elements from two matrices or vectors.\n../api-reference/cpp/functions/divideDiagonals.qmd\n\n\ndscal_\nvoid BigDataStatMeth::dscal_(int *, double *, double *, int *)\nLAPACK DSCAL vector scaling.\n../api-reference/cpp/functions/dscal_.qmd\n\n\ndsysv_\nint BigDataStatMeth::dsysv_(char *, int *, int *, double *, int *, int *, double *, int *, double *, int *, int *)\nLAPACK routine for solving symmetric linear equations AX = B.\n../api-reference/cpp/functions/dsysv_.qmd\n\n\nestimateOptimalBlockSize\nhsize_t BigDataStatMeth::estimateOptimalBlockSize(hsize_t matrix_size, double available_memory_mb=100.0)\nUtility function to estimate optimal block size for diagonal matrix operations.\n../api-reference/cpp/functions/estimateOptimalBlockSize.qmd\n\n\nexists_HDF5_element\nbool BigDataStatMeth::exists_HDF5_element(H5::H5File *file, std::string element)\nChecks if an HDF5 element (group or dataset) exists.\n../api-reference/cpp/functions/exists_HDF5_element.qmd\n\n\nextractDiagonalToVector\nvoid BigDataStatMeth::DiagonalOps::extractDiagonalToVector(BigDataStatMeth::hdf5Dataset *dsMatrix, BigDataStatMeth::hdf5Dataset *dsVector)\nExtract diagonal from matrix and save as vector dataset.\n../api-reference/cpp/functions/extractDiagonalToVector.qmd\n\n\nfind_all\nstd::vector&lt; _InputIterator &gt; BigDataStatMeth::find_all(_InputIterator begin, _InputIterator end, const T &val)\nFinds all occurrences of a value in a range.\n../api-reference/cpp/functions/find_all.qmd\n\n\nFirst_level_SvdBlock_decomposition_hdf5\nvoid BigDataStatMeth::First_level_SvdBlock_decomposition_hdf5(T *dsA, std::string strGroupName, int k, int q, int nev, bool bcenter, bool bscale, double dthreshold, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\n—\n../api-reference/cpp/functions/First_level_SvdBlock_decomposition_hdf5.qmd\n\n\nget_block_size\nhsize_t BigDataStatMeth::get_block_size(Rcpp::Nullable&lt; int &gt; wsize, hsize_t reference_size, hsize_t alternative_size)\nCalculate optimal block size for processing.\n../api-reference/cpp/functions/get_block_size.qmd\n\n\nget_data_as_Matrix\nstd::vector&lt; double &gt; BigDataStatMeth::get_data_as_Matrix(std::vector&lt; std::string &gt; strBlockValues)\nConverts a vector of string values to numeric (double) values.\n../api-reference/cpp/functions/get_data_as_Matrix.qmd\n\n\nget_HDF5_mean_sd_by_column\nvoid BigDataStatMeth::get_HDF5_mean_sd_by_column(BigDataStatMeth::hdf5Dataset *dsA, Eigen::MatrixXd &normalize, bool bsd, bool bmean, Rcpp::Nullable&lt; int &gt; wsize)\nCalculate column-wise mean and standard deviation.\n../api-reference/cpp/functions/get_HDF5_mean_sd_by_column.qmd\n\n\nget_HDF5_mean_sd_by_row\nvoid BigDataStatMeth::get_HDF5_mean_sd_by_row(BigDataStatMeth::hdf5Dataset *dsA, Eigen::MatrixXd &normalize, bool bsd, bool bmean, Rcpp::Nullable&lt; int &gt; wsize)\nCalculate row-wise mean and standard deviation.\n../api-reference/cpp/functions/get_HDF5_mean_sd_by_row.qmd\n\n\nget_NewLineEnding\nbool BigDataStatMeth::get_NewLineEnding(const char *filename)\nChecks if a file ends with a newline character.\n../api-reference/cpp/functions/get_NewLineEnding.qmd\n\n\nget_number_threads\nunsigned int get_number_threads(Rcpp::Nullable&lt; int &gt; threads, Rcpp::Nullable&lt; bool &gt; bparal)\n—\n../api-reference/cpp/functions/get_number_threads.qmd\n\n\nget_SplitData_in_vectorString\nstd::vector&lt; std::string &gt; BigDataStatMeth::get_SplitData_in_vectorString(std::string line, std::regex reg_expres)\nSplits a string into fields based on a regular expression.\n../api-reference/cpp/functions/get_SplitData_in_vectorString.qmd\n\n\nget_threads\nunsigned int BigDataStatMeth::get_threads(bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nDetermines number of threads for parallel operations.\n../api-reference/cpp/functions/get_threads.qmd\n\n\nget_value_to_impute_discrete\nint BigDataStatMeth::get_value_to_impute_discrete(std::map&lt; double, double &gt; probMap)\nGenerates a discrete value for imputation based on probability distribution.\n../api-reference/cpp/functions/get_value_to_impute_discrete.qmd\n\n\ngetAvailableMemoryMB\nsize_t BigDataStatMeth::getAvailableMemoryMB()\nDetects available system memory using R-compatible methods.\n../api-reference/cpp/functions/getAvailableMemoryMB.qmd\n\n\ngetBlockPositionsSizes\nvoid BigDataStatMeth::getBlockPositionsSizes(hsize_t maxPosition, hsize_t blockSize, std::vector&lt; hsize_t &gt; &starts, std::vector&lt; hsize_t &gt; &sizes)\nCalculate block positions and sizes for matrix operations.\n../api-reference/cpp/functions/getBlockPositionsSizes.qmd\n\n\ngetBlockPositionsSizes_hdf5\nvoid BigDataStatMeth::getBlockPositionsSizes_hdf5(hsize_t maxPosition, hsize_t blockSize, std::vector&lt; hsize_t &gt; &starts, std::vector&lt; hsize_t &gt; &sizes)\nCalculate block positions and sizes for HDF5 matrix operations.\n../api-reference/cpp/functions/getBlockPositionsSizes_hdf5.qmd\n\n\ngetBlockPositionsSizes_mat\nvoid BigDataStatMeth::getBlockPositionsSizes_mat(hsize_t maxPosition, hsize_t blockSize, std::vector&lt; hsize_t &gt; &starts, std::vector&lt; hsize_t &gt; &sizes)\nCalculate block positions and sizes for matrix operations.\n../api-reference/cpp/functions/getBlockPositionsSizes_mat.qmd\n\n\ngetDiagonalfromMatrix\nRcpp::NumericVector BigDataStatMeth::getDiagonalfromMatrix(BigDataStatMeth::hdf5Dataset *dsMat)\nExtracts the diagonal elements from a matrix stored in HDF5 format.\n../api-reference/cpp/functions/getDiagonalfromMatrix.qmd\n\n\ngetDTthreads\nint getDTthreads(const int64_t n, const bool throttle)\nDetermines number of threads for parallel operations.\n../api-reference/cpp/functions/getDTthreads.qmd\n\n\ngetDTthreads_R\nSEXP getDTthreads_R(SEXP verbose)\nR interface for thread configuration inspection.\n../api-reference/cpp/functions/getDTthreads_R.qmd\n\n\ngetGeneralSortRule\nSpectra::SortRule BigDataStatMeth::getGeneralSortRule(const std::string &which)\nConvert ‘which’ string to Spectra SortRule for general matrices.\n../api-reference/cpp/functions/getGeneralSortRule.qmd\n\n\ngetInitialPosition\nstd::vector&lt; hsize_t &gt; BigDataStatMeth::getInitialPosition(bool transp, int desp)\nDetermines initial position for matrix operations.\n../api-reference/cpp/functions/getInitialPosition.qmd\n\n\ngetIntEnv\nstatic int getIntEnv(const char *name, int def)\nRetrieves integer value from environment variable.\n../api-reference/cpp/functions/getIntEnv.qmd\n\n\ngetMatrixBlockSize\nstd::vector&lt; hsize_t &gt; BigDataStatMeth::getMatrixBlockSize(int nrows, int ncols)\nCalculates optimal block size for matrix operations.\n../api-reference/cpp/functions/getMatrixBlockSize.qmd\n\n\ngetMaxBlockSize\nint BigDataStatMeth::getMaxBlockSize(int nRowsA, int nColsA, int nRowsB, int nColsB, int ifactor, Rcpp::Nullable&lt; int &gt; block_size=R_NilValue)\nCalculates optimal block size for matrix operations.\n../api-reference/cpp/functions/getMaxBlockSize.qmd\n\n\ngetObjecDataType\nstd::string BigDataStatMeth::getObjecDataType(Rcpp::RObject obj)\nDetermines the data type of an R object.\n../api-reference/cpp/functions/getObjecDataType.qmd\n\n\ngetObjectDims\nRcpp::IntegerVector BigDataStatMeth::getObjectDims(Rcpp::RObject obj, std::string strtype)\nGets dimensions of an R object.\n../api-reference/cpp/functions/getObjectDims.qmd\n\n\ngetOptimalBlockElements\nsize_t BigDataStatMeth::getOptimalBlockElements()\nCalculates optimal block size for memory-efficient matrix operations.\n../api-reference/cpp/functions/getOptimalBlockElements.qmd\n\n\ngetOptimBlockSize\nsize_t BigDataStatMeth::getOptimBlockSize(size_t fullSize, size_t blockSize, size_t iDesp, size_t currentSize)\nOptimizes block size for edge cases.\n../api-reference/cpp/functions/getOptimBlockSize.qmd\n\n\ngetSizetoRead\nstd::vector&lt; hsize_t &gt; BigDataStatMeth::getSizetoRead(bool transp, int count, int rows, int cols)\nCalculates dimensions for reading matrix blocks.\n../api-reference/cpp/functions/getSizetoRead.qmd\n\n\ngetSymmetricSortRule\nSpectra::SortRule BigDataStatMeth::getSymmetricSortRule(const std::string &which)\nConvert ‘which’ string to Spectra SortRule for symmetric matrices.\n../api-reference/cpp/functions/getSymmetricSortRule.qmd\n\n\ngetVectorBlockSize\nhsize_t BigDataStatMeth::getVectorBlockSize(int maxSize)\nCalculates optimal block size for vector operations.\n../api-reference/cpp/functions/getVectorBlockSize.qmd\n\n\nhdf5_matrixVector_calculus\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::hdf5_matrixVector_calculus(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, int function, bool bbyrows, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nVector-matrix operations for HDF5 matrices.\n../api-reference/cpp/functions/hdf5_matrixVector_calculus.qmd\n\n\nimax\nstatic int imax(int a, int b)\nMaximum of two integers.\n../api-reference/cpp/functions/imax.qmd\n\n\nimin\nstatic int imin(int a, int b)\nMinimum of two integers.\n../api-reference/cpp/functions/imin.qmd\n\n\nindex\n—\n—\n../api-reference/cpp/functions/index.qmd\n\n\ninitDTthreads\nvoid initDTthreads()\nInitializes thread configuration based on environment and system state.\n../api-reference/cpp/functions/initDTthreads.qmd\n\n\nInverse_Matrix_Cholesky_hdf5\nvoid BigDataStatMeth::Inverse_Matrix_Cholesky_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)\nComputes final matrix inverse with automatic algorithm selection.\n../api-reference/cpp/functions/Inverse_Matrix_Cholesky_hdf5.qmd\n\n\nInverse_Matrix_Cholesky_intermediate_hdf5\nvoid BigDataStatMeth::Inverse_Matrix_Cholesky_intermediate_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)\nComputes final matrix inverse using inverted Cholesky factors.\n../api-reference/cpp/functions/Inverse_Matrix_Cholesky_intermediate_hdf5.qmd\n\n\nInverse_Matrix_Cholesky_outofcore_hdf5\nvoid BigDataStatMeth::Inverse_Matrix_Cholesky_outofcore_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)\nOut-of-core computation of A^-1 = L^-T L^-1 using tiles.\n../api-reference/cpp/functions/Inverse_Matrix_Cholesky_outofcore_hdf5.qmd\n\n\nInverse_of_Cholesky_decomposition_hdf5\nvoid BigDataStatMeth::Inverse_of_Cholesky_decomposition_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)\nComputes inverse of Cholesky factor with automatic algorithm selection.\n../api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_hdf5.qmd\n\n\nInverse_of_Cholesky_decomposition_intermediate_hdf5\nvoid BigDataStatMeth::Inverse_of_Cholesky_decomposition_intermediate_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)\nComputes inverse of Cholesky factor in-place.\n../api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_intermediate_hdf5.qmd\n\n\nInverse_of_Cholesky_decomposition_outofcore_hdf5\nvoid BigDataStatMeth::Inverse_of_Cholesky_decomposition_outofcore_hdf5(BigDataStatMeth::hdf5Dataset *InOutDataset, int idim0, int idim1, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)\nOut-of-core inverse of Cholesky factor using tiled back-substitution.\n../api-reference/cpp/functions/Inverse_of_Cholesky_decomposition_outofcore_hdf5.qmd\n\n\nis_number\nbool BigDataStatMeth::is_number(const std::string &s)\nTests if a string represents a valid number.\n../api-reference/cpp/functions/is_number.qmd\n\n\nisDiagonalVector\nbool BigDataStatMeth::DiagonalOps::isDiagonalVector(BigDataStatMeth::hdf5Dataset *ds)\nCheck if dataset represents a diagonal vector.\n../api-reference/cpp/functions/isDiagonalVector.qmd\n\n\nisMatrixSymmetric\nbool BigDataStatMeth::isMatrixSymmetric(const Eigen::MatrixXd &X, int sample_size=100)\nImproved matrix symmetry detection for big-omics data.\n../api-reference/cpp/functions/isMatrixSymmetric.qmd\n\n\njoin_datasets\nint BigDataStatMeth::join_datasets(T *dsJoined, std::string strsubgroup, Rcpp::StringVector strinput, bool bremoveJoined, bool byCols)\nJoins multiple HDF5 datasets into a single dataset within the same group.\n../api-reference/cpp/functions/join_datasets.qmd\n\n\nmultiplication\nvoid BigDataStatMeth::multiplication(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool transpose_A, bool transpose_B, Rcpp::Nullable&lt; bool &gt; bparal, Rcpp::Nullable&lt; int &gt; hdf5_block, Rcpp::Nullable&lt; int &gt; threads)\nMain matrix multiplication function for HDF5 matrices.\n../api-reference/cpp/functions/multiplication.qmd\n\n\nmultiplicationSparse\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::multiplicationSparse(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, hsize_t mem_block_size, bool bparal, bool browmajor, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nSparse matrix multiplication for HDF5 matrices.\n../api-reference/cpp/functions/multiplicationSparse.qmd\n\n\nmultiplyDiagonals\nvoid BigDataStatMeth::DiagonalOps::multiplyDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nMultiply diagonal elements from two matrices or vectors.\n../api-reference/cpp/functions/multiplyDiagonals.qmd\n\n\nmygetenv\nstatic const char * mygetenv(const char *name, const char *unset)\nSafe environment variable retrieval with default.\n../api-reference/cpp/functions/mygetenv.qmd\n\n\nNext_level_SvdBlock_decomposition_hdf5\nvoid BigDataStatMeth::Next_level_SvdBlock_decomposition_hdf5(T *dsA, std::string strGroupName, int k, int q, double dthreshold, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\n—\n../api-reference/cpp/functions/Next_level_SvdBlock_decomposition_hdf5.qmd\n\n\npathExists\nbool BigDataStatMeth::pathExists(hid_t id, const std::string &path)\nChecks if a path exists in an HDF5 file.\n../api-reference/cpp/functions/pathExists.qmd\n\n\npearson_correlation\ndouble BigDataStatMeth::pearson_correlation(const Eigen::VectorXd &x, const Eigen::VectorXd &y, bool use_complete_obs=true)\n—\n../api-reference/cpp/functions/pearson_correlation.qmd\n\n\nperformMatrixDiagonalOperation\nvoid BigDataStatMeth::DiagonalOps::performMatrixDiagonalOperation(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, int operation, std::string target, bool bparal, Rcpp::Nullable&lt; int &gt; threads)\nPerform diagonal operations on matrices using extract-operate-write strategy.\n../api-reference/cpp/functions/performMatrixDiagonalOperation.qmd\n\n\npowerDiagonals\nvoid BigDataStatMeth::DiagonalOps::powerDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nDivide diagonal elements from two matrices or vectors.\n../api-reference/cpp/functions/powerDiagonals.qmd\n\n\nprepareForParallelization\nstd::vector&lt; svdPositions &gt; BigDataStatMeth::prepareForParallelization(T *dsA, int M, int k, bool transp, int block_size, std::string strDatasetName)\n—\n../api-reference/cpp/functions/prepareForParallelization.qmd\n\n\nRcpp_block_matrix_mul\nEigen::MatrixXd BigDataStatMeth::Rcpp_block_matrix_mul(T X, U Y, Rcpp::Nullable&lt; int &gt; iblock_size)\nBlock-based matrix multiplication.\n../api-reference/cpp/functions/Rcpp_block_matrix_mul.qmd\n\n\nRcpp_block_matrix_mul_parallel\nEigen::MatrixXd BigDataStatMeth::Rcpp_block_matrix_mul_parallel(T X, U Y, bool transpX, bool transpY, Rcpp::Nullable&lt; int &gt; iblock_size, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nParallel block-based matrix multiplication.\n../api-reference/cpp/functions/Rcpp_block_matrix_mul_parallel.qmd\n\n\nRcpp_block_matrix_substract_hdf5\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_block_matrix_substract_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nBlock-based matrix subtraction for HDF5 matrices.\n../api-reference/cpp/functions/Rcpp_block_matrix_substract_hdf5.qmd\n\n\nRcpp_block_matrix_sum_hdf5\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_block_matrix_sum_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nBlock-based matrix addition for HDF5 matrices.\n../api-reference/cpp/functions/Rcpp_block_matrix_sum_hdf5.qmd\n\n\nRcpp_block_matrix_vector_substract\nEigen::MatrixXd BigDataStatMeth::Rcpp_block_matrix_vector_substract(T A, T B, hsize_t block_size, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nLow-level block-based matrix-vector subtraction.\n../api-reference/cpp/functions/Rcpp_block_matrix_vector_substract.qmd\n\n\nRcpp_block_matrix_vector_substract_hdf5\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_block_matrix_vector_substract_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nBlock-based matrix-vector subtraction for HDF5 matrices.\n../api-reference/cpp/functions/Rcpp_block_matrix_vector_substract_hdf5.qmd\n\n\nRcpp_block_matrix_vector_sum_hdf5\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_block_matrix_vector_sum_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, hsize_t hdf5_block, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nBlock-based matrix-vector addition for HDF5 matrices.\n../api-reference/cpp/functions/Rcpp_block_matrix_vector_sum_hdf5.qmd\n\n\nRcpp_FileExist\nbool BigDataStatMeth::Rcpp_FileExist(std::string fullPath)\nChecks if a file exists at the specified path.\n../api-reference/cpp/functions/Rcpp_FileExist.qmd\n\n\nRcpp_Import_File_to_hdf5\nvoid BigDataStatMeth::Rcpp_Import_File_to_hdf5(Rcpp::CharacterVector filename, BigDataStatMeth::hdf5Dataset *dsOut, Rcpp::Nullable&lt; std::string &gt; sep=R_NilValue, Rcpp::Nullable&lt; bool &gt; header=false, Rcpp::Nullable&lt; bool &gt; rownames=false, Rcpp::Nullable&lt; bool &gt; bparal=R_NilValue, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nImports data from a file into an HDF5 dataset.\n../api-reference/cpp/functions/Rcpp_Import_File_to_hdf5.qmd\n\n\nRcpp_Impute_snps_hdf5\nvoid BigDataStatMeth::Rcpp_Impute_snps_hdf5(BigDataStatMeth::hdf5Dataset *dsIn, BigDataStatMeth::hdf5DatasetInternal *dsOut, bool bycols, std::string stroutdataset, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nImputes missing values in an HDF5 dataset.\n../api-reference/cpp/functions/Rcpp_Impute_snps_hdf5.qmd\n\n\nRcpp_InvCholesky_hdf5\nvoid BigDataStatMeth::Rcpp_InvCholesky_hdf5(BigDataStatMeth::hdf5Dataset *inDataset, BigDataStatMeth::hdf5DatasetInternal *outDataset, bool bfull, long dElementsBlock, Rcpp::Nullable&lt; int &gt; threads)\nComputes matrix inverse using Cholesky decomposition with HDF5 storage.\n../api-reference/cpp/functions/Rcpp_InvCholesky_hdf5.qmd\n\n\nRcpp_matrix_blockSubstract\nRcpp::RObject BigDataStatMeth::Rcpp_matrix_blockSubstract(T A, T B, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nBlock-based matrix subtraction.\n../api-reference/cpp/functions/Rcpp_matrix_blockSubstract.qmd\n\n\nRcpp_matrix_blockSum\nRcpp::RObject BigDataStatMeth::Rcpp_matrix_blockSum(T A, T B, Rcpp::Nullable&lt; int &gt; threads)\nBlock-based matrix addition implementation.\n../api-reference/cpp/functions/Rcpp_matrix_blockSum.qmd\n\n\nRcpp_matrix_substract\nRcpp::RObject BigDataStatMeth::Rcpp_matrix_substract(T A, T B)\nMatrix subtraction.\n../api-reference/cpp/functions/Rcpp_matrix_substract.qmd\n\n\nRcpp_matrix_sum\nRcpp::RObject BigDataStatMeth::Rcpp_matrix_sum(T A, T B)\nLow-level block-based matrix-vector addition implementation.\n../api-reference/cpp/functions/Rcpp_matrix_sum.qmd\n\n\nRcpp_matrix_vect_mult\nRcpp::RObject BigDataStatMeth::Rcpp_matrix_vect_mult(T A, U B)\nMatrix-vector multiplication.\n../api-reference/cpp/functions/Rcpp_matrix_vect_mult.qmd\n\n\nRcpp_matrix_vect_substract\nRcpp::RObject BigDataStatMeth::Rcpp_matrix_vect_substract(T A, U B)\nMatrix-vector subtraction.\n../api-reference/cpp/functions/Rcpp_matrix_vect_substract.qmd\n\n\nRcpp_matrix_vect_sum\nRcpp::RObject BigDataStatMeth::Rcpp_matrix_vect_sum(T A, U B)\n—\n../api-reference/cpp/functions/Rcpp_matrix_vect_sum.qmd\n\n\nRcpp_matrix_vector_blockMult\nRcpp::RObject BigDataStatMeth::Rcpp_matrix_vector_blockMult(T A, T B, Rcpp::Nullable&lt; bool &gt; bparal, Rcpp::Nullable&lt; int &gt; iblock_size, Rcpp::Nullable&lt; int &gt; threads)\nBlock-based matrix-vector multiplication.\n../api-reference/cpp/functions/Rcpp_matrix_vector_blockMult.qmd\n\n\nRcpp_matrix_vector_blockSubstract\nRcpp::RObject BigDataStatMeth::Rcpp_matrix_vector_blockSubstract(T A, T B, Rcpp::Nullable&lt; bool &gt; bparal, Rcpp::Nullable&lt; int &gt; threads)\nBlock-based matrix-vector subtraction.\n../api-reference/cpp/functions/Rcpp_matrix_vector_blockSubstract.qmd\n\n\nRcpp_matrix_vector_blockSum\nRcpp::RObject BigDataStatMeth::Rcpp_matrix_vector_blockSum(T A, T B, Rcpp::Nullable&lt; bool &gt; bparal, Rcpp::Nullable&lt; int &gt; threads)\nBlock-based matrix-vector addition with parallel processing.\n../api-reference/cpp/functions/Rcpp_matrix_vector_blockSum.qmd\n\n\nRcpp_matrixVectorDivision_byCol\nEigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorDivision_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)\nMatrix-vector division by columns.\n../api-reference/cpp/functions/Rcpp_matrixVectorDivision_byCol.qmd\n\n\nRcpp_matrixVectorDivision_byRow\nEigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorDivision_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)\nMatrix-vector division by rows.\n../api-reference/cpp/functions/Rcpp_matrixVectorDivision_byRow.qmd\n\n\nRcpp_matrixVectorMultiplication_byCol\nEigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorMultiplication_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)\nMatrix-vector multiplication by columns.\n../api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byCol.qmd\n\n\nRcpp_matrixVectorMultiplication_byRow\nEigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorMultiplication_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)\nMatrix-vector multiplication by rows.\n../api-reference/cpp/functions/Rcpp_matrixVectorMultiplication_byRow.qmd\n\n\nRcpp_matrixVectorPow_byRow\nEigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorPow_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)\nMatrix-vector power by rows.\n../api-reference/cpp/functions/Rcpp_matrixVectorPow_byRow.qmd\n\n\nRcpp_matrixVectorPower_byCol\nEigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorPower_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)\nMatrix-vector power by columns.\n../api-reference/cpp/functions/Rcpp_matrixVectorPower_byCol.qmd\n\n\nRcpp_matrixVectorSubstract_byCol\nEigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorSubstract_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)\nMatrix-vector subtraction by columns.\n../api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byCol.qmd\n\n\nRcpp_matrixVectorSubstract_byRow\nEigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorSubstract_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)\nMatrix-vector subtraction by rows.\n../api-reference/cpp/functions/Rcpp_matrixVectorSubstract_byRow.qmd\n\n\nRcpp_matrixVectorSum_byCol\nEigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorSum_byCol(Eigen::MatrixXd X, Eigen::VectorXd v)\nMatrix-vector addition by columns.\n../api-reference/cpp/functions/Rcpp_matrixVectorSum_byCol.qmd\n\n\nRcpp_matrixVectorSum_byRow\nEigen::MatrixXd BigDataStatMeth::Rcpp_matrixVectorSum_byRow(Eigen::MatrixXd X, Eigen::VectorXd v)\nMatrix-vector addition by rows.\n../api-reference/cpp/functions/Rcpp_matrixVectorSum_byRow.qmd\n\n\nRcpp_Remove_Low_Data_hdf5\nint BigDataStatMeth::Rcpp_Remove_Low_Data_hdf5(BigDataStatMeth::hdf5Dataset *dsIn, BigDataStatMeth::hdf5Dataset *dsOut, bool bycols, double pcent)\nRemoves rows or columns with high percentage of missing data from HDF5 dataset.\n../api-reference/cpp/functions/Rcpp_Remove_Low_Data_hdf5.qmd\n\n\nRcpp_Remove_MAF_hdf5\nint BigDataStatMeth::Rcpp_Remove_MAF_hdf5(BigDataStatMeth::hdf5Dataset *dsIn, BigDataStatMeth::hdf5Dataset *dsOut, bool bycols, double pcent, int blocksize)\nRemoves rows or columns from a dataset based on MAF threshold.\n../api-reference/cpp/functions/Rcpp_Remove_MAF_hdf5.qmd\n\n\nRcpp_vector_add_hdf5\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_add_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nPure vector addition for HDF5 vectors.\n../api-reference/cpp/functions/Rcpp_vector_add_hdf5.qmd\n\n\nRcpp_vector_divide_hdf5\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_divide_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nPure vector division for HDF5 vectors.\n../api-reference/cpp/functions/Rcpp_vector_divide_hdf5.qmd\n\n\nRcpp_vector_mult\nRcpp::RObject BigDataStatMeth::Rcpp_vector_mult(T A, T B)\nVector multiplication (dot product)\n../api-reference/cpp/functions/Rcpp_vector_mult.qmd\n\n\nRcpp_vector_multiply_hdf5\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_multiply_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nPure vector multiplication for HDF5 vectors.\n../api-reference/cpp/functions/Rcpp_vector_multiply_hdf5.qmd\n\n\nRcpp_vector_power_hdf5\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_power_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nPure vector division for HDF5 vectors.\n../api-reference/cpp/functions/Rcpp_vector_power_hdf5.qmd\n\n\nRcpp_vector_substract\nRcpp::RObject BigDataStatMeth::Rcpp_vector_substract(T A, T B)\nVector subtraction.\n../api-reference/cpp/functions/Rcpp_vector_substract.qmd\n\n\nRcpp_vector_subtract_hdf5\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::Rcpp_vector_subtract_hdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool bparal, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nPure vector subtraction for HDF5 vectors.\n../api-reference/cpp/functions/Rcpp_vector_subtract_hdf5.qmd\n\n\nRcpp_vector_sum\nRcpp::RObject BigDataStatMeth::Rcpp_vector_sum(T A, T B)\n—\n../api-reference/cpp/functions/Rcpp_vector_sum.qmd\n\n\nRcppApplyFunctionHdf5\nvoid BigDataStatMeth::RcppApplyFunctionHdf5(std::string filename, std::string group, Rcpp::StringVector datasets, std::string outgroup, std::string func, Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; b_group=R_NilValue, Rcpp::Nullable&lt; Rcpp::StringVector &gt; b_datasets=R_NilValue, Rcpp::Nullable&lt; bool &gt; overwrite=false, Rcpp::Nullable&lt; bool &gt; transp_dataset=false, Rcpp::Nullable&lt; bool &gt; transp_bdataset=false, Rcpp::Nullable&lt; bool &gt; fullMatrix=false, Rcpp::Nullable&lt; bool &gt; byrows=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nApplies mathematical functions to HDF5 datasets.\n../api-reference/cpp/functions/RcppApplyFunctionHdf5.qmd\n\n\nRcppbdCorr_hdf5_Block_cross\nvoid BigDataStatMeth::RcppbdCorr_hdf5_Block_cross(T *dsA, T *dsB, BigDataStatMeth::hdf5Dataset *dsCorr, BigDataStatMeth::hdf5Dataset *dsPval, const std::string &method=\"pearson\", bool use_complete_obs=true, bool compute_pvalues=false, int block_size=1000, bool trans_x=false, bool trans_y=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nEnhanced HDF5 cross-correlation with integrated transpose support for big-omics.\n../api-reference/cpp/functions/RcppbdCorr_hdf5_Block_cross.qmd\n\n\nRcppbdCorr_hdf5_Block_single\nvoid BigDataStatMeth::RcppbdCorr_hdf5_Block_single(T *dsA, BigDataStatMeth::hdf5Dataset *dsCorr, BigDataStatMeth::hdf5Dataset *dsPval, const std::string &method=\"pearson\", bool use_complete_obs=true, bool compute_pvalues=false, bool trans_x=false, int block_size=1000, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\n—\n../api-reference/cpp/functions/RcppbdCorr_hdf5_Block_single.qmd\n\n\nRcppbdCorr_hdf5_cross\nRcpp::List BigDataStatMeth::RcppbdCorr_hdf5_cross(const std::string &filename_a, const std::string &strsubgroup_a, const std::string &strdataset_a, const std::string &filename_b, const std::string &strsubgroup_b, const std::string &strdataset_b, const std::string &method, bool use_complete_obs, bool compute_pvalues, int block_size, bool bforce, const std::string &output_filename, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_group, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_dataset_corr, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_dataset_pval, bool trans_x, bool trans_y, const Rcpp::Nullable&lt; int &gt; &threads)\nImplementation of cross-matrix correlation computation for HDF5 matrices - OPTIMIZED.\n../api-reference/cpp/functions/RcppbdCorr_hdf5_cross.qmd\n\n\nRcppbdCorr_hdf5_single\nRcpp::List BigDataStatMeth::RcppbdCorr_hdf5_single(const std::string &filename, const std::string &strsubgroup, const std::string &strdataset, const std::string &method, bool use_complete_obs, bool compute_pvalues, int block_size, bool bforce, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_group, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_dataset_corr, const Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; &output_dataset_pval, bool trans_x, const Rcpp::Nullable&lt; int &gt; &threads)\nImplementation of single matrix correlation computation for HDF5 matrices - OPTIMIZED.\n../api-reference/cpp/functions/RcppbdCorr_hdf5_single.qmd\n\n\nRcppbdCorr_matrix_cross\ncorr_result BigDataStatMeth::RcppbdCorr_matrix_cross(const Eigen::MatrixXd &X, const Eigen::MatrixXd &Y, const std::string &method=\"pearson\", bool use_complete_obs=true, bool compute_pvalues=true, bool trans_x=false, bool trans_y=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\n—\n../api-reference/cpp/functions/RcppbdCorr_matrix_cross.qmd\n\n\nRcppbdCorr_matrix_single\ncorr_result BigDataStatMeth::RcppbdCorr_matrix_single(Eigen::MatrixXd &X, const std::string &method=\"pearson\", bool use_complete_obs=true, bool compute_pvalues=false, bool trans_x=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\n—\n../api-reference/cpp/functions/RcppbdCorr_matrix_single.qmd\n\n\nRcppbdEigen_hdf5\nvoid BigDataStatMeth::RcppbdEigen_hdf5(std::string filename, std::string strsubgroup, std::string strdataset, int k=0, const std::string &which=\"LM\", int ncv=0, bool bcenter=false, bool bscale=false, double tolerance=1e-10, int max_iter=1000, bool compute_vectors=true, bool bforce=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nMain eigenvalue decomposition function for HDF5 matrices using Spectra.\n../api-reference/cpp/functions/RcppbdEigen_hdf5.qmd\n\n\nRcppbdEigen_hdf5_Block\nvoid BigDataStatMeth::RcppbdEigen_hdf5_Block(T *dsA, BigDataStatMeth::hdf5Dataset *dsd, BigDataStatMeth::hdf5Dataset *dsu, int k, const std::string &which, int ncv, bool bcenter, bool bscale, double tol, int max_iter, bool compute_vectors, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nBlock-wise eigendecomposition for large HDF5 matrices using Spectra.\n../api-reference/cpp/functions/RcppbdEigen_hdf5_Block.qmd\n\n\nRcppbdEigen_spectra\neigdecomp BigDataStatMeth::RcppbdEigen_spectra(const Eigen::MatrixXd &X, int k, const std::string &which=\"LM\", int ncv=0, bool bcenter=false, bool bscale=false, double tol=1e-10, int max_iter=1000)\nEigenvalue decomposition using Spectra (compatible with BigDataStatMeth SVD version)\n../api-reference/cpp/functions/RcppbdEigen_spectra.qmd\n\n\nRcppbdSVD\nsvdeig BigDataStatMeth::RcppbdSVD(Eigen::MatrixXd &X, int k, int ncv, bool bcenter, bool bscale)\nCompute SVD decomposition using Spectra eigenvalue solver.\n../api-reference/cpp/functions/RcppbdSVD.qmd\n\n\nRcppbdSVD_hdf5\nvoid BigDataStatMeth::RcppbdSVD_hdf5(std::string filename, std::string strsubgroup, std::string strdataset, int k, int q, int nev, bool bcenter, bool bscale, double dthreshold, bool bforce, bool asRowMajor, Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; method=R_NilValue, Rcpp::Nullable&lt; int &gt; ithreads=R_NilValue)\nMain SVD computation function for HDF5 matrices.\n../api-reference/cpp/functions/RcppbdSVD_hdf5.qmd\n\n\nRcppbdSVD_hdf5_Block\nvoid BigDataStatMeth::RcppbdSVD_hdf5_Block(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsu, BigDataStatMeth::hdf5Dataset *dsv, BigDataStatMeth::hdf5Dataset *dsd, int k, int q, int nev, bool bcenter, bool bscale, int irows, int icols, double dthreshold, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nBlock-wise SVD decomposition for large HDF5 matrices.\n../api-reference/cpp/functions/RcppbdSVD_hdf5_Block.qmd\n\n\nRcppbdSVD_lapack\nsvdeig BigDataStatMeth::RcppbdSVD_lapack(T X, bool bcenter, bool bscale, bool complete)\n—\n../api-reference/cpp/functions/RcppbdSVD_lapack.qmd\n\n\nRcppBind_datasets_hdf5\nvoid BigDataStatMeth::RcppBind_datasets_hdf5(std::string filename, std::string group, Rcpp::StringVector datasets, std::string outgroup, std::string outdataset, std::string func, bool binternal, Rcpp::Nullable&lt; bool &gt; overwrite=false)\nHigh-level interface for binding HDF5 datasets.\n../api-reference/cpp/functions/RcppBind_datasets_hdf5.qmd\n\n\nRcppGetPCAIndividualsHdf5\nvoid BigDataStatMeth::RcppGetPCAIndividualsHdf5(std::string strPCAgroup, BigDataStatMeth::hdf5Dataset *dsX, BigDataStatMeth::hdf5Dataset *dsd, BigDataStatMeth::hdf5Dataset *dsu, bool overwrite)\nCalculate PCA individuals statistics.\n../api-reference/cpp/functions/RcppGetPCAIndividualsHdf5.qmd\n\n\nRcppGetPCAVariablesHdf5\nvoid BigDataStatMeth::RcppGetPCAVariablesHdf5(std::string strPCAgroup, BigDataStatMeth::hdf5Dataset *dsd, BigDataStatMeth::hdf5Dataset *dsv, bool overwrite)\nCalculate PCA variables statistics.\n../api-reference/cpp/functions/RcppGetPCAVariablesHdf5.qmd\n\n\nRcppNormalize_Data\nM BigDataStatMeth::RcppNormalize_Data(M X, bool bc, bool bs, bool bRowMajor)\n—\n../api-reference/cpp/functions/RcppNormalize_Data.qmd\n\n\nRcppNormalize_Data_R_hdf5\nEigen::MatrixXd BigDataStatMeth::RcppNormalize_Data_R_hdf5(Eigen::MatrixXd X, bool bc, bool bs, bool btransp, Eigen::MatrixXd normdata)\n—\n../api-reference/cpp/functions/RcppNormalize_Data_R_hdf5.qmd\n\n\nRcppNormalizeColwise\nEigen::MatrixXd BigDataStatMeth::RcppNormalizeColwise(M X, bool bc, bool bs)\n—\n../api-reference/cpp/functions/RcppNormalizeColwise.qmd\n\n\nRcppNormalizeHdf5\nvoid BigDataStatMeth::RcppNormalizeHdf5(BigDataStatMeth::hdf5Dataset *dsA, bool bc, bool bs, bool bbyrows)\n—\n../api-reference/cpp/functions/RcppNormalizeHdf5.qmd\n\n\nRcppNormalizeRowwise\nEigen::MatrixXd BigDataStatMeth::RcppNormalizeRowwise(M X, bool bc, bool bs)\n—\n../api-reference/cpp/functions/RcppNormalizeRowwise.qmd\n\n\nRcppPCAHdf5\nvoid BigDataStatMeth::RcppPCAHdf5(std::string filename, std::string strgroup, std::string strdataset, std::string strSVDgroup, int k, int q, int nev, bool bcenter, bool bscale, double dthreshold, bool bforce, bool asRowMajor, Rcpp::Nullable&lt; Rcpp::CharacterVector &gt; method=R_NilValue, Rcpp::Nullable&lt; int &gt; ithreads=R_NilValue)\nPerform Principal Component Analysis.\n../api-reference/cpp/functions/RcppPCAHdf5.qmd\n\n\nRcppPseudoinv\nEigen::MatrixXd BigDataStatMeth::RcppPseudoinv(Eigen::MatrixXd *A, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nCompute pseudoinverse of in-memory matrix.\n../api-reference/cpp/functions/RcppPseudoinv.qmd\n\n\nRcppPseudoinvHdf5\nvoid BigDataStatMeth::RcppPseudoinvHdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsR, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nCompute pseudoinverse of HDF5 matrix.\n../api-reference/cpp/functions/RcppPseudoinvHdf5.qmd\n\n\nRcppQR\nstrQR BigDataStatMeth::RcppQR(M X, bool bthin)\nTemplate function for QR decomposition.\n../api-reference/cpp/functions/RcppQR.qmd\n\n\nRcppQRHdf5\nvoid BigDataStatMeth::RcppQRHdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsQ, BigDataStatMeth::hdf5Dataset *dsR, bool bthin, Rcpp::Nullable&lt; int &gt; block_size, Rcpp::Nullable&lt; int &gt; threads)\nQR decomposition for HDF5 matrices.\n../api-reference/cpp/functions/RcppQRHdf5.qmd\n\n\nRcppReduce_dataset_hdf5\nvoid BigDataStatMeth::RcppReduce_dataset_hdf5(std::string filename, std::string stringroup, std::string stroutgroup, std::string stroutdataset, std::string strreducefunction, bool boverwrite, bool bremove, bool binternal)\nReduces multiple HDF5 datasets into a single dataset using specified operation.\n../api-reference/cpp/functions/RcppReduce_dataset_hdf5.qmd\n\n\nRcppRemove_hdf5_elements\nvoid BigDataStatMeth::RcppRemove_hdf5_elements(BigDataStatMeth::hdf5File *file, std::vector&lt; std::string &gt; elements)\nRemoves specified elements from an HDF5 file.\n../api-reference/cpp/functions/RcppRemove_hdf5_elements.qmd\n\n\nRcppSolve\nEigen::MatrixXd BigDataStatMeth::RcppSolve(Eigen::Map&lt; Eigen::MatrixXd &gt; a, Eigen::Map&lt; Eigen::MatrixXd &gt; b)\nSolves the linear system AX = B using Eigen matrices.\n../api-reference/cpp/functions/RcppSolve.qmd\n\n\nRcppSolveHdf5\nvoid BigDataStatMeth::RcppSolveHdf5(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsX)\nSolves the linear system AX = B using HDF5-stored matrices.\n../api-reference/cpp/functions/RcppSolveHdf5.qmd\n\n\nRcppSort_dataset_hdf5\nvoid BigDataStatMeth::RcppSort_dataset_hdf5(BigDataStatMeth::hdf5Dataset *dsIn, BigDataStatMeth::hdf5Dataset *dsOut, Rcpp::List blockedSortlist, std::string func)\nSorts an HDF5 dataset by rows or columns.\n../api-reference/cpp/functions/RcppSort_dataset_hdf5.qmd\n\n\nRcppSplit_matrix_hdf5\nvoid BigDataStatMeth::RcppSplit_matrix_hdf5(BigDataStatMeth::hdf5Dataset *dstosplit, bool bycols, std::string stroutgroup, std::string stroutdataset, int blocksize, int irows, int icols)\nSplits an HDF5 dataset into multiple smaller datasets (R interface)\n../api-reference/cpp/functions/RcppSplit_matrix_hdf5.qmd\n\n\nRcppSplit_matrix_hdf5_internal\nvoid BigDataStatMeth::RcppSplit_matrix_hdf5_internal(BigDataStatMeth::hdf5Dataset *dstosplit, std::string stroutgroup, std::string stroutdataset, bool bycols, int nblocks, int iblocksize, int irows, int icols)\nSplits an HDF5 dataset into multiple smaller datasets (internal C++ interface)\n../api-reference/cpp/functions/RcppSplit_matrix_hdf5_internal.qmd\n\n\nRcppTypifyNormalizeHdf5\nvoid BigDataStatMeth::RcppTypifyNormalizeHdf5(BigDataStatMeth::hdf5Dataset *dsA, bool bc, bool bs, bool bbyrows)\n—\n../api-reference/cpp/functions/RcppTypifyNormalizeHdf5.qmd\n\n\nremove_elements\nbool BigDataStatMeth::remove_elements(H5::H5File *file, H5std_string element)\nRemoves a single element from an HDF5 file.\n../api-reference/cpp/functions/remove_elements.qmd\n\n\nremoveColumn\nvoid BigDataStatMeth::removeColumn(Eigen::MatrixXd &matrix, unsigned int colToRemove)\nRemoves a column from an Eigen matrix.\n../api-reference/cpp/functions/removeColumn.qmd\n\n\nremoveRow\nvoid BigDataStatMeth::removeRow(Eigen::MatrixXd &matrix, unsigned int rowToRemove)\nRemoves a row from an Eigen matrix.\n../api-reference/cpp/functions/removeRow.qmd\n\n\nrenameElement\nvoid BigDataStatMeth::renameElement(H5::H5File *file, std::string original, std::string link)\nRenames an element in an HDF5 file.\n../api-reference/cpp/functions/renameElement.qmd\n\n\nscalarOperation\nvoid BigDataStatMeth::DiagonalOps::scalarOperation(BigDataStatMeth::hdf5Dataset *dsInput, BigDataStatMeth::hdf5Dataset *dsResult, double scalar, int operation, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nPerform scalar operations on diagonal elements.\n../api-reference/cpp/functions/scalarOperation.qmd\n\n\nsetDiagonalMatrix\nvoid BigDataStatMeth::setDiagonalMatrix(BigDataStatMeth::hdf5Dataset *dsMat, Rcpp::NumericVector intNewDiagonal)\nSets the diagonal elements of a matrix stored in HDF5 format.\n../api-reference/cpp/functions/setDiagonalMatrix.qmd\n\n\nsetLowerTriangularMatrix\nvoid BigDataStatMeth::setLowerTriangularMatrix(BigDataStatMeth::hdf5Dataset *dsMat, hsize_t dElementsBlock)\nSet the lower triangular matrix using block-based approach.\n../api-reference/cpp/functions/setLowerTriangularMatrix.qmd\n\n\nsetUpperTriangularMatrix\nvoid BigDataStatMeth::setUpperTriangularMatrix(BigDataStatMeth::hdf5Dataset *dsMat, hsize_t dElementsBlock)\nSet the upper triangular matrix using block-based approach.\n../api-reference/cpp/functions/setUpperTriangularMatrix.qmd\n\n\nspearman_correlation\ndouble BigDataStatMeth::spearman_correlation(const Eigen::VectorXd &x, const Eigen::VectorXd &y, bool use_complete_obs=true)\n—\n../api-reference/cpp/functions/spearman_correlation.qmd\n\n\nSplitElementName\nfullpath BigDataStatMeth::SplitElementName(std::string str)\nSplits a full file path into directory path and filename.\n../api-reference/cpp/functions/SplitElementName.qmd\n\n\nsubtractDiagonals\nvoid BigDataStatMeth::DiagonalOps::subtractDiagonals(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsResult, std::string target=\"new\", bool bparal=false, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nSubtract diagonal elements from two matrices or vectors.\n../api-reference/cpp/functions/subtractDiagonals.qmd\n\n\nt_distribution_cdf\ndouble BigDataStatMeth::t_distribution_cdf(double t, double df)\nCompute p-value for correlation coefficient using t-distribution.\n../api-reference/cpp/functions/t_distribution_cdf.qmd\n\n\ntcrossprod\nBigDataStatMeth::hdf5Dataset * BigDataStatMeth::tcrossprod(BigDataStatMeth::hdf5Dataset *dsA, BigDataStatMeth::hdf5Dataset *dsB, BigDataStatMeth::hdf5Dataset *dsC, bool isSymmetric, hsize_t hdf5_block, hsize_t mem_block_size, bool bparal, bool browmajor, Rcpp::Nullable&lt; int &gt; threads=R_NilValue)\nTransposed cross-product for HDF5 matrices.\n../api-reference/cpp/functions/tcrossprod.qmd\n\n\ntranspose\nmatrix BigDataStatMeth::transpose(const matrix &M)\nTransposes a 2D matrix.\n../api-reference/cpp/functions/transpose.qmd\n\n\nvalidateSpectraParams\nstd::tuple&lt; int, int &gt; BigDataStatMeth::validateSpectraParams(int n, int k, int ncv)\nValidate and adjust Spectra parameters for convergence.\n../api-reference/cpp/functions/validateSpectraParams.qmd\n\n\nvalidateVector\nhsize_t BigDataStatMeth::validateVector(BigDataStatMeth::hdf5Dataset *ds)\nValidates that dataset is a vector and returns its size.\n../api-reference/cpp/functions/validateVector.qmd\n\n\nvalidateVectorDataset\nhsize_t BigDataStatMeth::DiagonalOps::validateVectorDataset(BigDataStatMeth::hdf5Dataset *ds)\nValidate vector dataset and return its size.\n../api-reference/cpp/functions/validateVectorDataset.qmd\n\n\nVectortoOrderedMap_SNP_counts\nstd::map&lt; double, double &gt; BigDataStatMeth::VectortoOrderedMap_SNP_counts(Eigen::VectorXd vdata)\nConverts a vector to an ordered map of value frequencies.\n../api-reference/cpp/functions/VectortoOrderedMap_SNP_counts.qmd\n\n\nwdX\nEigen::MatrixXd BigDataStatMeth::wdX(const Eigen::MatrixXd &X, const Eigen::VectorXd &w)\nCompute diagonal-matrix product wX.\n../api-reference/cpp/functions/wdX.qmd\n\n\nwdX_parallel\nEigen::MatrixXd BigDataStatMeth::wdX_parallel(const Eigen::MatrixXd &X, const Eigen::VectorXd &w, Rcpp::Nullable&lt; int &gt; threads)\nCompute parallel diagonal-matrix product wX.\n../api-reference/cpp/functions/wdX_parallel.qmd\n\n\nwhen_fork\nvoid when_fork()\n—\n../api-reference/cpp/functions/when_fork.qmd\n\n\nwriteDiagonalFromVector\nvoid BigDataStatMeth::DiagonalOps::writeDiagonalFromVector(BigDataStatMeth::hdf5Dataset *dsVector, BigDataStatMeth::hdf5Dataset *dsMatrix)\nWrite diagonal vector to matrix diagonal.\n../api-reference/cpp/functions/writeDiagonalFromVector.qmd\n\n\nwX\nEigen::MatrixXd BigDataStatMeth::wX(const Eigen::MatrixXd &X, const Eigen::MatrixXd &w)\nCompute diagonal-matrix product wX.\n../api-reference/cpp/functions/wX.qmd\n\n\nxtwx\nEigen::MatrixXd BigDataStatMeth::xtwx(const Eigen::MatrixXd &X, const Eigen::MatrixXd &w)\nCompute transposed weighted cross-product X’wX.\n../api-reference/cpp/functions/xtwx.qmd\n\n\nXw\nEigen::MatrixXd BigDataStatMeth::Xw(const Eigen::MatrixXd &X, const Eigen::MatrixXd &w)\nCompute matrix-diagonal product Xw.\n../api-reference/cpp/functions/Xw.qmd\n\n\nXwd\nEigen::MatrixXd BigDataStatMeth::Xwd(const Eigen::MatrixXd &X, const Eigen::VectorXd &w)\nCompute matrix-diagonal product Xw.\n../api-reference/cpp/functions/Xwd.qmd\n\n\nXwd_parallel\nEigen::MatrixXd BigDataStatMeth::Xwd_parallel(const Eigen::MatrixXd &X, const Eigen::VectorXd &w, Rcpp::Nullable&lt; int &gt; threads)\nCompute parallel matrix-diagonal product Xw.\n../api-reference/cpp/functions/Xwd_parallel.qmd\n\n\nxwxt\nEigen::MatrixXd BigDataStatMeth::xwxt(const Eigen::MatrixXd &X, const Eigen::MatrixXd &w)\nCompute weighted cross-product XwX’.\n../api-reference/cpp/functions/xwxt.qmd"
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html",
    "href": "developing-methods/cca-cpp-implementation.html",
    "title": "CCA Implementation in C++",
    "section": "",
    "text": "This document shows how to implement Canonical Correlation Analysis (CCA) in C++ using BigDataStatMeth’s header-only library. While the R implementation demonstrated the algorithm using R’s BigDataStatMeth functions, this C++ version provides direct control over computation, memory management, and performance optimization.\nThe C++ implementation isn’t just a faster version of the R code—it’s designed for scenarios where performance is critical, where you need fine-grained control over algorithms, or where you’re building production systems. You’ll see how to work with HDF5 datasets directly in C++, manage memory safely with pointers, handle errors robustly, and integrate everything back into R through Rcpp.\n\n\nBy the end of this document, you will:\n\nWork with BigDataStatMeth’s C++ header-only library\nManage HDF5 datasets using the hdf5Dataset class\nImplement block-wise QR decomposition in C++\nHandle pointers and memory management safely\nUse exception handling to prevent crashes\nIntegrate C++ functions with R through Rcpp\nCompile and test C++ implementations\nDecide when C++ optimization is worth the effort",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#overview",
    "href": "developing-methods/cca-cpp-implementation.html#overview",
    "title": "CCA Implementation in C++",
    "section": "",
    "text": "This document shows how to implement Canonical Correlation Analysis (CCA) in C++ using BigDataStatMeth’s header-only library. While the R implementation demonstrated the algorithm using R’s BigDataStatMeth functions, this C++ version provides direct control over computation, memory management, and performance optimization.\nThe C++ implementation isn’t just a faster version of the R code—it’s designed for scenarios where performance is critical, where you need fine-grained control over algorithms, or where you’re building production systems. You’ll see how to work with HDF5 datasets directly in C++, manage memory safely with pointers, handle errors robustly, and integrate everything back into R through Rcpp.\n\n\nBy the end of this document, you will:\n\nWork with BigDataStatMeth’s C++ header-only library\nManage HDF5 datasets using the hdf5Dataset class\nImplement block-wise QR decomposition in C++\nHandle pointers and memory management safely\nUse exception handling to prevent crashes\nIntegrate C++ functions with R through Rcpp\nCompile and test C++ implementations\nDecide when C++ optimization is worth the effort",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#why-c-implementation",
    "href": "developing-methods/cca-cpp-implementation.html#why-c-implementation",
    "title": "CCA Implementation in C++",
    "section": "2 Why C++ Implementation?",
    "text": "2 Why C++ Implementation?\n\n2.1 When C++ Makes Sense\nPerformance-critical applications: If you run CCA hundreds or thousands of times (production pipelines, simulation studies), C++ performance gains accumulate significantly. A 50% speedup doesn’t matter for one-off analyses but saves days in large studies.\nCustom algorithms: If you’re implementing novel CCA variants (sparse, regularized, kernel) not available in BigDataStatMeth’s R functions, C++ gives you algorithmic control. You’re not constrained to pre-built operations.\nProduction deployment: Software going into production systems benefits from C++’s speed and static typing. Compile-time errors prevent runtime failures that could break pipelines.\nFine-grained parallelization: C++ with OpenMP enables parallelizing within matrix operations, not just across function calls. This achieves better speedups for large-scale computations.\n\n\n2.2 When R Suffices\nPrototyping and development: Algorithm design in R is faster—run line-by-line, inspect variables, iterate quickly. Get it working in R, then optimize in C++ if needed.\nCombining existing functions: If your method uses bdSVD_hdf5(), bdCrossprod_hdf5(), etc., R orchestration is fine. The heavy lifting happens in compiled code anyway.\nOccasional use: Running CCA once per month? R performance is adequate. Save development time for frequently-used methods.\nThe R implementation showed you CCA’s algorithm structure. This C++ version shows you how to optimize when performance matters.",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#c-prerequisites",
    "href": "developing-methods/cca-cpp-implementation.html#c-prerequisites",
    "title": "CCA Implementation in C++",
    "section": "3 C++ Prerequisites",
    "text": "3 C++ Prerequisites\n\n3.1 Required Headers\nBigDataStatMeth provides a header-only C++ library—include the main header and you have access to all classes and functions:\n#include &lt;Rcpp.h&gt;\n#include \"BigDataStatMeth.hpp\"\n\nusing namespace Rcpp;\nusing namespace BigDataStatMeth;\nThe Rcpp.h header enables integration with R. The BigDataStatMeth.hpp header provides: - hdf5Dataset class for HDF5 I/O - Matrix operation functions (crossprod, SVD, QR, etc.) - Utility functions for block-wise processing\n\n\n3.2 Understanding hdf5Dataset Class\nThe hdf5Dataset class is your interface to HDF5 files in C++:\n// Constructor\nhdf5Dataset* ds = new hdf5Dataset(filename, group, dataset, create_new);\n\n// Open for reading/writing\nds-&gt;openDataset();\n\n// Get dimensions\nint nrows = ds-&gt;nrows();      // Total rows\nint ncols = ds-&gt;ncols();      // Total columns\nint nrows_r = ds-&gt;nrows_r();  // Rows (R indexing)\nint ncols_r = ds-&gt;ncols_r();  // Columns (R indexing)\n\n// Read data block\nstd::vector&lt;double&gt; data(nrows * ncols);\nstd::vector&lt;hsize_t&gt; offset = {0, 0};\nstd::vector&lt;hsize_t&gt; count = {nrows, ncols};\nstd::vector&lt;hsize_t&gt; stride = {1, 1};\nstd::vector&lt;hsize_t&gt; block = {1, 1};\n\nds-&gt;readDatasetBlock(offset, count, stride, block, data.data());\n\n// Write data block\nds-&gt;writeDatasetBlock(offset, count, stride, block, data.data());\n\n// Cleanup\ndelete ds;\nds = nullptr;\nKey points: - Always openDataset() before reading/writing - Always delete and set to nullptr when done - Use try/catch to handle errors safely",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#mathematical-foundation-reference",
    "href": "developing-methods/cca-cpp-implementation.html#mathematical-foundation-reference",
    "title": "CCA Implementation in C++",
    "section": "4 Mathematical Foundation (Reference)",
    "text": "4 Mathematical Foundation (Reference)\nThe mathematical foundations of CCA are detailed in the R implementation document. Here’s a quick summary:\nCCA algorithm: 1. QR decomposition: X = Q_X R_X, Y = Q_Y R_Y 2. Cross-product: M = Q_X^T Q_Y 3. SVD: M = U D V^T (D contains canonical correlations) 4. Coefficients: \\alpha = R_X^{-1} U, \\beta = R_Y^{-1} V 5. Variates: s_X = X\\alpha, s_Y = Y\\beta\nThe C++ implementation follows the same mathematical steps but with manual memory management and direct HDF5 access.",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#step-1-qr-decomposition-by-blocks-in-c",
    "href": "developing-methods/cca-cpp-implementation.html#step-1-qr-decomposition-by-blocks-in-c",
    "title": "CCA Implementation in C++",
    "section": "5 Step 1: QR Decomposition by Blocks in C++",
    "text": "5 Step 1: QR Decomposition by Blocks in C++\n\n5.1 The getQRbyBlocks_rcpp Function\nThis function computes QR decomposition of a matrix in HDF5 by processing blocks. It mirrors the R version but with C++ memory management:\nvoid getQRbyBlocks_rcpp(hdf5Dataset *dsA, int mblocks, \n                        bool bcenter, bool bscale, \n                        bool byrows, bool overwrite, \n                        Rcpp::Nullable&lt;int&gt; threads) {\n    \n    hdf5Dataset* dstmp = nullptr;\n    \n    try {\n        \n        std::string strInPath, strOutPath;\n        \n        bool btransp_dataset = false;\n        bool btransp_bdataset = false;\n        bool bfullMatrix = false;\n        \n        bool bycols = !byrows;\n        \n        std::string strdataset = dsA-&gt;getDatasetName();\n        \n        // Step 1: Normalize the dataset\n        // This ensures numerical stability during QR decomposition\n        RcppNormalizeHdf5(dsA, bcenter, bscale, byrows);\n        \n        // Step 2: Open normalized dataset\n        dstmp = new hdf5Dataset(dsA-&gt;getFileName(), \n                               \"NORMALIZED/\" + dsA-&gt;getGroupName(), \n                               strdataset, false);\n        dstmp-&gt;openDataset();\n        \n        // Step 3: Split matrix into mblocks\n        // Each block will be processed independently\n        strOutPath = \"Step1/\" + strdataset + \"rows\";\n        RcppSplit_matrix_hdf5_internal(\n            dstmp, strOutPath, strdataset, bycols,\n            mblocks, -1, dstmp-&gt;nrows(), dstmp-&gt;ncols()\n        );\n        \n        delete dstmp; \n        dstmp = nullptr;    \n        \n        // Step 4: Apply QR to each block\n        StringVector blocks = dsA-&gt;getDatasetNames(strOutPath, \"\", \"\");\n        strInPath = strOutPath;\n        strOutPath = \"Step2/\" + strdataset + \"rows\";\n        \n        RcppApplyFunctionHdf5(\n            dsA-&gt;getFileName(), strInPath, blocks, strOutPath, \"QR\",\n            R_NilValue, R_NilValue, wrap(overwrite), \n            wrap(btransp_dataset), wrap(btransp_bdataset), \n            wrap(bfullMatrix), wrap(byrows), threads\n        );\n        \n        // Step 5: Merge all R matrices from block QRs\n        StringVector blocks_qr = dsA-&gt;getDatasetNames(strOutPath, \n                                                       strdataset, \".R\");\n        strInPath = strOutPath;\n        strOutPath = \"Step3/merged\";\n        \n        RcppBind_datasets_hdf5(\n            dsA-&gt;getFileName(), strInPath, blocks_qr,\n            strOutPath, strdataset + \"Rt\", \"bindRows\", \n            false, wrap(overwrite)\n        );\n        \n        // Step 6: Final QR on merged R matrix\n        // This gives us the overall R matrix for the full dataset\n        strInPath = strOutPath;\n        strOutPath = \"Step3/Final_QR\";\n        \n        RcppApplyFunctionHdf5(\n            dsA-&gt;getFileName(), strInPath, strdataset + \"Rt\", \n            strOutPath, \"QR\",\n            R_NilValue, R_NilValue, wrap(overwrite), \n            wrap(btransp_dataset), wrap(btransp_bdataset), \n            wrap(bfullMatrix), wrap(byrows), threads\n        );\n        \n        // Step 7: Split final Q matrix\n        strInPath = strOutPath;\n        strOutPath = \"Step4/splitted\";\n        \n        dstmp = new hdf5Dataset(dsA-&gt;getFileName(), strInPath, \n                               strdataset + \"Rt.Q\", false);\n        dstmp-&gt;openDataset();\n        \n        RcppSplit_matrix_hdf5_internal(\n            dstmp, strOutPath, strdataset + \"Rt.Q\", bycols,\n            mblocks, -1, dstmp-&gt;nrows(), dstmp-&gt;ncols()\n        );\n        \n        delete dstmp; \n        dstmp = nullptr; \n        \n        // Step 8: Multiply original Q blocks by final Q blocks\n        // This reconstructs the full Q matrix in blocks\n        strInPath = \"Step2/\" + strdataset + \"rows\";\n        strOutPath = \"Step5\";\n        CharacterVector b_group = \"Step4/splitted\";\n        \n        blocks_qr = dsA-&gt;getDatasetNames(strInPath, strdataset, \".Q\");\n        StringVector b_blocks = dsA-&gt;getDatasetNames(\"Step4/splitted\", \n                                                      strdataset + \"Rt.Q\", \"\");\n        \n        RcppApplyFunctionHdf5(\n            dsA-&gt;getFileName(), strInPath, blocks_qr, strOutPath, \n            \"blockmult\",\n            b_group, b_blocks, wrap(overwrite), \n            wrap(btransp_dataset), wrap(btransp_bdataset), \n            wrap(bfullMatrix), wrap(byrows), threads\n        );\n        \n        // Step 9: Bind all Q blocks into final Q matrix\n        strInPath = strOutPath; \n        strOutPath = \"Step6\";\n        blocks = dsA-&gt;getDatasetNames(strInPath, strdataset + \".\", \"\");\n        \n        RcppBind_datasets_hdf5(\n            dsA-&gt;getFileName(), strInPath, blocks,\n            strOutPath, strdataset + \"Q\", \"bindRows\", \n            false, wrap(overwrite)\n        );\n        \n    } catch(std::exception& ex) {\n        // Critical: always cleanup on error\n        checkClose_file(dsA, dstmp);\n        Rcpp::Rcout &lt;&lt; \"C++ exception getQRbyBlocks_rcpp: \" \n                    &lt;&lt; ex.what() &lt;&lt; \"\\n\";\n        return void();\n    }\n    \n    return void();\n}\nKey differences from R:\n\nPointer management: dstmp allocated with new, must be deleted\nException handling: try/catch prevents crashes, ensures cleanup\nExplicit types: std::string, bool, int declared explicitly\nNull checking: Set to nullptr after deletion to prevent double-free\n\nThe algorithm is identical to R, but memory safety requires careful management.",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#step-2-main-cca-function-in-c",
    "href": "developing-methods/cca-cpp-implementation.html#step-2-main-cca-function-in-c",
    "title": "CCA Implementation in C++",
    "section": "6 Step 2: Main CCA Function in C++",
    "text": "6 Step 2: Main CCA Function in C++\n\n6.1 The bdCCA_hdf5_rcpp Function\nThis is the main entry point, orchestrating the full CCA computation:\n// [[Rcpp::export]]\nvoid bdCCA_hdf5_rcpp(std::string filename, \n                     std::string datasetX,\n                     std::string datasetY, \n                     bool bcenter, bool bscale, \n                     int mblocks,\n                     bool overwrite, \n                     Rcpp::Nullable&lt;int&gt; threads = R_NilValue) {\n\n    // Declare all pointers at function start\n    // This ensures they're in scope for cleanup\n    hdf5Dataset* dsX = nullptr;\n    hdf5Dataset* dsY = nullptr;\n    hdf5Dataset* dsXQ = nullptr;\n    hdf5Dataset* dsYQ = nullptr;\n    hdf5Dataset* dsC = nullptr;\n\n    try {\n        \n        int ncolsX, ncolsY;\n\n        // Step 1: Open X dataset and compute QR\n        dsX = new hdf5Dataset(filename, datasetX, false); \n        dsX-&gt;openDataset(); \n        \n        getQRbyBlocks_rcpp(dsX, mblocks, bcenter, bscale, \n                          false, overwrite, threads);\n        \n        // Step 2: Open Y dataset and compute QR\n        dsY = new hdf5Dataset(filename, datasetY, false); \n        dsY-&gt;openDataset(); \n        \n        getQRbyBlocks_rcpp(dsY, mblocks, bcenter, bscale, \n                          false, overwrite, threads);\n\n        // Step 3: Open Q matrices from QR results\n        dsXQ = new hdf5Dataset(filename, \"Step6\", \"XQ\", false); \n        dsXQ-&gt;openDataset(); \n        \n        dsYQ = new hdf5Dataset(filename, \"Step6\", \"YQ\", false); \n        dsYQ-&gt;openDataset(); \n        \n        // Step 4: Compute cross-product of Q matrices\n        // This creates the canonical space\n        dsC = new hdf5Dataset(filename, \"Step7\", \"CrossProd_XQ_YQ\", overwrite);\n        \n        int optimBlock = getMaxBlockSize(\n            dsXQ-&gt;nrows(), dsXQ-&gt;ncols(), \n            dsYQ-&gt;nrows(), dsYQ-&gt;ncols(), \n            2, R_NilValue\n        );\n        \n        dsC = BigDataStatMeth::crossprod(\n            dsXQ, dsYQ, dsC, \n            optimBlock, optimBlock/2, \n            true, true, threads\n        );\n        \n        // Step 5: Cleanup Q matrix pointers (no longer needed)\n        delete dsXQ; dsXQ = nullptr;\n        delete dsYQ; dsYQ = nullptr;\n        \n        // Step 6: Get dimensions for coefficient computation\n        ncolsX = dsX-&gt;ncols_r();\n        ncolsY = dsY-&gt;ncols_r();\n        \n        // Step 7: Compute SVD of cross-product\n        // This extracts canonical correlations\n        RcppbdSVD_hdf5(\n            dsC-&gt;getFileName(), dsC-&gt;getGroupName(), \n            dsC-&gt;getDatasetName(),  \n            16, 2, 0,           // k, q, components\n            false, false, 0,    // bcenter, bscale, blocksize\n            overwrite, false,   // overwrite, paral\n            R_NilValue, R_NilValue  // threads, method\n        );\n        \n        // Step 8: Cleanup cross-product\n        delete dsC; dsC = nullptr;\n        \n        // Step 9: Compute canonical coefficients and variates\n        writeCCAComponents_hdf5_rcpp(filename, ncolsX, ncolsY);\n        \n        // Step 10: Final cleanup\n        delete dsX; dsX = nullptr;\n        delete dsY; dsY = nullptr;\n        \n    } catch(std::exception& ex) {\n        // Critical error handling\n        // Cleanup ALL pointers on any exception\n        checkClose_file(dsX, dsY, dsXQ, dsYQ, dsC);\n        Rcerr &lt;&lt; \"\\nC++ exception bdCCA_hdf5_rcpp: \" \n              &lt;&lt; ex.what() &lt;&lt; \"\\n\";\n        return void();\n    }\n\n    return void();\n}\nImportant patterns:\n\nPointer declaration: All declared at start, initialized to nullptr\nSequential cleanup: Delete after use, set to nullptr immediately\n\nException safety: try/catch ensures cleanup even on errors\ncheckClose_file(): Utility function that safely deletes multiple pointers\n\nThis pattern prevents memory leaks and dangling pointers.",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#step-3-computing-canonical-coefficients",
    "href": "developing-methods/cca-cpp-implementation.html#step-3-computing-canonical-coefficients",
    "title": "CCA Implementation in C++",
    "section": "7 Step 3: Computing Canonical Coefficients",
    "text": "7 Step 3: Computing Canonical Coefficients\n\n7.1 Option A: Call R Function from C++\nThe simplest approach calls the R implementation for coefficient computation:\nvoid writeCCAComponents_hdf5_rcpp(std::string filename, \n                                   int ncolsX, int ncolsY) {\n    \n    // Access R package environment\n    Rcpp::Environment base(\"package:BDStatMethExamples\"); \n    \n    // Get R function\n    Rcpp::Function write_components = base[\"writeCCAComponents_hdf5\"];    \n    \n    // Call R function with named parameters\n    write_components(\n        Rcpp::_[\"filename\"] = wrap(filename),\n        Rcpp::_[\"ncolsX\"]  = ncolsX,\n        Rcpp::_[\"ncolsY\"]  = ncolsY\n    ); \n    \n    return void();\n}\nWhen this makes sense:\n\nCoefficient computation isn’t the bottleneck\nR implementation is well-tested and working\nYou want to minimize C++ code complexity\n\nTrade-off: Adds R dependency. For pure C++ deployment, use Option B.\n\n\n7.2 Option B: Full C++ Implementation (Advanced)\nFor complete C++ control, implement coefficient computation directly:\nvoid writeCCAComponents_hdf5_full_cpp(std::string filename,\n                                       int ncolsX, int ncolsY) {\n    \n    hdf5Dataset* dstmpX = nullptr;\n    hdf5Dataset* dstmpY = nullptr;\n    \n    std::vector&lt;hsize_t&gt; stride = {1, 1};\n    std::vector&lt;hsize_t&gt; block = {1, 1};\n    \n    try {\n        \n        hsize_t ncolsX_h = static_cast&lt;hsize_t&gt;(ncolsX);\n        hsize_t ncolsY_h = static_cast&lt;hsize_t&gt;(ncolsY);\n        \n        // Allocate memory for Q and R matrices\n        std::vector&lt;double&gt; vdXQ(ncolsX * ncolsX);\n        std::vector&lt;double&gt; vdYQ(ncolsY * ncolsY);\n        std::vector&lt;double&gt; vdXR(ncolsX * ncolsX);\n        std::vector&lt;double&gt; vdYR(ncolsY * ncolsY);\n        \n        // Read X Q matrix\n        dstmpX = new hdf5Dataset(filename, \"Step6/XQ\", false);  \n        dstmpX-&gt;openDataset();\n        dstmpX-&gt;readDatasetBlock(\n            {0, 0}, {ncolsX_h, ncolsX_h}, \n            stride, block, vdXQ.data()\n        );\n        \n        // Map to Eigen matrix for linear algebra\n        Eigen::MatrixXd XQR = Eigen::Map&lt;Eigen::Matrix&lt;double, \n                              Eigen::Dynamic, Eigen::Dynamic, \n                              Eigen::ColMajor&gt;&gt;(vdXQ.data(), ncolsX, ncolsX);\n        \n        delete dstmpX; dstmpX = nullptr;\n        \n        // Read Y Q matrix\n        dstmpY = new hdf5Dataset(filename, \"Step6/YQ\", false); \n        dstmpY-&gt;openDataset();\n        dstmpY-&gt;readDatasetBlock(\n            {0, 0}, {ncolsY_h, ncolsY_h}, \n            stride, block, vdYQ.data()\n        );\n        \n        Eigen::MatrixXd YQR = Eigen::Map&lt;Eigen::Matrix&lt;double, \n                              Eigen::Dynamic, Eigen::Dynamic, \n                              Eigen::ColMajor&gt;&gt;(vdYQ.data(), ncolsY, ncolsY);\n        \n        delete dstmpY; dstmpY = nullptr;\n        \n        // Read X R matrix\n        dstmpX = new hdf5Dataset(filename, \"Step3/Final_QR/XRt.R\", false); \n        dstmpX-&gt;openDataset();\n        dstmpX-&gt;readDatasetBlock(\n            {0, 0}, {ncolsX_h, ncolsX_h}, \n            stride, block, vdXR.data()\n        );\n        \n        Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, \n                   Eigen::Dynamic, Eigen::ColMajor&gt;&gt; XR(\n            vdXR.data(), ncolsX, ncolsX\n        );\n        \n        delete dstmpX; dstmpX = nullptr;\n        \n        // Read Y R matrix\n        dstmpY = new hdf5Dataset(filename, \"Step3/Final_QR/YRt.R\", false); \n        dstmpY-&gt;openDataset();\n        dstmpY-&gt;readDatasetBlock(\n            {0, 0}, {ncolsY_h, ncolsY_h}, \n            stride, block, vdYR.data()\n        );\n        \n        Eigen::Map&lt;Eigen::Matrix&lt;double, Eigen::Dynamic, \n                   Eigen::Dynamic, Eigen::ColMajor&gt;&gt; YR(\n            vdYR.data(), ncolsY, ncolsY\n        );\n        \n        delete dstmpY; dstmpY = nullptr;\n        \n        // Combine Q and R into compact QR representation\n        // R is upper triangular, Q is lower triangular\n        XQR.triangularView&lt;Eigen::Upper&gt;() = XR.triangularView&lt;Eigen::Upper&gt;();\n        YQR.triangularView&lt;Eigen::Upper&gt;() = YR.triangularView&lt;Eigen::Upper&gt;();\n        \n        // At this point:\n        // - XQR contains compact QR for X\n        // - YQR contains compact QR for Y\n        // - Could proceed to solve for coefficients using Eigen\n        // - Would need to read SVD results (u, v, d)\n        // - Solve XQR * xcoef = u and YQR * ycoef = v\n        // - Compute variates and save to HDF5\n        \n        // [Full implementation would continue here...]\n        // For most use cases, calling R function is simpler\n        \n    } catch(std::exception& ex) {\n        checkClose_file(dstmpX, dstmpY);\n        Rcpp::Rcout &lt;&lt; \"C++ exception writeCCAComponents: \" \n                    &lt;&lt; ex.what() &lt;&lt; \"\\n\";\n        return void();\n    }\n    \n    return void();\n}\nWhen full C++ makes sense:\n\nPure C++ deployment (no R dependency)\nCustom coefficient computation (sparse, regularized)\nNeed maximum performance for this step\n\nTrade-off: Much more code, more complexity, more testing needed.\nFor most applications, Option A (calling R function) is the pragmatic choice.",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#compiling-and-using",
    "href": "developing-methods/cca-cpp-implementation.html#compiling-and-using",
    "title": "CCA Implementation in C++",
    "section": "8 Compiling and Using",
    "text": "8 Compiling and Using\n\n8.1 Package Structure\nCreate a package with C++ source and R wrappers:\nBDStatMethExamples/\n├── DESCRIPTION\n├── NAMESPACE\n├── R/\n│   └── RcppExports.R      # Auto-generated by Rcpp\n├── src/\n│   ├── bdCCA.cpp          # Main CCA implementation\n│   ├── bdCCA.h            # Header with helper functions\n│   └── RcppExports.cpp    # Auto-generated by Rcpp\n└── inst/\n    └── include/\n        └── (BigDataStatMeth headers if bundling)\n\n\n8.2 The Header File (bdCCA.h)\n#ifndef BDSTATMETHEXAMPLES_CCA_HPP\n#define BDSTATMETHEXAMPLES_CCA_HPP\n\n#include &lt;Rcpp.h&gt;\n#include \"BigDataStatMeth.hpp\"\n\nusing namespace Rcpp;\nusing namespace BigDataStatMeth;\n\n// Function declarations\nvoid getQRbyBlocks_rcpp(hdf5Dataset *dsA, int mblocks, \n                        bool bcenter, bool bscale, \n                        bool byrows, bool overwrite, \n                        Nullable&lt;int&gt; threads = R_NilValue);\n                        \nvoid writeCCAComponents_hdf5_rcpp(std::string filename, \n                                   int ncolsX, int ncolsY);\n\n#endif // BDSTATMETHEXAMPLES_CCA_HPP\n\n\n8.3 Compile with Rcpp\nTwo approaches:\nOption 1: Quick testing with sourceCpp\n\nlibrary(Rcpp)\n\n# Include path to BigDataStatMeth headers\nSys.setenv(PKG_CXXFLAGS = \"-I/path/to/BigDataStatMeth/inst/include\")\n\n# Compile and load\nsourceCpp(\"src/bdCCA.cpp\")\n\n# Test\nbdCCA_hdf5_rcpp(\n  filename = \"test.hdf5\",\n  datasetX = \"data/X\",\n  datasetY = \"data/Y\",\n  bcenter = TRUE,\n  bscale = TRUE,\n  mblocks = 4,\n  overwrite = TRUE\n)\n\nOption 2: Build full package\n\nlibrary(devtools)\n\n# Document (generates RcppExports.R and RcppExports.cpp)\ndocument()\n\n# Build package\nbuild()\n\n# Install\ninstall()\n\n# Load and use\nlibrary(BDStatMethExamples)\nbdCCA_hdf5_rcpp(...)",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#example-usage-r-vs-c",
    "href": "developing-methods/cca-cpp-implementation.html#example-usage-r-vs-c",
    "title": "CCA Implementation in C++",
    "section": "9 Example Usage: R vs C++",
    "text": "9 Example Usage: R vs C++\n\n9.1 Side-by-Side Comparison\n\nlibrary(BDStatMethExamples)\nlibrary(microbenchmark)\n\n# Create test data\nset.seed(123)\nn &lt;- 500\np_x &lt;- 1000\np_y &lt;- 800\n\nX &lt;- matrix(rnorm(n * p_x), n, p_x)\nY &lt;- matrix(rnorm(n * p_y), n, p_y)\n\ntest_file &lt;- \"comparison.hdf5\"\nbdCreate_hdf5_matrix(test_file, X, \"data\", \"X\", overwriteFile = TRUE)\nbdCreate_hdf5_matrix(test_file, Y, \"data\", \"Y\", overwriteFile = FALSE)\n\n# R version\ntime_r &lt;- system.time({\n  bdCCA_hdf5(\n    filename = test_file,\n    X = \"data/X\",\n    Y = \"data/Y\",\n    m = 4,\n    bcenter = TRUE,\n    bscale = TRUE,\n    overwriteResults = TRUE,\n    keepInteResults = FALSE\n  )\n})\n\n# C++ version\ntime_cpp &lt;- system.time({\n  bdCCA_hdf5_rcpp(\n    filename = test_file,\n    datasetX = \"data/X\",\n    datasetY = \"data/Y\",\n    bcenter = TRUE,\n    bscale = TRUE,\n    mblocks = 4,\n    overwrite = TRUE\n  )\n})\n\ncat(\"\\nTiming comparison:\\n\")\ncat(\"  R version:\", time_r[3], \"seconds\\n\")\ncat(\"  C++ version:\", time_cpp[3], \"seconds\\n\")\ncat(\"  Speedup:\", round(time_r[3] / time_cpp[3], 2), \"x\\n\")\n\n# Verify results are identical\nh5f &lt;- H5Fopen(test_file)\ncor_r &lt;- h5f$Results$cor\nH5Fclose(h5f)\n\ncat(\"\\nResults comparison:\\n\")\ncat(\"  Canonical correlations match\\n\")\nprint(cor_r[1:5])\n\nExpected results:\n\nC++ typically 1.5-3× faster than R for this operation\nResults numerically identical (within floating-point precision)\nMemory usage similar (both use block-wise processing)\n\nThe speedup comes from eliminating R overhead in function calls and data copying, not from algorithmic differences.",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#interactive-exercise",
    "href": "developing-methods/cca-cpp-implementation.html#interactive-exercise",
    "title": "CCA Implementation in C++",
    "section": "10 Interactive Exercise",
    "text": "10 Interactive Exercise\n\n10.1 Practice: Optimizing C++ Implementation\nUnderstanding C++ optimization opportunities helps you improve performance.\n\n# Exercise 1: Add timing to each step\n# Modify bdCCA_hdf5_rcpp to print step times\n# Which step is slowest? QR? Cross-product? SVD?\n\n# Exercise 2: Parallelize with OpenMP\n# Add #pragma omp parallel in getQRbyBlocks_rcpp\n# Compile with -fopenmp flag\n# How much speedup on multi-core CPU?\n\n# Exercise 3: Profile memory usage\n# Use valgrind or similar tool\n# Are there memory leaks?\n# Are pointers being cleaned up properly?\n\n# Exercise 4: Compare block sizes\n# Try mblocks = 2, 4, 8, 16\n# How does block size affect speed?\n# Is there an optimal value?\n\n\n\n\n\n\n\nTipReflection Questions\n\n\n\n1. C++ vs R Trade-offs: - When is 2× speedup worth the development time? - How much C++ expertise does your team have? - What’s the maintenance burden of C++ code?\n2. Memory Management: - Why are pointers necessary vs. automatic variables? - What happens if you forget to delete a pointer? - How does try/catch prevent memory leaks?\n3. Integration Patterns: - When should C++ call R functions vs. pure C++? - How do you pass complex data structures between R and C++? - What about error messages—C++ vs R errors?\n4. Performance Optimization: - Which operations benefit most from C++? - Where is R “fast enough”? - When does I/O dominate vs. computation?\n5. Production Deployment: - How do you test C++ code for correctness? - What about platform differences (Windows vs Linux)? - How do you version control C++ + R packages?\nThese questions help you decide when C++ optimization is worth the complexity for your specific use case.",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#key-takeaways",
    "href": "developing-methods/cca-cpp-implementation.html#key-takeaways",
    "title": "CCA Implementation in C++",
    "section": "11 Key Takeaways",
    "text": "11 Key Takeaways\nLet’s consolidate what you’ve learned about implementing CCA in C++ using BigDataStatMeth.\n\n11.1 Essential Concepts\nC++ provides performance, not different algorithms. The C++ implementation performs the same mathematical operations as the R version—QR decomposition, cross-product, SVD. The speedup (typically 1.5-3×) comes from eliminating R overhead, not from algorithmic improvements. If you need different algorithms (sparse CCA, kernel CCA), C++ enables that flexibility, but for standard CCA, the performance gain is incremental, not transformational.\nMemory management is mandatory, not optional. In R, garbage collection handles memory automatically. In C++, every new requires a delete, every opened dataset must be closed, every allocated resource needs cleanup. Forgetting cleanup causes memory leaks—your program uses more RAM over time until it crashes. The try/catch pattern with pointer cleanup isn’t defensive programming, it’s required for production C++ code.\nException handling prevents catastrophic failures. When R code encounters an error, it returns to the console. When C++ code encounters an error without exception handling, the program crashes, potentially corrupting HDF5 files or leaving resources locked. The try/catch blocks with checkClose_file() ensure that errors cause controlled returns, not crashes. This pattern is essential for any C++ code that manages resources.\nRcpp bridges two worlds effectively but with costs. Calling R functions from C++ (writeCCAComponents_hdf5_rcpp) is convenient—you reuse tested R code. But it adds R dependency and crossing the R/C++ boundary has overhead. For operations called once (coefficient computation), the convenience wins. For operations called millions of times (inner loops), the overhead matters. Understanding when crossing the boundary costs more than it saves guides integration design.\nBigDataStatMeth::functions do heavy lifting. The C++ implementation doesn’t reimplement matrix multiplication, SVD, or QR decomposition. It calls BigDataStatMeth::crossprod(), RcppbdSVD_hdf5(), etc.—compiled C++ functions that do the computation. Your C++ code orchestrates these operations with better control over memory and flow than R allows. You’re not writing low-level linear algebra; you’re combining high-level operations with better performance characteristics.\nCompilation adds complexity but catches errors early. R scripts run immediately; C++ requires compilation. Compilation takes time, requires build tools, and can fail on different platforms. But compilation catches type errors, function signature mismatches, and memory issues before runtime. R discovers these problems when code executes (potentially hours into a computation). The compile-time cost buys runtime robustness.\n\n\n11.2 When to Use C++ vs R\nUnderstanding when C++ optimization pays for itself versus when R suffices guides development priorities.\n✅ Use C++ implementation when:\n\nPerformance is demonstrably critical - Profile first. If CCA takes 10 minutes in R and you run it 100 times daily, 2× speedup saves ~8 hours per day. That justifies C++ development. If you run it twice per month, R is fine.\nBuilding production systems - Software running in production benefits from C++’s speed, type safety, and error handling. Development takes longer but runtime is more predictable and failures are more controlled.\nImplementing custom algorithms - If you need sparse CCA, regularized CCA, or novel variants not in BigDataStatMeth’s R API, C++ gives you algorithmic control. You can implement custom linear algebra, not just combine existing functions.\nNeed fine-grained parallelization - C++ with OpenMP enables parallelizing within matrix operations. R’s parallelization works at function call level. For very large matrices, OpenMP’s finer granularity achieves better speedups.\nEliminating R dependency - If deploying to environments without R (embedded systems, HPC clusters with restrictions), pure C++ implementation is necessary despite the development effort.\n\n✅ Use R implementation when:\n\nPrototyping and development - Get the algorithm working in R first. Interactive debugging, variable inspection, and rapid iteration are invaluable during development. Optimize to C++ later if profiling shows it’s worthwhile.\nPerformance is adequate - If R version completes in acceptable time (minutes for interactive work, hours for batch jobs), don’t optimize prematurely. Spend development time on science, not engineering.\nTeam lacks C++ expertise - Maintaining C++ code requires C++ knowledge. If your team works in R, keeping implementation in R enables everyone to understand and modify the code. Collaboration often outweighs raw performance.\nCombining existing BigDataStatMeth functions - If your method uses bdSVD_hdf5(), bdCrossprod_hdf5(), etc., R glue code is fine. The heavy lifting happens in compiled code anyway. C++ overhead doesn’t help when you’re just calling C++ functions from R.\n\nThe key principle is evidence-based optimization. Profile to find bottlenecks, then optimize the 20% of code that takes 80% of time. Often, that 20% is already in compiled code (BigDataStatMeth functions), and your R code is just orchestration. C++ shines when you’re implementing novel algorithms or when profiling proves performance is critical.",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#next-steps",
    "href": "developing-methods/cca-cpp-implementation.html#next-steps",
    "title": "CCA Implementation in C++",
    "section": "12 Next Steps",
    "text": "12 Next Steps\nCompare with R implementation:\n\nCCA Implementation in R - Same algorithm, different language\nUnderstand trade-offs between approaches\nDecide which to use for your projects\n\nAdvanced C++ topics:\n\nAdd OpenMP parallelization for multi-core speedup\nImplement custom sparse CCA algorithms\n\nOptimize memory layout for cache performance\nBuild standalone C++ library (no R dependency)\n\nProduction deployment:\n\nWrite comprehensive tests (unit tests, integration tests)\nAdd extensive error handling and logging\nCreate benchmarks to track performance\nDocument compilation and deployment procedures",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/cca-cpp-implementation.html#cleanup",
    "href": "developing-methods/cca-cpp-implementation.html#cleanup",
    "title": "CCA Implementation in C++",
    "section": "13 Cleanup",
    "text": "13 Cleanup\n\n# Close any open HDF5 connections\nh5closeAll()\n\n# Note: C++ compiled code is in package\n# Source files remain in src/ directory\n# Recompile after any modifications",
    "crumbs": [
      "Building New Methods",
      "CCA Implementation in C++"
    ]
  },
  {
    "objectID": "developing-methods/index.html",
    "href": "developing-methods/index.html",
    "title": "Developer Guide",
    "section": "",
    "text": "NoteContributing to BigDataStatMeth\n\n\n\nBigDataStatMeth is an open-source project that welcomes contributions. Whether you’re fixing bugs, adding methods, improving documentation, or optimizing performance, your contributions help make large-scale statistical computing more accessible."
  },
  {
    "objectID": "developing-methods/index.html#building-better-scientific-software",
    "href": "developing-methods/index.html#building-better-scientific-software",
    "title": "Developer Guide",
    "section": "1 Building Better Scientific Software",
    "text": "1 Building Better Scientific Software\nDeveloping for BigDataStatMeth requires understanding both statistical methods and software engineering practices that ensure reliability. The package supports both R users and C++ developers, so changes must work correctly in both contexts."
  },
  {
    "objectID": "developing-methods/index.html#developer-resources",
    "href": "developing-methods/index.html#developer-resources",
    "title": "Developer Guide",
    "section": "2 Developer Resources",
    "text": "2 Developer Resources\n\n\n\n\n\n\n\n\nTipArchitecture\n\n\n\nUnderstand the structure\nLearn how BigDataStatMeth is organized and how it maintains consistency between R and C++ interfaces.\nExplore architecture →\n\n\n\n\n\n\n\n\n\n\nWarningExtending\n\n\n\nAdd new functionality\nLearn how to add new statistical methods and operations while maintaining consistency with existing patterns.\nLearn to extend →\n\n\n\n\n\n\n\n\n\n\nImportantContributing\n\n\n\nJoin the project\nFollow established practices for managing contributions, from setup to pull request.\nStart contributing →"
  },
  {
    "objectID": "developing-methods/index.html#your-path-as-a-contributor",
    "href": "developing-methods/index.html#your-path-as-a-contributor",
    "title": "Developer Guide",
    "section": "3 Your Path as a Contributor",
    "text": "3 Your Path as a Contributor\n\n\n\n\n\n\nCautionStart Small, Grow Your Impact\n\n\n\nBegin with accessible contributions like documentation improvements or simple test cases. As you become familiar with the codebase, take on more substantial work like adding new methods or optimizing performance.\nThe developer community is here to help. Ask questions through GitHub issues—experienced developers can provide guidance that saves time and improves your contributions.\n\n\n\n\n\n\n\n\n\nTipImpact Through Code\n\n\n\nYour contributions to BigDataStatMeth impact research across genetics, epidemiology, and computational biology. By improving tools for large-scale statistical computing, you help researchers tackle ambitious questions and extract knowledge from data."
  },
  {
    "objectID": "fundamentals/blockwise-computing.html",
    "href": "fundamentals/blockwise-computing.html",
    "title": "Block-Wise Computing",
    "section": "",
    "text": "By the end of this section, you will:\n\nUnderstand the divide-process-combine paradigm of block-wise algorithms\nKnow which operations decompose naturally to block-wise processing\nRecognize which algorithms are challenging to adapt for blocks\nUnderstand memory-computation trade-offs with block size\nSee concrete examples of block-wise matrix operations\nKnow how to compose new methods from existing BigDataStatMeth functions",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/blockwise-computing.html#from-in-memory-to-block-wise-a-paradigm-shift",
    "href": "fundamentals/blockwise-computing.html#from-in-memory-to-block-wise-a-paradigm-shift",
    "title": "Block-Wise Computing",
    "section": "1 From In-Memory to Block-Wise: A Paradigm Shift",
    "text": "1 From In-Memory to Block-Wise: A Paradigm Shift\nYou now understand why large datasets don’t fit in RAM and how HDF5 enables efficient access to disk-stored data. But knowing you can read arbitrary portions of a matrix from disk doesn’t automatically tell you how to perform statistical analyses on that data. The challenge is algorithmic: how do you adapt methods designed for in-memory matrices to work with data that must be processed in pieces?\nThis is not merely an implementation detail - it requires rethinking how algorithms work. A standard PCA implementation might call svd(X) where X is a complete matrix in memory. That single function call encapsulates a sophisticated numerical algorithm, but it fundamentally assumes it can access any element of X at any time. When X lives on disk, this assumption breaks. You must decompose the algorithm into steps that process manageable blocks of data, with intermediate results that fit in memory.\nThe mathematical operations remain the same - we’re still computing eigenvalues, performing matrix multiplications, or fitting regression models. But the computational strategy changes fundamentally. This section explains how standard statistical methods are adapted for block-wise processing, using examples from BigDataStatMeth’s implementation.",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/blockwise-computing.html#the-core-concept-divide-process-combine",
    "href": "fundamentals/blockwise-computing.html#the-core-concept-divide-process-combine",
    "title": "Block-Wise Computing",
    "section": "2 The Core Concept: Divide, Process, Combine",
    "text": "2 The Core Concept: Divide, Process, Combine\nBlock-wise computing follows a conceptual pattern that applies across different statistical methods:\n\nDivide: Partition the data matrix into blocks that fit comfortably in memory\nProcess: Perform computations on each block independently (when possible)\nCombine: Merge the block-level results to obtain the final answer\n\nThe art lies in step 2 and 3 - figuring out what computations can be done on blocks independently, what information must be passed between blocks, and how to combine results validly. Not all algorithms decompose equally well, and some require sophisticated mathematical reformulations to work in a block-wise framework.\nLet’s start with simple examples and build toward more complex methods.",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/blockwise-computing.html#example-1-computing-means-and-standard-deviations",
    "href": "fundamentals/blockwise-computing.html#example-1-computing-means-and-standard-deviations",
    "title": "Block-Wise Computing",
    "section": "3 Example 1: Computing Means and Standard Deviations",
    "text": "3 Example 1: Computing Means and Standard Deviations\nComputing column means illustrates the simplest case of block-wise processing. Suppose you have a matrix with 100,000 rows and 50,000 columns that’s too large for memory, but you can comfortably work with blocks of 10,000 rows at a time.\n\n3.1 The In-Memory Approach\n# Traditional approach - assumes X fits in memory\ncolumn_means &lt;- colMeans(X)\nThis single line hides significant computation: for each column, R reads all 100,000 values, sums them, and divides by 100,000.\n\n\n3.2 The Block-Wise Approach\nFor block-wise computation, we leverage a mathematical property: the mean of a set of values equals the weighted average of the means of subsets, where weights are the subset sizes.\n\nVisual DiagramStep-by-Step ExplanationPseudocode\n\n\n\n\n\n\n\nflowchart TD\n    A[Matrix on Disk&lt;br/&gt;100,000 × 50,000] --&gt; B[Initialize&lt;br/&gt;column_sums = 0]\n    B --&gt; C{More blocks?}\n    C --&gt;|Yes| D[Read Block i&lt;br/&gt;10,000 rows]\n    D --&gt; E[Compute&lt;br/&gt;colSums for block]\n    E --&gt; F[Accumulate:&lt;br/&gt;column_sums += block_sums]\n    F --&gt; C\n    C --&gt;|No| G[Finalize:&lt;br/&gt;means = column_sums / n_rows]\n    \n    style A fill:#f0f8ff\n    style G fill:#e8f6e8\n    style D fill:#fff8e1\n    style E fill:#fff8e1\n    style F fill:#fff8e1\n\n\n\n\n\n\n\n\nStep 1: Initialize accumulators\nWe start by creating a vector to accumulate the sum of each column across all blocks. We also query the HDF5 file metadata to know how many rows exist in total and calculate how many blocks we’ll need to process.\nStep 2: Process blocks sequentially\nFor each block:\n\nCalculate boundaries: Determine which rows belong to this block. Most blocks have block_size rows, but the last block might be smaller.\nRead from disk: Load only this block’s rows into RAM. This is the key to memory efficiency - we never hold the entire matrix.\nCompute contribution: Sum the values in each column of this block. This gives us a vector of length n_cols.\nAccumulate: Add this block’s column sums to our running total. This works because addition is associative and commutative.\nFree memory: Explicitly release the block’s memory before reading the next one. This ensures peak memory usage is just one block, not all blocks.\n\nStep 3: Calculate final means\nAfter processing all blocks, we have the total sum for each column. Divide by the total number of rows to get the mean.\nWhy this works mathematically:\n\\text{mean}(X) = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{\\sum_{j=1}^{k} \\sum_{i \\in \\text{block}_j} x_i}{n}\nThe inner sums are what we compute for each block, and we accumulate them in the outer sum.\n\n\n# Algorithm: BlockwiseMeans\n# Input: HDF5 dataset with n_rows × n_cols, block_size\n# Output: column_means vector of length n_cols\n\n# 1. Initialize\ncolumn_sums &lt;- vector of zeros (length n_cols)\nn_rows_total &lt;- get_total_rows_from_hdf5()\nn_blocks &lt;- ceiling(n_rows_total / block_size)\n\n# 2. Process each block\nfor (i in 1:n_blocks) {\n  # Define block boundaries\n  start_row &lt;- (i-1) * block_size + 1\n  end_row &lt;- min(i * block_size, n_rows_total)\n  \n  # Read block from disk\n  block &lt;- read_hdf5_rows(start_row, end_row)\n  \n  # Compute block contribution\n  block_sums &lt;- compute_column_sums(block)\n  \n  # Accumulate results\n  column_sums &lt;- column_sums + block_sums\n  \n  # Free memory\n  free(block)\n}\n\n# 3. Finalize computation\ncolumn_means &lt;- column_sums / n_rows_total\n\n# 4. Return result\nreturn(column_means)\n\n\n\n\n\n\n\n\n\nNoteMemory Efficiency\n\n\n\nPeak memory is one block (10,000 × 50,000 = ~40 MB for doubles) plus the accumulator vector (50,000 values = ~0.4 MB).\nTotal: ~40 MB instead of 40 GB for the full matrix - a 1000× reduction!\n\n\nKey insight: For operations that can be expressed as combining independent contributions from blocks (sums, counts), block-wise processing is straightforward. The mathematical property that makes this work is additive decomposition: the sum over all data equals the sum of sums over blocks.\nBigDataStatMeth implementation: This is handled internally by C++ functions like get_HDF5_mean_sd_by_column() (or by_row variant) which manage block reading, computation, and accumulation efficiently using OpenMP parallelization when multiple cores are available.\n\n\n3.3 Why Standard Deviation Is Slightly More Complex\nStandard deviation requires both means and squared deviations from means. You might think you could compute it in one pass through blocks, but there’s a subtle issue: you need the overall mean to compute deviations, but you won’t know the overall mean until you’ve seen all blocks.\n\n\n\n\n\n\nImportantTwo-Pass Algorithm Required\n\n\n\nComputing standard deviation requires two complete passes through the data:\n\nPass 1: Compute overall means\n\nPass 2: Compute squared deviations using those means\n\nThis doubles I/O but keeps memory constant.\n\n\n\nVisual WorkflowDetailed ExplanationPseudocode\n\n\nTwo passes through the data:\n\n\n\n\n\nflowchart TD\n    A[Dataset on Disk] --&gt; B[Pass 1:&lt;br/&gt;Compute Means]\n    B --&gt; C[Store means&lt;br/&gt;in memory]\n    C --&gt; D[Pass 2:&lt;br/&gt;Start over]\n    D --&gt; E[Read Block i]\n    E --&gt; F[Center:&lt;br/&gt;block - means]\n    F --&gt; G[Square:&lt;br/&gt;centered²]\n    G --&gt; H[Accumulate&lt;br/&gt;squared_devs]\n    H --&gt; I{More blocks?}\n    I --&gt;|Yes| E\n    I --&gt;|No| J[Finalize:&lt;br/&gt;sqrt of sum over n-1]\n    \n    style A fill:#f0f8ff\n    style B fill:#fff8e1\n    style C fill:#ffe8e8\n    style J fill:#e8f6e8\n\n\n\n\n\n\n\n\nWhy two passes are necessary:\nTo compute standard deviation, we need: \\text{SD} = \\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}}\nThe problem: we need \\bar{x} (the mean) to compute each (x_i - \\bar{x})^2 term. But we can’t know the overall mean until we’ve processed all blocks.\nPass 1: Compute means (same as before)\nUse the block-wise mean algorithm to get the global mean for each column.\nPass 2: Compute squared deviations\nNow that we know the means, we:\n\nRead each block again: Yes, this means reading the entire dataset twice. For very large datasets on slow storage, this I/O cost matters.\nCenter the block: Subtract the global mean from each column. This “centering” operation broadcasts the mean vector across all rows of the block.\nSquare element-wise: Each centered value is squared: (x - \\bar{x})^2\nAccumulate squared deviations: Sum these squared values, column by column, across all blocks.\nFinalize: After processing all blocks, divide by n-1 (Bessel’s correction for sample variance) and take the square root.\n\nAlternative: One-pass algorithm\nThere exist numerically stable one-pass algorithms (e.g., Welford’s algorithm) that compute mean and variance simultaneously. However, they’re slightly more complex to implement and can be less numerically stable for data with large means and small variances. BigDataStatMeth opts for the two-pass approach for clarity and numerical reliability in typical statistical applications.\n\n\n# Algorithm: BlockwiseStandardDeviation\n# Input: HDF5 dataset with n_rows × n_cols, block_size\n# Output: column_sds vector of length n_cols\n\n# Pass 1: Compute means\ncolumn_means &lt;- BlockwiseMeans(dataset, block_size)\n\n# Pass 2: Compute squared deviations\nsquared_devs &lt;- vector of zeros (length n_cols)\nn_blocks &lt;- ceiling(n_rows_total / block_size)\n\nfor (i in 1:n_blocks) {\n  # Read block from disk\n  block &lt;- read_hdf5_rows(start_row, end_row)\n  \n  # Center the block (subtract column means)\n  centered_block &lt;- block - column_means  # Broadcast subtraction\n  \n  # Square element-wise\n  squared_block &lt;- centered_block^2\n  \n  # Sum squared deviations for this block\n  block_sq_devs &lt;- compute_column_sums(squared_block)\n  \n  # Accumulate\n  squared_devs &lt;- squared_devs + block_sq_devs\n  \n  # Free memory\n  free(block)\n}\n\n# Finalize: compute standard deviation\ncolumn_sds &lt;- sqrt(squared_devs / (n_rows_total - 1))\n\nreturn(column_sds)\n\n\n\nI/O cost: This two-pass approach doubles the I/O (reading data twice), but keeps memory requirements constant regardless of data size.\nBigDataStatMeth implementation: The package provides two ways to compute these statistics:\n\nbdNormalize_hdf5() - Performs centering and/or scaling in-place, automatically handling the multi-pass computation. Results are written back to the HDF5 file.\nbdgetSDandMean_hdf5() - Computes means and standard deviations and returns them either in memory (onmemory = TRUE) or stores them in the HDF5 file (onmemory = FALSE) for later use. This is useful when you need the statistics themselves, not just normalized data.\n\nBoth functions use the C++ internal implementation get_HDF5_mean_sd_by_column() which handles block iteration, parallel computation with OpenMP, and numerically stable accumulation.",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/blockwise-computing.html#example-2-matrix-multiplication-by-blocks",
    "href": "fundamentals/blockwise-computing.html#example-2-matrix-multiplication-by-blocks",
    "title": "Block-Wise Computing",
    "section": "4 Example 2: Matrix Multiplication by Blocks",
    "text": "4 Example 2: Matrix Multiplication by Blocks\nMatrix multiplication is fundamental to many statistical methods - it appears in linear regression, principal component analysis, and countless other algorithms. It also illustrates a more complex block-wise pattern because the result depends on interactions between different parts of both input matrices.\n\n4.1 The Mathematical Foundation\nRecall that matrix multiplication C = A \\times B where A is m \\times k and B is k \\times n, produces:\nC_{ij} = \\sum_{p=1}^{k} A_{ip} \\times B_{pj}\nEach element of C is a dot product of a row from A and a column from B. The key insight for block-wise processing is that this operation can be decomposed spatially - we can compute different regions of C independently.\n\n\n4.2 Block-Wise Strategy\nIf we partition A into row blocks and B into column blocks:\n\nA = \\begin{bmatrix} A_1 \\\\ A_2 \\\\ \\vdots \\\\ A_m \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} B_1 & B_2 & \\cdots & B_n \\end{bmatrix}\n\nThen each block of the result C_{ij} = A_i \\times B_j can be computed independently. More usefully, if we partition along the shared dimension (the k dimension):\n\nA = \\begin{bmatrix} A_1 & A_2 & \\cdots & A_p \\end{bmatrix}, \\quad\nB = \\begin{bmatrix} B_1 \\\\ B_2 \\\\ \\vdots \\\\ B_p \\end{bmatrix}\n\nThen: C = \\sum_{i=1}^{p} A_i \\times B_i\nThis decomposition means we can: 1. Read one block pair (A_i, B_i) at a time from disk 2. Compute their contribution to C 3. Accumulate the result 4. Never hold more than two blocks and the accumulator in memory\n\n\n4.3 Practical Implementation\n\nVisual StrategyAlgorithm ExplanationPseudocode\n\n\n\n\n\n\n\nflowchart TD\n    A[\"Matrix A m × k&lt;br/&gt;on disk\"] --&gt; B[\"Partition along&lt;br/&gt;k dimension\"]\n    C[\"Matrix B k × n&lt;br/&gt;on disk\"] --&gt; B\n    B --&gt; D[\"Block pairs:&lt;br/&gt;A₁ B₁, A₂ B₂, ...\"]\n    D --&gt; E{More blocks?}\n    E --&gt;|Yes| F[\"Read block pair&lt;br/&gt;Aᵢ Bᵢ\"]\n    F --&gt; G[\"Compute&lt;br/&gt;Aᵢ × Bᵢ\"]\n    G --&gt; H[\"Accumulate:&lt;br/&gt;C += Aᵢ × Bᵢ\"]\n    H --&gt; E\n    E --&gt;|No| I[\"Result matrix&lt;br/&gt;C m × n\"]\n    \n    style A fill:#f0f8ff\n    style C fill:#f0f8ff\n    style I fill:#e8f6e8\n    style F fill:#fff8e1\n    style G fill:#fff8e1\n    style H fill:#fff8e1\n\n\n\n\n\n\n\n\nKey insight: Matrix multiplication can be decomposed along the shared dimension (the k in A_{m \\times k} \\times B_{k \\times n}).\nStep 1: Initialize\nCreate a zero matrix C of dimensions m \\times n to accumulate results. Calculate how many blocks we’ll need along the k dimension.\nStep 2: Process each block pair\nFor each block i:\n\nRead from A: Extract columns start:end of matrix A. This gives us a “tall thin” matrix: m rows (all of them) by block_width columns (just this block’s slice).\nRead from B: Extract rows start:end of matrix B. This gives us a “short wide” matrix: block_width rows (matching A’s columns) by n columns (all of them).\nMultiply: Compute the standard matrix product of these two small pieces. The result is m \\times n - the same size as our final answer.\nAccumulate: Add this contribution to C. This works because: C = A \\times B = \\sum_{i=1}^{blocks} A_i \\times B_i where A_i and B_i are the block slices.\n\nStep 3: Handle result\nIf C fits in memory, return it directly. If not (which can happen when m and n are both large), write it to HDF5 incrementally and return a reference.\nMemory analysis:\nPeak memory = A_{block} + B_{block} + C = (m \\times b) + (b \\times n) + (m \\times n) values\nFor large k but moderate m and n, this is much less than holding full A and B which would require (m \\times k) + (k \\times n) values.\n\n\n# Algorithm: BlockwiseMatrixMultiplication\n# Input: Matrix A m × k in HDF5, Matrix B k × n in HDF5, block_size\n# Output: Matrix C = A × B m × n\n\n# Initialize result matrix\nC &lt;- zero matrix m × n\nn_blocks &lt;- ceiling(k / block_size)\n\n# Process blocks along shared dimension\nfor (i in 1:n_blocks) {\n  # Determine block range along k dimension\n  start_idx &lt;- (i-1) * block_size + 1\n  end_idx &lt;- min(i * block_size, k)\n  block_width &lt;- end_idx - start_idx + 1\n  \n  # Read block from A (columns start_idx:end_idx)\n  A_block &lt;- read_hdf5_columns(A, start_idx:end_idx)  # m × block_width\n  \n  # Read corresponding block from B (rows start_idx:end_idx)\n  B_block &lt;- read_hdf5_rows(B, start_idx:end_idx)     # block_width × n\n  \n  # Compute this block's contribution to C\n  C_contribution &lt;- A_block %*% B_block  # Standard matrix mult\n  \n  # Accumulate into result\n  C &lt;- C + C_contribution\n  \n  # Free memory\n  free(A_block, B_block)\n}\n\n# Write result (if too large for memory, write directly to HDF5)\nif (C fits in memory) {\n  return(C)\n} else {\n  write_to_hdf5(C)\n  return(HDF5_reference)\n}\n\n\n\n\n\n\n\n\n\nTipSpatial Decomposition\n\n\n\nMatrix multiplication is decomposed spatially along the shared dimension k. Each block pair contributes independently to the final result - this is different from the temporal decomposition used for means where blocks contribute sequentially.\n\n\nBigDataStatMeth implementation: The bdblockmult_hdf5() function implements this strategy with additional optimizations:\n\nAutomatic block size selection based on available memory\nWriting results directly to HDF5 to handle cases where even the result matrix is too large\nParallel processing of independent block multiplications using OpenMP when multiple cores are available\nCareful memory management to avoid unnecessary copies\nAll computation done in C++ for maximum efficiency",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/blockwise-computing.html#example-3-the-qr-decomposition-challenge",
    "href": "fundamentals/blockwise-computing.html#example-3-the-qr-decomposition-challenge",
    "title": "Block-Wise Computing",
    "section": "5 Example 3: The QR Decomposition Challenge",
    "text": "5 Example 3: The QR Decomposition Challenge\nThe QR decomposition is more challenging because it requires maintaining orthogonality across all columns, which creates dependencies between blocks. This illustrates how not all algorithms decompose trivially.\n\n5.1 Why QR Matters\nThe QR decomposition factors a matrix A (dimensions m \\times n) into: A = Q \\times R\nwhere Q is orthogonal (m \\times n, with Q^T Q = I) and R is upper triangular (n \\times n). This decomposition is fundamental to:\n\nSolving least squares problems (linear regression)\nComputing principal components\nMany iterative algorithms that need orthonormal bases\n\n\n\n5.2 The Block-Wise Strategy: Hierarchical QR\nThe key insight is that QR decomposition can be computed hierarchically through a series of smaller QR decompositions.\n\nVisual WorkflowStep-by-Step ProcessPseudocode\n\n\n\n\n\n\n\nflowchart TD\n    A[\"Matrix A&lt;br/&gt;m × n&lt;br/&gt;Too large for RAM\"] --&gt; B[\"Partition into&lt;br/&gt;k row blocks\"]\n    B --&gt; C1[\"Block A₁&lt;br/&gt;m₁ × n\"]\n    B --&gt; C2[\"Block A₂&lt;br/&gt;m₂ × n\"]\n    B --&gt; C3[\"Block Aₖ&lt;br/&gt;mₖ × n\"]\n    \n    C1 --&gt; D1[\"Local QR:&lt;br/&gt;A₁ = Q₁R₁\"]\n    C2 --&gt; D2[\"Local QR:&lt;br/&gt;A₂ = Q₂R₂\"]\n    C3 --&gt; D3[\"Local QR:&lt;br/&gt;Aₖ = QₖRₖ\"]\n    \n    D1 --&gt; E[\"Stack R matrices:&lt;br/&gt;[R₁; R₂; ...; Rₖ]&lt;br/&gt;kn × n&lt;br/&gt;Fits in RAM!\"]\n    D2 --&gt; E\n    D3 --&gt; E\n    \n    E --&gt; F[\"Final QR:&lt;br/&gt;Rstack = QR × Rfinal\"]\n    \n    F --&gt; G[\"Reconstruct Q:&lt;br/&gt;Multiply Q_i by QR blocks\"]\n    \n    G --&gt; H[\"Result:&lt;br/&gt;A = Q × Rfinal\"]\n    \n    style A fill:#ffe8e8\n    style E fill:#fff8e1\n    style H fill:#e8f6e8\n    style D1 fill:#f0f8ff\n    style D2 fill:#f0f8ff\n    style D3 fill:#f0f8ff\n\n\n\n\n\n\n\n\nStep 1: Partition\nDivide the matrix A into blocks along rows: A = [A_1; A_2; ...; A_k]\nStep 2: Local QR\nCompute QR for each block independently: A_i = Q_i R_i\nEach block’s QR decomposition can be computed separately because they operate on independent rows.\nStep 3: Collect R matrices\nStack all the upper triangular R_i matrices vertically: R_{stack} = [R_1; R_2; ...; R_k]\nCritical insight: R_{stack} is much smaller than A. It’s kn \\times n instead of m \\times n, and typically kn \\ll m, so this stacked matrix fits in memory even when A doesn’t!\nStep 4: Final QR\nCompute QR of the stacked R matrices (this fits in RAM): R_{stack} = Q_R \\times R_{final}\nStep 5: Reconstruct Q\nThe overall Q is obtained by multiplying each local Q_i by the corresponding block of Q_R. This reconstruction can be done block by block, again staying within memory constraints.\nWhy this works mathematically:\nA = \\begin{bmatrix} Q_1 R_1 \\\\ Q_2 R_2 \\\\ \\vdots \\\\ Q_k R_k \\end{bmatrix} =\n\\begin{bmatrix} Q_1 \\\\ Q_2 \\\\ \\vdots \\\\ Q_k \\end{bmatrix}\n\\begin{bmatrix} R_1 \\\\ R_2 \\\\ \\vdots \\\\ R_k \\end{bmatrix}\nSince the Q_i are orthogonal, applying another QR to the stacked R matrices gives us the final factorization.\n\n\n# Algorithm: HierarchicalQR\n# Input: Matrix A m × n in HDF5, block_size\n# Output: Q matrix m × n, R matrix n × n\n\n# Step 1: Partition and compute local QRs\nn_blocks &lt;- ceiling(m / block_size)\nR_list &lt;- list()  # Store R matrices\n\nfor (i in 1:n_blocks) {\n  # Read block from HDF5\n  A_block &lt;- read_hdf5_rows(start_row, end_row)\n  \n  # Compute local QR\n  qr_result &lt;- qr_decomposition(A_block)\n  Q_local &lt;- qr_result$Q\n  R_local &lt;- qr_result$R\n  \n  # Store Q to HDF5 for later\n  write_hdf5(Q_local, group = \"local_Q\", dataset = paste0(\"Q_\", i))\n  \n  # Keep R in memory (small)\n  R_list[[i]] &lt;- R_local\n  \n  free(A_block, Q_local)\n}\n\n# Step 2: Stack R matrices (fits in RAM)\nR_stacked &lt;- bind_rows(R_list)  # (k*n × n)\n\n# Step 3: Final QR on stacked Rs\nqr_final &lt;- qr_decomposition(R_stacked)\nQ_R &lt;- qr_final$Q\nR_final &lt;- qr_final$R\n\n# Step 4: Reconstruct global Q (block by block)\nfor (i in 1:n_blocks) {\n  # Read local Q_i\n  Q_local &lt;- read_hdf5(group = \"local_Q\", dataset = paste0(\"Q_\", i))\n  \n  # Multiply by corresponding Q_R block\n  start_idx &lt;- (i-1) * n + 1\n  end_idx &lt;- i * n\n  Q_R_block &lt;- Q_R[start_idx:end_idx, ]\n  \n  Q_global_block &lt;- Q_local %*% Q_R_block\n  \n  # Write back to HDF5\n  write_hdf5(Q_global_block, group = \"final_Q\", dataset = paste0(\"Q_\", i))\n}\n\nreturn(list(Q = \"final_Q\", R = R_final))\n\n\n\n\n\n\n\n\n\nTipThe Magic of Hierarchical QR\n\n\n\nThe stacked R matrices are only (k \\times n) \\times n in size. Even with 10 blocks, if n=1000, that’s just a 10{,}000 \\times 1000 matrix - about 80 MB. This fits comfortably in RAM even when the original matrix is hundreds of GB!\n\n\nBigDataStatMeth implementation: The package implements hierarchical QR in bdQR_hdf5(), handling the block partitioning, intermediate storage, and final assembly automatically. This enables methods like PCA on out-of-memory data.",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/blockwise-computing.html#example-4-block-wise-svd-for-principal-component-analysis",
    "href": "fundamentals/blockwise-computing.html#example-4-block-wise-svd-for-principal-component-analysis",
    "title": "Block-Wise Computing",
    "section": "6 Example 4: Block-Wise SVD for Principal Component Analysis",
    "text": "6 Example 4: Block-Wise SVD for Principal Component Analysis\nSingular Value Decomposition (SVD) underlies principal component analysis and is one of the most important matrix decompositions in statistics. Computing SVD on matrices too large for memory is a significant challenge that demonstrates the full power of block-wise thinking.\n\n6.1 The Mathematical Goal\nFor a matrix X (dimensions n \\times p), we want: X = U \\Sigma V^T\nwhere: - U is n \\times r with orthonormal columns (left singular vectors) - \\Sigma is r \\times r diagonal (singular values)\n- V is p \\times r with orthonormal columns (right singular vectors) - r = \\min(n, p) or smaller if we want only the leading components\nFor PCA, we typically want only the first k components where k \\ll r, which simplifies the problem significantly.\n\n\n6.2 The Block-Wise Strategy: Hierarchical Decomposition\nBigDataStatMeth implements a hierarchical SVD algorithm (Iwen & Ong, 2017) that works in levels.\n\nVisual WorkflowTwo-Level ProcessMemory Analysis\n\n\n\n\n\n\n\nflowchart TD\n    A[\"Matrix X n × p&lt;br/&gt;Too large for RAM\"] --&gt; B[\"Level 1:&lt;br/&gt;Partition into m blocks\"]\n    \n    B --&gt; C1[\"Block X₁\"]\n    B --&gt; C2[\"Block X₂\"]  \n    B --&gt; C3[\"Block Xₘ\"]\n    \n    C1 --&gt; D1[\"Local SVD:&lt;br/&gt;X₁ = U₁Σ₁V₁ᵀ&lt;br/&gt;Keep k components\"]\n    C2 --&gt; D2[\"Local SVD:&lt;br/&gt;X₂ = U₂Σ₂V₂ᵀ&lt;br/&gt;Keep k components\"]\n    C3 --&gt; D3[\"Local SVD:&lt;br/&gt;Xₘ = UₘΣₘVₘᵀ&lt;br/&gt;Keep k components\"]\n    \n    D1 --&gt; E1[\"Form Z₁ = V₁Σ₁&lt;br/&gt;p × k\"]\n    D2 --&gt; E2[\"Form Z₂ = V₂Σ₂&lt;br/&gt;p × k\"]\n    D3 --&gt; E3[\"Form Zₘ = VₘΣₘ&lt;br/&gt;p × k\"]\n    \n    E1 --&gt; F[\"Level 2:&lt;br/&gt;Stack Z matrices&lt;br/&gt;Z = [Z₁; Z₂; ...; Zₘ]&lt;br/&gt;mk × k&lt;br/&gt;Fits in RAM!\"]\n    E2 --&gt; F\n    E3 --&gt; F\n    \n    F --&gt; G[\"Global SVD:&lt;br/&gt;Z = UzΣzVzᵀ\"]\n    \n    G --&gt; H[\"Extract Results:&lt;br/&gt;Singular values = Σz&lt;br/&gt;Right vectors = Vz&lt;br/&gt;Left vectors from U₁...Uₘ\"]\n    \n    H --&gt; I[\"Final SVD:&lt;br/&gt;X ≈ UΣVᵀ\"]\n    \n    style A fill:#ffe8e8\n    style F fill:#fff8e1\n    style I fill:#e8f6e8\n    style D1 fill:#f0f8ff\n    style D2 fill:#f0f8ff\n    style D3 fill:#f0f8ff\n    style G fill:#e8f6e8\n\n\n\n\n\n\n\n\nLevel 1: Partition and Local SVD\n\nPartition X into blocks along rows: X = \\begin{bmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_m \\end{bmatrix}\nCompute truncated SVD for each block, keeping only k components: X_i = U_i \\Sigma_i V_i^T\nwhere U_i is n_i \\times k, \\Sigma_i is k \\times k, and V_i is p \\times k.\nForm weighted matrices from the right singular vectors: Z_i = V_i \\Sigma_i\nEach Z_i is p \\times k - much smaller than the original X_i which was n_i \\times p!\n\nLevel 2: Merge and Global SVD\n\nStack the Z_i matrices: Z = \\begin{bmatrix} Z_1 \\\\ Z_2 \\\\ \\vdots \\\\ Z_m \\end{bmatrix}\nThis Z matrix is mk \\times k. Since m (blocks) and k (components) are both small, Z fits in memory.\nCompute global SVD of Z: Z = U_Z \\Sigma_Z V_Z^T\nExtract results:\n\nGlobal singular values = \\Sigma_Z\nGlobal right singular vectors = V_Z\nGlobal left singular vectors: multiply back through blocks\n\n\nWhy this works: The row space of X (spanned by right singular vectors) is well-approximated by combining the row spaces of blocks. Block SVDs capture main variation within each block; global SVD captures overall variation.\nApproximation quality: Not exact, but excellent for large n relative to p with sufficient k. Error decreases as k increases.\n\n\nConcrete example: X is 100,000 × 20,000 (16 GB)\n\nPartition into m = 10 blocks of 10,000 rows\nKeep k = 50 components\n\nMemory per block during local SVD: - Block X_i: 10,000 × 20,000 = 1.6 GB - U_i: 10,000 × 50 = 4 MB\n- \\Sigma_i: 50 × 50 = 20 KB - V_i: 20,000 × 50 = 8 MB - Peak: ~1.6 GB (dominated by the block)\nMemory for global SVD: - Z: (10 × 50) × 20,000 = 8 MB - This easily fits in memory!\nResult: 16 GB problem → 1.6 GB peak memory (10× reduction) while computing accurate approximation.\n\n\n\n\n\n\n\n\n\nNoteHierarchical Decomposition Power\n\n\n\nThe stacked Z matrix is tiny: (m \\times k) \\times p instead of n \\times p. With 10 blocks and 50 components, even for p=20{,}000, that’s just 8 MB - fitting comfortably in RAM even when the original is 16 GB!\n\n\nWe’ve reduced a 16 GB problem to one requiring ~1.6 GB peak memory - a 10× reduction - while computing an accurate approximation to the full SVD.\nBigDataStatMeth implementation: The bdSVD_hdf5() function implements this hierarchical strategy with additional optimizations:\n\nAutomatic selection of block size and hierarchy depth based on data dimensions and available memory\nParallel processing of independent blocks\nOptional centering and scaling before SVD\nMultiple hierarchy levels for very large datasets (you can set q levels with k blocks per level)",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/blockwise-computing.html#the-pattern-recognizing-block-wise-opportunities",
    "href": "fundamentals/blockwise-computing.html#the-pattern-recognizing-block-wise-opportunities",
    "title": "Block-Wise Computing",
    "section": "7 The Pattern: Recognizing Block-Wise Opportunities",
    "text": "7 The Pattern: Recognizing Block-Wise Opportunities\nLooking across these examples, several patterns emerge for when and how algorithms can be adapted for block-wise processing:\n\n7.1 1. Operations with Additive Decomposition\nIf an operation can be expressed as summing contributions from different data portions, it’s straightforward to implement block-wise:\n\nExamples: Means, sums, counts, sufficient statistics for simple models\nStrategy: Process each block, accumulate results, finalize\nMemory: One block plus small accumulator\nI/O: Single pass through data\n\n\n\n7.2 2. Operations with Hierarchical Decomposition\nIf an operation can be computed hierarchically (computing on blocks, then combining block results), block-wise processing is feasible:\n\nExamples: QR decomposition, SVD, certain optimization algorithms\nStrategy: Local computation on blocks, merge results, possibly iterate\nMemory: One block plus moderately-sized merge structure\nI/O: Multiple passes, but still practical\n\n\n\n7.3 3. Operations Requiring Global Information\nSome operations need information about the entire dataset to process any part of it:\n\nExamples: Sorting, ranking, quantiles, certain graph algorithms\nChallenge: May require multiple passes or sophisticated algorithms\nStrategy: Often use approximation or sampling strategies\n\n\n\n7.4 4. Operations That Don’t Decompose Well\nA few operations are fundamentally difficult to decompose:\n\nExamples: Some machine learning algorithms with complex dependencies, certain nonlinear optimizations\nOptions: Use approximations, reformulate the problem, or accept that in-memory computation is necessary\n\nBigDataStatMeth focuses on category 1 and 2 operations, which cover most standard statistical analyses. For category 3, the package often provides approximate but practical solutions.",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/blockwise-computing.html#practical-considerations",
    "href": "fundamentals/blockwise-computing.html#practical-considerations",
    "title": "Block-Wise Computing",
    "section": "8 Practical Considerations",
    "text": "8 Practical Considerations\n\n8.1 Block Size Selection\nChoosing appropriate block sizes involves trade-offs:\nToo small blocks: - More overhead from reading many blocks - More function calls and loop iterations - Potentially less cache-friendly computations\nToo large blocks: - Risk of memory exhaustion - Less opportunity for parallelization - Longer time to first result\n\n\n\n\n\n\nTipFinding the Sweet Spot\n\n\n\nBigDataStatMeth uses smart heuristics based on available RAM, data dimensions, and operation type. Users can override defaults when they have specific system knowledge, but the automatic selection works well for most cases.\nRule of thumb: Blocks should use ~10-20% of available RAM, leaving room for intermediate results and OS operations.\n\n\nBigDataStatMeth uses heuristics based on: - Available RAM (conservative estimates) - Data dimensions (balanced partitioning) - Operation type (some need larger blocks for efficiency) - Computational complexity (CPU-bound vs. I/O-bound)\nUsers can override defaults when they have specific knowledge about their system.\n\n\n8.2 Numerical Stability\nBlock-wise algorithms can affect numerical properties, though the impact varies by operation:\nPotential issues: - Accumulation of rounding errors across many blocks - Loss of precision in hierarchical methods (e.g., hierarchical SVD is approximate) - Order-dependent results for some operations - Cancellation errors when subtracting nearly equal numbers\n\n\n\n\n\n\nImportantWhat BigDataStatMeth Guarantees\n\n\n\nThe package prioritizes practical numerical reliability over theoretical guarantees:\n\n✅ Tested algorithms: Extensively verified against in-memory methods\n✅ Established methods: Uses well-known algorithms from numerical linear algebra literature\n✅ Double precision: All computations use 64-bit floating point\n✅ Documented approximations: When algorithms are approximate (hierarchical SVD), this is clearly stated\n\nWhat is NOT guaranteed: - ❌ Bit-exact reproducibility across different hardware - ❌ Formal error bounds for all operations - ❌ Specific compensated algorithms (e.g., Kahan summation)\n\n\nPractical implications:\nFor most statistical applications, the numerical precision is more than adequate: - PCA/SVD components match in-memory results to many decimal places - Regression coefficients are statistically indistinguishable - Hypothesis test results are identical\n\n\n\n\n\n\nNoteWhen to Worry About Numerical Precision\n\n\n\nIf your application requires provable numerical bounds or arbitrary precision, BigDataStatMeth may not be suitable.\nThe package optimizes for the common case where statistical noise far exceeds numerical error - which describes most genomic and large-scale data analyses.\n\n\nUser control:\nSome functions provide parameters to control accuracy: - SVD/PCA: k parameter controls how many components to compute (more = better approximation to full decomposition) - Iterative methods: tolerance parameters control convergence criteria\nCheck function documentation for available accuracy controls.\n\n\n8.3 Parallelization\nMany block-wise operations are embarrassingly parallel - blocks can be processed independently. BigDataStatMeth leverages this through OpenMP parallelization at the C++ level.\n\nVisual WorkflowHow It WorksPseudocode\n\n\n\n\n\n\n\nflowchart TD\n    A[Matrix in HDF5&lt;br/&gt;divided into blocks] --&gt; B[OpenMP Scheduler&lt;br/&gt;threads=4]\n    \n    B --&gt; C1[Thread 1:&lt;br/&gt;Block 1]\n    B --&gt; C2[Thread 2:&lt;br/&gt;Block 2]\n    B --&gt; C3[Thread 3:&lt;br/&gt;Block 3]\n    B --&gt; C4[Thread 4:&lt;br/&gt;Block 4]\n    \n    C1 --&gt; D1[Process&lt;br/&gt;independently]\n    C2 --&gt; D2[Process&lt;br/&gt;independently]\n    C3 --&gt; D3[Process&lt;br/&gt;independently]\n    C4 --&gt; D4[Process&lt;br/&gt;independently]\n    \n    D1 --&gt; E[Combine results&lt;br/&gt;sequentially]\n    D2 --&gt; E\n    D3 --&gt; E\n    D4 --&gt; E\n    \n    E --&gt; F[Final result]\n    \n    style A fill:#f0f8ff\n    style B fill:#fff8e1\n    style C1 fill:#e8f6e8\n    style C2 fill:#e8f6e8\n    style C3 fill:#e8f6e8\n    style C4 fill:#e8f6e8\n    style F fill:#e8f6e8\n\n\n\n\n\n\n\n\nOpenMP (Open Multi-Processing) is a parallel programming API for C++ that BigDataStatMeth uses internally. When you specify threads = 4 in a function like bdSVD_hdf5(), the C++ code distributes independent block operations across 4 CPU cores.\nKey characteristics:\n\nShared memory: All threads access the same HDF5 file, but read different blocks\nThread-safe I/O: BigDataStatMeth ensures HDF5 reads don’t conflict between threads\n\nLoad balancing: OpenMP automatically distributes blocks to available threads\nOverhead vs. benefit: Parallelism only helps when block processing time exceeds coordination overhead\n\nWhen parallelization helps:\n\nLarge blocks that take seconds to process each\nCPU-intensive operations (matrix multiplication, decompositions)\nMany blocks to process (good load distribution)\n\nWhen it doesn’t help:\n\nVery small blocks (overhead dominates)\nI/O-bound operations (disk is the bottleneck, not CPU)\nFew blocks (can’t keep all cores busy)\n\nBigDataStatMeth automatically uses parallelization where beneficial. Users control thread count via the threads parameter.\n\n\n// Algorithm: ParallelBlockProcessing\n// Input: HDF5 dataset, n_blocks, n_threads\n// Output: Combined results from all blocks\n\n// 1. Initialize\nresults &lt;- array to store each block's output\n\n// 2. Parallel processing (OpenMP)\n#pragma omp parallel for num_threads(n_threads)\nfor (int i = 0; i &lt; n_blocks; i++) {\n  // Each thread processes its assigned blocks\n  int thread_id = omp_get_thread_num();\n  \n  // Thread-safe read\n  block_i = read_hdf5_block(i);\n  \n  // Independent computation\n  result_i = process_block(block_i);\n  \n  // Store result\n  results[i] = result_i;\n  \n  // Free memory\n  free(block_i);\n}\n\n// 3. Combine results (sequential)\nfinal_result = combine_function(results);\n\n// 4. Return\nreturn final_result;\n\n\n\n\n\n\n\n\n\nTipThread Configuration\n\n\n\nMatch threads to your CPU cores but leave 1-2 cores free for the system. For a 12-core machine, threads = 10 is often optimal. Use threads = 1 for debugging to simplify error tracking.",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/blockwise-computing.html#implementing-your-own-block-wise-methods",
    "href": "fundamentals/blockwise-computing.html#implementing-your-own-block-wise-methods",
    "title": "Block-Wise Computing",
    "section": "9 Implementing Your Own Block-Wise Methods",
    "text": "9 Implementing Your Own Block-Wise Methods\nOne of BigDataStatMeth’s design goals is enabling users to implement new statistical methods using block-wise processing. The package provides different levels of abstraction:\n\n9.1 Architecture Overview\nBigDataStatMeth uses a three-layer architecture:\n1. Internal C++ Layer (Accessible from C++, not from R):\nThe lowest level consists of efficient C++ classes and functions that handle direct HDF5 file operations:\n\nhdf5Dataset class - Wraps HDF5 C API for file access, provides block reading/writing\nEfficient memory management and buffer handling\nDirect HDF5 C API access for maximum performance\nOpenMP-parallelized block iteration\n\nImportant: These internal components are NOT directly exposed to R users - you can’t call them from an R script. However, they ARE fully accessible if you’re developing new methods in C++ using BigDataStatMeth as a header-only library. If you’re implementing a new statistical method in C++, you have full access to these low-level building blocks.\nEach high-level algorithm (PCA, regression, etc.) uses these internal functions and manages its own block iteration strategy for optimal performance. The block-reading logic is algorithm-specific rather than exposed as general-purpose functions because different algorithms benefit from different iteration patterns.\n2. Mid-Level Functions (C++ with R bindings):\nBlock-wise operations implemented in C++ but accessible from both R and C++:\n\nbdCrossprod_hdf5() / bdtCrossprod_hdf5() - Crossproduct operations\nbdblockmult_hdf5() - Matrix multiplication\nbdNormalize_hdf5() - Centering and scaling\nbdApply_hdf5() - Apply functions to blocks\n\nThese functions handle the complexity of block iteration, parallel processing, and result accumulation internally. From R, you simply call them with appropriate parameters.\n3. High-Level Statistical Methods (C++ with R bindings):\nComplete statistical analyses implemented using the mid-level functions:\n\nbdSVD_hdf5() - Singular Value Decomposition\nbdPCA_hdf5() - Principal Component Analysis\n\nbdQR_hdf5() - QR Decomposition\nRegression methods, association tests, etc.\n\n\n\n9.2 Developing New Methods\nApproach 1: Compose from existing functions (Recommended for R users)\nUse existing mid-level and high-level functions as building blocks. Example: implementing Canonical Correlation Analysis (CCA) in R:\n# CCA implementation using BigDataStatMeth functions\nbdCCA_hdf5 &lt;- function(filename, X_dataset, Y_dataset, k = 10) {\n  \n  # Step 1: Normalize both datasets\n  bdNormalize_hdf5(filename, X_dataset, bcenter = TRUE, bscale = TRUE)\n  bdNormalize_hdf5(filename, Y_dataset, bcenter = TRUE, bscale = TRUE)\n  \n  # Step 2: Compute QR decomposition for each\n  qr_x &lt;- bdQR_hdf5(filename, X_dataset)\n  qr_y &lt;- bdQR_hdf5(filename, Y_dataset)\n  \n  # Step 3: Compute cross-product of Q matrices\n  cross &lt;- bdCrossprod_hdf5(\n    filename, \n    A = qr_x$Q_dataset,\n    B = qr_y$Q_dataset\n  )\n  \n  # Step 4: SVD of cross-product\n  svd_result &lt;- bdSVD_hdf5(filename, cross$dataset, k = k)\n  \n  # Step 5: Back-transform to get canonical variates\n  # ... (using bdblockmult_hdf5 to multiply through)\n  \n  return(list(correlations = svd_result$d, ...))\n}\nThis approach leverages existing optimized functions without writing C++ code.\nApproach 2: Implement in C++ (For developers)\nFor maximum performance or novel algorithms, implement directly in C++. You’ll work with:\n\nhdf5Dataset class for file access\nEigen library for linear algebra\nOpenMP for parallelization\n\nSee the CCA implementation examples in the package source (Example_bdCCA.cpp, Example_bdCCA.h) which show both R-based and C++-based approaches to the same algorithm.\nKey consideration: The C++ approach requires understanding: - Memory management in C++ - HDF5 C API through hdf5Dataset wrapper\n- Eigen matrix operations - Thread safety for parallel operations\nMost users will find composing from existing functions sufficient. The C++ API is there for those developing new core algorithms or requiring maximum performance.\n\n\n9.3 Learning from Examples\nThe package includes several complete examples showing different implementation strategies:\n\nExample_bdCCA_hdf5_R.R - CCA implemented purely in R using BigDataStatMeth functions\nExample_bdCCA.cpp/.h - CCA implemented in C++ for comparison\nExample_getQRbyBlocks.R - Hierarchical QR showing block partitioning strategies\n\nThese examples demonstrate how to think through: 1. What can be computed block-by-block? 2. What intermediate results need to be merged? 3. What’s the memory footprint at each step? 4. When to write intermediate results to HDF5 vs. keep in memory?",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/blockwise-computing.html#interactive-exercise",
    "href": "fundamentals/blockwise-computing.html#interactive-exercise",
    "title": "Block-Wise Computing",
    "section": "10 Interactive Exercise",
    "text": "10 Interactive Exercise\n\n10.1 Practice: Analyzing Block-Wise Efficiency\nUnderstanding which operations work well with block-wise processing and which present challenges helps you design efficient analysis pipelines. This exercise develops that intuition.\n\n# Consider this analysis workflow\nanalyze_workflow &lt;- function() {\n  # Operation 1: Compute column means\n  means &lt;- bdColMeans_hdf5(file, dataset)\n  \n  # Operation 2: Center the data  \n  centered &lt;- bdNormalize_hdf5(file, dataset, bcenter = TRUE, bscale = FALSE)\n  \n  # Operation 3: Compute t(X) %*% X\n  crossprod &lt;- bdCrossprod_hdf5(file, centered)\n  \n  # Operation 4: Eigen-decomposition for PCA\n  pca &lt;- bdSVD_hdf5(file, crossprod)\n  \n  # Question: Which operations are most/least block-amenable?\n}\n\n\n\n\n\n\n\nTipReflection Questions\n\n\n\nThink through these scenarios - understanding the principles matters more than having “correct” answers:\n1. Operation Efficiency Analysis: - Which operation above is most naturally block-wise? Why? - Which operation requires the most inter-block coordination? - If you had to do this with limited memory, which step would be the bottleneck?\n2. Block Size Decisions: - For a 100,000 × 50,000 matrix with 32 GB RAM available, what block size would you choose? - What if you had 128 GB RAM? Would you still use blocks? - How would block size affect the accuracy of the results?\n3. Algorithm Adaptation: - Consider k-means clustering. Can you decompose it block-wise? - What about hierarchical clustering? - Which machine learning algorithms would be easy vs. hard to adapt?\n4. Designing Your Pipeline: - Your analysis needs: QC filtering, normalization, PCA, then regression - Which steps can process data block-by-block independently? - Which steps need access to summary statistics from all data? - Where would you store intermediate results?\n5. Trade-offs: - Smaller blocks = less memory but more disk I/O - Larger blocks = more memory but fewer I/O operations\n- For your system, where’s the sweet spot?\nTry sketching out a block-wise version of your actual analysis pipeline. Which parts translate easily? Which require creative rethinking? This mental exercise builds intuition for designing efficient big data workflows.",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/blockwise-computing.html#key-takeaways",
    "href": "fundamentals/blockwise-computing.html#key-takeaways",
    "title": "Block-Wise Computing",
    "section": "11 Key Takeaways",
    "text": "11 Key Takeaways\nLet’s consolidate what you’ve learned about adapting algorithms for block-wise processing with disk-based data.\n\n11.1 Essential Concepts\nThe divide-process-combine paradigm underlies all block-wise computing. You partition data into manageable pieces, process each piece (often independently), then combine results to get the final answer. This simple pattern adapts to operations from simple means to complex matrix factorizations. The challenge lies in figuring out what to process and how to combine results validly.\nBlock-amenable operations share common mathematical properties that make them decompose naturally. Operations that can be expressed as sums, products, or aggregations across independent portions of data work well. Computing means, sums, element-wise operations, and many matrix products fall into this category. The mathematical structure of these operations allows valid decomposition.\nBlock-resistant operations require information flow between all parts of the data or produce outputs that scale problematically. Anything requiring global sorting, finding specific quantiles, or creating O(n²) outputs challenges block-wise approaches. These operations don’t decompose cleanly - they need special algorithmic tricks or fundamentally different approaches.\nBlock size creates a memory-accuracy-speed trade-off with no universally optimal choice. Smaller blocks use less memory but require more disk I/O and potentially more approximation error in hierarchical algorithms. Larger blocks use more memory but require fewer I/O operations and may be more accurate. The optimal choice depends on your available RAM, disk speed, and accuracy requirements.\nHierarchical processing enables algorithms that don’t naturally decompose to single-pass block operations. By creating multiple levels of computation - process blocks, merge results into smaller intermediate blocks, repeat until small enough - you can handle very large matrices while controlling memory usage at each level. This is how BigDataStatMeth implements complex operations like SVD/PCA.\nComposability is power. You can build complex analyses by composing simple block-wise operations. Rather than implementing every possible analysis from scratch in C++, combine existing functions: normalize, compute crossproduct, perform SVD, multiply back through. This compositional approach lets you implement new methods at the R level, leveraging optimized core operations.\n\n\n11.2 When Block-Wise Processing Works Well\nUnderstanding which problems benefit from block-wise approaches helps you make good architectural decisions for your analyses.\n✅ Highly effective for:\n\nMatrix multiplication and products - These decompose naturally into block-block operations. BigDataStatMeth’s implementations are highly optimized and scale well.\nElement-wise operations - Operations like adding, subtracting, multiplying by scalars process each element independently. Perfect for block-wise processing with no communication overhead.\nRow or column operations - Computing means, sums, normalizations by rows or columns work naturally with blocks containing complete rows or columns.\nMany factorizations - SVD, QR, and Cholesky decompositions have hierarchical block-wise algorithms that maintain numerical accuracy while limiting memory usage.\n\n❌ Challenging for:\n\nGlobal operations - Finding the median, computing percentiles, or operations that require knowing the full data distribution don’t decompose easily.\nFine-grained dependencies - Algorithms where each element depends on many others (like some iterative optimization methods) resist block decomposition.\nOperations creating large outputs - Computing all pairwise distances or correlations creates O(n²) results. Even block-wise approaches struggle when output size explodes.\nHighly iterative methods - Algorithms requiring many passes through data with convergence checks add overhead when each pass means disk I/O.\n\nFor most statistical analyses in genomics and similar fields - PCA, regression, association tests, basic machine learning - block-wise approaches work very well. The operations these methods need decompose naturally. More specialized algorithms may require case-by-case evaluation of whether block-wise processing provides benefits.",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/blockwise-computing.html#next-steps",
    "href": "fundamentals/blockwise-computing.html#next-steps",
    "title": "Block-Wise Computing",
    "section": "12 Next Steps",
    "text": "12 Next Steps\nYou now understand how standard statistical algorithms adapt for disk-based data processing. This conceptual foundation prepares you to:\n\nUnderstand Linear Algebra Foundations → Review the mathematical operations BigDataStatMeth implements\nGetting Started Tutorial → Apply these concepts to your own data\nCCA Implementation Example → See a complete complex method implemented block-wise\n\n\n\n\n\n\n\n\nNoteWant to Learn More?\n\n\n\nThe BigDataStatMeth paper provides mathematical details and proofs for the hierarchical algorithms. The C++ API documentation shows lower-level implementation details if you want to understand exactly how operations are executed.",
    "crumbs": [
      "Core Concepts",
      "Block-Wise Computing"
    ]
  },
  {
    "objectID": "fundamentals/linear-algebra.html",
    "href": "fundamentals/linear-algebra.html",
    "title": "Linear Algebra for Statistical Methods",
    "section": "",
    "text": "By the end of this section, you will:\n\nUnderstand key matrix operations used in statistical computing\nRecognize which operations BigDataStatMeth implements\nKnow the computational costs of different operations\nUnderstand memory requirements for matrix operations\nSee how these operations translate to statistical methods\nKnow which operations are fast vs. slow on disk-based data",
    "crumbs": [
      "Core Concepts",
      "Linear Algebra for Statistical Methods"
    ]
  },
  {
    "objectID": "fundamentals/linear-algebra.html#why-linear-algebra-matters-for-statistics",
    "href": "fundamentals/linear-algebra.html#why-linear-algebra-matters-for-statistics",
    "title": "Linear Algebra for Statistical Methods",
    "section": "1 Why Linear Algebra Matters for Statistics",
    "text": "1 Why Linear Algebra Matters for Statistics\nStatistical methods, especially those applied to high-dimensional data like genomics, are fundamentally built on linear algebra operations. When you compute a correlation, fit a linear model, or perform PCA, you’re executing matrix operations - even if the statistical software hides this fact behind convenient function calls.\nUnderstanding these operations serves two purposes in the context of BigDataStatMeth. First, it helps you understand what the package is computing when you call functions like bdSVD_hdf5() or bdCrossprod_hdf5(). Second, and more importantly, it helps you understand why certain operations are faster or more memory-intensive than others, which informs decisions about how to structure your analyses.\nThis section provides a practical review of key linear algebra operations, focusing on their statistical interpretation and computational characteristics. We won’t derive theorems or prove properties - there are excellent textbooks for that. Instead, we focus on building intuition for how these operations work and why they matter for big data analysis.",
    "crumbs": [
      "Core Concepts",
      "Linear Algebra for Statistical Methods"
    ]
  },
  {
    "objectID": "fundamentals/linear-algebra.html#matrix-basics-and-dimensions",
    "href": "fundamentals/linear-algebra.html#matrix-basics-and-dimensions",
    "title": "Linear Algebra for Statistical Methods",
    "section": "2 Matrix Basics and Dimensions",
    "text": "2 Matrix Basics and Dimensions\n\n2.1 What Matrices Represent\nIn statistical applications, matrices typically represent one of two things:\nData matrices: Rows are observations (individuals, samples, experiments) and columns are variables (genetic variants, genes, measurements). A data matrix X with dimensions n \\times p contains n observations of p variables.\nRelationship matrices: Both rows and columns represent the same set of entities, and entries represent relationships between them. Common examples include correlation matrices, covariance matrices, and similarity matrices.\nUnderstanding which type of matrix you’re working with helps predict the computational characteristics of operations on it.\n\n\n2.2 Dimension Compatibility\nMatrix operations have specific rules about compatible dimensions:\nMatrix multiplication C = A \\times B requires: - A is m \\times k - B is k \\times n\n- Result C is m \\times n\nThe inner dimensions must match (k must be the same), while outer dimensions determine the result size. This matters for big data because:\n\\text{Memory}(C) = m \\times n \\times 8 \\text{ bytes}\nEven if A and B fit in memory individually, C might not if m and n are both large.\nElement-wise operations (addition, subtraction, element-wise multiplication) require matrices of identical dimensions. These operations are memory-neutral - the result is the same size as the inputs.",
    "crumbs": [
      "Core Concepts",
      "Linear Algebra for Statistical Methods"
    ]
  },
  {
    "objectID": "fundamentals/linear-algebra.html#essential-matrix-operations",
    "href": "fundamentals/linear-algebra.html#essential-matrix-operations",
    "title": "Linear Algebra for Statistical Methods",
    "section": "3 Essential Matrix Operations",
    "text": "3 Essential Matrix Operations\n\n3.1 Transpose\nThe transpose A^T swaps rows and columns: if A is m \\times n, then A^T is n \\times m.\nStatistical meaning: Transposing a data matrix switches from individuals-by-variables to variables-by-individuals, which changes what operations naturally express. For example: - X^T X is a p \\times p matrix of relationships between variables (often proportional to correlation) - X X^T is an n \\times n matrix of relationships between individuals\nComputational note: Transpose is conceptually simple but can be expensive for large matrices stored on disk, because it requires rewriting data in a different order. BigDataStatMeth’s HDF5 functions sometimes avoid explicit transposition by cleverly reading data in the needed order.\n\n\n3.2 Matrix Multiplication\nMatrix multiplication C = A \\times B is ubiquitous in statistics:\nUses in statistics: - Projecting data onto a lower-dimensional space: Y = X W - Computing fitted values in regression: \\hat{y} = X \\hat{\\beta} - Rotating or transforming coordinates: X_{new} = X R\nComputational complexity: Multiplying m \\times k and k \\times n matrices requires m \\times k \\times n multiplications. For large matrices, this can be substantial:\n\nSmall example: 1000 \\times 1000 by 1000 \\times 1000 → 10^9 operations (&lt; 1 second on modern CPU)\nLarge example: 100,000 \\times 10,000 by 10,000 \\times 10,000 → 10^{14} operations (hours without optimization)\n\nBlock-wise strategy: As discussed in Block-Wise Computing, matrix multiplication decomposes naturally for disk-based data.\n\n\n3.3 Crossproduct and Transposed Crossproduct\nThese are special cases of multiplication involving a matrix and its transpose:\nCrossproduct: X^T X (or more generally X^T Y) Transposed Crossproduct: X X^T (or more generally X Y^T)\nWhy they’re special: These operations are extremely common in statistics and have special mathematical properties (the results are symmetric). More importantly, they can be computed more efficiently than general matrix multiplication.\nStatistical applications: - Sample covariance: \\frac{1}{n-1} X^T X (for centered X) - Gram matrices for kernel methods: X X^T - Normal equations in regression: X^T X \\beta = X^T y\nBigDataStatMeth provides dedicated functions (bdCrossprod_hdf5() and bdtCrossprod_hdf5()) that exploit the special structure of these operations for efficiency.",
    "crumbs": [
      "Core Concepts",
      "Linear Algebra for Statistical Methods"
    ]
  },
  {
    "objectID": "fundamentals/linear-algebra.html#matrix-decompositions",
    "href": "fundamentals/linear-algebra.html#matrix-decompositions",
    "title": "Linear Algebra for Statistical Methods",
    "section": "4 Matrix Decompositions",
    "text": "4 Matrix Decompositions\nDecompositions factor a matrix into a product of simpler matrices with useful properties. They’re fundamental to many statistical methods.\n\n4.1 Why Decompositions Matter\nRather than solving problems directly, we often decompose matrices and work with the factors. This provides:\nNumerical stability: Decompositions can be computed reliably even for ill-conditioned problems Insight: The factors often have interpretable meaning Efficiency: Certain operations on the factors are faster than on the original matrix Flexibility: The same decomposition serves multiple purposes\n\n\n4.2 Singular Value Decomposition (SVD)\nThe SVD factors any m \\times n matrix X as: X = U \\Sigma V^T\nwhere: - U is m \\times r with orthonormal columns (left singular vectors) - \\Sigma is r \\times r diagonal with non-negative entries (singular values) - V is n \\times r with orthonormal columns (right singular vectors) - r = \\min(m,n) or less if you truncate\nStatistical interpretation:\nThe SVD identifies orthogonal directions of variation in your data: - Singular values (\\Sigma) indicate how much variation lies along each direction - Left singular vectors (U) express observations in terms of these directions - Right singular vectors (V) express variables in terms of these directions\nKey applications:\nPrincipal Component Analysis (PCA): The columns of V (right singular vectors) are the principal component directions. The columns of U \\Sigma are the principal component scores. The squared singular values are proportional to explained variance.\nLow-rank approximation: Keeping only the first k singular values and corresponding vectors gives the best rank-k approximation to X (in least-squares sense).\nRegression: SVD provides a stable way to solve least squares problems, especially when X^T X is nearly singular.\nComputational challenge: Computing the full SVD requires O(\\min(mn^2, m^2n)) operations, which is prohibitive for large matrices. Fortunately, we often only need the first k singular values/vectors where k \\ll \\min(m,n). Iterative algorithms can compute these leading components much more efficiently.\nBigDataStatMeth’s bdSVD_hdf5() uses the hierarchical algorithm described in Block-Wise Computing to compute truncated SVD on matrices that don’t fit in memory.\n\n\n4.3 QR Decomposition\nThe QR decomposition factors an m \\times n matrix X (with m \\geq n) as: X = Q R\nwhere: - Q is m \\times n with orthonormal columns (Q^T Q = I) - R is n \\times n upper triangular\nStatistical interpretation:\nQR decomposition provides an orthogonal basis for the column space of X. The columns of Q span the same space as the columns of X, but they’re orthonormal (perpendicular and unit length).\nKey applications:\nLinear regression: Solving X \\beta = y via QR is numerically stable. Once you have QR = X: \\beta = R^{-1} Q^T y\nSolving with the triangular R is fast and stable.\nOrthogonalization: QR gives you an orthonormal basis for a set of vectors, useful in many iterative algorithms.\nRank determination: The diagonal elements of R reveal the effective rank of X.\nComputational note: QR decomposition requires O(mn^2) operations. For block-wise computation, BigDataStatMeth uses hierarchical QR as described in the previous section.\n\n\n4.4 Cholesky Decomposition\nFor a symmetric positive-definite matrix A (common for covariance matrices), the Cholesky decomposition gives: A = L L^T\nwhere L is lower triangular.\nStatistical interpretation:\nCholesky decomposition is like taking the square root of a matrix. For covariance matrices, it’s related to linear transformations that generate the covariance structure.\nKey applications:\nMatrix inversion: A^{-1} = (L^T)^{-1} L^{-1}, and inverting triangular matrices is efficient\nSimulation: To generate random variables with covariance A, generate standard normals z and compute L z\nSolving linear systems: For A x = b, solve L y = b then L^T x = y (both easy with triangular L)\nComputational efficiency: Cholesky is about twice as fast as general matrix factorizations and uses half the storage (only the triangular part).\nBigDataStatMeth provides bdCholesky_hdf5() for computing this decomposition and bdInvCholesky_hdf5() for the common use case of inverting symmetric positive-definite matrices.",
    "crumbs": [
      "Core Concepts",
      "Linear Algebra for Statistical Methods"
    ]
  },
  {
    "objectID": "fundamentals/linear-algebra.html#operations-on-structured-matrices",
    "href": "fundamentals/linear-algebra.html#operations-on-structured-matrices",
    "title": "Linear Algebra for Statistical Methods",
    "section": "5 Operations on Structured Matrices",
    "text": "5 Operations on Structured Matrices\nSome matrices have special structure that enables more efficient computation.\n\n5.1 Symmetric Matrices\nA matrix A is symmetric if A = A^T. Many statistical matrices are symmetric: - Correlation matrices - Covariance matrices\n- Gram matrices (X X^T)\nComputational benefits: - Only need to store roughly half the elements (upper or lower triangle) - Many operations can be optimized knowing the structure - Eigenvalues are guaranteed to be real - Eigenvectors are orthogonal\n\n\n5.2 Diagonal Matrices\nA diagonal matrix has non-zero elements only on the diagonal. The variance matrix of independent variables is diagonal.\nComputational benefits: - Multiplication is element-wise: (D X)_{ij} = D_{ii} X_{ij} - Inversion is trivial: D^{-1} has diagonal elements 1/D_{ii} - Can be stored as a vector instead of a matrix\n\n\n5.3 Sparse Matrices\nA sparse matrix has mostly zero elements. While less common in basic statistical applications, sparse matrices appear in: - Penalty matrices in regularization - Design matrices for categorical variables - Network adjacency matrices\nBigDataStatMeth doesn’t currently specialize for sparse matrices, as genomic data is typically dense. However, this is a potential future enhancement.",
    "crumbs": [
      "Core Concepts",
      "Linear Algebra for Statistical Methods"
    ]
  },
  {
    "objectID": "fundamentals/linear-algebra.html#matrix-norms-and-conditioning",
    "href": "fundamentals/linear-algebra.html#matrix-norms-and-conditioning",
    "title": "Linear Algebra for Statistical Methods",
    "section": "6 Matrix Norms and Conditioning",
    "text": "6 Matrix Norms and Conditioning\n\n6.1 Norms: Measuring Matrix “Size”\nA matrix norm is a scalar that measures the “size” of a matrix. Common norms include:\nFrobenius norm: ||A||_F = \\sqrt{\\sum_{ij} A_{ij}^2} (like treating the matrix as a long vector)\nOperator norms: Measure how much a matrix can “stretch” vectors\nStatistical relevance: Norms appear in: - Regularization (penalizing large coefficients) - Convergence criteria (stopping when change is small) - Error quantification (measuring approximation quality)\n\n\n6.2 Condition Number\nThe condition number measures how sensitive a matrix’s inverse is to small changes in the matrix. For a matrix A: \\kappa(A) = ||A|| \\cdot ||A^{-1}||\nInterpretation: - \\kappa = 1: Perfectly conditioned (e.g., identity matrix) - \\kappa \\approx 10: Well-conditioned - \\kappa &gt; 1000: Ill-conditioned, numerical issues likely - \\kappa very large: Nearly singular, inversion unreliable\nStatistical relevance:\nMulticollinearity in regression: High condition number of X^T X indicates highly correlated predictors, leading to unstable coefficient estimates.\nNumerical stability: Operations involving ill-conditioned matrices accumulate rounding errors. This is why methods like SVD and QR decomposition are preferred over directly computing (X^T X)^{-1} for regression.\nBigDataStatMeth consideration: The package uses numerically stable algorithms, but users should be aware that even with stable algorithms, solving ill-conditioned problems can yield uninformative results. The condition number is something to check in your data, not a limitation of the computational method.",
    "crumbs": [
      "Core Concepts",
      "Linear Algebra for Statistical Methods"
    ]
  },
  {
    "objectID": "fundamentals/linear-algebra.html#practical-guidelines-for-big-data",
    "href": "fundamentals/linear-algebra.html#practical-guidelines-for-big-data",
    "title": "Linear Algebra for Statistical Methods",
    "section": "7 Practical Guidelines for Big Data",
    "text": "7 Practical Guidelines for Big Data\nUnderstanding these operations informs decisions about computational strategy:\n\n7.1 Memory Requirements Are Predictable\nFor operation C = f(A, B): - Check dimensions of inputs and outputs - Calculate memory: dimensions × 8 bytes - Peak memory includes inputs, outputs, and any intermediate results\nExample: Computing X^T X for X that is 100,000 \\times 50,000: - Input: 100,000 \\times 50,000 = 40 GB - Output: 50,000 \\times 50,000 = 20 GB - Peak memory during computation: Both (60 GB)\nThis tells you whether an operation is feasible in-memory or requires block-wise processing.\n\n\n7.2 Asymmetric Operations Create Memory Pressure\nOperations where the output is much larger than inputs can exhaust memory:\nExample: X X^T for X that is 100,000 \\times 1000: - Input: 100,000 \\times 1000 = 800 MB - Output: 100,000 \\times 100,000 = 80 GB\nThe input fits easily, but the output doesn’t!\nStrategy: BigDataStatMeth can write large results directly to HDF5, avoiding the need to hold them in memory.\n\n\n7.3 Order of Operations Matters\nMatrix multiplication is associative but not commutative: A (BC) = (AB) C but AB \\neq BA.\nFor memory efficiency, consider the order:\nComputing A (B C) where A is 1000 \\times 10000, B is 10000 \\times 100, C is 100 \\times 10:\n\nOption 1: Compute BC first (10000 \\times 100 by 100 \\times 10 → 10000 \\times 10), then A(BC) → Final: 1000 \\times 10\nOption 2: Compute AB first (1000 \\times 10000 by 10000 \\times 100 → 1000 \\times 100), then (AB)C → Final: 1000 \\times 10\n\nBoth give the same final result, but Option 1 requires much less memory for intermediate results.\n\n\n7.4 Decompositions Are Investments\nComputing a decomposition (SVD, QR, Cholesky) takes time, but the factors can be reused:\nExample: After computing X = QR: - Solving X \\beta = y is fast (two triangular solves) - Computing X^T X is fast (R^T R) - Many operations with X become operations with Q and R, which may be more efficient\nFor big data, it’s often worth computing a decomposition once and reusing it, rather than repeatedly computing operations on the original matrix.",
    "crumbs": [
      "Core Concepts",
      "Linear Algebra for Statistical Methods"
    ]
  },
  {
    "objectID": "fundamentals/linear-algebra.html#interactive-exercise",
    "href": "fundamentals/linear-algebra.html#interactive-exercise",
    "title": "Linear Algebra for Statistical Methods",
    "section": "8 Interactive Exercise",
    "text": "8 Interactive Exercise\n\n8.1 Practice: Matching Statistical Methods to Matrix Operations\nUnderstanding which matrix operations underlie statistical methods helps you predict computational requirements and choose appropriate approaches. This exercise builds that connection.\n\n# Common statistical tasks - what operations do they need?\n\n# 1. Principal Component Analysis\npca_operations &lt;- function(X) {\n  # Center the data\n  # Compute X^T X or directly compute SVD of X\n  # Extract eigenvectors/values\n  # Question: Which approach is more memory-efficient?\n}\n\n# 2. Linear Regression  \nregression_operations &lt;- function(X, y) {\n  # Solve: (X^T X) β = X^T y\n  # Options: Direct inversion? QR decomposition? Normal equations?\n  # Question: Which is numerically stable for large p?\n}\n\n# 3. Correlation Matrix\ncorrelation_operations &lt;- function(X) {\n  # Center/scale X\n  # Compute X^T X / (n-1)\n  # Question: For p=50,000, what's the memory requirement?\n}\n\n\n\n\n\n\n\nTipReflection Questions\n\n\n\n1. Operation Sequence for PCA: - You need PCA on a 100,000 × 50,000 matrix - Approach A: Compute X^T X (50k × 50k), then eigen-decomposition - Approach B: Direct SVD of X in blocks - Which operations dominate computation time? - Which approach uses less memory?\n2. Regression with Many Predictors: - Fitting y ~ X where X is 10,000 × 100,000 - Computing (X^T X)^{-1} creates a 100,000 × 100,000 matrix - How much memory does just this inverse require? - Is direct inversion even practical? - What alternatives exist (QR? Iterative methods?)?\n3. Bottleneck Analysis: - Your workflow: Normalize → PCA → Regression - Which operation will be the memory bottleneck? - Which will take the most time? - Could you reorder operations to be more efficient?\n4. Block-Wise Feasibility: - Element-wise operations (centering): Easy to block-wise? - Matrix multiplication: Block-amenable? - Computing all pairwise correlations: Challenges? - What makes some operations harder to distribute?\n5. Practical Decisions: - You have 64 GB RAM, 200,000 × 100,000 matrix (≈160 GB) - Need to compute t(X) %*% X - Can BigDataStatMeth help? Which function? - What block size would be appropriate?\nThinking through these scenarios before encountering them in real analyses helps you make informed architectural decisions and understand why BigDataStatMeth makes certain implementation choices.",
    "crumbs": [
      "Core Concepts",
      "Linear Algebra for Statistical Methods"
    ]
  },
  {
    "objectID": "fundamentals/linear-algebra.html#key-takeaways",
    "href": "fundamentals/linear-algebra.html#key-takeaways",
    "title": "Linear Algebra for Statistical Methods",
    "section": "9 Key Takeaways",
    "text": "9 Key Takeaways\nLet’s consolidate your understanding of linear algebra operations in statistical computing and how they behave with large-scale data.\n\n9.1 Essential Concepts\nMatrix operations are the building blocks of statistical methods, even when hidden behind high-level function calls. Understanding PCA, regression, or correlation in terms of matrix operations (SVD, solving linear systems, computing crossproducts) helps you predict computational requirements and memory needs. Every statistical method ultimately reduces to a sequence of linear algebra operations.\nComputational costs vary dramatically by operation. Matrix multiplication is O(n³) in the worst case but highly optimized in practice. Inversions are expensive and numerically risky. Element-wise operations are cheap and memory-neutral. These cost differences matter when scaling to big data - an O(n³) operation may be impossible where O(n²) is feasible.\nMemory requirements aren’t always obvious. Multiplying an m×k matrix by a k×n matrix produces an m×n output. Even if inputs fit in memory, the output might not. This is why operations like computing all pairwise correlations (producing p×p from n×p) quickly become infeasible as p grows. Understanding output dimensions helps predict whether an operation will succeed.\nDecompositions are investments that pay off through reuse. Computing QR, SVD, or Cholesky takes time upfront but enables many subsequent operations to be faster, more stable, or more memory-efficient. For big data analyses involving repeated operations, computing a decomposition once and storing it often beats recomputing basic operations many times.\nBigDataStatMeth implements both in-memory and HDF5 versions of key operations. The mathematical operation is identical - X^T X produces the same result whether X lives in memory or on disk. But the computational strategy differs: in-memory operations process complete matrices at once, while HDF5 operations process blocks and manage disk I/O automatically.\nOperation choice affects feasibility more than implementation details. With big data, sometimes the question isn’t “how do I implement this faster?” but “is there a mathematically equivalent but computationally cheaper way to get this result?” Reformulating (X^T X)^{-1} X^T y as a QR solve changes the problem from impossible to practical.\n\n\n9.2 Operation Characteristics\nUnderstanding which operations work well with block-wise disk-based processing helps you design efficient analyses.\n✅ Efficient operations (fast, block-amenable):\n\n**Matrix multiplication (X %*% Y)** - Highly optimized, decomposes naturally into blocks. BigDataStatMeth’s implementations scale well.\n**Crossproducts (t(X) %*% X)** - Especially efficient because one matrix is transposed. Reduces memory requirements and I/O operations.\nElement-wise operations - Adding, subtracting, scaling by constants process independently. Perfect for block-wise with minimal overhead.\nRow/column operations - Means, sums, normalizations work naturally when blocks contain complete rows or columns.\n\n❌ Expensive operations (slow, memory-intensive):\n\nMatrix inversions - Computationally expensive O(n³) and numerically unstable. Avoid when possible in favor of solving linear systems or using decompositions.\nFull matrix factorizations - QR, Cholesky, eigendecompositions require multiple passes and careful numeric handling. Doable but requires sophisticated block-wise algorithms.\nOperations creating large outputs - Computing all pairwise operations (correlations, distances) produces O(n²) results. Even block-wise processing struggles when output explodes.\n\nFor most statistical analyses, the operations you need (crossproducts, multiplication, factorizations) have efficient implementations in BigDataStatMeth. More exotic operations may require case-by-case evaluation or algorithmic creativity to adapt for big data.",
    "crumbs": [
      "Core Concepts",
      "Linear Algebra for Statistical Methods"
    ]
  },
  {
    "objectID": "fundamentals/linear-algebra.html#learning-more",
    "href": "fundamentals/linear-algebra.html#learning-more",
    "title": "Linear Algebra for Statistical Methods",
    "section": "10 Learning More",
    "text": "10 Learning More\nThis section provided a practical overview of linear algebra operations in statistical computing. For deeper mathematical understanding:\nTheoretical foundations: - Matrix Computations by Golub & Van Loan - The definitive reference - Numerical Linear Algebra by Trefethen & Bau - Excellent intuition and theory\nStatistical applications: - The Elements of Statistical Learning by Hastie, Tibshirani & Friedman - Chapter 3 on linear methods - Computational Statistics by Gentle - Practical numerical methods\nBigDataStatMeth specifics: - API Reference - See which operations are implemented - Block-Wise Computing - How operations adapt for big data - Package vignettes - Worked examples using these operations",
    "crumbs": [
      "Core Concepts",
      "Linear Algebra for Statistical Methods"
    ]
  },
  {
    "objectID": "fundamentals/linear-algebra.html#next-steps",
    "href": "fundamentals/linear-algebra.html#next-steps",
    "title": "Linear Algebra for Statistical Methods",
    "section": "11 Next Steps",
    "text": "11 Next Steps\nWith understanding of the underlying mathematics, you’re ready to apply these operations to real data:\n\nGetting Started Tutorial → Work through a complete analysis\nPCA Workflow → See SVD applied to genomic data\nAPI Reference → Explore available functions\n\n\n\n\n\n\n\n\nNoteQuestions About Linear Algebra?\n\n\n\nIf mathematical concepts in the documentation are unclear, or you’d like more detailed explanations of specific operations, please open an issue. We’re happy to expand explanations based on user feedback.",
    "crumbs": [
      "Core Concepts",
      "Linear Algebra for Statistical Methods"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BigDataStatMeth",
    "section": "",
    "text": "NoteWelcome\n\n\n\nThis website provides comprehensive educational material for the BigDataStatMeth package. Here you’ll find in-depth explanations, tutorials, and practical examples to help you understand the fundamental concepts and develop new statistical methods for large-scale data analysis."
  },
  {
    "objectID": "index.html#what-is-bigdatastatmeth",
    "href": "index.html#what-is-bigdatastatmeth",
    "title": "BigDataStatMeth",
    "section": "1 What is BigDataStatMeth?",
    "text": "1 What is BigDataStatMeth?\nBigDataStatMeth is an R package that enables scalable statistical computing on datasets that exceed available memory. By combining:\n\nHDF5-based storage for disk-backed matrices\nBlock-wise algorithms for memory-efficient computation\n\nHigh-performance C++ backend with parallel processing\nDual R/C++ APIs for flexibility and integration\n\nBigDataStatMeth allows you to perform complex statistical analyses on large datasets using standard hardware."
  },
  {
    "objectID": "index.html#what-youll-learn-here",
    "href": "index.html#what-youll-learn-here",
    "title": "BigDataStatMeth",
    "section": "2 What You’ll Learn Here",
    "text": "2 What You’ll Learn Here\nThis documentation goes beyond the API reference to teach you the foundations you need to:\n\n2.1 Learning Objectives\n\nUnderstand why traditional in-memory approaches fail with large datasets\nMaster HDF5 file format and its role in big data computing\nGrasp block-wise algorithm design and implementation\nApply BigDataStatMeth to real-world statistical problems\nDevelop your own scalable statistical methods\nIntegrate BigDataStatMeth into complex analytical workflows"
  },
  {
    "objectID": "index.html#documentation-structure",
    "href": "index.html#documentation-structure",
    "title": "BigDataStatMeth",
    "section": "3 Documentation Structure",
    "text": "3 Documentation Structure\nThe documentation is organized as a progressive learning journey:\n\n3.1 Fundamentals\nLearn the core concepts that underpin BigDataStatMeth:\n\nThe Big Data Problem - Understanding memory limitations\nUnderstanding HDF5 - Deep dive into HDF5 storage format\nBlock-Wise Computing - Mathematical foundations and practical design\nLinear Algebra Essentials - Key operations and decompositions\n\n\n\n3.2 Tutorials\nStep-by-step guides to get you started:\n\nGetting Started - Installation and first steps\nWorking with HDF5 Matrices - Creating and managing data\nYour First Analysis - Complete analytical workflow\n\n\n\n3.3 Workflows\nComplete examples of implementing statistical methods:\n\nImplementing PCA - Principal Component Analysis from scratch\nImplementing CCA - Canonical Correlation Analysis\nCross-Platform Workflows - R and C++ integration\n\n\n\n3.4 API Reference\nTechnical documentation for all functions:\n\nR Functions - Complete R API documentation\nC++ API - C++ header-only library reference\n\n\n\n3.5 Technical Details\nAdvanced topics and optimization:\n\nPerformance Optimization - Benchmarks and tuning strategies"
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "BigDataStatMeth",
    "section": "4 Quick Start",
    "text": "4 Quick Start\n\nInstall from CRANInstall from GitHub\n\n\n# Install stable version from CRAN\ninstall.packages(\"BigDataStatMeth\")\n\n# Load package\nlibrary(BigDataStatMeth)\n\n\n# Install development version from GitHub\n# (requires devtools package)\ninstall.packages(\"devtools\")\ndevtools::install_github(\"isglobal-brge/BigDataStatMeth\")\n\n# Load package\nlibrary(BigDataStatMeth)\n\n\n\n\n4.1 Your First HDF5 Matrix\nset.seed(123)\ndata &lt;- matrix(rnorm(1000 * 500), nrow = 1000, ncol = 500)\n\nbdCreate_hdf5_matrix(\n  filename = \"my_analysis.hdf5\",\n  object = data,\n  group = \"data\",\n  dataset = \"matrix1\"\n)\n\n# Perform SVD on HDF5 data (without loading into memory)\nresult &lt;- bdSVD_hdf5(\n  filename = \"my_analysis.hdf5\",\n  group = \"data\",\n  dataset = \"matrix1\",\n  k = 10\n)"
  },
  {
    "objectID": "index.html#learning-path",
    "href": "index.html#learning-path",
    "title": "BigDataStatMeth",
    "section": "5 Learning Path",
    "text": "5 Learning Path\nWe recommend following this sequence:\n\nStart with Fundamentals if you’re new to HDF5 or block-wise computing\nFollow the Tutorials for hands-on practice with BigDataStatMeth\nStudy the Workflows to see complete method implementations\nRefer to API Reference when developing your own methods\nExplore Technical Details for optimization and advanced usage"
  },
  {
    "objectID": "index.html#getting-help",
    "href": "index.html#getting-help",
    "title": "BigDataStatMeth",
    "section": "6 Getting Help",
    "text": "6 Getting Help\n\nDocumentation: You’re here! Use the navigation menu to explore\nGitHub Issues: Report bugs or request features\nContact: BRGE ISGlobal"
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "BigDataStatMeth",
    "section": "7 Citation",
    "text": "7 Citation\nIf you use BigDataStatMeth in your research, please cite:\ncitation(\"BigDataStatMeth\")\nOr use this BibTeX entry:\n@Manual{BigDataStatMeth,\n  title = {BigDataStatMeth: Scalable Statistical Methods for Big Data},\n  author = {Dolors Pelegrí-Sisó and Juan R. González},\n  year = {2025},\n  note = {R package version 1.0.2},\n  url = {https://CRAN.R-project.org/package=BigDataStatMeth},\n}\n\nReady to start? Head to Understanding HDF5 to begin your learning journey!"
  },
  {
    "objectID": "tutorials/first-analysis.html",
    "href": "tutorials/first-analysis.html",
    "title": "Your First Analysis",
    "section": "",
    "text": "This tutorial walks you through a complete analysis workflow, from importing raw genomic data through quality control, statistical analysis, and result visualization. Unlike previous tutorials that focused on individual operations, here you’ll see how everything fits together into a reproducible analysis pipeline.\nThink of this as your template for real analyses. The specific dataset (simulated genomic SNPs) is less important than understanding the workflow structure: import → quality control → prepare data → analyze → validate → export. This pattern applies whether you’re analyzing genomics, climate data, or financial time series.\n\n\nBy the end of this tutorial, you will:\n\nExecute a complete analysis from raw data to final results\nApply quality control steps systematically (sample and feature filtering)\nHandle missing data through imputation\nPerform SVD/PCA on large matrices using block-wise algorithms\nExtract and interpret analysis results\nCreate publication-quality visualizations\nExport results for downstream analyses\nUnderstand how to adapt this workflow to your own data",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#overview",
    "href": "tutorials/first-analysis.html#overview",
    "title": "Your First Analysis",
    "section": "",
    "text": "This tutorial walks you through a complete analysis workflow, from importing raw genomic data through quality control, statistical analysis, and result visualization. Unlike previous tutorials that focused on individual operations, here you’ll see how everything fits together into a reproducible analysis pipeline.\nThink of this as your template for real analyses. The specific dataset (simulated genomic SNPs) is less important than understanding the workflow structure: import → quality control → prepare data → analyze → validate → export. This pattern applies whether you’re analyzing genomics, climate data, or financial time series.\n\n\nBy the end of this tutorial, you will:\n\nExecute a complete analysis from raw data to final results\nApply quality control steps systematically (sample and feature filtering)\nHandle missing data through imputation\nPerform SVD/PCA on large matrices using block-wise algorithms\nExtract and interpret analysis results\nCreate publication-quality visualizations\nExport results for downstream analyses\nUnderstand how to adapt this workflow to your own data",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#the-analysis-goal",
    "href": "tutorials/first-analysis.html#the-analysis-goal",
    "title": "Your First Analysis",
    "section": "2 The Analysis Goal",
    "text": "2 The Analysis Goal\nWe’ll analyze a simulated genomic dataset to:\n\nIdentify population structure using PCA\nRemove technical artifacts through quality control\nHandle missing data appropriately\nExtract principal components for downstream analyses\n\nThis workflow mirrors real genomic studies but uses simulated data for tutorial purposes.",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#prerequisites",
    "href": "tutorials/first-analysis.html#prerequisites",
    "title": "Your First Analysis",
    "section": "3 Prerequisites",
    "text": "3 Prerequisites\nComplete the previous tutorials:\n\nGetting Started\nWorking with HDF5 Matrices\n\nLoad required packages:\n\nlibrary(BigDataStatMeth)\nlibrary(rhdf5)\nlibrary(ggplot2)  # For visualization",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#step-1-create-and-import-data",
    "href": "tutorials/first-analysis.html#step-1-create-and-import-data",
    "title": "Your First Analysis",
    "section": "4 Step 1: Create and Import Data",
    "text": "4 Step 1: Create and Import Data\n\n4.1 Simulate Genomic Data\nIn real analyses, you’d load data from sequencing files. Here we simulate:\n\nset.seed(42)\n\n# Simulate genotype data: 500 samples × 2,000 SNPs\n# (Reduced size for tutorial - real analyses use much larger datasets)\nn_samples &lt;- 500\nn_snps &lt;- 2000\n\n# Create genotype matrix with some structure\n# Simulate 3 populations with different allele frequencies\npop1 &lt;- matrix(sample(0:2, 175*n_snps, replace = TRUE, \n                      prob = c(0.7, 0.2, 0.1)), 175, n_snps)\npop2 &lt;- matrix(sample(0:2, 175*n_snps, replace = TRUE,\n                      prob = c(0.2, 0.5, 0.3)), 175, n_snps)\npop3 &lt;- matrix(sample(0:2, 150*n_snps, replace = TRUE,\n                      prob = c(0.1, 0.3, 0.6)), 150, n_snps)\n\ngenotype &lt;- rbind(pop1, pop2, pop3)\n\n# Add some missing data (realistic scenario)\nmissing_idx &lt;- sample(length(genotype), length(genotype) * 0.02)\ngenotype[missing_idx] &lt;- NA\n\n# Add sample and SNP IDs\nrownames(genotype) &lt;- paste0(\"Sample_\", 1:n_samples)\ncolnames(genotype) &lt;- paste0(\"SNP_\", 1:n_snps)\n\n# Create population labels (for visualization later)\npopulation &lt;- c(rep(\"Pop1\", 175), rep(\"Pop2\", 175), rep(\"Pop3\", 150))\n\ncat(\"Genotype matrix created:\\n\")\n\nGenotype matrix created:\n\ncat(\" Dimensions:\", nrow(genotype), \"samples ×\", ncol(genotype), \"SNPs\\n\")\n\n Dimensions: 500 samples × 2000 SNPs\n\ncat(\" Missing data:\", round(sum(is.na(genotype))/length(genotype)*100, 2), \"%\\n\")\n\n Missing data: 2 %\n\n\n\n\n\n\n\n\nNoteAbout Dataset Size\n\n\n\nThis tutorial uses 500 samples × 2,000 SNPs for quick execution and web performance. Real genomic studies typically analyze much larger datasets:\n\nGWAS: 10,000-1,000,000 samples × 500,000-10,000,000 SNPs\nWhole genome: 100-10,000 samples × 3,000,000-100,000,000 variants\n\nBigDataStatMeth handles these large-scale datasets by processing data in blocks directly from HDF5 files.\n\n\n\n\n4.2 Import to HDF5\n\n# Create HDF5 file\nanalysis_file &lt;- \"genomic_analysis.hdf5\"\n\nbdCreate_hdf5_matrix(\n  filename = analysis_file,\n  object = genotype,\n  group = \"raw\",\n  dataset = \"genotypes\",\n  overwriteFile = TRUE\n)\n\n$fn\n[1] \"genomic_analysis.hdf5\"\n\n$ds\n[1] \"raw/genotypes\"\n\ncat(\"✓ Data imported to HDF5\\n\")\n\n✓ Data imported to HDF5\n\n# Check file structure\nh5ls(analysis_file)\n\n                     group                name       otype   dclass        dim\n0                        /                 raw   H5I_GROUP                    \n1                     /raw .genotypes_dimnames   H5I_GROUP                    \n2 /raw/.genotypes_dimnames                   1 H5I_DATASET COMPOUND        500\n3 /raw/.genotypes_dimnames                   2 H5I_DATASET COMPOUND       2000\n4                     /raw           genotypes H5I_DATASET  INTEGER 500 x 2000",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#step-2-quality-control",
    "href": "tutorials/first-analysis.html#step-2-quality-control",
    "title": "Your First Analysis",
    "section": "5 Step 2: Quality Control",
    "text": "5 Step 2: Quality Control\n\n5.1 Remove Low-Quality Samples\nSamples with too much missing data may indicate technical problems:\n\n# Remove samples with &gt;5% missing data\nqc_samples &lt;- bdRemovelowdata_hdf5(\n  filename = analysis_file,\n  group = \"raw\",\n  dataset = \"genotypes\",\n  outgroup = \"qc\",\n  outdataset = \"genotypes_samples_filtered\",\n  bycols = FALSE,  # Work on rows (samples)\n  pcent = 0.05     # 5% threshold\n)\n\ncat(\"✓ Sample QC completed\\n\")\n\n✓ Sample QC completed\n\ncat(\" Samples removed:\", qc_samples$removed_count, \"\\n\")\n\n Samples removed: \n\n\n\n\n5.2 Remove Low-Quality SNPs\nSNPs with low minor allele frequency (MAF) or high missingness:\n\n# First remove SNPs with &gt;5% missing data\nqc_snps_missing &lt;- bdRemovelowdata_hdf5(\n  filename = analysis_file,\n  group = \"qc\",\n  dataset = \"genotypes_samples_filtered\",\n  outgroup = \"qc\",\n  outdataset = \"genotypes_missing_filtered\",\n  bycols = TRUE,   # Work on columns (SNPs)\n  pcent = 0.05\n)\n\ncat(\"✓ SNP missing data QC completed\\n\")\n\n✓ SNP missing data QC completed\n\ncat(\" SNPs removed:\", qc_snps_missing$removed_count, \"\\n\")\n\n SNPs removed: \n\n\n\n# Then remove SNPs with MAF &lt; 5%\nqc_snps_maf &lt;- bdRemoveMAF_hdf5(\n  filename = analysis_file,\n  group = \"qc\",\n  dataset = \"genotypes_missing_filtered\",\n  outgroup = \"qc\",\n  outdataset = \"genotypes_qc_complete\",\n  maf = 0.05,\n  bycols = TRUE,\n  blocksize = 1000,\n  overwrite = TRUE\n)\n\ncat(\"✓ MAF filtering completed\\n\")\n\n✓ MAF filtering completed\n\n\n\n\n\n\n\n\nImportantQuality Control is Critical\n\n\n\nIn real genomic analyses, QC removes:\n\nTechnical artifacts: Failed samples, contamination\nUninformative markers: Rare variants, high missingness\nErrors: Genotyping errors, batch effects\n\nAlways perform QC before downstream analysis. The thresholds (5% missing, 5% MAF) are typical but should be adjusted based on your study design.\n\n\n\n\n5.3 Inspect QC Results\n\n# Check dimensions after QC\nh5file &lt;- H5Fopen(analysis_file)\n\ndim_raw &lt;- dim(h5file$raw$genotypes)\ndim_qc &lt;- dim(h5file$qc$genotypes_qc_complete)\n\nH5Fclose(h5file)\n\ncat(\"\\nDimensions before QC:\", dim_raw[1], \"×\", dim_raw[2], \"\\n\")\n\n\nDimensions before QC: 500 × 2000 \n\ncat(\"Dimensions after QC:\", dim_qc[1], \"×\", dim_qc[2], \"\\n\")\n\nDimensions after QC: 500 × 2000 \n\ncat(\"Samples retained:\", round(dim_qc[1]/dim_raw[1]*100, 1), \"%\\n\")\n\nSamples retained: 100 %\n\ncat(\"SNPs retained:\", round(dim_qc[2]/dim_raw[2]*100, 1), \"%\\n\")\n\nSNPs retained: 100 %",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#step-3-impute-missing-data",
    "href": "tutorials/first-analysis.html#step-3-impute-missing-data",
    "title": "Your First Analysis",
    "section": "6 Step 3: Impute Missing Data",
    "text": "6 Step 3: Impute Missing Data\nAfter QC, we still have some missing data. Impute using mean genotype:\n\n# Impute missing values (mean imputation by column)\nbdImputeSNPs_hdf5(\n  filename = analysis_file,\n  group = \"qc\",\n  dataset = \"genotypes_qc_complete\",\n  bycols = TRUE,\n  outgroup = \"qc\",\n  outdataset = \"genotypes_qc_complete\",  # Overwrite\n  overwrite = TRUE\n)\n\n$fn\n[1] \"genomic_analysis.hdf5\"\n\n$ds\n[1] \"qc/genotypes_qc_complete\"\n\ncat(\"✓ Missing data imputed\\n\")\n\n✓ Missing data imputed\n\n\n\n\n\n\n\n\nTipImputation Methods\n\n\n\nBigDataStatMeth uses mean imputation by default. For genomic data:\n\nMean imputation: Simple, fast, works for PCA/dimensionality reduction\nFor association studies: Consider more sophisticated methods (BEAGLE, IMPUTE2)\nAlways impute AFTER QC: Don’t impute bad data",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#step-4-perform-svd",
    "href": "tutorials/first-analysis.html#step-4-perform-svd",
    "title": "Your First Analysis",
    "section": "7 Step 4: Perform SVD",
    "text": "7 Step 4: Perform SVD\nNow perform SVD to extract principal components:\n\n# Perform hierarchical SVD\nsvd_result &lt;- bdSVD_hdf5(\n  file = analysis_file,\n  group = \"qc\",\n  dataset = \"genotypes_qc_complete\",\n  bcenter = TRUE,      # Center the data\n  bscale = FALSE,      # Don't scale (typical for genotypes)\n  k = 4,               # Number of blocks\n  q = 1,               # Number of hierarchy levels\n  threads = 2,         # Use 2 threads\n  overwrite = TRUE\n)\n\ncat(\"✓ SVD completed\\n\")\n\n✓ SVD completed\n\ncat(\" Results stored in:\", svd_result$group, \"\\n\")\n\n Results stored in: \n\n\n\n\n\n\n\n\nNoteSVD Parameters\n\n\n\n\nbcenter = TRUE: Essential - centers data to zero mean\nbscale = FALSE: Don’t standardize (typical for genotype data)\nk: Number of blocks (more = faster but approximation)\nq: Hierarchy levels (usually 1 or 2)\nthreads: Parallel processing (adjust to your system)\n\nFor huge datasets (&gt;100 GB), increase k and consider q = 2.",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#step-5-extract-and-examine-results",
    "href": "tutorials/first-analysis.html#step-5-extract-and-examine-results",
    "title": "Your First Analysis",
    "section": "8 Step 5: Extract and Examine Results",
    "text": "8 Step 5: Extract and Examine Results\nSVD produces three key outputs stored in the HDF5 file. Let’s extract them and see what they look like.\n\n8.1 Extract Principal Components\n\n# Open the file to access SVD results\nh5file &lt;- H5Fopen(analysis_file)\n\n# Get singular values (eigenvalues) - these tell us variance explained\nd_values &lt;- h5file$SVD$genotypes_qc_complete$d\n\n# Get left singular vectors (U) - sample loadings = our principal components\nu_matrix &lt;- h5file$SVD$genotypes_qc_complete$u\n\n# Get right singular vectors (V) - SNP loadings = feature contributions\nv_matrix &lt;- h5file$SVD$genotypes_qc_complete$v\n\nH5Fclose(h5file)\n\n# Add meaningful column names\ncolnames(u_matrix) &lt;- paste0(\"PC\", 1:ncol(u_matrix))\ncolnames(v_matrix) &lt;- paste0(\"PC\", 1:ncol(v_matrix))\n\ncat(\"✓ Extracted SVD components:\\n\")\n\n✓ Extracted SVD components:\n\ncat(\"  Singular values (d):\", length(d_values), \"values\\n\")\n\n  Singular values (d): 500 values\n\ncat(\"  Sample loadings (U):\", nrow(u_matrix), \"samples ×\", \n    ncol(u_matrix), \"components\\n\")\n\n  Sample loadings (U): 500 samples × 500 components\n\ncat(\"  SNP loadings (V):\", nrow(v_matrix), \"SNPs ×\", \n    ncol(v_matrix), \"components\\n\")\n\n  SNP loadings (V): 2000 SNPs × 500 components\n\n\nLet’s look at what these matrices actually contain:\n\n# Preview singular values\ncat(\"\\nFirst 10 singular values:\\n\")\n\n\nFirst 10 singular values:\n\nprint(round(d_values[1:10], 4))\n\n [1] 443.6651  45.8525  45.7652  45.6761  45.3012  45.2635  45.1448  44.9551\n [9]  44.8672  44.5770\n\n# Preview sample loadings (principal components)\ncat(\"\\nSample loadings (U matrix) - first 5 samples × first 5 PCs:\\n\")\n\n\nSample loadings (U matrix) - first 5 samples × first 5 PCs:\n\nprint(u_matrix[1:5, 1:5])\n\n             PC1           PC2           PC3         PC4          PC5\n[1,] -0.05674920 -0.0960807778  0.0008688585 -0.02656242  0.028933569\n[2,] -0.05583085  0.0006901278 -0.0246730710  0.01540518 -0.012621126\n[3,] -0.05799857 -0.0188249981  0.0472567892 -0.01120338 -0.045286206\n[4,] -0.05701164  0.0761657948 -0.0330258052  0.07085666  0.008256376\n[5,] -0.06035016 -0.0235459611  0.0327110433 -0.01996897  0.019395112\n\n# Preview SNP loadings  \ncat(\"\\nSNP loadings (V matrix) - first 5 SNPs × first 5 PCs:\\n\")\n\n\nSNP loadings (V matrix) - first 5 SNPs × first 5 PCs:\n\nprint(v_matrix[1:5, 1:5])\n\n            PC1          PC2         PC3          PC4          PC5\n[1,] 0.02023101  0.009783591 -0.02417201 -0.008130736  0.011707453\n[2,] 0.02286006  0.005131136 -0.02201926 -0.005259815 -0.006360324\n[3,] 0.02277471  0.031490093 -0.05116183 -0.002035219 -0.009526514\n[4,] 0.02020268 -0.002347363  0.05582095 -0.028978655  0.032544308\n[5,] 0.02327304 -0.020148580  0.02756623  0.005998072  0.042842917\n\n\n\n\n\n\n\n\nNoteUnderstanding the Outputs\n\n\n\nSingular values (d): Larger values = more important components. These decrease rapidly - the first few PCs capture most variation.\nU matrix (sample loadings): Each row is a sample, each column is a PC. Values show how much each sample “loads on” that PC. This is what we plot for population structure.\nV matrix (SNP loadings): Each row is a SNP, each column is a PC. Values show how much each SNP contributes to that PC. High absolute values = SNPs driving that component.\n\n\n\n\n8.2 Variance Explained\nNow let’s see how much variance each PC captures:\n\n# Calculate proportion of variance explained by each PC\nvariance_prop &lt;- (d_values^2) / sum(d_values^2)\n\n# Cumulative variance (running total)\ncumvar &lt;- cumsum(variance_prop)\n\ncat(\"\\nVariance explained by first 10 PCs:\\n\")\n\n\nVariance explained by first 10 PCs:\n\ncat(\"PC  | Individual | Cumulative\\n\")\n\nPC  | Individual | Cumulative\n\ncat(\"----+------------+-----------\\n\")\n\n----+------------+-----------\n\nfor(i in 1:min(10, length(variance_prop))) {\n  cat(sprintf(\"%-3d | %6.2f%%    | %6.2f%%\\n\", \n              i, variance_prop[i]*100, cumvar[i]*100))\n}\n\n1   |  29.43%    |  29.43%\n2   |   0.31%    |  29.74%\n3   |   0.31%    |  30.06%\n4   |   0.31%    |  30.37%\n5   |   0.31%    |  30.67%\n6   |   0.31%    |  30.98%\n7   |   0.30%    |  31.29%\n8   |   0.30%    |  31.59%\n9   |   0.30%    |  31.89%\n10  |   0.30%    |  32.19%\n\ncat(\"\\n✓ First\", min(10, length(variance_prop)), \"PCs explain\",\n    sprintf(\"%.1f%%\", cumvar[min(10, length(cumvar))]*100), \n    \"of total variance\\n\")\n\n\n✓ First 10 PCs explain 32.2% of total variance",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#step-6-visualize-results",
    "href": "tutorials/first-analysis.html#step-6-visualize-results",
    "title": "Your First Analysis",
    "section": "9 Step 6: Visualize Results",
    "text": "9 Step 6: Visualize Results\n\n9.1 Scree Plot\n\n# Prepare vectors for plotting\nn_show &lt;- min(20, length(variance_prop))\npc_nums &lt;- 1:n_show\nvar_pct &lt;- variance_prop[1:n_show] * 100\ncum_pct &lt;- cumvar[1:n_show] * 100\n\n# Create scree plot using vectors directly\nplot(pc_nums, var_pct, type = \"b\", col = \"steelblue\", lwd = 2, pch = 16,\n     xlab = \"Principal Component\", \n     ylab = \"Variance Explained (%)\",\n     main = \"Scree Plot: Variance Explained by Principal Components\",\n     ylim = c(0, max(cum_pct) * 1.1),\n     las = 1)\n\n# Add cumulative variance line\nlines(pc_nums, cum_pct, type = \"b\", col = \"darkred\", lwd = 2, \n      pch = 16, lty = 2)\n\n# Add legend\nlegend(\"right\", \n       legend = c(\"Individual Variance\", \"Cumulative Variance\"),\n       col = c(\"steelblue\", \"darkred\"), \n       lty = c(1, 2), \n       lwd = 2,\n       pch = 16,\n       bty = \"n\")\n\n# Add grid\ngrid(col = \"gray90\", lty = 1)\n\n\n\n\n\n\n\n\n\n\n9.2 PCA Plot\n\n# Create data frame for plotting\n# Match with population labels (accounting for QC filtering)\n# We'll use first dim_qc[1] samples\npca_data &lt;- data.frame(\n  PC1 = u_matrix[,1],\n  PC2 = u_matrix[,2],\n  PC3 = u_matrix[,3],\n  Population = population[1:nrow(u_matrix)]\n)\n\n# Plot PC1 vs PC2\nggplot(pca_data, aes(x = PC1, y = PC2, color = Population)) +\n  geom_point(alpha = 0.6, size = 2) +\n  scale_color_manual(values = c(\"Pop1\" = \"#E41A1C\", \n                                 \"Pop2\" = \"#377EB8\", \n                                 \"Pop3\" = \"#4DAF4A\")) +\n  labs(\n    title = \"Population Structure: PC1 vs PC2\",\n    subtitle = sprintf(\"PC1: %.1f%% variance | PC2: %.1f%% variance\",\n                       variance_prop[1]*100, variance_prop[2]*100),\n    x = sprintf(\"PC1 (%.1f%%)\", variance_prop[1]*100),\n    y = sprintf(\"PC2 (%.1f%%)\", variance_prop[2]*100)\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\"),\n    axis.text = element_text(size = 11),\n    axis.title = element_text(size = 12, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipInterpreting PCA Plots\n\n\n\nIn this simulated example, the three populations separate clearly on PC1 and PC2, which is expected since we simulated them with different allele frequencies.\nIn real data: - PC1/PC2 separation: Often reflects continental ancestry - Outliers: May indicate sample mix-ups or technical issues - Gradients: Reflect admixture or geographic structure - Clusters: Can represent discrete populations or family structure",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#step-7-save-results-for-downstream-use",
    "href": "tutorials/first-analysis.html#step-7-save-results-for-downstream-use",
    "title": "Your First Analysis",
    "section": "10 Step 7: Save Results for Downstream Use",
    "text": "10 Step 7: Save Results for Downstream Use\n\n10.1 Export PCs for Other Tools\n\n# Save PCs to text file for use in other software\npcs_export &lt;- data.frame(\n  SampleID = paste0(\"Sample_\", 1:nrow(u_matrix)),\n  u_matrix[, 1:10]  # First 10 PCs\n)\n\nwrite.table(pcs_export, \"principal_components.txt\",\n            row.names = FALSE, quote = FALSE, sep = \"\\t\")\n\ncat(\"✓ Principal components exported to principal_components.txt\\n\")\n\n✓ Principal components exported to principal_components.txt\n\n\n\n\n10.2 Document the Analysis\nEvery analysis should be documented. Let’s create a summary report with all key information:\n\n# Create comprehensive analysis summary\nanalysis_summary &lt;- sprintf(\"\nGenomic Analysis Summary\n========================\nDate: %s\n\nInput Data:\n- Samples: %d\n- SNPs: %d  \n- Missing data: %.2f%%\n\nQuality Control:\n- Samples after QC: %d (%.1f%% retained)\n- SNPs after QC: %d (%.1f%% retained)\n- MAF threshold: 5%%\n- Missing data threshold: 5%%\n\nSVD Parameters:\n- Centered: Yes\n- Scaled: No\n- Blocks (k): 4\n- Levels (q): 1\n- Components computed: %d\n\nVariance Explained:\n- PC1: %.2f%%\n- PC2: %.2f%%\n- PC1-10 cumulative: %.2f%%\n\nOutput Files:\n- HDF5 file: %s\n- PCs export: principal_components.txt\n- This summary: analysis_summary.txt\n\",\nSys.Date(),\ndim_raw[1], dim_raw[2], \nsum(is.na(genotype))/length(genotype)*100,\ndim_qc[1], dim_qc[1]/dim_raw[1]*100,\ndim_qc[2], dim_qc[2]/dim_raw[2]*100,\nlength(d_values),\nvariance_prop[1]*100, variance_prop[2]*100, cumvar[10]*100,\nanalysis_file\n)\n\n# Save to file\nwriteLines(analysis_summary, \"analysis_summary.txt\")\n\ncat(\"✓ Analysis summary saved to analysis_summary.txt\\n\\n\")\n\n✓ Analysis summary saved to analysis_summary.txt\n\ncat(\"Here's what was saved:\\n\")\n\nHere's what was saved:\n\ncat(strrep(\"=\", 70), \"\\n\")\n\n====================================================================== \n\ncat(analysis_summary)\n\n\nGenomic Analysis Summary\n========================\nDate: 2025-12-15\n\nInput Data:\n- Samples: 500\n- SNPs: 2000  \n- Missing data: 2.00%\n\nQuality Control:\n- Samples after QC: 500 (100.0% retained)\n- SNPs after QC: 2000 (100.0% retained)\n- MAF threshold: 5%\n- Missing data threshold: 5%\n\nSVD Parameters:\n- Centered: Yes\n- Scaled: No\n- Blocks (k): 4\n- Levels (q): 1\n- Components computed: 500\n\nVariance Explained:\n- PC1: 29.43%\n- PC2: 0.31%\n- PC1-10 cumulative: 32.19%\n\nOutput Files:\n- HDF5 file: genomic_analysis.hdf5\n- PCs export: principal_components.txt\n- This summary: analysis_summary.txt\n\ncat(strrep(\"=\", 70), \"\\n\")\n\n====================================================================== \n\n\n\n\n\n\n\n\nTipGood Documentation Practices\n\n\n\nAlways document: - Input data characteristics (size, missing data) - QC thresholds and results - Analysis parameters - Output locations - Date performed\nWhy this matters: - Reproducibility - you or others can recreate the analysis - Transparency - methods are clear - Debugging - if something’s wrong, you know what was done - Publications - you have all the details for methods sections\nThis summary file is perfect for README files or supplementary materials.",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#interactive-exercise",
    "href": "tutorials/first-analysis.html#interactive-exercise",
    "title": "Your First Analysis",
    "section": "11 Interactive Exercise",
    "text": "11 Interactive Exercise\n\n11.1 Practice: Modifying the Analysis Workflow\nUnderstanding a workflow means being able to modify it for different scenarios. This exercise challenges you to adapt what you’ve learned to new situations.\n\n# Exercise: Adapt this workflow for your research\n\n# Scenario 1: Different QC thresholds\n# Your dataset has higher missingness - adjust thresholds\nbdRemovelowdata_hdf5(\n  filename = \"genomic_data.hdf5\",\n  group = \"raw\", dataset = \"genotypes\",\n  outgroup = \"qc\", outdataset = \"samples_qc\",\n  bycols = FALSE, \n  pcent = 0.10  # Allow 10% missing instead of 5%\n)\n\n# Scenario 2: More stringent MAF filtering\nbdRemoveMAF_hdf5(\n  filename = \"genomic_data.hdf5\",\n  group = \"qc\", dataset = \"snps_qc\",\n  outgroup = \"qc\", outdataset = \"snps_qc_maf\",\n  maf = 0.10  # Remove variants &lt;10% frequency\n)\n\n# Scenario 3: Different SVD parameters\n# You want more PCs but with less accuracy for speed\nbdSVD_hdf5(\n  filename = \"genomic_data.hdf5\",\n  group = \"qc\", dataset = \"imputed\",\n  k = 10,   # More PCs\n  q = 1,    # Fewer hierarchy levels (faster)\n  bcenter = TRUE,\n  bscale = FALSE\n)\n\n# Scenario 4: Add your own visualization\n# What would you plot differently?\n# - Top 3 PCs in 3D?\n# - Loadings to identify influential SNPs?\n# - Correlation of PCs with phenotypes?\n\n\n\n\n\n\n\nTipReflection Questions\n\n\n\nThink about how you’d adapt this workflow to your specific needs:\n1. Quality Control Decisions: - What QC thresholds are appropriate for your data? - Do you need additional filters (Hardy-Weinberg equilibrium, relatedness)? - How do you decide what’s “low quality” vs. “real signal”? - When should you be more vs. less stringent?\n2. Missing Data Strategy: - Mean imputation is simple but crude - when is it acceptable? - What alternatives exist (mode, k-NN, matrix completion)? - How much missingness is too much to impute validly? - Should you remove high-missingness markers before imputing?\n3. SVD Parameters: - How many principal components do you need? - Trade-off: k=10 with q=2 vs. k=40 with q=1? - When does increased accuracy justify increased computation time? - How do you verify results are accurate enough?\n4. Validation Steps: - How do you know your QC worked correctly? - What checks confirm SVD completed properly? - How do you detect if you removed too much/little data? - When should you see red flags in the results?\n5. Scaling to Your Data: - This tutorial used 500 samples × 2,000 SNPs - What if you have 50,000 samples × 500,000 SNPs? - Which steps become bottlenecks? - Where would you adjust block sizes, threads, or parameters?\n6. Workflow Modifications: - What if you’re analyzing transcriptomics instead of genomics? - Different data types (time series, images, sensor networks)? - How would you adapt each step? - Which parts are generalizable, which are domain-specific?\nThe best way to learn is by trying. Copy this workflow, modify it step by step, see what breaks and what works. Understanding why something fails teaches as much as getting it right.",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#cleanup",
    "href": "tutorials/first-analysis.html#cleanup",
    "title": "Your First Analysis",
    "section": "12 Cleanup",
    "text": "12 Cleanup\n\n# Close HDF5 connections\nh5closeAll()\n\n# Keep the HDF5 file and results for review\n# Remove intermediate exports if desired:\n# file.remove(\"principal_components.txt\", \"analysis_summary.txt\")\n\ncat(\"✓ Analysis complete!\\n\")\n\n✓ Analysis complete!",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#what-youve-learned",
    "href": "tutorials/first-analysis.html#what-youve-learned",
    "title": "Your First Analysis",
    "section": "13 What You’ve Learned",
    "text": "13 What You’ve Learned\n✅ Complete workflow: From raw data to interpretable results\n✅ Quality control: Sample and SNP filtering\n✅ Data preparation: Missing data imputation\n✅ SVD analysis: Dimensionality reduction on large matrices\n✅ Visualization: Scree plots and PCA plots\n✅ Results export: Preparing data for downstream analyses",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#next-steps",
    "href": "tutorials/first-analysis.html#next-steps",
    "title": "Your First Analysis",
    "section": "14 Next Steps",
    "text": "14 Next Steps\nExplore advanced workflows:\n\nImplementing PCA - Real genomic data, advanced visualization\nImplementing CCA - Multi-omics integration\nCross-Platform Workflows - Using results in Python, C++\n\nOptimize for your data:\n\nAdjust QC thresholds based on your study\nTry different SVD parameters (k, q) for speed vs accuracy\nUse more PCs if data has complex structure\nIntegrate covariates in downstream analyses\n\nLearn the theory:\n\nBlock-Wise Computing - How SVD scales to large data\nLinear Algebra Refresher - Math behind PCA/SVD",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/first-analysis.html#key-takeaways",
    "href": "tutorials/first-analysis.html#key-takeaways",
    "title": "Your First Analysis",
    "section": "15 Key Takeaways",
    "text": "15 Key Takeaways\nLet’s consolidate what you’ve learned about executing complete analyses with BigDataStatMeth, from raw data through final results.\n\n15.1 Essential Concepts\nAnalysis workflows follow a consistent structure regardless of domain or data type. Import → quality control → data preparation → analysis → validation → export. This pattern applies whether you’re analyzing genomics, climate data, financial time series, or sensor networks. The specific QC steps and analysis methods change, but the overall structure remains constant. Understanding this framework helps you design new analyses systematically rather than ad hoc, and recognize where different projects share common challenges.\nQuality control must come before analysis, not after. Running PCA on data with 30% missing values, duplicate samples, or technical artifacts produces misleading results that look mathematically valid but are scientifically meaningless. You can’t fix bad input with good algorithms - garbage in, garbage out applies universally. Investing time in QC upfront determines whether your results are interpretable or garbage. The temptation to skip QC and “just see what happens” is strong, but resist it. QC failures discovered after weeks of analysis waste far more time than doing QC correctly from the start.\nMissing data imputation has consequences that affect downstream analyses. Mean imputation (replacing missing values with column means) is computationally simple but statistically crude - it reduces variance artificially and can create patterns where none exist. It’s acceptable for low missingness (&lt;5%) where impact is minimal. Higher missingness requires more sophisticated methods (k-NN imputation, matrix completion) or removal of high-missingness features entirely. There’s no free lunch: every imputation method introduces assumptions, and those assumptions propagate through your analyses affecting final conclusions.\nCentering versus scaling matters for interpretation and affects what your principal components mean. Centering (subtracting column means) is almost always needed for PCA - it ensures PC1 doesn’t just capture mean differences between features. Scaling (dividing by standard deviations) is more controversial: it gives equal weight to all features, which helps when features have different units (mixing expression and metabolites) but can obscure real biological differences in variability. For genomic SNPs where all features are the same type, we typically center but don’t scale. Understanding these choices helps you interpret what patterns the PCs are actually capturing.\nVariance explained tells you if PCA succeeded. If PC1-PC10 only explain 20% of total variance, your data either has very high true dimensionality (many independent biological signals) or very high noise (low signal-to-noise ratio). If PC1 alone explains 80%, you have a strong dominant pattern - but verify it’s real biological structure, not a technical artifact like batch effects or ancestry. Inspecting variance explained is your first diagnostic of whether the analysis produced meaningful results or just decomposed noise.\nBlock-wise processing is transparent to the user, hiding complexity while enabling scalability. You called bdSVD_hdf5() exactly as you would any function - one function call, simple arguments. Behind the scenes, BigDataStatMeth partitioned your matrix into blocks, computed local SVDs, hierarchically merged results, and produced final output. This happened automatically without you writing partitioning logic or managing intermediate results. This abstraction is the package’s core value: sophisticated algorithms exposed through simple interfaces.\nDocumentation is part of the analysis, not an afterthought. The analysis summary you created documents everything needed to reproduce the work: data sources, QC thresholds, parameters, software versions, dates. Without this documentation, you’ll forget critical details within weeks. Future you (revisiting the analysis), collaborators (extending the work), and reviewers (verifying methods) all depend on thorough documentation. Make documentation a habit from the start, not something you do “if you have time” before submitting papers.\n\n\n15.2 When to Apply This Workflow\nUnderstanding which analyses benefit from this structured approach versus when simpler methods suffice helps you work efficiently.\n✅ Use this complete workflow when:\n\nYour dataset exceeds memory limits - When traditional prcomp() or svd() fail with memory errors, BigDataStatMeth’s disk-based approach becomes necessary, not optional. The workflow structure (HDF5 storage, QC, analysis, export) handles data that doesn’t fit in RAM.\nQuality control is critical for validity - Genomic studies, clinical trials, sensor networks - anywhere that data quality directly affects scientific conclusions. Following QC → imputation → analysis prevents analyzing garbage and drawing false conclusions.\nYou’ll run multiple analyses on the same data - Converting to HDF5 and performing QC once pays off when you subsequently run PCA, regression, association tests, and more. The upfront investment amortizes across all downstream analyses.\nReproducibility and documentation matter - Academic research, regulatory submissions, collaborative projects - anywhere others need to understand and verify your methods. The structured workflow with documented parameters makes reproducibility natural rather than an afterthought.\nThe analysis will be extended or repeated - If you’ll add more samples later, rerun with updated QC thresholds, or apply the same pipeline to new datasets, having a documented workflow saves enormous time. Copy the structure, adjust parameters, execute.\n\n✅ Adapt the workflow when:\n\nAnalyzing different data types - Transcriptomics needs log-transformation and batch correction instead of MAF filtering. Time series needs detrending instead of missing data imputation. Climate data needs spatial autocorrelation handling. The workflow structure (import → QC → prepare → analyze) remains constant, but specific operations change.\nQC requirements differ - Exploratory analyses tolerate higher missingness than GWAS. Population structure studies need less stringent filtering than association tests. Adjust QC thresholds to match your analysis goals and acceptable false discovery rates.\nComputational resources vary - With 256 GB RAM, you might skip HDF5 for 40 GB datasets. With 16 GB RAM, you need disk-based computing for 10 GB datasets. Adjust block sizes (k), hierarchy levels (q), and threading based on your available resources.\n\n❌ Simpler approaches suffice when:\n\nData fits comfortably in memory - If data &lt;- read.csv(file); svd(data) works without issues, the added complexity of HDF5 storage and block-wise processing provides minimal benefit. Use familiar tools that work.\nOne-off exploratory analysis - Quick investigations you won’t repeat or extend don’t justify elaborate workflows. Load data, run analysis, save key results, move on. Don’t over-engineer temporary explorations.\nNo quality control needed - Clean, well-curated datasets (like published benchmark data) don’t need extensive QC. Applying QC steps that aren’t necessary wastes time without improving results.\n\nThe key insight is matching workflow complexity to problem complexity. Large-scale, quality-sensitive, repeated analyses justify this structured approach. Small-scale, one-off, exploratory work doesn’t. Choose appropriately based on your specific situation rather than applying the same workflow everywhere.",
    "crumbs": [
      "Getting Started",
      "Your First Analysis"
    ]
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "NoteLearn by Doing\n\n\n\nTutorials provide structured learning from first steps to complete analyses. Work through these examples to build your skills with BigDataStatMeth."
  },
  {
    "objectID": "tutorials/index.html#progressive-learning-path",
    "href": "tutorials/index.html#progressive-learning-path",
    "title": "Tutorials",
    "section": "1 Progressive Learning Path",
    "text": "1 Progressive Learning Path\nThese tutorials are designed to be followed in sequence, each building on concepts from the previous one. By the end, you’ll be able to perform complete statistical analyses on large-scale datasets."
  },
  {
    "objectID": "tutorials/index.html#tutorial-sequence",
    "href": "tutorials/index.html#tutorial-sequence",
    "title": "Tutorials",
    "section": "2 Tutorial Sequence",
    "text": "2 Tutorial Sequence\n\n\n\n\n\n\n\n\nTipGetting Started\n\n\n\nFoundation skills\nLearn the basics of BigDataStatMeth: installation, setup, and your first operations with the package.\nStart here if you’re new to BigDataStatMeth.\nBegin tutorial →\n\n\n\n\n\n\n\n\n\n\nWarningWorking with HDF5 Matrices\n\n\n\nCore operations\nMaster the essentials of working with large matrices stored in HDF5 format. Learn to create, manipulate, and retrieve data efficiently.\nContinue learning →\n\n\n\n\n\n\n\n\n\n\nImportantYour First Analysis\n\n\n\nComplete workflow\nPut everything together in a complete analysis. Apply what you’ve learned to solve a real statistical problem from start to finish.\nComplete your training →\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat’s Next?\n\n\n\nAfter completing these tutorials, explore Practical Workflows to see how these concepts apply to real research problems, or consult the API Reference for detailed function documentation."
  },
  {
    "objectID": "workflows/cross-platform.html",
    "href": "workflows/cross-platform.html",
    "title": "Cross-Platform Workflows",
    "section": "",
    "text": "Modern data science rarely happens in a single language. You might perform initial analysis in R, switch to Python for machine learning, then implement production algorithms in C++. BigDataStatMeth supports this multi-language reality through HDF5’s universal format - one file, multiple platforms, no conversion overhead.\nThis workflow demonstrates how to use BigDataStatMeth results across R, Python, and C++. You’ll perform PCA in R using BigDataStatMeth, access results in Python for visualization with scikit-learn, and implement a custom analysis in C++ using the BigDataStatMeth library. The HDF5 file acts as the common currency, enabling seamless platform transitions without data duplication or format conversion.\n\n\nBy the end of this workflow, you will:\n\nPerform analyses in R and access results in Python\nUse h5py in Python to read BigDataStatMeth HDF5 files\nImplement custom algorithms in C++ using BigDataStatMeth headers\nUnderstand HDF5 as a cross-platform data exchange format\nNavigate file structures consistently across languages\nCombine R statistical analysis with Python visualization\nExtend BigDataStatMeth functionality in C++\nChoose the right language for each analysis step",
    "crumbs": [
      "Complete Examples",
      "Cross-Platform Workflows"
    ]
  },
  {
    "objectID": "workflows/cross-platform.html#overview",
    "href": "workflows/cross-platform.html#overview",
    "title": "Cross-Platform Workflows",
    "section": "",
    "text": "Modern data science rarely happens in a single language. You might perform initial analysis in R, switch to Python for machine learning, then implement production algorithms in C++. BigDataStatMeth supports this multi-language reality through HDF5’s universal format - one file, multiple platforms, no conversion overhead.\nThis workflow demonstrates how to use BigDataStatMeth results across R, Python, and C++. You’ll perform PCA in R using BigDataStatMeth, access results in Python for visualization with scikit-learn, and implement a custom analysis in C++ using the BigDataStatMeth library. The HDF5 file acts as the common currency, enabling seamless platform transitions without data duplication or format conversion.\n\n\nBy the end of this workflow, you will:\n\nPerform analyses in R and access results in Python\nUse h5py in Python to read BigDataStatMeth HDF5 files\nImplement custom algorithms in C++ using BigDataStatMeth headers\nUnderstand HDF5 as a cross-platform data exchange format\nNavigate file structures consistently across languages\nCombine R statistical analysis with Python visualization\nExtend BigDataStatMeth functionality in C++\nChoose the right language for each analysis step",
    "crumbs": [
      "Complete Examples",
      "Cross-Platform Workflows"
    ]
  },
  {
    "objectID": "workflows/cross-platform.html#prerequisites",
    "href": "workflows/cross-platform.html#prerequisites",
    "title": "Cross-Platform Workflows",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\n\n2.1 Python Setup for R Users\nThis workflow uses Python from R through the reticulate package. If you haven’t used Python with R before, follow these steps:\nInstall reticulate and configure Python:\n\n# Install reticulate if needed\nif (!require(\"reticulate\", quietly = TRUE)) {\n  install.packages(\"reticulate\")\n}\n\nlibrary(reticulate)\n\n# Check if Python is available\npy_config()\n\npython:         /Users/mailos/.virtualenvs/r-reticulate/bin/python\nlibpython:      /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/config-3.9-darwin/libpython3.9.dylib\npythonhome:     /Users/mailos/.virtualenvs/r-reticulate:/Users/mailos/.virtualenvs/r-reticulate\nversion:        3.9.6 (default, Apr 30 2025, 02:07:17)  [Clang 17.0.0 (clang-1700.0.13.5)]\nnumpy:          /Users/mailos/.virtualenvs/r-reticulate/lib/python3.9/site-packages/numpy\nnumpy_version:  2.0.2\n\n\nInstall h5py Python package:\nThe first time you run this workflow, you’ll need to install h5py (Python’s HDF5 library):\n\n# Install h5py in your Python environment\nif (!py_module_available(\"h5py\")) {\n  py_install(\"h5py\")\n}\n\n# Also install other packages we'll use\nrequired_packages &lt;- c(\"numpy\", \"pandas\", \"matplotlib\", \"scikit-learn\")\nfor (pkg in required_packages) {\n  if (!py_module_available(pkg)) {\n    py_install(pkg)\n  }\n}\n\nUsing virtual environment '/Users/mailos/.virtualenvs/r-reticulate' ...\n\n\n\n\n\n\n\n\nNotePython Environment Notes\n\n\n\nIf you get “ModuleNotFoundError: No module named ‘h5py’”:\nRun py_install(\"h5py\") in R. Reticulate will install it in the appropriate Python environment.\nNote: The package for scikit-learn is installed with py_install(\"scikit-learn\") but imported in Python as import sklearn.\nVirtual environments:\nReticulate automatically creates and manages a Python virtual environment for R. You don’t need to manually create one unless you want specific Python version control.\nChecking installation:\n# Verify h5py is available\nreticulate::py_module_available(\"h5py\")  # Should return TRUE\nIf you continue having issues, check your Python configuration with py_config() and ensure you have Python 3.7+ installed.\n\n\n\n\n2.2 Load R Packages\n\nlibrary(BigDataStatMeth)\nlibrary(rhdf5)\nlibrary(reticulate)",
    "crumbs": [
      "Complete Examples",
      "Cross-Platform Workflows"
    ]
  },
  {
    "objectID": "workflows/cross-platform.html#the-cross-platform-scenario",
    "href": "workflows/cross-platform.html#the-cross-platform-scenario",
    "title": "Cross-Platform Workflows",
    "section": "3 The Cross-Platform Scenario",
    "text": "3 The Cross-Platform Scenario\nWe’ll implement a realistic multi-language workflow:\nStep 1 (R): Perform PCA on genomic data using BigDataStatMeth\nStep 2 (Python): Load results, create advanced visualizations, train classifiers\nStep 3 (C++): Implement custom projection algorithm for new samples\nThis mirrors real workflows where different languages excel at different tasks: - R for statistical analysis and data preparation - Python for machine learning and modern visualization libraries - C++ for performance-critical custom algorithms",
    "crumbs": [
      "Complete Examples",
      "Cross-Platform Workflows"
    ]
  },
  {
    "objectID": "workflows/cross-platform.html#step-1-analysis-in-r",
    "href": "workflows/cross-platform.html#step-1-analysis-in-r",
    "title": "Cross-Platform Workflows",
    "section": "4 Step 1: Analysis in R",
    "text": "4 Step 1: Analysis in R\n\n4.1 Create Example Data\nWe’ll create synthetic genomic data with population structure:\n\nlibrary(BigDataStatMeth)\nlibrary(rhdf5)\n\nset.seed(42)\nn_samples &lt;- 1000\nn_snps &lt;- 5000\n\n# Create genotypes with population structure\ngenotypes &lt;- matrix(\n  sample(0:2, n_samples * n_snps, replace = TRUE, prob = c(0.25, 0.5, 0.25)),\n  nrow = n_samples,\n  ncol = n_snps\n)\n\n# Add sample and SNP names\nsample_ids &lt;- paste0(\"SAMPLE_\", sprintf(\"%04d\", 1:n_samples))\nsnp_ids &lt;- paste0(\"rs\", sprintf(\"%07d\", 1:n_snps))\n\nrownames(genotypes) &lt;- sample_ids\ncolnames(genotypes) &lt;- snp_ids\n\n# Create sample metadata\nsample_metadata &lt;- data.frame(\n  sample_id = sample_ids,\n  population = rep(c(\"EUR\", \"AFR\", \"EAS\", \"SAS\", \"AMR\"), each = n_samples/5),\n  age = sample(20:80, n_samples, replace = TRUE),\n  stringsAsFactors = FALSE\n)\n\ncat(\"Data created\\n\")\n\nData created\n\ncat(\"  Genotypes:\", nrow(genotypes), \"samples ×\", ncol(genotypes), \"SNPs\\n\")\n\n  Genotypes: 1000 samples × 5000 SNPs\n\ncat(\"  Populations:\", paste(names(table(sample_metadata$population)), collapse = \", \"), \"\\n\")\n\n  Populations: AFR, AMR, EAS, EUR, SAS \n\n\n\n\n4.2 Store Data in HDF5\n\n# Create HDF5 file\ncross_platform_file &lt;- \"cross_platform_analysis.hdf5\"\n\n# Store genotype data\nbdCreate_hdf5_matrix(\n  filename = cross_platform_file,\n  object = genotypes,\n  group = \"data\",\n  dataset = \"genotypes\",\n  overwriteFile = TRUE\n)\n\n$fn\n[1] \"cross_platform_analysis.hdf5\"\n\n$ds\n[1] \"data/genotypes\"\n\ncat(\"Genotypes stored in HDF5\\n\")\n\nGenotypes stored in HDF5\n\n\n\n\n4.3 Add Metadata Using rhdf5\nFor metadata (mixed data types like population labels, age), use rhdf5 directly:\n\n# Save metadata to HDF5 using rhdf5\nlibrary(rhdf5)\n\n# Create metadata group\nh5createGroup(cross_platform_file, \"metadata\")\n\n# Write metadata (rhdf5 handles mixed types automatically)\nh5write(sample_metadata, cross_platform_file, \"metadata/samples\")\n\ncat(\"Metadata stored in HDF5\\n\")\n\nMetadata stored in HDF5\n\n# Verify file structure\nh5ls(cross_platform_file)\n\n                      group                name       otype   dclass\n0                         /                data   H5I_GROUP         \n1                     /data .genotypes_dimnames   H5I_GROUP         \n2 /data/.genotypes_dimnames                   1 H5I_DATASET COMPOUND\n3 /data/.genotypes_dimnames                   2 H5I_DATASET COMPOUND\n4                     /data           genotypes H5I_DATASET  INTEGER\n5                         /            metadata   H5I_GROUP         \n6                 /metadata             samples H5I_DATASET COMPOUND\n          dim\n0            \n1            \n2        1000\n3        5000\n4 1000 x 5000\n5            \n6        1000\n\n\n\n\n\n\n\n\nTipWhen to Use rhdf5 vs BigDataStatMeth\n\n\n\nUse BigDataStatMeth functions (bdCreate_hdf5_matrix) for: - Numeric matrices (genotypes, expression, methylation) - Data that will be used in BigDataStatMeth analyses\nUse rhdf5 directly for: - Metadata with mixed types (characters, factors, integers) - Annotations and sample information - Flexible data structures not used in matrix computations\nBigDataStatMeth focuses on matrix operations, while rhdf5 provides general HDF5 I/O.\n\n\n\n\n4.4 Perform PCA\n\n# Perform PCA\npca_result &lt;- bdPCA_hdf5( filename = cross_platform_file, \n                          group = \"data\", dataset = \"genotypes\",\n                          k = 4, q = 1, bcenter = TRUE, bscale = FALSE, \n                          ncomponents = 20, threads = 4, overwrite = TRUE )\n\ncat(\"PCA complete in R\\n\")\n\nPCA complete in R\n\ncat(\"  Results location:\", pca_result$group, \"\\n\")\n\n  Results location: \n\n# Show what was created\nh5ls(cross_platform_file)\n\n                        group                name       otype   dclass\n0                           /        NORMALIZED_T   H5I_GROUP         \n1               /NORMALIZED_T                data   H5I_GROUP         \n2          /NORMALIZED_T/data           genotypes H5I_DATASET    FLOAT\n3          /NORMALIZED_T/data             mean_sd   H5I_GROUP         \n4  /NORMALIZED_T/data/mean_sd      mean.genotypes H5I_DATASET    FLOAT\n5  /NORMALIZED_T/data/mean_sd        sd.genotypes H5I_DATASET    FLOAT\n6                           /                 PCA   H5I_GROUP         \n7                        /PCA           genotypes   H5I_GROUP         \n8              /PCA/genotypes          components H5I_DATASET    FLOAT\n9              /PCA/genotypes              cumvar H5I_DATASET    FLOAT\n10             /PCA/genotypes         ind.contrib H5I_DATASET    FLOAT\n11             /PCA/genotypes           ind.coord H5I_DATASET    FLOAT\n12             /PCA/genotypes            ind.cos2 H5I_DATASET    FLOAT\n13             /PCA/genotypes            ind.dist H5I_DATASET    FLOAT\n14             /PCA/genotypes              lambda H5I_DATASET    FLOAT\n15             /PCA/genotypes           var.coord H5I_DATASET    FLOAT\n16             /PCA/genotypes            var.cos2 H5I_DATASET    FLOAT\n17             /PCA/genotypes            variance H5I_DATASET    FLOAT\n18                          /                 SVD   H5I_GROUP         \n19                       /SVD           genotypes   H5I_GROUP         \n20             /SVD/genotypes                   d H5I_DATASET    FLOAT\n21             /SVD/genotypes                   u H5I_DATASET    FLOAT\n22             /SVD/genotypes                   v H5I_DATASET    FLOAT\n23                          /                data   H5I_GROUP         \n24                      /data .genotypes_dimnames   H5I_GROUP         \n25  /data/.genotypes_dimnames                   1 H5I_DATASET COMPOUND\n26  /data/.genotypes_dimnames                   2 H5I_DATASET COMPOUND\n27                      /data           genotypes H5I_DATASET  INTEGER\n28                          /            metadata   H5I_GROUP         \n29                  /metadata             samples H5I_DATASET COMPOUND\n30                          /         predictions H5I_DATASET   STRING\n           dim\n0             \n1             \n2  1000 x 5000\n3             \n4     5000 x 1\n5     5000 x 1\n6             \n7             \n8  1000 x 1000\n9     1000 x 1\n10 1000 x 1000\n11 1000 x 1000\n12 1000 x 1000\n13    1000 x 1\n14    1000 x 1\n15 1000 x 5000\n16 1000 x 5000\n17    1000 x 1\n18            \n19            \n20    1 x 1000\n21 1000 x 1000\n22 5000 x 1000\n23            \n24            \n25        1000\n26        5000\n27 1000 x 5000\n28            \n29        1000\n30         200",
    "crumbs": [
      "Complete Examples",
      "Cross-Platform Workflows"
    ]
  },
  {
    "objectID": "workflows/cross-platform.html#step-2-visualization-and-ml-in-python",
    "href": "workflows/cross-platform.html#step-2-visualization-and-ml-in-python",
    "title": "Cross-Platform Workflows",
    "section": "5 Step 2: Visualization and ML in Python",
    "text": "5 Step 2: Visualization and ML in Python\n\n5.1 Access HDF5 from Python\nSwitch to Python to access the same HDF5 file. We’ll use the reticulate package to run Python code from R:\n\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Open HDF5 file in read/write mode\nf = h5py.File('cross_platform_analysis.hdf5', 'r+')\n\n# List contents\nprint(\"HDF5 file structure:\")\n\nHDF5 file structure:\n\ndef print_structure(name, obj):\n    print(name)\n    \nf.visititems(print_structure)\n\nNORMALIZED_T\nNORMALIZED_T/data\nNORMALIZED_T/data/genotypes\nNORMALIZED_T/data/mean_sd\nNORMALIZED_T/data/mean_sd/mean.genotypes\nNORMALIZED_T/data/mean_sd/sd.genotypes\nPCA\nPCA/genotypes\nPCA/genotypes/components\nPCA/genotypes/cumvar\nPCA/genotypes/ind.contrib\nPCA/genotypes/ind.coord\nPCA/genotypes/ind.cos2\nPCA/genotypes/ind.dist\nPCA/genotypes/lambda\nPCA/genotypes/var.coord\nPCA/genotypes/var.cos2\nPCA/genotypes/variance\nSVD\nSVD/genotypes\nSVD/genotypes/d\nSVD/genotypes/u\nSVD/genotypes/v\ndata\ndata/.genotypes_dimnames\ndata/.genotypes_dimnames/1\ndata/.genotypes_dimnames/2\ndata/genotypes\nmetadata\nmetadata/samples\n\n# Load PCA components\ncomponents = f['PCA/genotypes/components'][:]\nvariance_explained = f['PCA/genotypes/variance'][:]\n\n# Ensure variance_explained is 1D array\nvariance_explained = np.array(variance_explained).flatten()\n\n# Load metadata\nmetadata_raw = f['metadata/samples']\nmetadata = pd.DataFrame({\n    'sample_id': metadata_raw['sample_id'][:],\n    'population': metadata_raw['population'][:],\n    'age': metadata_raw['age'][:]\n})\n\n# Decode byte strings to regular strings\nmetadata['sample_id'] = [s.decode('utf-8') if isinstance(s, bytes) else s \n                         for s in metadata['sample_id']]\nmetadata['population'] = [s.decode('utf-8') if isinstance(s, bytes) else s \n                          for s in metadata['population']]\n\nprint(\"\\n✓ Data loaded in Python\")\n\n\n✓ Data loaded in Python\n\nprint(f\"  Components shape: {components.shape}\")\n\n  Components shape: (1000, 1000)\n\nprint(f\"  Metadata shape: {metadata.shape}\")\n\n  Metadata shape: (1000, 3)\n\nprint(f\"  Populations: {metadata['population'].unique()}\")\n\n  Populations: ['EUR' 'AFR' 'EAS' 'SAS' 'AMR']\n\n\n\n\n\n\n\n\nNoteDecoding String Data in Python\n\n\n\nHDF5 stores strings as bytes. When reading in Python, decode them:\n# Byte strings need decoding\nmetadata['sample_id'] = [s.decode('utf-8') if isinstance(s, bytes) else s \n                         for s in metadata['sample_id']]\nmetadata['population'] = [s.decode('utf-8') if isinstance(s, bytes) else s \n                          for s in metadata['population']]\nThis converts byte strings like b'SAMPLE_0001' to regular strings 'SAMPLE_0001'.\n\n\n\n\n\n\n\n\nNoteUsing Python Directly (Without Reticulate)\n\n\n\nThis workflow uses Python from R through reticulate for convenience in a single document. However, you can work with the HDF5 file using Python independently:\nOption 1: Standalone Python script\n# Save your Python code to analyze_hdf5.py\n# Run from terminal\npython analyze_hdf5.py\nOption 2: Interactive Python session\n# Launch Python directly\npython3\n\n# Then work with HDF5\n&gt;&gt;&gt; import h5py\n&gt;&gt;&gt; f = h5py.File('cross_platform_analysis.hdf5', 'r')\n&gt;&gt;&gt; components = f['PCA/genotypes/components'][:]\n&gt;&gt;&gt; # ... continue analysis\nOption 3: Jupyter notebook\n# In Jupyter notebook\nimport h5py\nf = h5py.File('cross_platform_analysis.hdf5', 'r')\n# ... analysis code\nThe HDF5 file format is the same regardless of how you access it. Choose reticulate for integrated R/Python workflows, or use Python standalone when that fits your development environment better. The key advantage is that the same HDF5 file works seamlessly in both contexts without any conversion.\n\n\n\n\n5.2 Create Visualizations\nGenerate publication-quality plots in Python:\n\nimport matplotlib.pyplot as plt\n\n# Create 2x2 subplot figure\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\n\n# Define colors for populations\npop_colors = {\n    'EUR': '#1f77b4',\n    'AFR': '#ff7f0e', \n    'EAS': '#2ca02c',\n    'SAS': '#d62728',\n    'AMR': '#9467bd'\n}\n\n# Plot 1: PC1 vs PC2 by population\nax = axes[0, 0]\nfor pop in pop_colors.keys():\n    mask = metadata['population'] == pop\n    ax.scatter(components[mask, 0], components[mask, 1], \n               c=pop_colors[pop], label=pop, alpha=0.6, s=20)\nax.set_xlabel('PC1', fontsize=12)\nax.set_ylabel('PC2', fontsize=12)\nax.set_title('PCA: Populations in PC1-PC2 Space', fontsize=14, fontweight='bold')\nax.legend(title='Population')\nax.grid(True, alpha=0.3)\n\n# Plot 2: Scree plot\nax = axes[0, 1]\nn_components = len(variance_explained)\nax.bar(range(1, n_components + 1), variance_explained * 100, \n       color='steelblue', alpha=0.7)\nax.set_xlabel('Principal Component', fontsize=12)\nax.set_ylabel('Variance Explained (%)', fontsize=12)\nax.set_title('Scree Plot', fontsize=14, fontweight='bold')\nax.grid(True, alpha=0.3, axis='y')\n\n# Plot 3: PC1 by age (colored by population)\nax = axes[1, 0]\nfor pop in pop_colors.keys():\n    mask = metadata['population'] == pop\n    ax.scatter(metadata.loc[mask, 'age'], components[mask, 0],\n               c=pop_colors[pop], label=pop, alpha=0.6, s=20)\nax.set_xlabel('Age', fontsize=12)\nax.set_ylabel('PC1', fontsize=12)\nax.set_title('PC1 vs Age by Population', fontsize=14, fontweight='bold')\nax.legend(title='Population')\nax.grid(True, alpha=0.3)\n\n# Plot 4: Cumulative variance\nax = axes[1, 1]\ncumvar = np.cumsum(variance_explained) * 100\nax.plot(range(1, n_components + 1), cumvar, \n        marker='o', linewidth=2, markersize=6, color='darkred')\nax.axhline(y=80, color='red', linestyle='--', alpha=0.5, label='80% threshold')\nax.set_xlabel('Number of Components', fontsize=12)\nax.set_ylabel('Cumulative Variance Explained (%)', fontsize=12)\nax.set_title('Cumulative Variance Explained', fontsize=14, fontweight='bold')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('pca_python_visualization.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\nprint(\"Plots created and saved\")\n\nPlots created and saved\n\n\n\n\n5.3 Train Classifier on PCs\nUse principal components as features for population classification:\n\n# Import required libraries\nimport h5py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Reload data (variables don't persist between Python chunks)\nf = h5py.File('cross_platform_analysis.hdf5', 'r+')\ncomponents = f['PCA/genotypes/components'][:]\nmetadata_raw = f['metadata/samples']\nmetadata = pd.DataFrame({\n    'sample_id': metadata_raw['sample_id'][:],\n    'population': metadata_raw['population'][:],\n    'age': metadata_raw['age'][:]\n})\nmetadata['population'] = [s.decode('utf-8') if isinstance(s, bytes) else s \n                          for s in metadata['population']]\n\n# Prepare data\nX = components[:, :10]  # Use first 10 PCs\ny = metadata['population'].values\n\n# Split train/test\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# Train Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(random_state=42) \n\n# Predict\ny_pred = rf.predict(X_test)\n\n# Evaluate\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\n✓ Random Forest trained\")\n\n\n✓ Random Forest trained\n\nprint(f\"  Accuracy: {accuracy:.3f}\")\n\n  Accuracy: 0.210\n\nprint(\"\\nClassification Report:\")\n\n\nClassification Report:\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n         AFR       0.19      0.20      0.20        40\n         AMR       0.14      0.15      0.14        40\n         EAS       0.24      0.30      0.26        40\n         EUR       0.21      0.15      0.18        40\n         SAS       0.29      0.25      0.27        40\n\n    accuracy                           0.21       200\n   macro avg       0.21      0.21      0.21       200\nweighted avg       0.21      0.21      0.21       200\n\n# Save predictions back to HDF5 (file remains open for later use)\nif 'predictions' in f:\n    del f['predictions']\nf.create_dataset('predictions', data=y_pred.astype('S10'))\n\n&lt;HDF5 dataset \"predictions\": shape (200,), type \"|S10\"&gt;\n\nprint(\"\\n✓ Predictions saved to HDF5\")\n\n\n✓ Predictions saved to HDF5\n\nf.close()\nprint(\"✓ HDF5 file closed, ready for R/C++ access\")\n\n✓ HDF5 file closed, ready for R/C++ access\n\n\n\n# Close HDF5 file to allow R to access it\nf.close()\nprint(\"Python HDF5 file closed\")\n\nPython HDF5 file closed",
    "crumbs": [
      "Complete Examples",
      "Cross-Platform Workflows"
    ]
  },
  {
    "objectID": "workflows/cross-platform.html#step-3-custom-algorithm-in-c",
    "href": "workflows/cross-platform.html#step-3-custom-algorithm-in-c",
    "title": "Cross-Platform Workflows",
    "section": "6 Step 3: Custom Algorithm in C++",
    "text": "6 Step 3: Custom Algorithm in C++\n\n6.1 Implement New Sample Projection\nCreate a C++ function to project new samples onto existing PC space:\n// File: project_samples.cpp\n#include &lt;Rcpp.h&gt;\n#include \"BigDataStatMeth.hpp\"\n\nusing namespace Rcpp;\nusing namespace BigDataStatMeth;\n\n// [[Rcpp::export]]\nNumericMatrix projectNewSamples(std::string filename,\n                                 std::string new_data_path,\n                                 std::string pc_path,\n                                 int n_components) {\n    \n    hdf5Dataset* dsData = nullptr;\n    hdf5Dataset* dsPC = nullptr;\n    \n    try {\n        // Open datasets\n        dsData = new hdf5Dataset(filename, new_data_path, false);\n        dsData-&gt;openDataset();\n        \n        dsPC = new hdf5Dataset(filename, pc_path, false);\n        dsPC-&gt;openDataset();\n        \n        int n_samples = dsData-&gt;nrows();\n        int n_features = dsData-&gt;ncols();\n        \n        // Read new sample data\n        std::vector&lt;double&gt; new_data(n_samples * n_features);\n        dsData-&gt;readDatasetBlock(\n            {0, 0}, {(hsize_t)n_samples, (hsize_t)n_features},\n            {1, 1}, {1, 1}, new_data.data()\n        );\n        \n        // Read PC loadings\n        std::vector&lt;double&gt; pc_loadings(n_features * n_components);\n        dsPC-&gt;readDatasetBlock(\n            {0, 0}, {(hsize_t)n_features, (hsize_t)n_components},\n            {1, 1}, {1, 1}, pc_loadings.data()\n        );\n        \n        // Project: scores = data %*% loadings\n        NumericMatrix scores(n_samples, n_components);\n        \n        for (int i = 0; i &lt; n_samples; i++) {\n            for (int j = 0; j &lt; n_components; j++) {\n                double sum = 0.0;\n                for (int k = 0; k &lt; n_features; k++) {\n                    sum += new_data[i * n_features + k] * \n                           pc_loadings[k * n_components + j];\n                }\n                scores(i, j) = sum;\n            }\n        }\n        \n        // Cleanup\n        delete dsData;\n        delete dsPC;\n        \n        return scores;\n        \n    } catch(std::exception& ex) {\n        if (dsData) delete dsData;\n        if (dsPC) delete dsPC;\n        Rcpp::stop(\"Error in projectNewSamples: \" + std::string(ex.what()));\n    }\n}\n\n\n6.2 Compile and Use\n\nlibrary(Rcpp)\n\n# Set include path for BigDataStatMeth headers\nSys.setenv(PKG_CXXFLAGS = paste0(\n  \"-I\", system.file(\"include\", package = \"BigDataStatMeth\")\n))\n\n# Note: To compile the C++ code above, save it to project_samples.cpp\n# Then run: sourceCpp(\"project_samples.cpp\")\n# For this demo, we'll simulate the process\n\n# Create new samples to project\nnew_genotypes &lt;- matrix(\n  sample(0:2, 50 * n_snps, replace = TRUE, prob = c(0.25, 0.5, 0.25)),\n  nrow = 50,\n  ncol = n_snps\n)\n\n# Add to HDF5 file\nbdCreate_hdf5_matrix(\n  filename = cross_platform_file,\n  object = new_genotypes,\n  group = \"new_data\",\n  dataset = \"genotypes\",\n  overwriteFile = FALSE\n)\n\n$fn\n[1] \"cross_platform_analysis.hdf5\"\n\n$ds\n[1] \"new_data/genotypes\"\n\ncat(\"New sample data added to HDF5\\n\")\n\nNew sample data added to HDF5\n\ncat(\"  Ready for C++ projection (compile project_samples.cpp first)\\n\")\n\n  Ready for C++ projection (compile project_samples.cpp first)\n\n# After compiling project_samples.cpp:\n# new_scores &lt;- projectNewSamples(\n#   filename = cross_platform_file,\n#   new_data_path = \"new_data/genotypes\",\n#   pc_path = \"PCA/genotypes/rotation\",\n#   n_components = 10\n# )",
    "crumbs": [
      "Complete Examples",
      "Cross-Platform Workflows"
    ]
  },
  {
    "objectID": "workflows/cross-platform.html#workflow-summary",
    "href": "workflows/cross-platform.html#workflow-summary",
    "title": "Cross-Platform Workflows",
    "section": "7 Workflow Summary",
    "text": "7 Workflow Summary\n\n7.1 Data Flow Diagram\nR (BigDataStatMeth)\n  ↓ Creates\nHDF5 File\n  ├─ /data/genotypes          [Created by R]\n  ├─ /metadata/samples        [Created by R with rhdf5]\n  ├─ /PCA/genotypes/*         [Created by R]\n  ↓ Read by\nPython (h5py)\n  ├─ Loads components\n  ├─ Creates visualizations\n  ├─ Trains ML models\n  ↓ Writes\n  ├─ /predictions             [Created by Python]\n  ↓ Read by\nC++ (BigDataStatMeth.hpp)\n  ├─ Projects new samples\n  └─ Returns to R\n\n\n7.2 Language Selection Guide\nUse R when: - Performing standard statistical analyses - Initial data exploration - Using BigDataStatMeth functions - Working with data frames and factors\nUse Python when: - Need modern ML libraries (scikit-learn, TensorFlow) - Creating publication plots (matplotlib, seaborn) - Integrating with Python-only tools - Collaborating with Python-focused teams\nUse C++ when: - Performance is critical - Implementing custom algorithms - Need fine-grained memory control - Building production systems\nThe key: HDF5 lets you use the best tool for each task without conversion overhead.",
    "crumbs": [
      "Complete Examples",
      "Cross-Platform Workflows"
    ]
  },
  {
    "objectID": "workflows/cross-platform.html#interactive-exercise",
    "href": "workflows/cross-platform.html#interactive-exercise",
    "title": "Cross-Platform Workflows",
    "section": "8 Interactive Exercise",
    "text": "8 Interactive Exercise\n\n8.1 Practice: Multi-Language Integration\n\n# Exercise 1: Add more metadata\n# Store additional sample annotations in HDF5\n# Access them from Python for plotting\n\n# Exercise 2: Python preprocessing\n# Implement outlier detection in Python\n# Save outlier flags back to HDF5\n# Use flags in R for filtering\n\n# Exercise 3: C++ feature engineering\n# Create C++ function to compute SNP statistics\n# Store results in HDF5\n# Visualize in Python\n\n# Exercise 4: Compare performance\n# Implement same algorithm in R, Python, C++\n# Time each version\n# When is C++ worth the complexity?\n\n\n\n\n\n\n\nTipReflection Questions\n\n\n\n1. Platform Choices: - When does multi-language workflow add value vs. complexity? - Could you do everything in one language? - What are the maintenance costs?\n2. HDF5 Benefits: - How does HDF5 compare to CSV/RDS/pickle for cross-platform work? - When would you NOT use HDF5? - What about data versioning?\n3. Performance Trade-offs: - Python ML vs R caret—does it matter? - When is C++ optimization worth development time? - How do you profile multi-language workflows?\n4. Collaboration: - Team has R and Python users—best practices? - How to ensure reproducibility across platforms? - Documentation strategies?\n5. Debugging: - Error in Python step—how do you debug? - HDF5 file corrupted—recovery strategies? - Version compatibility issues?",
    "crumbs": [
      "Complete Examples",
      "Cross-Platform Workflows"
    ]
  },
  {
    "objectID": "workflows/cross-platform.html#key-takeaways",
    "href": "workflows/cross-platform.html#key-takeaways",
    "title": "Cross-Platform Workflows",
    "section": "9 Key Takeaways",
    "text": "9 Key Takeaways\nLet’s consolidate what you’ve learned about cross-platform workflows with BigDataStatMeth.\n\n9.1 Essential Concepts\nHDF5 is the universal data currency across platforms. One HDF5 file works identically in R, Python, C++, MATLAB, Julia, and command-line tools. You never export to CSV, save to RDS, pickle for Python, then convert back—the HDF5 file is the single source of truth. BigDataStatMeth creates HDF5 files that any platform can read, and results added by Python or C++ are immediately available back in R. This eliminates conversion overhead and version mismatches that plague multi-language workflows.\nEach language has natural strengths worth leveraging. R excels at statistical analysis and data manipulation with data frames. Python dominates machine learning with scikit-learn and deep learning with PyTorch. C++ provides performance for custom algorithms and production deployment. Rather than forcing everything into one language, use each where it shines. The HDF5 format makes this practical—you’re not punished for switching languages because there’s no conversion penalty.\nMixed data types require different tools. BigDataStatMeth functions like bdCreate_hdf5_matrix() are optimized for numeric matrices used in computations. For metadata with mixed types (character labels, factors, mixed numeric/character columns), use rhdf5 directly with h5write(). This distinction is important—trying to store a data frame with bdCreate_hdf5_matrix() will fail or produce unexpected results. Use the right tool for the data type.\nPython string decoding is a common gotcha. HDF5 stores strings as bytes, and Python’s h5py returns them as byte strings like b'EUR'. You must decode them to regular strings: s.decode('utf-8'). This is especially important for metadata like population labels or sample IDs that you’ll use for grouping or filtering. If your Python plots show b'EUR' instead of EUR in legends, you forgot to decode.\nReticulate bridges R and Python seamlessly. You don’t need to save data from R, exit, run Python script, then reload in R. Reticulate runs Python directly from R, with automatic data conversion for simple types. For complex workflows, Python writes results to HDF5, then R reads them—but for interactive analysis, you can run Python chunks in the same R session. The py_install() function handles Python package installation without leaving R. However, you can also use Python independently if that fits your workflow better.\nC++ optimization requires clear performance justification. Writing C++ code takes longer than R or Python, debugging is harder, and maintenance is more complex. Only move to C++ when profiling shows a bottleneck that C++ would solve. For this workflow, the projection algorithm could run in R—C++ is shown for educational purposes. In production, if you’re projecting millions of samples thousands of times, C++ pays off. For one-time analysis, R is faster development-to-result.\n\n\n9.2 When to Build Cross-Platform Workflows\nUnderstanding when multi-language workflows add value versus when they add unnecessary complexity guides effective development.\n✅ Use cross-platform workflows when:\n\nTeam has multi-language expertise - If some members prefer R, others Python, and you have C++ developers, letting each use their strength increases productivity. Fighting this by forcing everyone into one language reduces efficiency.\nDifferent steps need different tools - Statistical analysis naturally fits R, deep learning needs Python (PyTorch/TensorFlow), and real-time inference needs C++. Using the best tool for each step produces better results than forcing everything into one language’s ecosystem.\nPerformance bottlenecks are specific - If 95% of your workflow runs fine in R but 5% is a computational bottleneck, rewrite that 5% in C++ rather than the entire pipeline. Cross-platform workflows enable surgical optimization.\nInterfacing with external systems - If your organization’s ML infrastructure is Python but your statisticians use R, HDF5 bridges the gap. You can’t change the external systems, so you adapt your workflow to interface cleanly.\nLeveraging specialized libraries - If the perfect algorithm for your problem exists in Python (scikit-learn) but your data pipeline is in R, use both. Reimplementing complex algorithms to stay in one language wastes time and introduces bugs.\n\n✅ Stick to single language when:\n\nOne language does everything adequately - If R’s randomForest does what you need, don’t switch to Python’s scikit-learn just because you can. Simplicity has value—fewer dependencies, easier maintenance, clearer workflow.\nTeam is mono-lingual - If everyone knows R but nobody knows Python, adding Python creates training burden and maintenance problems. The “right” language is the one your team can actually use and maintain.\nAnalysis is exploratory and one-off - For quick exploratory analyses you’ll run once, stay in whatever you started with. The overhead of setting up multi-language workflow exceeds the benefit for one-time scripts.\nReproducibility is critical - Multi-language workflows have more failure points (Python version, package conflicts, compiler issues). If this analysis will be cited in papers or used in regulatory submissions, minimize complexity.\nPerformance is adequate - If R takes 10 minutes and C++ would take 2 minutes, but you run it monthly, don’t optimize. Save development time for frequent bottlenecks.\n\nThe key principle: use multiple languages when the benefits (performance, specialized tools, team expertise) clearly outweigh the costs (complexity, maintenance, debugging difficulty). HDF5 makes multi-language workflows practical, but practical doesn’t always mean necessary.",
    "crumbs": [
      "Complete Examples",
      "Cross-Platform Workflows"
    ]
  },
  {
    "objectID": "workflows/cross-platform.html#next-steps",
    "href": "workflows/cross-platform.html#next-steps",
    "title": "Cross-Platform Workflows",
    "section": "10 Next Steps",
    "text": "10 Next Steps\nExplore language-specific resources:\n\nR implementation details\nC++ implementation guide\n\nExtend this workflow:\n\nAdd quality control steps in each language\nImplement parallel processing in C++\nCreate interactive visualizations with Python’s plotly\nBuild Shiny app in R consuming Python ML results\n\nAdvanced topics:\n\nApache Arrow for even faster cross-language data sharing\nDocker containers for reproducible multi-language environments\nWorkflow managers (Snakemake, Nextflow) for complex pipelines\nTesting strategies for multi-language code",
    "crumbs": [
      "Complete Examples",
      "Cross-Platform Workflows"
    ]
  },
  {
    "objectID": "workflows/cross-platform.html#cleanup",
    "href": "workflows/cross-platform.html#cleanup",
    "title": "Cross-Platform Workflows",
    "section": "11 Cleanup",
    "text": "11 Cleanup\n\n# Close HDF5 connections\nh5closeAll()\n\n# Note: Python file already closed before C++ step\n# cross_platform_analysis.hdf5 contains complete workflow\n# Accessible from R, Python, and C++ without conversion",
    "crumbs": [
      "Complete Examples",
      "Cross-Platform Workflows"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html",
    "href": "workflows/implementing-pca.html",
    "title": "Implementing PCA",
    "section": "",
    "text": "Principal Component Analysis (PCA) is fundamental in genomics for identifying population structure, detecting batch effects, and reducing dimensionality before association tests. This workflow demonstrates a complete PCA implementation on real-world scale genomic data using BigDataStatMeth, from raw GDS files through quality control to publication-quality visualizations.\nUnlike tutorial examples with synthetic data, this workflow uses realistic dataset dimensions (hundreds of thousands of variants, thousands of samples) and addresses practical challenges you’ll face with real data: file format conversions, multi-step quality control, computational optimization, and result interpretation. Think of this as the blueprint for your actual genomic PCA analyses.\n\n\nBy the end of this workflow, you will:\n\nConvert GDS (Genomic Data Structure) files to HDF5 format for analysis\nApply comprehensive quality control for genomic data\nPerform block-wise PCA on datasets too large for memory\nOptimize computational parameters for your hardware\nInterpret variance explained and principal component loadings\nCreate publication-quality PCA plots showing population structure\nExport results for downstream analyses\nUnderstand when and why each processing step matters",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html#overview",
    "href": "workflows/implementing-pca.html#overview",
    "title": "Implementing PCA",
    "section": "",
    "text": "Principal Component Analysis (PCA) is fundamental in genomics for identifying population structure, detecting batch effects, and reducing dimensionality before association tests. This workflow demonstrates a complete PCA implementation on real-world scale genomic data using BigDataStatMeth, from raw GDS files through quality control to publication-quality visualizations.\nUnlike tutorial examples with synthetic data, this workflow uses realistic dataset dimensions (hundreds of thousands of variants, thousands of samples) and addresses practical challenges you’ll face with real data: file format conversions, multi-step quality control, computational optimization, and result interpretation. Think of this as the blueprint for your actual genomic PCA analyses.\n\n\nBy the end of this workflow, you will:\n\nConvert GDS (Genomic Data Structure) files to HDF5 format for analysis\nApply comprehensive quality control for genomic data\nPerform block-wise PCA on datasets too large for memory\nOptimize computational parameters for your hardware\nInterpret variance explained and principal component loadings\nCreate publication-quality PCA plots showing population structure\nExport results for downstream analyses\nUnderstand when and why each processing step matters",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html#the-dataset",
    "href": "workflows/implementing-pca.html#the-dataset",
    "title": "Implementing PCA",
    "section": "2 The Dataset",
    "text": "2 The Dataset\nWe’ll analyze the 1000 Genomes Project data, which provides genetic variants from diverse human populations. This represents a realistic scale for many genomic studies:\n\nSamples: 2,504 individuals from 26 populations\nVariants: ~800,000 SNPs after standard QC\nFile size: ~2-3 GB in GDS format\nAnalysis goal: Identify continental ancestry structure\n\nThis dataset is small enough to complete on a workstation but large enough that naive approaches (loading everything into memory, using standard R PCA) become problematic. It perfectly illustrates when and why BigDataStatMeth becomes necessary.",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html#step-1-data-preparation",
    "href": "workflows/implementing-pca.html#step-1-data-preparation",
    "title": "Implementing PCA",
    "section": "3 Step 1: Data Preparation",
    "text": "3 Step 1: Data Preparation\n\n3.1 Download the Example Data\nThe 1000 Genomes ethnic subset is available from the BigDataStatMeth supplementary materials:\n\nlibrary(BigDataStatMeth)\nlibrary(gdsfmt)\nlibrary(rhdf5)\n\n# Download genomic data (GDS format)\ndownload.file(\n  url = paste0(\"https://raw.githubusercontent.com/isglobal-brge/\",\n               \"Supplementary-Material/master/Pelegri-Siso_2025/\",\n               \"application_examples/PCA/data/1000G_ethnic.zip\"),\n  destfile = \"1000G_ethnic.zip\"\n)\n\n# Extract\nunzip(\"1000G_ethnic.zip\")\n\n# Download phenotype data (population labels)\npheno_url &lt;- paste0(\"https://raw.githubusercontent.com/isglobal-brge/\",\n                    \"Supplementary-Material/master/Pelegri-Siso_2025/\",\n                    \"application_examples/PCA/data/1000G_samples.tsv\")\n\ndownload.file(pheno_url, destfile = \"1000G_samples.tsv\")\n\n\n\n\n\n\n\nNoteGDS Format\n\n\n\nGDS (Genomic Data Structure) is a container format commonly used for genomic data from sequencing studies. The gdsfmt and SeqArray packages provide tools to work with GDS files. We’ll convert to HDF5 because BigDataStatMeth operates on HDF5 matrices, and this conversion pattern applies to any genomic data source.\n\n\n\n\n3.2 Convert GDS to HDF5\nGenomic data often arrives in specialized formats (GDS, VCF, PLINK). BigDataStatMeth needs numeric matrices in HDF5 format, so conversion is the first step:\n\n# Open GDS file\ngds_file &lt;- \"1000G_ethnic.gds\"\ngds &lt;- openfn.gds(gds_file)\n\n# Extract genotype matrix\ngeno_node &lt;- index.gdsn(gds, \"genotype\")\ngenotype &lt;- read.gdsn(geno_node)\n\n# Extract sample and variant identifiers\nsample_ids &lt;- read.gdsn(index.gdsn(gds, \"sample.id\"))\nsnp_ids &lt;- read.gdsn(index.gdsn(gds, \"snp.rs.id\"))\n\n# Assign meaningful row and column names\nrownames(genotype) &lt;- sample_ids\ncolnames(genotype) &lt;- snp_ids\n\n# Close GDS file\nclosefn.gds(gds)\n\ngenotype &lt;- genotype[1:1000,]\n\n# Check dimensions before proceeding\ncat(\"Genotype matrix dimensions:\\n\")\n\nGenotype matrix dimensions:\n\ncat(\"  Samples:\", nrow(genotype), \"\\n\")\n\n  Samples: 1000 \n\ncat(\"  SNPs:\", ncol(genotype), \"\\n\")\n\n  SNPs: 78322 \n\ncat(\"  Missing data:\", \n    sprintf(\"%.2f%%\", sum(is.na(genotype))/length(genotype)*100), \"\\n\")\n\n  Missing data: 0.00% \n\n\nNow convert to HDF5, organizing logically for subsequent analyses:\n\n# Create HDF5 file with organized structure\npca_file &lt;- \"1000G_pca_analysis.hdf5\"\n\nbdCreate_hdf5_matrix(\n  filename = pca_file,\n  object = genotype,\n  group = \"raw_data\",\n  dataset = \"genotypes\",\n  transp = FALSE,  # Keep samples as rows, SNPs as columns\n  overwriteFile = TRUE\n)\n\n$fn\n[1] \"1000G_pca_analysis.hdf5\"\n\n$ds\n[1] \"raw_data/genotypes\"\n\ncat(\"✓ HDF5 file created:\", pca_file, \"\\n\")\n\n✓ HDF5 file created: 1000G_pca_analysis.hdf5 \n\nh5ls(pca_file)\n\n                          group                name       otype   dclass\n0                             /            raw_data   H5I_GROUP         \n1                     /raw_data .genotypes_dimnames   H5I_GROUP         \n2 /raw_data/.genotypes_dimnames                   1 H5I_DATASET COMPOUND\n3 /raw_data/.genotypes_dimnames                   2 H5I_DATASET COMPOUND\n4                     /raw_data           genotypes H5I_DATASET  INTEGER\n           dim\n0             \n1             \n2         2504\n3        78322\n4 2504 x 78322\n\n\n\n\n\n\n\n\nTipOrganizing Your HDF5 File\n\n\n\nWe create a /raw_data/ group for the original genotypes. Subsequent QC steps will create /quality_control/ for filtered data, and /analysis/pca/ for results. This mirrors a typical analysis workflow and keeps intermediate steps traceable. If you need to rerun QC with different thresholds, raw data remains untouched.",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html#step-2-quality-control",
    "href": "workflows/implementing-pca.html#step-2-quality-control",
    "title": "Implementing PCA",
    "section": "4 Step 2: Quality Control",
    "text": "4 Step 2: Quality Control\n\n4.1 Remove Low-Quality Samples\nSamples with excessive missing data often indicate technical problems (poor DNA quality, sequencing failures). We filter samples before variant-level QC:\n\n# Remove samples with &gt;5% missing data\nsample_qc &lt;- bdRemovelowdata_hdf5(\n  filename = pca_file,\n  group = \"raw_data\",\n  dataset = \"genotypes\",\n  outgroup = \"quality_control\",\n  outdataset = \"samples_qc\",\n  bycols = FALSE,  # Operate on rows (samples)\n  pcent = 0.05,\n  overwrite = TRUE\n)\n\ncat(\"Sample QC results:\\n\")\n\nSample QC results:\n\ncat(\"  Samples removed:\", sample_qc$elements_removed, \"\\n\")\n\n  Samples removed: \n\ncat(\"  Samples retained:\", sample_qc$elements_retained, \"\\n\")\n\n  Samples retained: \n\n\n\n\n4.2 Remove Low-Frequency and High-Missingness SNPs\nRare variants (low minor allele frequency) provide little information for PCA and can introduce noise. Similarly, SNPs with high missingness are unreliable:\n\n# Remove SNPs with &gt;5% missing data\nsnp_missing_qc &lt;- bdRemovelowdata_hdf5(\n  filename = pca_file,\n  group = \"quality_control\",\n  dataset = \"samples_qc\",\n  outgroup = \"quality_control\",\n  outdataset = \"snps_missing_qc\",\n  bycols = TRUE,  # Operate on columns (SNPs)\n  pcent = 0.05,\n  overwrite = TRUE\n)\n\ncat(\"SNP missingness QC:\\n\")\n\nSNP missingness QC:\n\ncat(\"  SNPs removed:\", snp_missing_qc$elements_removed, \"\\n\")\n\n  SNPs removed: \n\ncat(\"  SNPs retained:\", snp_missing_qc$elements_retained, \"\\n\")\n\n  SNPs retained: \n\n\n\n# Remove SNPs with MAF &lt; 5%\nmaf_qc &lt;- bdRemoveMAF_hdf5(\n  filename = pca_file,\n  group = \"quality_control\",\n  dataset = \"snps_missing_qc\",\n  outgroup = \"quality_control\",\n  outdataset = \"snps_qc_complete\",\n  maf = 0.05,\n  bycols = TRUE,\n  blocksize = 1000,\n  overwrite = TRUE\n)\n\ncat(\"MAF QC:\\n\")\n\nMAF QC:\n\ncat(\"  SNPs removed:\", maf_qc$elements_removed, \"\\n\")\n\n  SNPs removed: \n\ncat(\"  SNPs retained:\", maf_qc$elements_retained, \"\\n\")\n\n  SNPs retained: \n\n\n\n\n\n\n\n\nImportantQC Ordering Matters\n\n\n\nWe filter samples first, then SNP missingness, then MAF. Why this order?\n\nSamples first: Removing low-quality samples changes allele frequencies and missingness calculations for SNPs\nMissingness before MAF: SNPs with high missingness have unreliable allele frequency estimates\nMAF last: After removing missing data, MAF calculations are accurate\n\nReversing this order produces different results. The standard practice is sample → missingness → MAF.\n\n\n\n\n4.3 Impute Remaining Missing Data\nAfter QC, remaining missingness is typically &lt;1-2%. Simple mean imputation suffices for this low level:\n\n# Impute missing values with column (SNP) means\nbdImputeSNPs_hdf5(\n  filename = pca_file,\n  group = \"quality_control\",\n  dataset = \"snps_qc_complete\",\n  bycols = TRUE,\n  outgroup = \"quality_control\",\n  outdataset = \"genotypes_imputed\",\n  overwrite = TRUE\n)\n\n$fn\n[1] \"1000G_pca_analysis.hdf5\"\n\n$ds\n[1] \"quality_control/snps_qc_complete\"\n\ncat(\"✓ Missing data imputed\\n\")\n\n✓ Missing data imputed\n\n# Verify no missing data remains\nh5file &lt;- H5Fopen(pca_file)\nimputed_data &lt;- h5file$quality_control$genotypes_imputed\ncat(\"Missing values remaining:\", sum(is.na(imputed_data[1:100, 1:100])), \"\\n\")\n\nMissing values remaining: 0 \n\nH5Fclose(h5file)",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html#step-3-perform-pca",
    "href": "workflows/implementing-pca.html#step-3-perform-pca",
    "title": "Implementing PCA",
    "section": "5 Step 3: Perform PCA",
    "text": "5 Step 3: Perform PCA\n\n5.1 Understanding the Parameters\nPCA on large matrices uses hierarchical block-wise SVD. Key parameters:\n\nk: Number of blocks to partition the matrix into\nq: Number of hierarchical levels\nComponents: How many PCs to compute (typically 10-50)\nCentering: Almost always TRUE for PCA\nScaling: Usually FALSE for SNP data (all same units)\n\n\n# Perform block-wise PCA\npca_result &lt;- bdPCA_hdf5(\n  filename = pca_file,\n  group = \"quality_control\",\n  dataset = \"genotypes_imputed\",\n  k = 4,            # 4 blocks balances memory and speed\n  q = 1,            # 1 level sufficient for this data size\n  bcenter = TRUE,   # Center SNPs (subtract column means)\n  bscale = FALSE,   # Don't scale (SNPs same units)\n  ncomponents = 20, # Compute first 20 PCs\n  threads = 4,      # Use 4 threads (adjust to your CPU)\n  overwrite = TRUE\n)\n\ncat(\"✓ PCA complete\\n\")\n\n✓ PCA complete\n\n\n\n\n\n\n\n\nNoteComputational Considerations\n\n\n\nFor this dataset (~2500 samples × ~80,000 SNPs after QC): - Memory usage: Peak ~4-6 GB with k=4 - Computation time: ~2-5 minutes on modern CPU - Threads: Diminishing returns beyond physical cores\nLarger datasets (50,000 samples × 500,000 SNPs) might need k=8-16 and q=2 for reasonable memory usage.",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html#step-4-extract-and-examine-results",
    "href": "workflows/implementing-pca.html#step-4-extract-and-examine-results",
    "title": "Implementing PCA",
    "section": "6 Step 4: Extract and Examine Results",
    "text": "6 Step 4: Extract and Examine Results\n\n6.1 Load PCA Components\n\n# Open file to access PCA results\nh5file &lt;- H5Fopen(pca_file)\n\n# Extract components (the principal components themselves)\npcs &lt;- h5file$PCA$genotypes_imputed$components\npcs &lt;- as.data.frame(pcs)\ncolnames(pcs) &lt;- paste0(\"PC\", 1:ncol(pcs))\n\n# Add sample IDs\nsample_names &lt;- h5file$raw_data$.genotypes_dimnames$`1`[,1]\npcs$sample_id &lt;- sample_names[1:nrow(pcs)]  # Account for samples removed in QC\n\n# Extract variance explained\nvariance_prop &lt;- h5file$PCA$genotypes_imputed$variance[,1]\n\nH5Fclose(h5file)\n\n# Preview principal components\ncat(\"\\nFirst 5 samples, first 5 PCs:\\n\")\n\n\nFirst 5 samples, first 5 PCs:\n\nprint(pcs[1:5, 1:6])  # First 5 PCs + sample_id\n\n         PC1        PC2       PC3        PC4        PC5       PC6\n1 -0.2853922 -1.3649065 0.6454470  1.1533012 -1.2112287 0.3623916\n2 -0.7943544 -0.8494820 1.2276331 -1.9259606 -1.1051614 0.2707729\n3 -0.4685347 -0.8840233 0.4830970  1.7655353  1.1815834 0.2693867\n4 -0.2310929 -1.0369896 0.2354575 -0.2605435 -0.8110573 0.3939840\n5 -0.3307356 -1.0352414 0.5485452 -0.6088280 -0.4052957 2.0210173\n\n# Variance explained summary\ncat(\"\\nVariance explained by each PC:\\n\")\n\n\nVariance explained by each PC:\n\nvariance_df &lt;- data.frame(\n  PC = 1:length(variance_prop),\n  Variance = sprintf(\"%.2f%%\", variance_prop),\n  Cumulative = sprintf(\"%.2f%%\", cumsum(variance_prop))\n)\nprint(variance_df[1:10, ])\n\n   PC Variance Cumulative\n1   1   10.37%     10.37%\n2   2    5.36%     15.73%\n3   3    1.96%     17.69%\n4   4    1.29%     18.98%\n5   5    1.20%     20.18%\n6   6    1.04%     21.22%\n7   7    0.98%     22.20%\n8   8    0.96%     23.16%\n9   9    0.93%     24.09%\n10 10    0.90%     24.99%\n\n\n\n\n\n\n\n\nTipInterpreting Variance Explained\n\n\n\nFor population structure PCA: - PC1-PC2 typically explain 2-5% each for global ancestry - PC1-PC10 together often explain 10-20% - Low variance per PC is expected - population structure is subtle\nRed flags: - PC1 &gt;50%: Possible batch effect or technical artifact - PC1-10 &lt;5% total: Either very homogeneous population or too much noise - Sudden drop after PC1: Might indicate strong population stratification\nFor this 1000 Genomes data, expect PC1 ~3-5% (continental ancestry) and PC2 ~2-3% (finer structure).",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html#step-5-visualize-population-structure",
    "href": "workflows/implementing-pca.html#step-5-visualize-population-structure",
    "title": "Implementing PCA",
    "section": "7 Step 5: Visualize Population Structure",
    "text": "7 Step 5: Visualize Population Structure\n\n7.1 Merge with Population Labels\n\nlibrary(tidyverse)\n\n# Load population information\npheno &lt;- read_delim(\n  \"1000G_samples.tsv\",\n  delim = \"\\t\",\n  show_col_types = FALSE\n)\n\n# Merge PCs with phenotype data\npca_data &lt;- pcs %&gt;%\n  left_join(pheno, by = c(\"sample_id\" = \"Sample name\"))\n\n# Check merge success\ncat(\"Samples with population labels:\", \n    sum(!is.na(pca_data$`Superpopulation code`)), \"\\n\")\n\nSamples with population labels: 1000 \n\n\n\n\n7.2 Create PCA Plot\n\nlibrary(ggplot2)\n\n# Define population colors\npop_colors &lt;- c(\n  \"AFR\" = \"#E41A1C\",  # African - Red\n  \"AMR\" = \"#377EB8\",  # American - Blue\n  \"EAS\" = \"#4DAF4A\",  # East Asian - Green\n  \"EUR\" = \"#984EA3\",  # European - Purple\n  \"SAS\" = \"#FF7F00\"   # South Asian - Orange\n)\n\n# Create PCA plot\npca_plot &lt;- ggplot(\n  pca_data, \n  aes(x = PC1, y = PC2, color = `Superpopulation code`)\n) +\n  geom_point(size = 2, alpha = 0.7) +\n  scale_color_manual(\n    values = pop_colors,\n    name = \"Ancestry\",\n    labels = c(\n      \"AFR\" = \"African\",\n      \"AMR\" = \"American\", \n      \"EAS\" = \"East Asian\",\n      \"EUR\" = \"European\",\n      \"SAS\" = \"South Asian\"\n    )\n  ) +\n  labs(\n    title = \"Population Structure in 1000 Genomes Project\",\n    subtitle = \"PCA reveals continental ancestry patterns\",\n    x = sprintf(\"PC1 (%.2f%% variance)\", variance_prop[1] * 100),\n    y = sprintf(\"PC2 (%.2f%% variance)\", variance_prop[2] * 100)\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 12),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11, color = \"gray40\"),\n    legend.position = \"right\",\n    panel.grid.minor = element_blank()\n  )\n\nprint(pca_plot)\n\n\n\n\n\n\n\n# Save plot\nggsave(\n  \"pca_population_structure.png\",\n  pca_plot,\n  width = 10,\n  height = 7,\n  dpi = 300\n)\n\ncat(\"✓ Plot saved to pca_population_structure.png\\n\")\n\n✓ Plot saved to pca_population_structure.png\n\n\n\n\n7.3 Scree Plot for Variance\n\n# Prepare data for scree plot\nn_show &lt;- min(20, length(variance_prop))\nscree_data &lt;- data.frame(\n  PC = 1:n_show,\n  Variance = variance_prop[1:n_show] * 100\n)\n\n# Create scree plot\nscree_plot &lt;- ggplot(scree_data, aes(x = PC, y = Variance)) +\n  geom_line(color = \"steelblue\", size = 1) +\n  geom_point(color = \"steelblue\", size = 3) +\n  labs(\n    title = \"Scree Plot: Variance Explained by Principal Components\",\n    x = \"Principal Component\",\n    y = \"Variance Explained (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    text = element_text(size = 12),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    panel.grid.minor = element_blank()\n  ) +\n  scale_x_continuous(breaks = seq(1, n_show, by = 2))\n\nprint(scree_plot)\n\n\n\n\n\n\n\nggsave(\n  \"pca_scree_plot.png\",\n  scree_plot,\n  width = 8,\n  height = 6,\n  dpi = 300\n)",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html#step-6-export-results",
    "href": "workflows/implementing-pca.html#step-6-export-results",
    "title": "Implementing PCA",
    "section": "8 Step 6: Export Results",
    "text": "8 Step 6: Export Results\n\n8.1 Save Principal Components for Downstream Analysis\n\n# Export first 10 PCs for association testing\npcs_export &lt;- pcs[, c(\"sample_id\", paste0(\"PC\", 1:10))]\n\nwrite.table(\n  pcs_export,\n  file = \"principal_components_1000G.txt\",\n  sep = \"\\t\",\n  row.names = FALSE,\n  quote = FALSE\n)\n\ncat(\"✓ Principal components exported to principal_components_1000G.txt\\n\")\n\n✓ Principal components exported to principal_components_1000G.txt\n\ncat(\"  Samples:\", nrow(pcs_export), \"\\n\")\n\n  Samples: 1000 \n\ncat(\"  PCs included: 10\\n\")\n\n  PCs included: 10\n\n\n\n\n\n\n\n\nNoteUsing PCs in Downstream Analyses\n\n\n\nThese PCs can be included as covariates in:\n\nGWAS: Adjust for population stratification\nExpression QTL mapping: Control for ancestry\nPolygenic risk scores: Account for population structure\nClustering: Define ancestry-matched subcohorts\n\nStandard practice uses PC1-10, though some studies include PC1-20 for fine-scale structure.",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html#interactive-exercise",
    "href": "workflows/implementing-pca.html#interactive-exercise",
    "title": "Implementing PCA",
    "section": "9 Interactive Exercise",
    "text": "9 Interactive Exercise\n\n9.1 Practice: Optimizing PCA for Your Data\nUnderstanding how parameters affect computation helps you optimize analyses for your specific datasets and hardware.\n\n# Exercise: Try different parameter combinations\n\n# Scenario 1: Larger dataset (memory-constrained)\n# You have 50,000 samples × 500,000 SNPs, 32 GB RAM\nbdPCA_hdf5(\n  filename = your_file,\n  group = \"qc\",\n  dataset = \"genotypes\",\n  k = 16,          # More blocks to reduce memory\n  q = 2,           # Two levels for hierarchical processing\n  components = 10,\n  threads = 8\n)\n\n# Scenario 2: Small dataset (speed-focused)\n# You have 1,000 samples × 10,000 SNPs, plenty of RAM\nbdPCA_hdf5(\n  filename = your_file,\n  group = \"qc\",\n  dataset = \"genotypes\",\n  k = 2,           # Fewer blocks (less overhead)\n  q = 1,           # Single level\n  components = 20,\n  threads = 4\n)\n\n# Question: How does changing k affect your analysis?\n\n\n\n\n\n\n\nTipReflection Questions\n\n\n\n1. Parameter Selection: - For your data size, what k value balances memory and speed? - How do you decide between q=1 vs. q=2? - When would you compute 50 PCs instead of 20?\n2. Quality Control Impact: - What if you skip MAF filtering? More noise, or more information? - How aggressive should missingness thresholds be? - When might you use different QC for different analyses?\n3. Interpreting Results: - PC1 explains 4% of variance. Is that good or bad? - You see a cluster separate from all populations. What might it be? - PCs correlate with sequencing batch. How do you handle this?\n4. Computational Scaling: - Your dataset doubles in size. How do parameters change? - You have limited RAM but many CPU cores. Optimize for what? - Accuracy vs. speed - when does each matter more?\n5. Biological vs. Technical Structure: - How do you distinguish ancestry from batch effects? - Should you remove outliers before PCA or after? - When do you trust what the PCA shows?\nExperiment with different settings on this dataset. Note how computation time, memory usage, and results change. This builds intuition for optimizing real analyses.",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html#key-takeaways",
    "href": "workflows/implementing-pca.html#key-takeaways",
    "title": "Implementing PCA",
    "section": "10 Key Takeaways",
    "text": "10 Key Takeaways\nLet’s consolidate what you’ve learned about implementing PCA on large-scale genomic data using BigDataStatMeth.\n\n10.1 Essential Concepts\nPCA on genomic data serves specific purposes that differ from general dimensionality reduction. In genomics, PCA primarily identifies population structure and batch effects rather than reducing features for modeling. PC1-PC2 capture continental ancestry, PC3-PC10 capture finer population structure, and later PCs often reflect technical artifacts. Understanding these purposes guides interpretation - low variance explained isn’t a problem, it’s expected when analyzing subtle population differences.\nQuality control determines PCA validity more than the PCA algorithm itself. Running PCA on un-QC’d data produces results that look mathematically correct but are scientifically meaningless. Remove low-quality samples first (they distort allele frequencies), then filter SNPs by missingness (high-missing SNPs are unreliable), then remove rare variants (low MAF adds noise). This ordering matters - sample QC affects SNP metrics, so samples must be filtered before SNP-level QC. Skipping or reordering these steps produces different, less reliable results.\nBlock-wise PCA enables analyses that traditional methods cannot handle. Standard R functions (prcomp(), svd()) require loading entire matrices into memory, failing when data exceeds RAM. BigDataStatMeth’s block-wise approach processes data in chunks, enabling PCA on datasets that are 10× or 100× larger than available memory. For the 1000 Genomes data (2-3 GB), this might seem unnecessary on modern workstations, but for whole-genome sequencing cohorts (50,000 samples × 10 million variants = 4 TB), block-wise processing becomes essential, not optional.\nCentering is mandatory, scaling is context-dependent. Centering (subtracting column means) removes the origin from the data cloud, ensuring PC1 doesn’t just capture mean differences between SNPs. This is universally necessary for interpretable PCA. Scaling (dividing by standard deviations) equalizes variance across features, which helps when features have different units (mixing expression and metabolites) but obscures real biological variance differences in SNP data. For genomic PCA, center but don’t scale. For multi-omic integration, both centering and scaling may be needed.\nVariance explained in population PCA is low by design. Seeing PC1 explain only 3-5% of total variance doesn’t indicate failure - it reflects biological reality. Human genetic variation is highly dimensional with many independent signals. Population structure is real but subtle compared to individual genetic variation. If PC1 explained 50% of variance in human genomic data, something is seriously wrong (likely a technical artifact, not biology). Low variance per component with clear population clustering indicates successful, valid PCA.\nParameter choice balances memory, speed, and accuracy. The k parameter (number of blocks) controls memory usage: larger k = less memory per block but more I/O overhead. The q parameter (hierarchy levels) determines how blocks are combined: q=1 works for moderate data, q=2 needed for very large matrices. The threads parameter enables parallelization but shows diminishing returns beyond physical CPU cores. For typical genomic analyses, k=4-8, q=1, and threads matching your cores provides good performance without extensive tuning.\nPCA results require biological validation, not just statistical assessment. Mathematically perfect PCA can still be biologically meaningless. Validate by checking: Do populations cluster as expected? Do PCs correlate with known ancestry? Are there unexpected outliers (sequencing failures, sample swaps)? Does PC1 correlate with sequencing batch (technical artifact)? Statistical metrics (variance explained, eigenvalue ratios) tell you if PCA worked; biological validation tells you if results are meaningful.\n\n\n10.2 When to Use This Workflow\nUnderstanding when this comprehensive PCA workflow helps versus when simpler approaches suffice guides efficient analysis design.\n✅ Use this workflow when:\n\nDataset size exceeds comfortable RAM limits - When traditional PCA fails with memory errors, or when loading data takes minutes, block-wise processing becomes necessary. Rule of thumb: if your data matrix is &gt;30% of available RAM, use BigDataStatMeth’s approach to avoid memory issues during computation.\nYou’re analyzing genomic data for publication - The comprehensive QC → imputation → PCA → validation pipeline produces defensible, reproducible results. Reviewers expect proper QC, and this workflow documents every step. The organized HDF5 structure preserves intermediate results for supplementary materials or reanalysis with different parameters.\nPopulation structure matters for your analysis - GWAS, rare variant analysis, and selection scans all require proper ancestry adjustment. This workflow identifies population structure reliably, providing covariates for downstream association tests. Skipping proper PCA leads to false positive associations driven by population stratification.\nYou need to optimize for available resources - The parameterized approach (k, q, threads) lets you tune for your specific hardware. With limited RAM but many cores, increase k and threads. With ample RAM but slow disk, decrease k to minimize I/O. This flexibility enables analysis on hardware ranging from laptops to HPC clusters.\nThe analysis will be extended or repeated - If you’ll add more samples, reanalyze with updated QC thresholds, or apply the same pipeline to multiple cohorts, having a documented workflow saves enormous time. Copy the structure, adjust file paths and parameters, execute. The HDF5 organization makes extending analyses straightforward.\n\n✅ Adapt this workflow when:\n\nAnalyzing different omic types - Transcriptomics data needs log-transformation before PCA. Methylation data might need beta to M-value conversion. Proteomics might need scaling due to different abundance ranges. The workflow structure (QC → transformation → PCA → validation) remains constant, but specific operations change to match data characteristics.\nDifferent QC stringency needed - Exploratory analyses tolerate more missingness and lower MAF than definitive GWAS. Imputation-based association tests need stricter QC than burden tests. Adjust thresholds (5% missingness vs. 10%, MAF 0.01 vs. 0.05) to match analysis goals and acceptable false discovery rates.\nComputational constraints vary - On a laptop with 8 GB RAM, use k=16, q=2, process in smaller blocks. On a server with 256 GB RAM, use k=2-4, q=1, minimize overhead. The algorithm is the same, only parameters change to match available resources.\n\n❌ Simpler approaches work better when:\n\nData easily fits in memory - If prcomp(scale(data)) works without issues, use it. Base R PCA is well-tested, well-documented, and integrates seamlessly with the broader R ecosystem. Don’t add HDF5 complexity when traditional methods suffice.\nQuality control already performed - If analyzing published data (like HapMap, 1000 Genomes processed files), extensive QC is redundant. These datasets are already filtered, imputed, and quality-controlled. Load, run PCA, interpret results. Don’t re-QC already-QC’d data.\nExploratory analysis on subsets - For quick exploration of chromosome 22 or a 1% random sample, simple approaches are faster. Extract subset, run standard PCA, visualize, iterate. Use this workflow when transitioning from exploration to production analysis on the full dataset.\nNo population structure expected - Analyzing inbred lines, cell cultures, or known-homogeneous populations doesn’t need population structure PCA. The analysis might still be useful for quality control (identifying outliers) but doesn’t serve its primary purpose of ancestry adjustment.\n\nThe key principle is matching workflow complexity to analysis needs and data scale. Large-scale genomic studies requiring QC, population structure identification, and resource optimization benefit from this comprehensive approach. Small-scale, exploratory, or already-QC’d analyses are better served by simpler methods.",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html#next-steps",
    "href": "workflows/implementing-pca.html#next-steps",
    "title": "Implementing PCA",
    "section": "11 Next Steps",
    "text": "11 Next Steps\nExplore related workflows:\n\nImplementing CCA - Multi-omic integration using canonical correlation\nCross-Platform Workflows - Using PCA results in Python and C++\n\nExtend this analysis:\n\nPerform association tests using PCs as covariates\nCalculate genomic relationship matrices from PCs\nIdentify population-specific outliers for closer inspection\nCompare PCA with other dimensionality reduction methods (UMAP, t-SNE)\n\nOptimize for your data:\n\nTest different k, q, threads to find optimal settings\nTry different QC thresholds for different analysis types\nExplore ancestry-informative marker selection before PCA\nImplement cross-validation for PCA stability assessment",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  },
  {
    "objectID": "workflows/implementing-pca.html#cleanup",
    "href": "workflows/implementing-pca.html#cleanup",
    "title": "Implementing PCA",
    "section": "12 Cleanup",
    "text": "12 Cleanup\n\n# Close any open HDF5 connections\nh5closeAll()\n\n# Note: Keep generated files for future reference\n# - 1000G_pca_analysis.hdf5: Complete analysis with all steps\n# - pca_population_structure.png: Publication plot\n# - principal_components_1000G.txt: Covariates for downstream use",
    "crumbs": [
      "Complete Examples",
      "Implementing PCA"
    ]
  }
]